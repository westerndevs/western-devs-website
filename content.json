{"meta":{"title":"Western Devs","subtitle":null,"description":"We're but a humble group of amazing developers with a common trait: we're all west of somewhere else.","author":"Western Devs","url":"https://westerndevs.com"},"posts":[{"title":"Docker COPY not Finding Files","authorId":"simon_timms","slug":"cannot-copy","date":"2023-11-23 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/cannot-copy/","link":"","permalink":"https://westerndevs.com/_/cannot-copy/","excerpt":"","raw":"---\ntitle:  Docker COPY not Finding Files \nauthorId: simon_timms\ndate: 2023-11-23\noriginalurl: https://blog.simontimms.com/2023/11/23/cannot-copy\nmode: public\n---\n\n\n\nMy dad once told me that there are no such things a problems just solutions waiting to be applied. I don't know what book he'd just read or course he'd just been on to spout such nonsense but I've never forgotten it. \n\nToday my not problem was running a docker build wasn't copying the files I was expecting it to. In particular I had a `themes` directory which was not ending up in the image and in fact the build was failing with something like \n\n```\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref b1f3faa4-fdeb-41ed-b016-fac3862d370a::pjh3jwhj2huqmcgigjh9udlh2: \"/themes\": not found\n```\n\nI was really confused because `themes` absolutly did exist on disk. It was as if it wasn't being added to the build context. In fact it wasn't being added and, as it turns out, this was because my .dockerignore file contained \n```\n**\n```\n\nWhich ignores everything from the local directory. That seemed a bit extreme so I changed it to \n```\n** \n!themes\n```\n\nWith this in place the build worked as expected.","categories":[],"tags":[]},{"title":"Connect to a Service Container in Github Actions","authorId":"simon_timms","slug":"connect-to-service-container","date":"2023-11-20 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/connect-to-service-container/","link":"","permalink":"https://westerndevs.com/_/connect-to-service-container/","excerpt":"","raw":"---\ntitle:  Connect to a Service Container in Github Actions\nauthorId: simon_timms\ndate: 2023-11-20\noriginalurl: https://blog.simontimms.com/2023/11/20/connect-to-service-container\nmode: public\n---\n\n\n\nIncreasingly there is a need to run containers during a github action build to run realistic tests. In my specific scenario I had a database integration test that I wanted to run against a postgres database with our latest database migrations applied.\n\nWe run our builds inside a multi-stage docker build so we actually need to have a build container communicate with the database container during the build phase. This is easy enough in the run phase but in the build phase there is just a flag you can pass to the build called `network` which takes an argument but the arguments it can take don't appear to be documented anywhere. After significant trial and error I found that the argument it takes that we want is `host`. This will build the container using the host networking. As we surfaced the ports for postgres in our workflow file like so \n```yml\npostgres:\n    image: postgres:15.3\n    ports:\n        - 5432:5432\n    env:\n        POSTGRES_DB: default\n        POSTGRES_USER: webapp_user\n        POSTGRES_PASSWORD: password\n    options: >-\n        --health-cmd pg_isready\n        --health-interval 10s\n        --health-timeout 5s\n        --health-retries 5\n```\n\nWe are able to access the database from the build context with `127.0.0.1`. So we can pass in a variable to our container build \n```dockerfile\ndocker build --network=host . --tag ${{ env.DOCKER_REGISTRY_NAME }}/${{ env.DOCKER_IMAGE_NAME }}:${{ github.run_number }} --build-arg 'DATABASE_CONNECTION_STRING=${{ env.DATABASE_CONNECTION_STRING }}'\n```\n\nWith all this in place the tests run nicely in the container during the build. Phew. ","categories":[],"tags":[]},{"title":"Setting up SMTP for Keycloak Using Mailgun","authorId":"simon_timms","slug":"smtp-setup-for-mailgun","date":"2023-10-30 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/smtp-setup-for-mailgun/","link":"","permalink":"https://westerndevs.com/_/smtp-setup-for-mailgun/","excerpt":"","raw":"---\ntitle:  Setting up SMTP for Keycloak Using Mailgun\nauthorId: simon_timms\ndate: 2023-10-30\noriginalurl: https://blog.simontimms.com/2023/10/30/smtp_setup_for_mailgun\nmode: public\n---\n\n\n\nQuick entry here about setting up Mailgun as the email provider in Keycloak. To do this first you'll need to create SMTP credentials in Mailgun and note the generated password\n\n![](/images/2023-10-30-smtp_setup_for_mailgun.md/2023-10-30-17-34-55.png))\n\nNext in Keycloak set the credentials up in the realm settings under email. You'll want the host to be smtp.mailgun.org and the port to be 465. Enable all the encryptions and use the full email address as the username.\n![](/images/2023-10-30-smtp_setup_for_mailgun.md/2023-10-30-17-34-24.png))\n\nCheck both the SSL boxes and give it port 465.","categories":[],"tags":[]},{"title":"Load Testing with Artillery","authorId":"simon_timms","slug":"artillery","date":"2023-10-14 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/artillery/","link":"","permalink":"https://westerndevs.com/_/artillery/","excerpt":"","raw":"---\ntitle:  Load Testing with Artillery\n# Scenario \n# Getting started \nauthorId: simon_timms\ndate: 2023-10-14\noriginalurl: https://blog.simontimms.com/2023/10/14/artillery\nmode: public\n---\n\n\n\nLoad testing a site or an API can be a bit involved. There are lots of things to consider like what the traffic on your site typically looks like, what peaks look like and so forth. That's mostly outside the scope of this article which is just about load testing with artillery.\n\n## Scenario \n\nWe have an API that we call which is super slow and super fragile. We were recently told by the team that maintains it that they'd made improvements and increased our rate limit from something like 200 requests per minute to 300 and could we test it. So sure, I guess we can do your job for you. For this we're going to use the load testing tool [artillery](https://www.artillery.io/).\n\n## Getting started \n\nArtillery is a node based tool so you'll need to have node installed.  You can install artillery with `npm install -g artillery`.\n\nYou then write a configuration file to tell artillery what to do. Here's the one I used for this test (with the names of the guilty redacted):\n\n```yaml\nconfig: \n  target: https://some.company.com\n  phases:\n    - duration: 1\n      arrivalRate: 1\n  http:\n    timeout: 100\nscenarios:\n - flow:\n    - log: \"Adding new user\n    - post:\n        url: /2020-04/graphql\n        body: |\n          {\"query\":\"query readAllEmployees($limit: Int!, $cursor: String, $statusFilter: [String!]!) {\\n company {\\n employees(filter: {status: {in: $statusFilter}}, pagination: {first: $limit, after: $cursor}) {\\n pageInfo {\\n hasNextPage\\n startCursor\\n endCursor\\n hasPreviousPage\\n }\\n nodes {\\n id\\n firstName\\n lastName\\n\\t\\tmiddleName\\n birthDate\\n displayName\\n employmentDetail {\\n employmentStatus\\n hireDate\\n terminationDate\\n }\\n taxIdentifiers {\\n taxIdentifierType\\n value\\n }\\n payrollProfile {\\n preferredAddress {\\n streetAddress1\\n streetAddress2\\n city\\n zipCode\\n county\\n state\\n country\\n }\\n preferredEmail\\n preferredPhone\\n compensations {\\n id\\n timeWorked {\\n unit\\n value\\n }\\n active\\n amount\\n multiplier\\n employerCompensation {\\n id\\n name\\n active\\n amount\\n timeWorked {\\n unit\\n value\\n }\\n multiplier\\n }\\n }\\n }\\n }\\n }\\n }\\n}\\n\",\"variables\":{\n          \"limit\": 100,\n          \"cursor\": null,\n          \"statusFilter\": [\n          \"ACTIVE\",\n          \"TERMINATED\",\n          \"NOTONPAYROLL\",\n          \"UNPAIDLEAVE\",\n          \"PAIDLEAVE\"\n          ]\n          }}\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer <redacted>\n```\n\nAs you can see this is graphql and it is a private API so we need to pass in a bearer token. The body I just stole from our postman collection so it isn't well formatted. \n\nRunning this is as simple as running `artillery run <filename>`.\n\nAt the top you can see arrival rates and duration. This is saying that we want to ramp up to 1 requests per second over the course of 1 second. So basically this is just proving that our request works. The first time I ran this I only got back 400 errors. To get the body of the response to allow me to see why I was getting a 400 I set \n```\nexport DEBUG=http,http:capture,http:response\n```\n\nOnce I had the simple case working I was able to increase the rates to higher levels. To do this I ended up adjusting the phases to look like \n```yaml\n  phases:\n    - duration: 30\n      arrivalRate: 30\n      maxVusers: 150\n```\n\nThis provisions 30 users a second up to a maximum of 150 users - so that takes about 5 seconds to saturate. I left the duration higher because I'm lazy and artillery is smart enough to not provision more. Then to ensure that I was pretty constantly hitting the API with the maximum number of users I added a loop to the scenario like so:\n\n```yaml\nscenarios:\n - flow:\n    - log: \"New virtual user running\"\n    - loop:\n      - post:\n          url: /2020-04/graphql\n          body: |\n            {\"query\":\"query readAllEmployeePensions($limit: Int!, $cursor: String, $statusFilter: [String!]!) {\\n company {\\n employees(filter: {status: { in: $statusFilter }}, pagination: {first: $limit, after: $cursor}) {\\n pageInfo {\\n hasNextPage\\n startCursor\\n endCursor\\n hasPreviousPage\\n }\\n nodes {\\n id\\n displayName\\n payrollProfile {\\n pensions {\\n id\\n active\\n employeeSetup {\\n amount {\\n percentage\\n value\\n }\\n cappings {\\n amount\\n timeInterval\\n }\\n }\\n employerSetup {\\n amount {\\n percentage\\n value\\n }\\n cappings {\\n amount\\n timeInterval\\n }\\n }\\n employerPension {\\n id\\n name\\n statutoryPensionPolicy\\n }\\n customFields {\\n name\\n value\\n }\\n }\\n }\\n }\\n }\\n }\\n}\\n\",\"variables\":{\n            \"limit\": 100,\n            \"statusFilter\": [\n            \"ACTIVE\"\n            ]\n            }}\n          headers:\n            Content-Type: application/json\n            Authorization: Bearer <redacted>\n      count: 100\n```\n\nPay attention to that count at the bottom. \n\nI was able to use this to fire thousands of requests at the service and prove out that our rate limit was indeed higher than it was before and we could raise our concurrency.","categories":[],"tags":[]},{"title":"Alerting on Blob Storage Throttling","authorId":"simon_timms","slug":"storage-alert-rules","date":"2023-03-30 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/storage-alert-rules/","link":"","permalink":"https://westerndevs.com/_/storage-alert-rules/","excerpt":"","raw":"---\ntitle:  Alerting on Blob Storage Throttling \nauthorId: simon_timms\ndate: 2023-03-30\noriginalurl: https://blog.simontimms.com/2023/03/30/storage-alert-rules\nmode: public \n---\n\n\n\nBlob storage is the workhorse of Azure. It is one of the original services and has grown with the times to allow storing data in a variety of formats. It is able to scale perhaps not to the moon but certainly to objects in low earth orbit(LEO). \n\nOne of my clients has a fair bit of data stored in a file share hosted in Azure Storage. They do nightly processing on this data using a legacy IaaS system. We were concerned that we might saturate the blob storage account with our requests. Fortunately, there are metrics we can use to understand what's going on inside blob storage. Nobody wants to monitor these all the time so we set up some alerting rules for the storage account. \n\nAlert rules can easily be created by going to the file share in the storage account and clicking on metrics. Then in the top bar click on `New Alert Rule`\n\nThe typical rules we applied were \n1. Alerting if we reach a certain % of capacity. We set this to about 90%\n![](/images/2023-03-30-storage-alert-rules.md/2023-03-30-07-32-46.png))\n2. Alerting if we see the number of transactions fall outside a typical range. We used a dynamic rule for this to account for how the load on this batch processing system changes overnight. \n![](/images/2023-03-30-storage-alert-rules.md/2023-03-30-07-35-25.png))\n\nHowever there was one additional metric we wanted to catch: when we have hit throttling. This was a bit trickier to set up because we've never actually hit this threshold. This means that the dimensions to filter on don't actually show up in the portal. They must be entered by hand. \n\nThese are the normal values we see\n![](/images/2023-03-30-storage-alert-rules.md/2023-03-30-07-38-07.png))\n\nBy clicking on add custom value we were able to add 3 new response codes \n\n* ClientAccountBandwidthThrottlingError\n* ClientShareIopsThrottlingError\n* ClientThrottlingError\n\n![](/images/2023-03-30-storage-alert-rules.md/2023-03-30-07-40-59.png))\n\nWith these in place we can be confident that should these ever occur we'll be alerted to it","categories":[],"tags":[]},{"title":"Importing Vuetify via ESM","authorId":"simon_timms","slug":"vuetify-esm","date":"2023-03-15 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/vuetify-esm/","link":"","permalink":"https://westerndevs.com/_/vuetify-esm/","excerpt":"","raw":"---\ntitle:  Importing Vuetify via ESM\nauthorId: simon_timms\ndate: 2023-03-15\noriginalurl: https://blog.simontimms.com/2023/03/15/vuetify-esm\nmode: public\n---\n\n\n\nIf you want a quick way to add Vue 3 and Vuetify to your project via the UMD CDN then you can do so using ESM. ESM are ECMAScript Modules and are now supported by the majority of browsers. This is going to look like \n\n```html\n<script type=\"importmap\">\n    { \"imports\": \n        { \"vue\": \"https://unpkg.com/vue@3.2.47/dist/vue.esm-browser.js\" }\n          \"vuetify\": \"https://unpkg.com/vuetify@3.1.10/dist/vuetify.esm.js\"\n        }\n</script>\n<script type=\"module\">\n    import {createApp} from 'vue'\n    import { createVuetify } from 'vuetify'\n    const vuetify = createVuetify()\n\n    createApp({\n      data() {\n        return {\n          message: 'Hello Vue!'\n        }\n      }\n    }).use(vuetify).mount('#app')\n</script>\n```\n\nThe import map is a way to map a module name to a URL. This is necessary because the Vuetify ESM module imports from Vue. Don't forget you'll also need to add in the CSS for Vuetify","categories":[],"tags":[]},{"title":"App Service Quota Issue","authorId":"simon_timms","slug":"app-service-quota","date":"2023-03-04 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/app-service-quota/","link":"","permalink":"https://westerndevs.com/_/app-service-quota/","excerpt":"","raw":"---\ntitle:  App Service Quota Issue \nauthorId: simon_timms\ndate: 2023-03-04\noriginalurl: https://blog.simontimms.com/2023/03/04/app-service-quota\nmode: public\n---\n\n\n\nI was deploying an app service in a new region today and ran into a quota issue. The error message was:\n\n```\nError: creating Service Plan: (Serverfarm Name \"***devplan\" / Resource Group \"***_dev\"): web.AppServicePlansClient#CreateOrUpdate: Failure sending request: StatusCode=401 -- Original Error: Code=\"Unauthorized\" Message=\"This region has quota of 0 instances for your subscription. Try selecting different region or SKU.\"\n```\n\nThis was a pretty simple deployment to an S1 app service plan. I've run into this before and it's typically easy to request a bump in quota in the subscription. My problem today was that it isn't obvious what CPU quota I need to request. I Googled around and found some suggestion that S1 ran on A series VMs but that wasn't something I had any limits on.\n\nCreating in the UI gave the same error\n\n![](/images/2023-02-10-app-service-quota.md/2023-02-10-20-45-53.png))\n\nI asked around and eventually somebody in the know was able to look into the consumption in that region. The cloud was full! Well not full but creation of some resources was restricted. Fortunately this was just a dev deployment so I was able to move to a different region and get things working. It would have been pretty miserable if this was a production deployment or if I was adding onto an existing deployment.","categories":[],"tags":[]},{"title":"Azure KeyVault Reference Gotcha","authorId":"simon_timms","slug":"azure-keyvaul-reference-gotcha","date":"2023-03-04 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-keyvaul-reference-gotcha/","link":"","permalink":"https://westerndevs.com/_/azure-keyvaul-reference-gotcha/","excerpt":"","raw":"---\ntitle:  Azure KeyVault Reference Gotcha\nauthorId: simon_timms\ndate: 2023-03-04\noriginalurl: https://blog.simontimms.com/2023/03/04/azure-keyvaul-reference-gotcha\nmode: public\n---\n\n\n\nI was working on a deployment today and ran into an issue with a keyvault reference. In the app service the keyvault reference showed that it wasn't able to get the secret. The reference seemed good but I wasn't seeing what I wanted to see which was a couple of green checkmarks\n\n![](/images/2023-03-03-azure-keyvaul-reference-gotcha.md/2023-03-03-21-10-36.png))\n\nThe managed identity on the app service had only GET access to the keyvault. I added LIST access and the reference started working. I'm not sure why this is but I'm guessing that the reference is doing a LIST to get the secret and then a GET to get the secret value.","categories":[],"tags":[]},{"title":"Allow Comments in JSON Payload in ExpressJS","authorId":"simon_timms","slug":"express-json-comments","date":"2023-02-16 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/express-json-comments/","link":"","permalink":"https://westerndevs.com/_/express-json-comments/","excerpt":"","raw":"---\ntitle:  Allow Comments in JSON Payload in ExpressJS\nauthorId: simon_timms\ndate: 2023-02-16\noriginalurl: https://blog.simontimms.com/2023/02/16/express-json-comments\nmode: public\n---\n\n\n\nOfficially comments are not supported in the JSON format. In fact this lack of ability to comment is one of the reasons that lead to the downfall of the JSON based project system during the rewrite of the .NET some years back. However they sure can be useful to support. In my case I wanted to add some comments to the body of a request to explain a parameter in Postman. I like to keep comments as close to the thing they describe as possible so I didn't want this on a wiki somewhere nobody would ever find. \n\nThe content looked something like \n\n```json\n{\n    \"data\": {\n        \"form_data\": {\n            \"effective_date\": \"2023-02-23\",\n            \"match_on_per_pay_period_basis\": 0, /* 0 if yes, 1 if no */\n            \"simple_or_tiered\": 1, /* 0 if simple 1 if tiered */\n        }\n    }\n}\n```\n\nThis was going to an ExpressJS application which was parsing the body using `body-parser`. These days we can just use `express.json()` and avoid taking on that additional dependency. The JSON parsing in both these is too strict to allow for comments. Fortunately, we can use middleware to resolve the issue. There is a swell package called `strip-json-comments` which does the surprisingly difficult task of stripping comments. We can use that. \n\nThe typical json paring middleware looks like\n\n```javascript\napp.use(express.json())\n\nor \n\napp.use(bodyParser.json())\n```\n\nInstead we can do \n\n```javascript\nimport stripJsonComments from 'strip-json-comments';\n\n...\n\napp.use(express.text{\n    type: \"application/json\" // \n}) //or app.use(bodyParser.text({type: \"application/json}))\napp.use((req,res,next)=> {\n    if(req.body){\n        req.body = stripJsonComments(req.body);\n    }\n    next();\n})\n```\n\nThis still allows us to take advantage of the compression and character encoding facilities in the original parser while also intercepting and cleaning up the JSON payload.","categories":[],"tags":[]},{"title":"Excel and Ruby","authorId":"simon_timms","slug":"ruby-excel","date":"2023-02-15 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/ruby-excel/","link":"","permalink":"https://westerndevs.com/_/ruby-excel/","excerpt":"","raw":"---\ntitle:  Excel and Ruby\n# Spreadsheet\n# RubyXL\n# CAXLSX\n# Fast Excel\nauthorId: simon_timms\ndate: 2023-02-15\noriginalurl: https://blog.simontimms.com/2023/02/15/ruby-excel\nmode: public\n---\n\n\n\nExcel is the king of spreadsheets and I often find myself in situation where I have to write our Excel files in an application. I'd say that as an application grows the probability of needing Excel import or export approaches 1. Fortunately, there are lots of libraries out there to help with Excel across just about every language. The quality and usefuleness of these libraries varies a lot. In Ruby land there seem to be a few options.\n\n## Spreadsheet\n\nhttps://github.com/zdavatz/spreadsheet/\n\nAs the name suggests this library deals with Excel spreadsheets. It is able to both read and write them by using Spreadsheet::Excel Library and the ParseExcel Library. However it only supports the older XLS file format. While this is still widely used it is not the default format for Excel 2007 and later. I try to stay clear of the format as much as possible. There have not been any releases of this library in about 18 months but there haven't been any releases of the XLS file format for decades so it doesn't seem like a big deal. \n\nThe library can be installed using \n\n```\ngem install spreadsheet\n```\n\nThen you can use it like so\n\n```ruby\nrequire 'spreadsheet'\n\nworkbook = Spreadsheet.open(\"test.xls\")\nworksheet = workbook.worksheet 0\nworksheet.rows[1][1] = \"Hello there!\"\nworkbook.write(\"test2.xls\")\n```\n\nThere are some limitations around editing files such as cell formats not updating but for most things it should be fine. \n\n## RubyXL\n\nhttps://github.com/weshatheleopard/rubyXL\n\nThis library works on the more modern XLSX file formats. It is able to read and write files with modifications. However there are some limitations such as being unable to insert images \n\n```ruby\nrequire 'rubyXL'\n\n  # only do this if you don't care about memory usage, otherwise you can load submodules separately\n  # depending on what you need\nrequire 'rubyXL/convenience_methods'\n\nworkbook = RubyXL::Parser.parse(\"test.xlsx\")\nworksheet = workbook[0]\ncell = worksheet.cell_at('A1')\ncell.change_contents(\"Hello there!\")\nworkbook.write(\"test2.xlsx\")\n```\n\n## CAXLSX\n\nhttps://github.com/caxlsx/caxlsx\n\nThis library is the community supported version of AXLSX. It is able to generate XLSX files but not read them or modify them. There is rich support for charts, images and other more advanced excel features. The \n\nInstall using \n\n```\ngem install caxlsx\n```\n\nAnd then a simple example looks like\n\n```ruby\nrequire 'axlsx'\n\np = Axlsx::Package.new\nworkbook = p.workbook\n\nwb.add_worksheet(name: 'Test') do |sheet|\n  sheet.add_row ['Hello there!']\nend\n\np.serialize \"test.xlsx\"\n\n```\n\nOf all the libraries mentioned here the documentation for this one is the best. It is also the most actively maintained. The examples directory https://github.com/caxlsx/caxlsx/tree/master/examples gives a plethora of examples of how to use the library.\n\n\n## Fast Excel\n\nhttps://github.com/Paxa/fast_excel\n\nThis library focuses on being the fastest excel library for ruby. It is actually written in C to speed it up so comes with all the caveats about running native code. Similar to CAXLSX it is only able to read and write files and not modify them. \n\n```ruby\nrequire 'fast_excel'\n\n  # constant_memory: true streams changes to disk so it means that you cannot\n  # modify an already written record\nworkbook = FastExcel.open(\"test.xlsx\", constant_memory: true)\nworksheet = workbook.add_worksheet(\"Test\")\n\nbold = workbook.bold_format\nworksheet.set_column(0, 0, FastExcel::DEF_COL_WIDTH, bold)\nworksheet << [\"Hello World\"]\nworkbook.close\n```\n\nAs you can see here the library really excels at adding consistently shaped rows. You're unlikely to get a complex spreadsheet with headers and footers built using this tooling. ","categories":[],"tags":[]},{"title":"Defining the Problem before the Solution","authorId":"david_wesst","slug":"defining-problem-before-the-soltuion","date":"2023-01-27 23:24:33+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"musings/defining-problem-before-the-soltuion/","link":"","permalink":"https://westerndevs.com/musings/defining-problem-before-the-soltuion/","excerpt":"Before you create a solution, you need to understand the problem. It sounds obvious enough, yet I see developers (including myself) getting into the code and design phase before they really understand the problem they are trying to fix. These are the steps I take a properly understand a problem I am trying to solve, prior to coding or solution-ing anything.","raw":"---\ntitle: Defining the Problem before the Solution\ndate: 2023-01-27T18:24:33.221Z\ntags:\n  - solution-architecture\n  - problem-definition\n  - requirements-gathering\n  - defining-value\n  - code\ncategories:\n  - musings\noriginalurl: https://www.davidwesst.com/blog/defining-problem-before-the-soltuion\nauthorId: david_wesst\nexcerpt: Before you create a solution, you need to understand the problem. It\n  sounds obvious enough, yet I see developers (including myself) getting into\n  the code and design phase before they really understand the problem they are\n  trying to fix. These are the steps I take a properly understand a problem I am\n  trying to solve, prior to coding or solution-ing anything.\n---\n\n[1]: https://github.com/davidwesst/website/\n[2]: https://www.davidwesst.com/talks/concensus-in-the-chaos/\n[3]: https://www.davidwesst.com/talks\n[4]: https://www.davidwesst.com/talks/cots-to-cloud/\n\nDevelopers love to code.\n\nI know this, because I am a developer. My heart constantly wants to code up the solution to...well anything. What I have learned over the developing and architecting enterprise software solutions, and as the solo developer of [my website project][1] is how this love of code can actually slow down and sometimes halt the development of a project or feature because we get too caught up in the tech, we don't take the time to reflect and solve the actual problem.\n\nHow do you fix this habit? Before you start coding up a solution, make sure you understand the problem you are trying to solve. Seems simple enough yet developers (like me) have the habit of jumping right into the code before they even really know what they are trying to solve. \n\nThrough my years of experience solving problems with technology, I have a couple of steps I go through to help inform my solution design for problems of a variety of problems. I apply these steps when I am trying to figure out how to integrate two enterprise systems and when I'm trying to figure out the best way to implement a new feature on my website. \n\nThe steps are the same, although the effort required will vary. \n\n# Understanding the Problem\n\nAnd I don't mean coding problem. \n\nI mean _business problem_ or _real life problem_ or whatever you want to call it, but it's not a code problem. Never have I ever been asked by a client to \"implement a binary tree\" or \"write a sorting algorithm for sorting an array\". That's not to say those aren't problems, but they aren't _business problems_. These are technical problems, and they are fun to work on...sometimes. ðŸ˜…\n\nBusiness problems are the reason clients engage with software developers. The client wants software to fix their problem, and they seem to think that software is the solution. Before you code _anything_, take a few moments to answer the following about the problem you're preparing to solve with code.\n\n## 1) Why is this a problem?\n\nI am not suggesting you second guess the client, but rather try and empathize with your client and really understand why their problem is what it is. This is where you can start to understand whether or not software development fits into the solution to the problem. I have come across this many times, where after revisiting the problem with the client, we found the best solution was a change in their business process rather than adding tools to it.\n\nLet's assume, for the sake of this post, that you see where software can help play a role in solving the problem.\n\n## 2) What happens if we do nothing?\n\n![An faded wooden sign that says the word \"Nothing\" in large blocky letters, set against a clear blue sky. Photo by Evan Buchholz on Unsplash.com](/images/2023-01-27-defining-problem-before-the-soltuion/nothing-sign.jpg)\n\nSounds silly, I know, but doing nothing is always an option and people do it all the time. But why would someone choose to do nothing? Because _the risk doesn't outweigh the reward_. \n\nBy answering this question with your client, you get to understand the risks associated with the problem. This will inform your solution design, as if the risks are high you may want to invest more time and effort into parts of the design than others. It will also give you context on the priority of your solution in the mind of your client.\n\n## 3) What KPIs or Success Metrics can your client define upfront?\n\nThe last thing I try to do is try and pull any key performance indicators (KPIs) or metrics that will help define success for the solution. I find that most of the time, this is about turning qualitative terms and statements into quantitative ones. \n\nFor example, \"We need to process these forms faster\" should change to something like \"We should be able to process at least 100 forms an hour\". See the difference?\n\nYou are adding clear, measurable, success criteria for your solution. The terms \"these forms\" and \"faster\" are too vague to build on. Maybe fast enough to you is 1 form a day, oy maybe 1 form a second. Your client is the expert in their business, so you should ask them so you can understand the goals and potential constraints your solution needs to address.\n\n# Redefine the Problem\n\nI know-- your hands are itchy from not coding, but assuming you took the time to understand the problem, the next step is to confirm your new found knowledge. The easiest way to do that is by explaining it to someone else, like your client. If your client agrees you nailed it, you nailed it and now you're ready to start \n_designing_ (not coding) your solution.\n\nOne thing that is not uncommon is that your definition of the problem may sound different than the problem your client originally described. This is _normal_, as _you_ are the technology problem solving expert. \n\nThe fact that your definition of the problem differs from your client's isn't necessarily a bad thing either. Many times, I have found that through my problem definition process, the client gains a better understanding of root cause of their problem and their mind will shift from their presumed solution, to something else. \n\n# Example: Adding Non-Blog Content to my Website\n\nLet me walk you though the process on something not so enterprise-level, but small scale, like a solo-developed website project.\n\nI hit a problem planning the next release of my website where I realized that it was going to be very complicated and cumbersome to add non-blog content to my website, such as the presentation materials from Prairie Dev Con [here][2] and [here][4]. At this point, here is what we know:\n\n> 1) Client = Me\n> 2) Problem = Adding non-blog content to the website is difficult.\n\nLike a good developer, I immediately started down the path of designing a custom application that would automate all the things that make adding content difficult. It was very fun, but after a couple of hours, I caught myself and took a step back and applied my problem definition process.\n\nLet's go through it, and we start by understanding the problem.\n\n## 1) Why is it a problem?\n\nIt is a problem because I want to continue to add different types of content to the website. The whole purpose of the site is to create a central hub for all my work, almost like a portfolio, but more like a \"hub\" for all things I create a share. The website is built to handle blog posts or document style content, but when you add more complicated content that is made up of more than just an article or webpage, you need to add links to other data (like files) which is a manual process and is error prone.\n\nIn short, it is a problem because maintaining non-article data will be difficult.\n\n## 2) What happens if we do nothing to solve the problem?\n\nYou can see in the [talks page][3] I have already added some non-article data, which is all currently managed through a JSON file that the website generator pickups and creates pages for. I also needed to upload the files to a public storage host (Azure Blob Storage) and use copy and paste the links into the JSON, which I messed up a few times. \n\nThis was my first attempt at \"doing nothing\" for this problem, and it was difficult. The plan is to add the back catalogue of presentations I have done over the past 10 years (or more probably), which will make that JSON file exceptionally difficult to manage.\n\nWhen you frame it in the context of risk: doing nothing will very likely result in an massive increase in the number of errors in the data. \n\n## 3) What KPIs can we use to measure solution success?\n\nIf we look at the original problem statement \"Adding non-blog content to the website is difficult\", we need to translate the term \"difficult\" into a quantitative one. This would give us a measure to determine how much easier it is to add new content.\n\nPulling from the answer to question 2, it's really managing the JSON file that makes things difficult. And so I asked myself (the client), what makes managing a JSON file so difficult? There are plenty of tools for that already. And this is where the _real problem_ revealed itself.\n\nThe relationships between the data leads to errors. Maintaining these relationships manually is exceptionally difficult, and we only have two relationships so far: presentation to event, and presentation to the presentation materials.\n\n## Redefining the Problem\n\nNow that we know the _real_ problem, we can redefine problem:\n\n> Problem = The process of manually managing the relationships between content types and data is exceptionally error prone and not scalable.\n\nThis updated problem is one that will inform the solution design moving forward. If you want to get specific about the tech needed, we have a very powerful and mature tool that will help solve data relationships: a relational database. How it informs the solution, is a whole other blog post or posts, but at least now we _know_ what we are trying to solve and can use our technical expertise to solve it.\n\n# Conclusion / TL;DR;\n\nBefore you start designing solutions or coding, take the time to clearly define the problem you are working to solve with your client (which can be you, if its your own project). To define the problem, answer these questions first:\n\n1) Why is it a problem?\n2) What happens if we do nothing to solve the problem?\n3) What KPIs can we use to measure solution success?\n\nOnce you have that, redefine the problem by wording it in a way that highlights the root issue to solve, along with the way to measure success. Assuming the client agrees with your redefined problem, you are ready to start using the big, beautiful brain of yours and start solution-ing!\n\nThanks for playing.\n\n~ DW\n\n---\n\n# Image Credit\n- \"Nothing Sign\" Photo by <a href=\"https://unsplash.com/@vnbuchholz92?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Evan Buchholz</a> on <a href=\"https://unsplash.com/photos/z-Hu8pnt23s?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n\n","categories":[{"name":"musings","slug":"musings","permalink":"https://westerndevs.com/categories/musings/"}],"tags":[{"name":"code","slug":"code","permalink":"https://westerndevs.com/tags/code/"},{"name":"solution-architecture","slug":"solution-architecture","permalink":"https://westerndevs.com/tags/solution-architecture/"},{"name":"problem-definition","slug":"problem-definition","permalink":"https://westerndevs.com/tags/problem-definition/"},{"name":"requirements-gathering","slug":"requirements-gathering","permalink":"https://westerndevs.com/tags/requirements-gathering/"},{"name":"defining-value","slug":"defining-value","permalink":"https://westerndevs.com/tags/defining-value/"}]},{"title":"Docker Build Hangs When Adding Key with apt-key in WSL2","authorId":"david_wesst","slug":"docker-build-hangs-on-apt-key-in-wsl2","date":"2023-01-12 02:43:58+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"coding/problem-solution/docker-build-hangs-on-apt-key-in-wsl2/","link":"","permalink":"https://westerndevs.com/coding/problem-solution/docker-build-hangs-on-apt-key-in-wsl2/","excerpt":"The solution to the problem where an apt-key command seems to run forever in your docker build.","raw":"---\ntitle: Docker Build Hangs When Adding Key with apt-key in WSL2\ndate: 2023-01-11T21:43:58.005Z\ncategories:\n  - coding\n  - problem-solution\ntags:\n  - docker\n  - wsl2\n  - apt-key\n  - debian\n  - linux\n  - devcontainer\nimage-alt: A light blue cartoon style whale, meant to resemble the Docker logo,\n  with a yellow-orange beak, like the Linux penguin, holding a key in it's\n  mouth. Drawn with very small squares.\nimage-type: image/png\nimage-credit:\n  derived-from: https://labs.openai.com/s/z8Gdu46DdNOGjAKCTXiY1mmW\noriginalurl: https://www.davidwesst.com/blog/docker-build-hangs-on-apt-key-in-wsl2\nauthorId: david_wesst\nexcerpt: The solution to the problem where an apt-key command seems to run\n  forever in your docker build.\n---\n\n[1]: https://www.mono-project.com/download/stable/#download-lin-debian\n[2]: https://github.com/davidwesst/inky/tree/8a5809f0b5f0a480b37b759443479fa13b9cf18c\n[3]: https://github.com/davidwesst/inky/commit/52b9d1a2e577061ae1da735e05cf466712bb9279\n[4]: https://unix.stackexchange.com/a/128704 \n[5]: https://manpages.debian.org/testing/apt/apt-key.8.en.html\n[6]: https://containers.dev/\n\n## Problem \n\nWhen trying to add a key using `apt-key` on a Debian 11 docker image, the step seems to run infinitely.  \n\nThe screenshot below highlights this problem when adding a key that is necessary to validate the mono-complete package.\n\n![A terminal window showing the steps of a docker build command along with their run times. The command that is currently being run is an apt-key command that is still running after 8078.8 seconds](/images/2023-01-11-docker-build-hangs-on-apt-key-in-wsl2/console-screenshot.png)\n\n### Details \n\nI setup a [DevContainer][6] to build Inky, a interactive fiction editor I like for game projects, without having to install all the build dependencies on my local machine. The Docker container build worked on my Linux machine, but would hang on my Windows 11 box, using Docker Desktop with WSL2. More specifically, it would run forever on the `apt-key` command, as specified by the [mono install instructions][1]. \n\nIf you need an example, take a look at [my Inky repository fork at that specific point][2].\n\n## Solution \n \nThe issue was that the command specifically references port 80 in the URL to the keyserver. In the end, I changed: \n \n`sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF` \n \nto  \n \n`sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF`\n \nYou can see the specifics in the [next commit in my example repository][3] in the following commit here. \n \n### Reference \n \nI was put on the right track with a Stack Overflow post trying to solve a similar issue with `apt-key`. Scrolling through the answers, I found this one: [LINK][4]\n \n### `apt-key` Deprecation Notice \n \nIf you look at the [Debian documentation for `apt-key`][5] or try running the command yourself, you might notice the deprecation warning. Underneath the hood, it runs the appropriate command in Debian 11, but will be gone after Debian 11 and Ubuntu 22.04. \n \nJust something to note for those looking over this solution in the future. \n \n## Conclusion / TL;DR; \n \nI needed to remove the port number from the keyserver URL used in my `apt-key` command. \n \nThanks for playing.\n\n~ DW","categories":[{"name":"coding","slug":"coding","permalink":"https://westerndevs.com/categories/coding/"},{"name":"problem-solution","slug":"coding/problem-solution","permalink":"https://westerndevs.com/categories/coding/problem-solution/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://westerndevs.com/tags/linux/"},{"name":"wsl2","slug":"wsl2","permalink":"https://westerndevs.com/tags/wsl2/"},{"name":"apt-key","slug":"apt-key","permalink":"https://westerndevs.com/tags/apt-key/"},{"name":"debian","slug":"debian","permalink":"https://westerndevs.com/tags/debian/"},{"name":"devcontainer","slug":"devcontainer","permalink":"https://westerndevs.com/tags/devcontainer/"}]},{"title":"Highlight Reel for 2022","authorId":"david_wesst","slug":"highlight-reel-for-2022","date":"2023-01-05 21:47:02+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"musings/highlight-reel-for-2022/","link":"","permalink":"https://westerndevs.com/musings/highlight-reel-for-2022/","excerpt":"When a new year arrives, it is a great opportunity to take a moment to reflect on where you started at the beginning of the year, and where you ended. When I look back on 2022, I noticed events reflected in my GitHub contribution graph that highlight some common cycles in my own behaviour. I want to take a moment to document this, and hopefully you can use this an example to examine your own progress and behaviour patterns that might be aiding (or impeding) your own personal and professional growth.","raw":"---\ntitle: Highlight Reel for 2022\ndate: 2023-01-05T16:47:02.098Z\ntags:\n  - github\n  - mvp\n  - code\n  - typescript\n  - javascript\n  - prdc-2022\n  - burnout\ncategories:\n  - musings\noriginalurl: https://www.davidwesst.com/blog/highlight-reel-for-2022\nauthorId: david_wesst\nexcerpt: When a new year arrives, it is a great opportunity to take a moment to\n  reflect on where you started at the beginning of the year, and where you\n  ended. When I look back on 2022, I noticed events reflected in my GitHub\n  contribution graph that highlight some common cycles in my own behaviour. I\n  want to take a moment to document this, and hopefully you can use this an\n  example to examine your own progress and behaviour patterns that might be\n  aiding (or impeding) your own personal and professional growth.\n---\n\n[1]: https://www.githubunwrapped.com/davidwesst\n[2]: https://github.com/cocobokostudios/videogamelibrary\n[3]: https://github.com/davidwesst/website/releases\n[4]: https://www.davidwesst.com/tags/mvp/\n[5]: https://www.davidwesst.com/blog/prairie-dev-con-2022-takeaways/\n[6]: https://www.davidwesst.com/blog/open-graph-tools-and-resources-for-web-nerds/\n[7]: https://www.davidwesst.com/blog/does-gdpr-apply-to-personal-websites/\n[8]: https://www.davidwesst.com/talks/concensus-in-the-chaos/\n[9]: http://localhost:8080/talks/cots-to-cloud/\n\nWhile working on revamping my website and blog, I revisited a number of my old posts. Two of my favourites are the Highlight Reels for [2014][1] and [2015][2] as they document how I felt about myself and my accomplishments at that point in my life and career. Considering that this past year has been one of pretty extreme personal transformation, I thought it would be appropriate to document my self-reflection for 2022 in a similar style. The difference this time around will be that I focus on observed behaviours related to event and work, rather than the events themselves. More specifically, the behaviours that I like, love, and need to improve.\n\nDon't worry-- it's not all feelings. It's data too. All my observed behaviours relate to my projects I worked on throughout the year on GitHub, which provides great insights into my contributions. I'll be using my GitHub contributions for 2022 to highlight the spots where I can identify the behaviour. \n\nWhy do this? Because I want to remind myself an others that if you feel like you are stuck, you are better off finding the source of the problem-- even if it makes you face some hard truths. By understanding the root problem, you can work at resolving it, even it it involves changing what you believe is your best approach to work.\n\n## I _love to learn_ about code\n\n![The GitHub contribution graph for user davidwesst, consisting of coloured squares ranging from grey for no contributions to bright green for many, for all the weeks in the year. There is a bright red, mouse-written work \"learning\" with an arrow pointing to a bright red circle around the squares for January and February 2022 where there appears to be a consistent amount of git activity.](./gh2022-contributions_learning.png)\n\nAt the end of 2021, I started looking at the job market and started to notice that the jobs I wanted (or thought I wanted) relied on skills that I have not been able to practice as part of my day job. Coding is no longer one of my responsibilities, only planning, designing, and providing oversight. This sparked the urge to refresh my skills and prove to myself that I _could_ do for these jobs, and that all I had to do was put in the time.\n\nAnd so began a series of LeetCode challenges, learning exercises, and review of various problems so that I could skill up and strengthen those coding muscles again. This is what you see in the contribution graph for the first 2-3 months of 2022.\n\nAlthough this spark eventually faded as it does, I realized something about myself. I realized that it's not the code I love, but the learning about code and how to apply code in various ways. New languages, patterns and practices, solution architecture, whatever-- if it involves coding something, you can count I'll be interested.\n\n### Lesson Learned: Burnout\n\nThis is example highlight Q1 of 2022, yet there are plenty of other times where I spent time learning new tech. Experimenting with Go and Rust as part of my [VGL project][2] (more about that later). A brief experiment with Q# back in 2019 early 2020, and my continual urge to learn C/C++ along with the DevOps tools around it. These are all things that have sparked that _love of learning code_ over the past few years, and each time it's the same pattern: spark of interest, dive deep into the learning, burn out because you don't know where to go with this knowledge.\n\nWhich brings me to the lesson learned: I need to direct my learning energy towards a goal. This way, when the excitement of learning something new fades I will still have a goal in my sights and continue to channel that energy towards something, rather then letting it fade out.\n\n## I _love to build things_ out of code\n\nAt the end of the year, I looked at my [GitHub Unwrapped video][1] and was surprised by my top languages for 2022.\n\n![A blue and white stocking is hung on an imaginary wall with snow coming down and a TypeScript logo sitting on top of it, denoting that TypeScript was my \"top language\" for the year, followed up by JavaScript in second, and HTML in third](/images/2023-01-05-highlight-reel-for-2022/gh2022-toplanguages.png)\n\nI was trying to figure out where I had written so much TypeScript, considering that for the past few months I have been living in JavaScript and HTML. Again, going back to my contribution graph I noticed another spike in activity in May.\n\n![The GitHub contribution graph made up of squares where a large number of the squares for May 2022 show contributions, along with many more from October to December. Both sections of the graph are circled in bright red, and between them is the word \"building\" with arrows pointing at them.](./gh2022-contributions_building.png)\n\nI remembered that I decided to repurpose my learning strategy, and rather than just doing LeetCode exercises and textbook studying to strengthen my atrophied coding muscles, I would study by building something. Something that _I_ found useful, all while further strengthening my skills! This was the beginning of the [Video Game Library or \"VGL\" project][2] where I spent time building a TypeScript-React project and included some experiments with both Go and Rust to determine which language allowed me to leverage WASM (which was yet another rabbit hole I became excited about).\n\nIn the end I shelved the project because I was letting my learning drive the project. This meant anything I wanted to learn, I added scope to the project. In the end, it become too big and my original vision was lost, but the urge to build never fades, only the \"something\" that I am building. \n\nLooking back beyond 2022-- the idea of building something has always driven me. Building a business, a video game, or a product. It doesn't matter, as long as _I am building it_.\n\nWhere it falls over is when the scope gets too large and overwhelming. This is not uncommon amongst creative types (just ask any game developer) but building something, ideally out of code, is something that drives me. If I can channel that excitement and passion on something I believe is worth it, I think I could produce and finish something I could be proud of.\n\nI started to make this realization about myself and my drive to build things later in the year. This is why I came back to building my website that I had let fall into dormant. I wanted to channel that excitement, energy, and knowledge into something I found valuable. My website is something I have talked about improving for years and started redoing countless times. Looking back at the contribution graph, this represents a large part of the contributions for October, November, and December for 2022. This is further evidenced by the [releases of the website][3] I published through the same time period.\n\n### Same Drive, Different Approach\n\nReflecting on my behaviour during the VGL project in May and the website work in the last quarter of the year-- the behaviour and drive was the same. I _loved building something_, yet the VGL project went onto the shelf, and the website finally managed to get some traction. \n\nThe difference was in my approach. \n\nFor the Video Game Library project, I let the excitement of learning and drive its development, which led to scope creep and dilution of the original project vision. With the website I took the time to plan and force myself to complete releases-- no matter how small.\n\nThis change in approach enabled in a longer focus on a single project. Ultimately, that kept my excitement for my website project going longer and I kept coming back to it over and over again to make small (or sometimes larger) improvements. In fact, I am continuing those improvements today as the website is far from complete-- but it's starting to represent the vision I started.\n\n## I _like_ (not love) to share\n\nI have tried sharing and producing content in various forms over the years, but with [Prairie Dev Con returning in 2022][5], I thought I would focus some energy into preparing and share content like I used to in my [Microsoft MVP days][4]. This meant lecture-style presentations and blogging. \n\nAfter three live events in 2022, and almost a blog post a week since mid-October, I realized that I don't love sharing like I used to...rather I only _like it_. It's a subtle difference, but it is definitely different than it once was. \n\nI like it because it is a _practical_ way to document my work. I love learning and building things, and sharing those things is an easy way to document my progress for others-- but more importantly myself. With the blog posts, I documented things I learned for my website like the [Open Graph protocol][6] or my [implementation of GDPR compliance][7]. For the presentations, I focused on what I knew and delivered two original sessions; one about [my day job and what it is means to be an IT Architect][8] and the other a [case study on how to do my day job][9]. \n\nThough this experience this year, I found that I liked the process-- but didn't love it like I used to. To me, the presentations and blog posts were necessary for other outcomes. More specifically, the presentations were my ticket to touch base with other real-life speakers and tech professionals after a multi-year hiatus. The blog posts were my way of documenting, analyzing, and appreciating my own effort into my various side projects.\n\nIn the past, with the MVP program, I blogged and shared to receive validation from my peers and the MVP program itself. Those goals are not bad ones by any stretch, but since I don't have the MVP program pushing me, I need something else to help push me. That \"something\" is myself, and the outcomes I mentioned previously. Personally, I think that means I've grown quite a bit since I was an MVP and is an great example of how 2022 has been a year filled with huge change for myself and my attitude towards work.\n\n![The same GitHub contribution graph from the previous two pictures, except this time the mouse-drawn word \"sharing\" with an arrow leading from the word to a red circle which highlights the months of October, November and December of 2022 as there are a large number of green squares with varying degree of brightness denoted that there were a large number of contributions during these months.](./gh2022-contributions_sharing.png)\n\n## The _need_ to improve\n\nI have mentioned the good things, the changed things, and now I will go over the things I need to improve (in my opinion).\n\n### I _need_ to accept my own skills and abilities\n\nEverybody is different and bring different value to the table. I have led a very privileged career and have had massive success in many different areas, yet for years I have rarely taken the time to appreciate those accomplishments.\n\nInstead, I would get caught up in comparing myself to others and what I _couldn't_ do, rather than what I _could_ do. I would dwell on my lack of recent coding experience, rather than celebrate the time I've spent [migrating legacy systems into the cloud][9]. I would focus on the jobs I did not qualify for, rather than the ones that I did qualify for. \n\nThis cycle of focusing on what is missing is lose-lose situation. There will never be enough success. The grass will always be greener on the other side of fence, no matter how many times I jump over it.\n\nI need to remind myself of this moving forward, and hopefully you can remember that for yourself as well.\n\n### I _need_ to do more, and talk less\n\nPeople refer to me as \"a talker\", as in, I like to talk and I'm pretty good at it.\n\nI leverage my talking skill in my day-to-day job, but when it comes what I am trying to build for myself I need to focus on doing the work rather than talking about it.\n\nIt might be clichÃ©, but \"talk is cheap\" and I need to talk less and do more. Plain and simple.\n\n## Conclusion / TL;DR;\n\nIn short, I identified cyclical behaviours and patterns in myself that relate to the work I put into my various side projects and personal (and professional) development. In 2022, I noticed the following about myself:\n\n1) I _love_ to learn about code\n2) I _love_ to build things out of the things I learn (in code)\n3) I _like_ to share what I build and learn (for future me, and anyone else willing to listen)\n\nThe first two are my way of channelling creativity, which is why I love them so much. Although I used to _love_ sharing my knowledge, at this point in my career and life, I _like_ it as it is a practical way for me to document things as I discover them and connect with others, rather than as a method to be validated and rewarded.\n\nIn terms of how I can improve: \n\n4) I _need_ to accept my own skills and abilities\n5) I _need_ to do more and talk less (but talk about it once it is actually done)\n\nI _need_ to accept and embrace my current skills and abilities, rather than focusing on what I think I am lacking. I also need to focus more on implementing my ideas rather that talking about them. Once I have something built, then I can talk more about it-- but until it's built, I need to focus my energy and excitement on the build rather than the talk.\n\nThanks for playing.\n\n~ DW\n","categories":[{"name":"musings","slug":"musings","permalink":"https://westerndevs.com/categories/musings/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"typescript","slug":"typescript","permalink":"https://westerndevs.com/tags/typescript/"},{"name":"github","slug":"github","permalink":"https://westerndevs.com/tags/github/"},{"name":"prdc-2022","slug":"prdc-2022","permalink":"https://westerndevs.com/tags/prdc-2022/"},{"name":"mvp","slug":"mvp","permalink":"https://westerndevs.com/tags/mvp/"},{"name":"code","slug":"code","permalink":"https://westerndevs.com/tags/code/"},{"name":"burnout","slug":"burnout","permalink":"https://westerndevs.com/tags/burnout/"}]},{"title":"Happy Holidays from the Western Devs","authorId":"all","slug":"happy-holidays-2022","date":"2022-12-24 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/happy-holidays-2022/","link":"","permalink":"https://westerndevs.com/_/happy-holidays-2022/","excerpt":"From our computers to yours, we wanted to take a moment to share with your part of our annual Western Devs Holiday Extranvaganza virtual celebration!","raw":"---\ntitle: Happy Holidays from the Western Devs \ndate: 2022-12-24T12:00:00-05:00\ntags:\n  - happy-holidays\n  - poem\n  - fun\nauthorId: all\nexcerpt: \"From our computers to yours, we wanted to take a moment to share with your part of our annual Western Devs Holiday Extranvaganza virtual celebration!\"\n---\n\nEvery year, the Western Devs try to put together a virtual holiday gathering across 7 timezones to catch-up, have fun, and celebrate the holiday season. To mark the occasion, we had AI generate a poem to help us enjoy the season along with an image for which we will not provide context for.\n\n> 'Twas the night before Christmas, and all through the land\n> The Western Devs group was busily at hand\n> Working on projects, coding with care\n> In hopes that the deadlines would soon be met, with flair\n> \n> he computers were humming, the monitors aglow\n> As the developers worked on code, to and fro\n> Some were debugging, some were designing\n> All with the goal of creating something exciting\n> \n> But as the night wore on, and the work was almost through\n> The developers stopped, and a cheer rose anew\n> They had made it, they had finished the task\n> Now it was time to put down the code and unwind at last\n> \n> So they gathered around, for the holiday was here\n> Raised a glass to a year of hard work and good cheer\n> Here's to the Western Devs group, may your future be bright\n> Merry Christmas to all, and to all a good night!\n\nHave yourselves a great holiday season and new year!\n\n![a portrait of the lobster dressed for bed and sipping a glass of champagne after a one night stand in Seattle. The lobster is sitting on a hotel bed, wearing an oversized aqua robe, with a shirt and bowtie, but no pants, and a set of ski goggles.](/images/2022-12-24-happy-holidays-2022/lobster.png)\n\nCheers!\n","categories":[],"tags":[{"name":"happy-holidays","slug":"happy-holidays","permalink":"https://westerndevs.com/tags/happy-holidays/"},{"name":"poem","slug":"poem","permalink":"https://westerndevs.com/tags/poem/"},{"name":"fun","slug":"fun","permalink":"https://westerndevs.com/tags/fun/"}]},{"title":"Speaking at Tech Events Helps You Grow","authorId":"david_wesst","slug":"speaking-at-tech-events-helps-you-grow","date":"2022-12-20 21:59:37+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/speaking-at-tech-events-helps-you-grow/","link":"","permalink":"https://westerndevs.com/_/speaking-at-tech-events-helps-you-grow/","excerpt":"A large part of my career growth is tied directly to speaking at tech events like conferences and user groups. I have had many people ask me about my experience and wanted to share it for others who might be wondering what benefits actually are.","raw":"---\ntitle: Speaking at Tech Events Helps You Grow\ndate: 2022-12-20T16:59:37.185Z\ntags:\n  - public-speaking\n  - prdc\n  - prairie-dev-con\n  - community\n  - presentations\n  - conference\noriginalurl: https://www.davidwesst.com/blog/speaking-at-tech-events-helps-you-grow\nauthorId: david_wesst\nexcerpt: \"A large part of my career growth is tied directly to speaking at tech\n  events like conferences and user groups. I have had many people ask me about\n  my experience and wanted to share it for others who might be wondering what\n  benefits actually are. \"\n---\n\n[1]: https://www.prairiedevcon.com\n[2]: https://www.westerndevs.com\n\nI was lucky enough to be selected to speak at all three [Prairie Dev Con 2022][1] events this year, after a hiatus from speaking at live tech events. The experience of submitting and delivering a couple of session reminded me how important speaking at these sorts of events has been over the years and now they account for a large part of my career growth. On top of that, I have had many people who are entering tech ask me about my experience and wanted to share it for others who might be wondering what benefits actually are. \n\n# You need to sell yourself and your session \n\nWhen the call for speakers opens up, you are required to submit a summary of your talk and yourself. I call this the pitch process, as your submission is your moment to convince the event organizers you are worth betting on. \n\nIt might sound stressful, but its not. It's a pretty low key process considering you are just filling out a form, and it's low stakes. If you don't make the cut then, you cam try again next time. \n\nThe point is that you need take the time to think about wht you're worth the effort, because you are definitely worth it! You know it, so now is your chance to practice. \n\n# Connecting with Other Speakers \n\nOnce you're accepted, you get a chance to connect with other speakers. These folks are like minded people who are willing to spend their time sharing their experiences and expertise. Sit with people you don't know and have conversations. Introduce yourself. Talk about what you do and listen to what they do. When you're done, find them on LinkedIn and remind them where you met them. \n\nI have met some of the best people this way and have continued to stay connected beyond the conference (shout out to the [WesternDevs][2]). \n\n# A Live Studio Audience \n\nAs much as I appreciate livestreaming and virtualized meetings, speaking in the same room as other humans is very different and definitely develops a different set of skills and strengths. The interaction you get with your audience during and after you deliver your session is something I have not been able to replicate in the digitally transformed world we live in today, in 2022. \n\nJust to be clear, something will go wrong...and that's okay.  \n\nNo matter how much you prep, something will go wrong. A demo will fail, a slide will be out of order, a question will be asked that you don't have the answer to. The key  is in how you react and respond to the situation. These \"mistakes\" are what has made me a better presenter in my day job. It has also helped me learn to stay calm and collected when pressure is being applied. \n\n![\"Five rows of tables with white table clothes with people attentively looking forward at the speaker, who is not in frame, with a PowerPoint slide on the back wall describing solution architecture at the University of Manitoba](/images/2022-12-20-speaking-at-tech-events-helps-you-grow/prdc2022-audience.png)\n\n# Side Note: Considerations Before Committing \n\nAs a side, I wanted to note that not all conferences are created equally. \n\nBefore you submit your session take note on what the conference does to support their speakers. A few questions to ask yourself before you commit your time and effort to a conference:\n- Do they cover your travel and accommodation?  \n- Does it include your admission into the conference?  \n- Do you still own your content when your done the conference? \n\nThere are no right or wrong answers to these questions, but you should consider what you're getting out of the deal when you submit sessions to a conference beyond professional development. \n\nJust remember that the speakers are the talent that makes a conference possible. Your work is valuable, and the conference team should ensure you feel appreciated, ome way or another. \n\n# TL;DR; / Conclusion \n\nSpeaking at in-person events, like tech conferences and user groups, is a a great way to grow as a professional. Key benefits are:\n- You learn promote yourself and your session through the session submission process (a.k.a. the pitch process)\n- Connecting with other speakers who are as passionate as you are about sharing their tech experience\n- Reacting to a live audience and the mistakes you make in front of them\n- Don't forget that your effort is valuable and before you submit a session, make sure the event makes you feel like you are getting a fair deal\n\nThanks for playing.\n\n~ DW","categories":[],"tags":[{"name":"prairie-dev-con","slug":"prairie-dev-con","permalink":"https://westerndevs.com/tags/prairie-dev-con/"},{"name":"public-speaking","slug":"public-speaking","permalink":"https://westerndevs.com/tags/public-speaking/"},{"name":"prdc","slug":"prdc","permalink":"https://westerndevs.com/tags/prdc/"},{"name":"community","slug":"community","permalink":"https://westerndevs.com/tags/community/"},{"name":"presentations","slug":"presentations","permalink":"https://westerndevs.com/tags/presentations/"},{"name":"conference","slug":"conference","permalink":"https://westerndevs.com/tags/conference/"}]},{"title":"Docker Desktop for Linux is not the same as Docker Engine","authorId":"david_wesst","slug":"docker-desktop-is-not-docker-engine","date":"2022-12-14 02:28:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/docker-desktop-is-not-docker-engine/","link":"","permalink":"https://westerndevs.com/_/docker-desktop-is-not-docker-engine/","excerpt":"With Docker for Desktop available for Linux (which I like), I managed to get myself confused regarding its role on my Linux-based development machine. This post clarifies a few things I discovered while triaging an issue I had trying to test my GitHub Workflows locally.","raw":"---\ntitle: Docker Desktop for Linux is not the same as Docker Engine\ndate: 2022-12-13T21:28:36.457Z\ntags:\n  - docker\n  - docker engine\n  - docker desktop\n  - linux\n  - github workflow\noriginalurl: https://www.davidwesst.com/blog/docker-desktop-is-not-docker-engine\nauthorId: david_wesst\nexcerpt: With Docker for Desktop available for Linux (which I like), I managed\n  to get myself confused regarding its role on my Linux-based development\n  machine. This post clarifies a few things I discovered while triaging an issue\n  I had trying to test my GitHub Workflows locally.\n---\n\n[1]: https://docs.docker.com/desktop/install/linux-install/\n[2]: https://docs.docker.com/engine/\n[3]: https://github.com/nektos/act\n[4]: https://github.com/nektos/act#necessary-prerequisites-for-running-act \n[5]: https://github.com/nektos/act/issues/1051\n[6]: https://docs.docker.com/engine/context/working-with-contexts/\n\nI like Docker Desktop. It provides me an easy-to-use GUI (graphical user interface) to manage my docker images I use for various tasks for building software. I use it on Windows, and now I'll be using it on Linux as [it is available for some of the the more common distros][1]. Regardless of its greatness, it is not the same as [Docker Engine][2] running right on the metal (rather than a virtual machine, like Docker Desktop), and some of those differences caught me while testing my GitHub Workflows with [nektos/act][3], which depends on Docker Engine to work.\n\nTo be fair, I should highlight that this is definitely a self-induced problem. The Docker Engine prerequisite is listed right on the  [README for the nektos/act][4], and had I reviewed the documentation I probably would have saved myself the trouble. Still, in my web sleuthing for solutions to the problem I created for myself, I found others had hit similar issues, hence this post.\n\n## The Context\n\nI discovered the problem when I attempted to test my GitHub Workflows locally using [nektos/act][3] which is a tool I have been using for the past few years in my software development. It does this by pulling down a docker image that simulates the GitHub runner and runs the workflow in that Docker container. I have done this a few times over, so went to one of my older projects where I set this up and pulled in the code to get it running.\n\nBeing that this was a fresh Linux install, I had not installed Docker yet. When I searched out the installation instructions for Docker on Linux, I was greeted with this announcement:\n\n![Docker documentation page with a banner highlighting that Docker for Desktop now exists for Linux](/images/2022-12-13-docker-desktop-is-not-docker-engine/docker-desktop-for-linux-notice.jpeg)\n\nI have been using Docker for Desktop on Windows for a while now, and I am always happy to have software that exists across my Windows-Linux development environment ecosystem, and so I went about installing Docker for Desktop as my new Docker install.\n\nAfter testing my new and shiny Docker (for Desktop) installation with the standard `docker run hello-world`, I was ready to get back to coding!\n\nOr so I thought...\n\n## The Problem (and Triaging it)\n\nThis is where things went sideways and the problem appeared. I ran `act -j build` to my run my `build job` in a  workflow I know has worked previously and was greeted with the following error message:\n\n> Cannot connect to Docker daemon. Is the docker daemon running?\n\nNot what I expected, considering I just tested out my fresh Docker install, but I tried pulling the image down myself with the `docker pull` command just to make sure things didn't break, and everything worked as expected.\n\nWith a bit of web sleuthing, I came across others who [reported the same issue][5] and noticed this link in particular:\n\n> You could check if `/var/run` actually contains `docker.sock` \n\nWhen checking this, I found that `docker.sock` was in fact NOT present. I immediate associated it with the Docker for Desktop installation, as that was the only new variable from my previous development environment.\n\n## The Root Cause (Probably)\n\nThis is part where I waste my time trying to figure out why did Docker for Desktop not install docker.sock. Rather that figuring out how to install the docker components that are missing.\n\nAlthough I am no Docker expert, my understanding is that Docker for Desktop runs docker inside a VM rather than on the system itself, unlike Docker Engine. In fact, you can see a separate [Docker context][6] when you list out the contexts.\n\n![Screenshot of a Linux terminal showing the Docker CLI output for docker context list command that lists the default docker context, which is the Docker Engine context, and the Docker for Desktop context for the user](./docker-context-output.jpeg).\n\n**It should be noted** that default context for Docker was listed, even though I had not installed Docker Engine yet. This lead me to believe something I installed was incorrectly configured, but really it was the fact that I had not installed the software I needed.\n\n## The Solution / TL;DR;\n\nAs technical as I made it sound, the real problem was that I was missing software. Specifically I was missing \"docker\" on my Linux machine, even though I installed Docker for Desktop. ðŸ˜Š\n\nWell, if the problem is that I am missing software, then the solution must be to install the software. That software is [Docker Engine][2], which sets up the Docker API right on the machine rather than though a VM like Docker for Desktop (as far as I understand it).\n\nIn conclusion, install the software dependencies the tools If you're running a Linux distro, as great as Docker for Desktop is-- you may still want to install Docker Engine. You can always switch contexts on where to run your own docker commands with the `docker context set` command, but it's worth double checking to make sure the tool you are using supports Docker for Desktop on Linux platforms.\n\nThanks for playing.\n\n~ DW\n\n","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://westerndevs.com/tags/linux/"},{"name":"github workflow","slug":"github-workflow","permalink":"https://westerndevs.com/tags/github-workflow/"},{"name":"docker engine","slug":"docker-engine","permalink":"https://westerndevs.com/tags/docker-engine/"},{"name":"docker desktop","slug":"docker-desktop","permalink":"https://westerndevs.com/tags/docker-desktop/"}]},{"title":"Prairie Dev Con 2022 Takeaways","authorId":"david_wesst","slug":"prairie-dev-con-2022-takeaways","date":"2022-12-07 20:37:20+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/prairie-dev-con-2022-takeaways/","link":"","permalink":"https://westerndevs.com/_/prairie-dev-con-2022-takeaways/","excerpt":"The talented speakers that come together for Prairie Dev Con always bring fresh ideas to my mind and give me pause to think about my own efforts and how I can learn from their experience. Although not a complete list, these ideas the ones that stood out the most from Prairie Dev Con 2022.","raw":"---\ntitle: Prairie Dev Con 2022 Takeaways\ndate: 2022-12-07T15:37:20.777Z\ntags:\n  - prairie-dev-con\n  - prdc-2022\n  - api-design\n  - open-api\n  - developer-velocity-index\n  - e2e-testing\n  - playwright\n  - adiad\noriginalurl: https://www.davidwesst.com/blog/prairie-dev-con-2022-takeaways\nauthorId: david_wesst\nexcerpt: The talented speakers that come together for Prairie Dev Con always\n  bring fresh ideas to my mind and give me pause to think about my own efforts\n  and how I can learn from their experience. Although not a complete list, these\n  ideas the ones that stood out the most from Prairie Dev Con 2022.\n---\n\n[1]: https://swagger.io\n[2]: https://www.openapis.org\n[3]: https://azure.microsoft.com/en-us/solutions/developer-velocity/\n[4]: https://playwright.dev\n[5]: https://www.linkedin.com/in/joelhebert/\n[6]: https://www.linkedin.com/in/ajenns/\n[7]: https://www.davepaquette.com\n[8]: https://www.linkedin.com/in/lavanya-mohan/\n[9]: https://www.linkedin.com/in/adam-krieger-7a087048/\n[10]: https://www.rodpaddock.com\n\nUntil I walked through the doors of the Prairie Dev Con in 2022, I did not realize how much I missed it. The talented speakers that come together always bring fresh ideas to my mind and give me pause to think about my own efforts and how I can learn from their experience. Although not a complete list, these ideas the ones that stood out the most from Prairie Dev Con 2022.  \n\n## API First Design ([JoÃ«l HÃ©bert][5])  \n\nJoel did a great session about API first design, which was a very dense session, but he delivered the content in a way that was very approachable and allowed me to think of the benefits of doing API first design with tools like [Swagger.io][1] and [OpenAPI][2].  \n\nIt was great seeing the value of these tools, and hearing about the patterns and practices experienced API developers like Joel use to implement consistent and secure APIs.  \n\n## Developer Velocity Index ([AJ Enns][6]) \n\nI went into this session thinking I was going to be fascinated with the subject, but that the concept would apply only to development leads or possibly coders, rather than an architect like me.. \n\nI was wrong.  \n\nThe [Developer Velocity Index (DVI)][3], is a way for any team (even if it is a one-person team, like me on my side projects) can help frame up and scope the abstract problem of figuring out how do to deliver more value.  \n\nI plan on applying the DVI to my side project adventures, self-development, and my enterprise day-job efforts as soon as possible.  \n\n## End to End Testing ([Dave Paquette][7] and [Lavanya Mohan][8])  \n\nAlthough Dave and Lavanya delivered two completely separate sessions related to testing, the content they delivered worked together in a very interesting way.   \n\nDave demonstrated and discussed [Playwright][4] and end-to-end testing framework that resolved or improved the problems we commonly see with end-to-end testing. Lavanya demonstrated how someone _should apply_ proper code management and development techniques when creating test code using a framework, like Playwright.  \n\nFor me, together they demonstrated why the test recorded features of end-to-end frameworks is not the \"best approach\" to creating tests, but rather it is only the first step.   \n\nI feel that these ideas will be seeping into both my day-job and side projects in the very near future. \n\n## Cloud Security ([Adam Krieger][9]) \n\nAdam closed the Prairie Dev Con season with his session, and managed to leave me with a lot of ideas and helped me identify gaps that I have been living with as a developer and as a solution architect. \n\nEnsuring that developers are security-aware is something I didn't realize I have been missing in my own skills, but also should be looking for in the implementation of my solution designs. \n\n## A Deal is a Deal ([Rod Paddock][10]) \n\nRod delivered a keynote in both Regina and Winnipeg, and each time I walked away with a positive outlook on my own professional and personal growth, but also with the reminder: A Deal Is A Deal.  \n\nSounds simple enough, but in the past I have frequently found myself regretting decisions or deals I had made with myself or others. But, a deal is a deal, and even if you don't like it or regret it, you need to take a moment to learn from it and ensure the next deal is one you won't regret. \n\n## TL;DR; / Conclusion \n\nIn short, there were a lot of good ideas at Prairie Dev Con 2022. These are the ones that stood out to me the most: \n\n- Consider API First Design with tools like Swagger.io and OpenAPI (JoÃ«l HÃ©bert)\n- The Developer Velocity Index (DVI) is NOT just for developers, but for anyone looking to deliver value (AJ Enns) \n- End-to-End Testing is a thing that requires effort, but has major benefits with the right tools and patterns in practice (Dave Paquette and Lavanya Mohan) \n- Cloud Developers need not should be security-aware and not just depend others (Adam Krieger) \n- A deal is a deal, and if you don't like it, learn from it so the next one is better (Rod Paddock) \n\nThanks for playing. \n\n~ DW ","categories":[],"tags":[{"name":"prairie-dev-con","slug":"prairie-dev-con","permalink":"https://westerndevs.com/tags/prairie-dev-con/"},{"name":"prdc-2022","slug":"prdc-2022","permalink":"https://westerndevs.com/tags/prdc-2022/"},{"name":"api-design","slug":"api-design","permalink":"https://westerndevs.com/tags/api-design/"},{"name":"open-api","slug":"open-api","permalink":"https://westerndevs.com/tags/open-api/"},{"name":"developer-velocity-index","slug":"developer-velocity-index","permalink":"https://westerndevs.com/tags/developer-velocity-index/"},{"name":"e2e-testing","slug":"e2e-testing","permalink":"https://westerndevs.com/tags/e2e-testing/"},{"name":"playwright","slug":"playwright","permalink":"https://westerndevs.com/tags/playwright/"},{"name":"adiad","slug":"adiad","permalink":"https://westerndevs.com/tags/adiad/"}]},{"title":"Open Graph Tools and Resources for Web Nerds (Like Me)","authorId":"david_wesst","slug":"open-graph-tools-and-resources-for-web-nerds","date":"2022-12-01 09:18:34+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/open-graph-tools-and-resources-for-web-nerds/","link":"","permalink":"https://westerndevs.com/_/open-graph-tools-and-resources-for-web-nerds/","excerpt":"A compilation of tools and resources I used to implemented the Open Graph Protocol (OGP) for my website to make posts and pages more engaging on LinkedIn and other social networks.","raw":"---\ntitle: Open Graph Tools and Resources for Web Nerds (Like Me)\ndate: 2022-12-01T04:18:34.509Z\ntags:\n  - open graph\n  - web development\n  - seo\n  - linkedin\n  - facebook\n  - twitter\noriginalurl: https://www.davidwesst.com/blog/open-graph-tools-and-resources-for-web-nerds\nauthorId: david_wesst\nexcerpt: A compilation of tools and resources I used to implemented the Open\n  Graph Protocol (OGP) for my website to make posts and pages more engaging on\n  LinkedIn and other social networks.\n---\n\n[1]: https://ogp.me\n[2]: https://github.com/davidwesst/website/releases\n[3]: https://chrome.google.com/webstore/detail/social-share-preview/ggnikicjfklimmffbkhknndafpdlabib\n[4]: https://addons.mozilla.org/en-US/firefox/addon/social-share-preview/\n[5]: https://www.linkedin.com/post-inspector/\n[6]: https://developers.facebook.com/tools/debug/\n\nThe [Open Graph Protocol (OGP)][1] is an open standard that allows web pages to have deeper integration with a social graph, such as Facebook, Twitter, or LinkedIn. You know those cards that appear on Twitter or LinkedIn with tailored images for a link to a blog post? That is OGP in action.\n\nWith [my recent adventures with reimplementing my website][2], I wanted to leverage this on pages and posts, specifically with LinkedIn and it took a little more research to get it working right. So, for the web nerds like me looking to implement OGP on their projects, I wanted to share the resources I found useful to hopefully save them some time in finding the right resources.\n\n## [`ogp.me`][1]\n\nI am calling this the specification, or \"spec\", and it probably the most important resource. The best part about this site is how approachable it is. \n\nThere are code snippets, explanations of all the object types and their properties, and its own list of tools (although they differ from the ones I am including on this list).\n\nIf you take one thing away from this post for your work with OGP, take this one.\n\n### Reference\n\n- [Open Graph protocol page][1]\n\n## LinkedIn (and Facebook) Post Inspectors\n\nBoth Facebook and LinkedIn provide a developer tool to analyze and verify your implemenation of OGP and has the added feature of busting whatever the social networks have cached for the pages you share. \n\nThese tools for triaging or assesing publically shared pages, but not so much when it comes to local development. That is where the next tool comes into play.\n\n### Reference\n\n- [LinkedIn Post Inspector][5]\n- [Facebook Sharing Debugger][6]\n\n## Social Share Preview Web Extension\n\nAvailable for both [Chromium Browsers][3] and [Firefox][4], this web extension allows you simulate what should appears for any page loaded up in your browser.\n\nThis tool saved me from having to continually publish the content to a public location for the post inspector, but note that it is just a _simulation_ of what the tool thinks it should appear. It does not replace post inspector or proper testing on the site you are looking to share to.\n\n![A window displaying a preview of what a LinkedIn post of the 'How much is enough documentation?' blog post on davidwesst.com](/images/2022-11-30-open-graph-tools-and-resources-for-web-nerds/social-share-preview-example.png)\n\n### Reference\n\n- [Chrome Extension][3]\n- [Firefox Add-On][4]\n\n## Browser Dev Tools (Obviously)\n\nIf you are reading the post, then this one is an obvious one-- but sometimes we (like me) get so caught up on exploring new ways to solve my problem, we forget about the obvious ones.\n\nOGP tags live in the `<head>` of your HTML page. If you are unsure why things are not working, make sure you run your browser dev tools of choice and check the `<head>` of the document and make sure the OGP tags you are expecting appear where they should be.\n\nIt seems simple, but depending on what tool, engine, or framework to output HTML, you may be surprised what shows up.\n\n### Reference\n\nOpen this post on a desktop browser and press the key combination `Ctrl + Shift + i` and you should see your browser dev tools pop open for the site.\n\n## Conclusion / TL;DR;\n\nRead the [aproachable spec document][1]. That is the most important part takeaway from my OGP implemenation. It is very approachable and gives you a strong foundation to work from as you use other tools to triage and assess your implementation.\n\nThese are the tools I used to implement LinkedIn support, along with my browser dev tools. \n\n- [LinkedIn Post Inspector][5]\n- [Facebook Sharing Debugger][6]\n- [Chrome Extension][3]\n- [Firefox Add-On][4]\n- `Ctrl + Shift + i` on your desktop browser\n\nThanks for playing.\n\n~ DW\n\n","categories":[],"tags":[{"name":"twitter","slug":"twitter","permalink":"https://westerndevs.com/tags/twitter/"},{"name":"open graph","slug":"open-graph","permalink":"https://westerndevs.com/tags/open-graph/"},{"name":"web development","slug":"web-development","permalink":"https://westerndevs.com/tags/web-development/"},{"name":"seo","slug":"seo","permalink":"https://westerndevs.com/tags/seo/"},{"name":"linkedin","slug":"linkedin","permalink":"https://westerndevs.com/tags/linkedin/"},{"name":"facebook","slug":"facebook","permalink":"https://westerndevs.com/tags/facebook/"}]},{"title":"Does GDPR Apply to Personal Websites?","authorId":"david_wesst","slug":"does-gdpr-apply-to-personal-websites","date":"2022-11-19 01:10:07+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/does-gdpr-apply-to-personal-websites/","link":"","permalink":"https://westerndevs.com/_/does-gdpr-apply-to-personal-websites/","excerpt":"While rebuilding my personal website in 2022, I wanted to know how or if GDPR applied to my little side project. My internet sleuthing did not bring up any clear and cut answers, but I put together some thoughts that might help others answer it for themselves.","raw":"---\ntitle: Does GDPR Apply to Personal Websites?\ndate: 2022-11-18T20:10:07.083Z\ntags:\n  - gdpr\n  - ldgp\n  - ccpa\n  - privacy\n  - cookies\n  - website\noriginalurl: https://www.davidwesst.com/blog/does-gdpr-apply-to-personal-websites\nauthorId: david_wesst\nexcerpt: While rebuilding my personal website in 2022, I wanted to know how or\n  if GDPR applied to my little side project. My internet sleuthing did not bring\n  up any clear and cut answers, but I put together some thoughts that might help\n  others answer it for themselves.\n---\n\n[1]: https://gdpr.eu/cookies/\n[2]: https://github.com/davidwesst/website/releases/tag/v10.0.1\n[3]: https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview\n[4]: https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375\n[5]: https://iapp.org/media/pdf/resource_center/Brazilian_General_Data_Protection_Law.pdf\n[6]: https://www.priv.gc.ca/en/privacy-topics/privacy-laws-in-canada/the-personal-information-protection-and-electronic-documents-act-pipeda\n[7]: https://gdpr.eu/eu-gdpr-personal-data/\n[8]: https://techstory.in/eu-declares-google-analytics-illegal-heres-why/\n[9]: https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-retention-privacy\n[10]: https://learn.microsoft.com/en-us/azure/azure-monitor/app/javascript?tabs=snippet#cookie-handling\n[11]: https://www.davidwesst.com/about\n[12]: https://gdpr.eu/compliance/\n[13]: https://gdpr.eu/\n\nA few weeks back, I [released v10.0.1 of my website][2]. I use a static site generator to generate all the pages and publish it out into the internet for all the world to read. In that release, I added [Application Insights][3] to provide me with some performance data, but also to get a bit of usage data (for those willing to share it).\n\nWhat I found odd was that all the links and articles I came across seemed to talk about things at a high-level (i.e. defining GDPR) or assumed I was working at a large scale (i.e. enterprise software), but nothing small projects like my personal website.\n\nStill, I managed to draw some of my own conclusions on how to handle GDPR for my personal website and wanted to document them somewhere.\n\n## DISCLAIMER: This is not legal advice\n\nI am not a lawyer, so this is just an opinion from a developer. As a rule of thumb, I avoid taking legal advice from random folks on the internet. If you take advice from this article, take that bit and keep it. \n\nI hope others (like you) use this post to draw your own conclusions or how you want to proceed with your own plan for handling GDPR.\n\nBut if you want _real_ advice. Get a lawyer and talk to them.\n\n## Short Answer: Yes\n\nYes, it does apply to your personal website **if** are tracking information about your users **and** you are developing your own website or application.\n\n### Developing Your Own Website or Application\n\nI mean developing as it coding it, publishing that code, and hosting it somewhere like Microsoft Azure or GitHub Pages. If you are publishing your own code, GDPR may apply to you.\n\nIf you are using a third party tool or platform, like Facebook or LinkedIn to host your blog posts-- you appear to be in the clear. When you use a third-party platform, the _platform_, not you, is responsible for GDPR compliance.\n\nEven if you think you are clear of GDPR responsibility, make sure that you trust your chosen platform to comply to GDPR and other regulatory bodies out there, as your site depends on it. \n\n### Tracking Information\n\nThe GDPR is all about protecting personal information and giving control back to people navigating the internet. GDPR is not the only set of laws in play, as [California][4], [Brazil][5], and [Canada][6] have their own versions of similar legislation, but many of these laws seem to have been inspired by GDPR and why I tend to focus on it.\n\nAt the personal website level, you need to consider whether or not you are collecting personal information from your users. This includes things like [IP addresses or cookie identifiers][7].\n\nIf you are NOT collecting information like that, you are good to go! Just remember that services like Google Analytics or Disqus Comments use personally identifiable information to operate, so if you have decided to include one of those services on your site then you need to think about GDPR compliance.\n\n## My Solution Highlights\n\nI concluded the GDPR-like laws apply to my personal website if I want to do any kind of usage tracking and understand how users are using my site.. This means it needs to be an opt-in policy that gives the user the option to do just that, _opt-in_.\n\n![A screenshot of the davidwesst.com blog page with a dialogue docked to the bottom with the statement: 'This site uses cookies to track usage in order to help improve the user experience. By clicking \"Accept\", you consent to our use of cookies.' along with gray 'Accept' and 'Decline' buttons, and a blue link with the text 'Privacy Statement'](/images/2022-11-18-does-gdpr-apply-to-personal-websites/my-gdpr-dialogue.png)\n\nThe dialogue above is the only real visual evidence on the site now. As simple as that looks, a lot of thought went into it prior to implementation. Rather than doing a complete code review, I figured I would share the highlights.\n\n### Understanding My Tools\n\nMy default would just be to include something like Google Analytics, and be done with it, but with [GA being made illegal in the EU][8] and more countries creating their own GDPR-like legislation, I thought I would stay away from it and try something different.\n\nI chose [Application Insights][3] and took the time to learn how [it handles data privacy and retention][9] and how the [JavaScript SDK uses cookies][10].\n\nRegardless of what you choose for your analytics or tracking tool, the important part is that you understand how the tools are GDPR compliant and how the tracking technology works.\n\n### Opt-In for Cookies\n\nYou've seen million of them already, but those cookie banners have purpose. The [GDPR website outlines the requirements][1] around using cookies, and many tools use them. The important thing is that _you_ know how your website works, along with all the dependencies _you choose_ to include.\n\nIn my case, the cookie banner enables cookies in Application Insights, which in turn enable usage data collection, only if they click \"Accept\". \n\n### Transparency\n\nThis last point is less technical, and more about design. I am designing with transparency in the front of my mind. I added a [privacy statement to my about page][11] to explain the \"why\" around using Application Insights, and will share more specifics and document them accordingly.\n\n## Conclusion / TL;DR;\n\nGDPR and the various GDPR-like laws definitely apply to you and your personal website or app project if you are building the code yourself, assuming you want to track information about your users.\n\nThe short story on this is that you need to draw your own conclusions and take responsibility for what you include in your website. If you are developing something to share outward into the world, you need to take the time to understand how the various tools you are included (such as Google Analytics or Application Insights) as well as the requirements for compliance. \n\n### GDPR Resources\n\nTwo resources I found useful in explaining GDPR requirements are provided on the site [GDPR.eu][13]. If you are looking for more information, I definitely suggest checking out these links:\n\n- [Cookies, the GDPR, and the ePrivacy Directive][1]\n- [Everything you need to know about GDPR compliance][12]\n\nThanks for playing.\n\n~ DW\n","categories":[],"tags":[{"name":"website","slug":"website","permalink":"https://westerndevs.com/tags/website/"},{"name":"gdpr","slug":"gdpr","permalink":"https://westerndevs.com/tags/gdpr/"},{"name":"ldgp","slug":"ldgp","permalink":"https://westerndevs.com/tags/ldgp/"},{"name":"ccpa","slug":"ccpa","permalink":"https://westerndevs.com/tags/ccpa/"},{"name":"privacy","slug":"privacy","permalink":"https://westerndevs.com/tags/privacy/"},{"name":"cookies","slug":"cookies","permalink":"https://westerndevs.com/tags/cookies/"}]},{"title":"Bulk Insert SQL Geometry on .NET Core","authorId":"simon_timms","slug":"bulk-insert-sql-geometry","date":"2022-11-17 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/bulk-insert-sql-geometry/","link":"","permalink":"https://westerndevs.com/_/bulk-insert-sql-geometry/","excerpt":"","raw":"---\ntitle:  Bulk Insert SQL Geometry on .NET Core \n# Gotchas\nauthorId: simon_timms\ndate: 2022-11-17\noriginalurl: https://blog.simontimms.com/2022/11/17/bulk_insert_sql_geometry\nmode: public\n---\n\n\n\nI have been updating an application from full framework to .NET 6 this week. One of the things this app does is bulk load data into SQL Server. Normally this works just fine but some of the data is geography data which requires a special package to be installed: `Microsoft.SqlServer.Types`. This package is owned by the SQL server team so, as you'd expect, it is ridiculously behind the times. Fortunately, they are working on updating it and it is now available for Netstandard 2.1 in a preview mode. \n\nThe steps I needed to take to update the app were: \n\n1. Install the preview package for `Microsoft.SqlServer.Types`\n2. Update the SQL client package from System.Data.SqlClient to Microsoft.Data.SqlClient\n\nAfter that the tests we had for inserting polygons worked just great. This has been a bit of a challenge over the years but I'm delighted that we're almost there. We just need a non-preview version of the types package and we should be good to go. \n\n## Gotchas\n\nWhen I'd only done step 1 I ran into errors like \n\n```\nSystem.InvalidOperationException : The given value of type SqlGeometry from the data source cannot be converted to type udt of the specified target column.\n---- System.ArgumentException : Specified type is not registered on the target server. Microsoft.SqlServer.Types.SqlGeometry, Microsoft.SqlServer.Types, Version=16.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91.\n```\n\nI went down a rabbit hole on that one before spotting a post from MVP Erik Jensen https://github.com/ErikEJ/EntityFramework6PowerTools/issues/103 which sent me in the right direction. ","categories":[],"tags":[]},{"title":"Removing Azure Backups in Terraform","authorId":"simon_timms","slug":"azure-backus-protip","date":"2022-11-11 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-backus-protip/","link":"","permalink":"https://westerndevs.com/_/azure-backus-protip/","excerpt":"","raw":"---\ntitle:  Removing Azure Backups in Terraform\nauthorId: simon_timms\ndate: 2022-11-11\noriginalurl: https://blog.simontimms.com/2022/11/11/azure-backus-protip\nmode: public\n---\n\n\n\nIf you have a VM backup in your Terraform state and need to get rid of it be aware that it is probably going to break your [deployment pipeline](https://blog.simontimms.com/2022/11/01/theory-of-terraform-github.-actions/). The reason is that Terraform will delete the item but then find that the resource still there. This is because backup deletion takes a while (say 14 days). Eventually the backup will delete but not before Terraform times out. \n\nThe solution I'm using is to just go in an manually delete the backup from the terraform state to unblock my pipelines. \n\n```bash\nterraform state list | grep <name of your backup>\n-- make note of the resource identifier --\nterraform state rm <found resource identifier>\n```\n\nEditing Terraform state seems scary but it's not too bad after you do it a bit. Take backups!","categories":[],"tags":[]},{"title":"How much is enough documentation?","authorId":"david_wesst","slug":"how-much-is-enough-documentation","date":"2022-11-10 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/how-much-is-enough-documentation/","link":"","permalink":"https://westerndevs.com/_/how-much-is-enough-documentation/","excerpt":"Documentation is important, but it takes a lot of time and if you are a solo developer, what documentation to you really need? Still, good docs can provide the context I forget after putting a project on the shelf, or explains how to fix something in older code I use, but haven't touched in a long time. So how much is 'enough' documentation and what types of documentation do I need to invest in give my future self the the best value for the effort I put in?","raw":"---\n\ntitle: \"How much is enough documentation?\"\ndate: \"2022-11-10T12:00:00-05:00\"\ntags:\n- madr\n- c4-model\n- decision\n- diagram\n- documentation\n- README.md\n- CONTRIBUTING.md \nexcerpt: \"Documentation is important, but it takes a lot of time and if you are a solo developer, what documentation to you really need? Still, good docs can provide the context I forget after putting a project on the shelf, or explains how to fix something in older code I use, but haven't touched in a long time. So how much is 'enough' documentation and what types of documentation do I need to invest in give my future self the the best value for the effort I put in?\"\nthumbnail: \"repo-with-docs.png\"\nthumnbail_alt: \"File tree of a source code repository with a 'docs' folder containing a sub-folder entitled 'decisions' with a series of markdown files documenting technical decisions for the project.\"\noriginalUrl: \"https://www.davidwesst.com/blog/how-much-is-enough-documentation/\"\nauthorId: david_wesst\n\n---\n\n[1]: https://github.com/davidwesst/website/blob/main/CHANGELOG.md\n[2]: https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/\n[3]: https://github.com/davidwesst/website/tree/main/docs/decisions\n[4]: https://adr.github.io\n[5]: https://github.com/davidwesst/website/blob/main/docs/decisions/0001-decisions-with-madr.md\n[6]: https://c4model.com\n[7]: https://github.com/davidwesst/website/blob/main/CHANGELOG.md\n[8]: https://keepachangelog.com/en/1.0.0/\n\nDocumentation takes a lot of different forms. Decision requests, diagrams, and just plain ol' word filled documents. Historically speaking, I have been guilty of being that developer that loathed documentation, waited until the last minute, and usually produced something that won't provide much help when it is actually needed.\n\nBeing a solution architect during the day, I wanted to apply some of my new found skills (and appreciation) for documentation while [working on v10 of my website][1]. Ultimately, documentation is _necessary_, even on personal projects. If I think back to my own experience with my own projects, they can end up sitting on the shelve for a long time. When go back to revisit it, other than analyzing my own code (on prototype stuff) can be a serious time sink to even get things in a running state without any decent documentation.\n\n## What do you _need_?\n\nAnd I do mean _needed_ not _wanted_. Everyone _wants_ documentation of all kinds, but what does an audience of one (i.e., your future self) _need_ to get the project back off the shelve and into working order?\n\nLike any good solution architect, I started to read, learn, and figure out what others consider \"enough documentation\" or \"good documentation\". I also spent time defining the problem I needed the documentation to solve, and landed on the following docs being \"enough\".\n\n### The README\n\nIt might seem obvious but, I have read enough of my own empty or default `README.md` files to know that this is easily the most important piece of documentation you write. Without it, the project will require code analysis to figure out what it _actually_ is, and that is never good.\n\nThere are a lot of great examples `README.md` files on GitHub to look at, but I would suggest you start simple if you're just getting off the ground. My take was to include system requirements and the steps to setup, build, and start the project for the developer.\n\nWhen searching for info on this, I really like [this article from Hillary Nyakundi][2] provided a great \"how-to\" on making a good `README.md`.\n\n### Decisions (also known as ADs or Architecture Decisions)\n\nThis is one I picked up from my day job being a Solution Architect in a large enterprise. Decisions you make along the way need to be documented, even if it is only for yourself.\n\nThe idea is to document decisions that will have a long term impact on your project. Decided to document decisions? That can be documented. Decided that you only want your project to run on Azure? That can be documented. Decided to design your solution around a specific pattern? That can be documented.\n\nYou can document as many or as few decisions as you want. In the case of my website project, [I documented a few core decisions early on][3] because I wanted to remember _why I built it this way_. Even though I am adding content regularly and tweaking features frequently enough, I could shelve the development at any point.\n\nIn terms of format, there are plenty of ways to document decisions and why it is important, but I am not going to spend time explaining that. Instead I would recommend reading [how GitHub documents decisions][4]. That is where I started, and they have a great breakdown of the different format and tools that can support you, if you're inclined to get into the tooling.\n\nFor the website, I [decided to use MADR as my decision document template][5] and documented \"why\" I chose it as the first decision for the project and [documented it][5].\n\n### Diagrams\n\nThe last bit of documentation I feel I _need_ (although it is not as important as the previous two) are solution diagrams.\n\nJust like decision documentation, this is something that can take a lot of different forms. Personally, I am not a huge fan of diving into UML or any of the traditional diagram styles. I like diagrams that present well to multiple audiences and explain _one thing_ well. \n\n![A solution diagram example from the website project](/images/2022-11-10-how-much-is-enough-documentation/website-solution-overview.png)\n\nThe above diagram is one I created to explain how I setup all the pieces inside of Microsoft Azure to host my website. The diagram answers the question \"What is necessary to host your application?\" which goes beyond the code in my case.\n\nThere is no real format that I applied here, but I scoped it to focus on the Azure Infrastructure and service I needed to rebuild the solution in Azure from scratch. Almost like a high-level guide to explain all the different pieces that need to be setup and handled.\n\nIn regards to diagram formatting, although I did not use it in this example, the [C4 model][6] is something I have been messing around with to describe systems and projects in my day job. If you need a little direction, or are struggling to figure out \"how to diagram\" your project, it might be worth a look.\n\n### Notable Mention: `CHANGELOG.md`\n\nI wanted to highlight this, but also point out that it is definitely not required. A `CHANGELOG.md` allows you to document your progress.\n\nI based [my CHANGELOG file][7] off of the format described at [keepachangelog.com][8]. It forced me to take a bit of time (really, like 15 minutes or so) to reflect on my effort and appreciate the effort I have put into the project. Plus, it tells the story of how the project has evolved over time; which, just like the decisions, provides context on how things got to where they are.\n\n## Conclusion / TL;DR;\n\nIn short, the documentation I _need_ (not _want_) consists of the following, with the following priority:\n\n1. `README.md` (that at least says how to setup, build, and run the project)\n2. Decisions (using [MADR][4] or some other format of your choosing)\n3. Solution Diagrams (that answer _one specific question_)\n4. `CHANGELOG.md` (not required, but provides more context and forces you to appreciate the effort you have put into your project)\n\nThanks for playing.\n\n~ DW\n\n","categories":[],"tags":[{"name":"madr","slug":"madr","permalink":"https://westerndevs.com/tags/madr/"},{"name":"c4-model","slug":"c4-model","permalink":"https://westerndevs.com/tags/c4-model/"},{"name":"decision","slug":"decision","permalink":"https://westerndevs.com/tags/decision/"},{"name":"diagram","slug":"diagram","permalink":"https://westerndevs.com/tags/diagram/"},{"name":"documentation","slug":"documentation","permalink":"https://westerndevs.com/tags/documentation/"},{"name":"README.md","slug":"README-md","permalink":"https://westerndevs.com/tags/README-md/"},{"name":"CONTRIBUTING.md","slug":"CONTRIBUTING-md","permalink":"https://westerndevs.com/tags/CONTRIBUTING-md/"}]},{"title":"How to fork (a repo) like a boss!","authorId":"david_wesst","slug":"how-to-fork-like-a-boss","date":"2022-11-02 21:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/how-to-fork-like-a-boss/","link":"","permalink":"https://westerndevs.com/_/how-to-fork-like-a-boss/","excerpt":"Everyone thinks about forking. It is a natural thing, yet how do get the job done and fork? I used to ask myself the same thing, until I learned these pro-tips and forked like a boss.","raw":"---\n\ntitle: \"How to fork (a repo) like a boss!\"\ndate: 2022-11-02T12:00-05:00\ntags:\n- github\n- pull requests\n- contribution\n- open source\nexcerpt: \"Everyone thinks about forking. It is a natural thing, yet how do get the job done and fork? I used to ask myself the same thing, until I learned these pro-tips and forked like a boss.\"\nauthorId: david_wesst\noriginalUrl: https://www.davidwesst.com/blog/how-to-fork-like-a-boss/\n\n---\n\n[1]:https://github.com/solution-loisir/markdown-it-eleventy-img\n[2]:https://github.com/solution-loisir/markdown-it-eleventy-img/pull/9\n[3]:https://github.com/solution-loisir/markdown-it-eleventy-img/issues/8\n[4]:https://github.blog/2012-09-17-contributing-guidelines/\n\nMy name is David Wesst and today I am going to teach you to fork like a boss! No special tools. No special creams. Just your clicking finger and a bit of confidence to use your coding skills.\n\nBefore I forked, I thought I forking wasn't for me. I thought, I am too old to fork, but man oh man was I wrong. But then came the day where a library I was using was missing a critical feature, and a quick search through the repository issues found [that others were looking for that feature too][3].\n\n![Screenshot of GitHub issue with title \"Question about whether the relative path is base on current working dir or current md file?\" with the first entry in the issue describing how their blog posts have images in the same directory as the post](/images/2022-11-02-how-to-fork-like-a-boss/gh-issue.jpg)\n\n## Choosing to Fork\n\nThis is the moment where I got to choose. I had a options for my next move:\n\n1. Add to the thread and hope for the best \n2. Built my own solution from scratch \n3. Take a look through the code and see if its forkable\n\nThe first choice makes sense if you don't have the knowledge or skills.\n\nThe second choice feels easier, but that is only your fear of contributing getting the best of you. When you add to your codebase, you are adding more code to support in the long run and all comes with that.\n\nThe last choice might make you nervous of you haven't forked in a long time, but I assure you, if you can code, you can fork. So browse through the code and see if you can find the spot your forking can help.\n\n## Defining Forkability\n\nThis is very subjective, but when it comes to forkable projects for me, I look for the following things, in this order:\n\n1. `CONTRIBUTING.md`, to give me a breakdown on how the community wants people to contribute\n2. Tests, so I know I can mess around with the code without breaking existing functionality\n3. Existing Issues and Pull Requests (PRs) to see what users, developers, and project owners are currently working on and their focus.\n\nIn my recent contribution to [markdown-it-eleventy-img][1], I went through the repo trying to figure out whether or not it was forkable. Although I didn't find a `CONTRIBUTING.md` ([but that could be a future PR][4]) but I found a set of tests, and even though I forgot in the moment, there was an existing issue from someone else about the same issue I was hoping to contribute!\n\nAnd with that, I knew this project was forkable. So I pulled out my finger and clicked \"FORK\" like boss and coded up my solution, and [submitted a PR][2].\n\n## Fork with Confidence and Respect\n\nIf you look through the [thread of the PR][2] you'll see that my solution went through a few iterations and changes after receiving feedback from the project owner.\n\nThis was a great conversation and it lead to a better solution implementation than my original submission, which made me exceptionally happy (and proud) of my contribution.\n\nEven though it is volunteer labour, remember that both you AND the project owner/admins are choosing to spend their time reviewing and analyzing your work. Everyone is involved in the fork is investing time, and everyone should be treated with respect and as a professional.\n\nPlusâ€”this is a great opportunity to level-up your development soft skills. Enjoy yourself, but be timely and respect the investment everyone is making.\n\n## Conclusion / TL;DR;\n\nTo fork like a boss, all you need is a project ready for contributions, some confidence, and respect for others on the project:\n\n1. A `CONTRIBUTING.md`\n2. Tests\n3. Existing Issues and Pull Requests (PRs)\n\nThanks for playing.\n\n~ DW\n\n","categories":[],"tags":[{"name":"github","slug":"github","permalink":"https://westerndevs.com/tags/github/"},{"name":"pull requests","slug":"pull-requests","permalink":"https://westerndevs.com/tags/pull-requests/"},{"name":"contribution","slug":"contribution","permalink":"https://westerndevs.com/tags/contribution/"},{"name":"open source","slug":"open-source","permalink":"https://westerndevs.com/tags/open-source/"}]},{"title":"Dealing with Set-Output Depreciation Warnings in Terraform github-actions","authorId":"simon_timms","slug":"set-output-terraform","date":"2022-11-01 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/set-output-terraform/","link":"","permalink":"https://westerndevs.com/_/set-output-terraform/","excerpt":"","raw":"---\ntitle:  Dealing with Set-Output Depreciation Warnings in Terraform github-actions\nauthorId: simon_timms\ndate: 2022-11-01\noriginalurl: https://blog.simontimms.com/2022/11/01/set-output-terraform\nmode: public\n---\n\n\n\nI've got a build that is running terraform on github actions (I actually have loads of them) and I've been noticing that they are very chatty about warnings now. \n\n![](/images/2022-11-01-set-output-terraform.md/2022-11-01-06-46-18.png))\n\nThe warning is \n\n```\nThe `set-output` command is deprecated and will be disabled soon. Please upgrade to using Environment Files. For more information see: https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/\n```\n\nThe history here without reading that link is basically that github are changing how we push variables to the pipeline for use in later steps. There were some security implications with the old approach and the new approach should be better\n\n```yml\n- name: Save variable\n  run: echo \"SOMENAME=PICKLE\" >> $GITHUB_STATE\n\n- name: Set output\n  run: echo \"SOMENAME=PICKLE\" >> $GITHUB_OUTPUT\n  ```\n\n  Problem was that the steps on which I was having trouble didn't obviously use the `set-output` command. \n  \n  ```yml\n ...\n- name: Init Terraform\n  run: terraform init \n- name: Validate Terraform\n  run: terraform validate\n...        \n```\n  \n  I had to dig a bit to find out that it was actually the `terraform` command that was causing the problem. See as part of the build I install the terraform cli using the \n\n  ```yml\n  - name: HashiCorp - Setup Terraform\n    uses: hashicorp/setup-terraform@v2.0.2\n    with:\n        terraform_version: 1.1.9\n        terraform_wrapper: true\n```\n\nTurns out that as of writing the latest version of the wrapper installed by the setup-terraform task makes use of an older version of the `@actions/core` package. This package is what is used to set the output and before version 1.10 it did so using `set-output`. A fix has been merged into the setup-terraform project but no update released yet. \n\nFor now I found that I had no need for the wrapper so I disabled it with \n\n```yml\n  - name: HashiCorp - Setup Terraform\n    uses: hashicorp/setup-terraform@v2.0.2\n    with:\n        terraform_version: 1.1.9\n        terraform_wrapper: false\n```\n\nbut for future readers if there is a more recent version of setup-terraform than 2.0.2 then you can update to that to remove the warnings. Now my build is clean \n\n![](/images/2022-11-01-set-output-terraform.md/2022-11-01-06-57-27.png))\n","categories":[],"tags":[]},{"title":"My Theory of GitHub Actions and IaC","authorId":"simon_timms","slug":"theory-of-terraform-github-actions","date":"2022-11-01 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/theory-of-terraform-github-actions/","link":"","permalink":"https://westerndevs.com/_/theory-of-terraform-github-actions/","excerpt":"","raw":"---\ntitle:  My Theory of GitHub Actions and IaC\n# Source Code\n# Builds\n# Tags \n# Permissions\n# Achieving our Goals \n# Future Steps \nauthorId: simon_timms\ndate: 2022-11-01\noriginalurl: https://blog.simontimms.com/2022/11/01/theory-of-terraform-github.-actions\nmode: public\n---\n\n\n\nI do all sorts of random work and one of those is helping out on some infrastructure deployments on Azure. Coming from a development background I'm allergic to clicking around inside an Azure website to configure things in a totally non-repeatable way. So I've been using Terraform to do the deployments. We have built up a pretty good history of using Terraform  - today I might use Pulumi instead but the actual tool isn't all that important as opposed to the theory. \n\nWhat I'm looking to achieve is a number of things \n\n1. Make deployments easy for people to do\n2. Make deployments repeatable - we should be able to us the same deployment scripts to set up a dev enviornment or recover from a disaster with minimal effort\n3. Ensure that changes are reviewed before they are applied\n\nTo meet these requirements a build pipeline in GitHub actions (or Azure DevOps, for that matter) is an ideal fit. We maintain our Terraform scripts in a repository. Typically we use one repository per resource group but your needs may vary around that. There isn't any monetary cost to having multiple repositories but there can be some cognitive load to remembering where the right repository is (more on that later).\n\n## Source Code\n\nChanges to the infrastructure definition code are checked into a shared repository. Membership over this code is fairly relaxed. Developers and ops people can all make changes to the code. We strive to make use of normal code review approaches when checking in changes. We're not super rigorous about changes which are checked in because many of the people checking in changes have ops backgrounds and aren't all that well versed in the PR process. I want to make this as easy for them as possible so they aren't tempted to try to make changes directly in Azure. \n\nIn my experience there is a very strong temptation for people to abandon rigour when a change is needed at once to address a business need. We need to change a firewall rule - no time to review that let's just do it. I'm not saying that this is a good thing but it is a reality. Driving people to the Terraform needs to be easy. Having their ad-hoc changes overwritten by a Terraform deploy will also help drive the point home. Stick and carrot.\n\n## Builds\n\nA typical build pipeline for us will include 3 stages. \n\n![](/images/2022-11-01-theory-of-terraform-github.-actions.md/2022-11-01-07-22-50.png))\n\nThe build step runs on a checkin trigger. This will run an initial build step which validates the terraform scripts are syntactically correct and well linted. A small number of our builds stop here. Unlike application deployments we typically want these changes to be live right away or at most during some maintenance window shortly after the changes have been authored. That deployments are run close to the time the changes were authored helps with our lack of rigour around code reviews.\n\nThe next stage is to preview what changes will be performed by Terraform. This stage is gated such that it need somebody to actually approve it. It is low risk because no changes to made - we run a `terraform plan` and see what changes will be made. Reading over these changes is very helpful because we often catch unintended consequences here. Accidentally destroying and recreating a VM instead of renaming it? Caught here. Removing a tag that somebody manually applied to a resource and that should be preserved? Caught here. \n\n![](/images/2022-11-01-theory-of-terraform-github.-actions.md/2022-11-01-07-45-43.png))\n\nThe final stage in the pipeline is to run the Terraform changes. This step is also gated to prevent us from deploying it without proper approvals. Depending on the environment we might need 2 approvals or at least one approval that isn't the person writing the change. More eyes on a change will catch problems more easily and also socialize changes so that it isn't a huge shock to the entire ops team that we now have a MySQL server in the enviornment or whatever it may be. \n\n## Tags \n\nMost Azure resources support tagging. These are basically just labels that you can apply to resources. We use tags to help us organize our resources. We have a tag called `environment` which is used to indicate what environment the resource is in. We have a tag called `owner` which is used to indicate who owns the resource. We have a tag called `project` which is used to indicate what project the resource is associated with. But for these builds the most important tags are `IaC Technology` and `IaC Source`. The first is used to tell people that the resources are part of a Terraform deployment. The second is used to indicate where on GitHub the Terraform scripts are located. These tags make it really easy for people to find the Terraform scripts for a resource and get a change in place.\n\n![](/images/2022-11-01-theory-of-terraform-github.-actions.md/2022-11-01-07-33-43.png))\n\n## Permissions\n\nI mentioned earlier that we like to guide ops people to make enviornment changes in Terraform rather than directly in Azure. One of the approaches we take around that is to not grant owner or writer permissions to resources directly to people be they ops or dev. Instead we have a number of permission restricted service principals that are used to make changes to resources. These service principals are granted permissions to specific resource groups and these service principals are what's used in the pipeline to make the changes. This means that if somebody wants to make a change to a resource they need to go through the Terraform pipeline.\n\nWe keep the client id and secret in the secrets of the github pipeline \n\n![](/images/2022-11-01-theory-of-terraform-github.-actions.md/2022-11-01-07-42-51.png))\n\nIn this example we just keep a single repository wide key because we only have one enviornment. We'd make use of enviornment specific secrets if we had more than one environment. \n\nThis approach has the added bonus of providing rip stops in the event that we leak some keys somewhere. At worst that service principal has access only to one resource group so an attacker is limited to being able to mess with that group and not escape to the larger enviornment. \n\n## Achieving our Goals \n\nTo my mind this approach is exactly how IaC was meant to be used. We have a single source of truth for our infrastructure. We can make changes to that infrastructure in a repeatable way. We can review those changes before they are applied. All this while keeping the barrier to entry low for people who are not familiar with the code review process.\n\n## Future Steps \n\nWe already make use of Terraform modules for most of our deployment but we're not doing a great job of reusing these modules from project to project. We're hoping to keep a library of these modules around which can help up standardize things. For instance our VM module doesn't just provision a VM - it sets up backups and uses a standardized source image. \n\nI also really like the idea of using the build pipeline to annotate pull requests with the Terraform changes using https://github.com/marketplace/actions/terraform-pr-commenter. Surfacing this directly on the PR would save the reviewers the trouble of going through the pipeline to see what changes are being made. However it would be added friction for our ops team as they'd have to set up PRs. ","categories":[],"tags":[]},{"title":"Registering Terraform Providers","authorId":"simon_timms","slug":"register-providers","date":"2022-08-12 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/register-providers/","link":"","permalink":"https://westerndevs.com/_/register-providers/","excerpt":"","raw":"---\ntitle:  Registering Terraform Providers\nauthorId: simon_timms\ndate: 2022-08-12\noriginalurl: https://blog.simontimms.com/2022/08/12/register_providers\nmode: public\n---\n\n\n\nIf you're setting up a new Terraform project on Azure you might find yourself needing to register providers if you're running with an identity that doesn't have wide ranging access to the subscription. I ran into this today with the error \n\n```\n\nâ”‚ Error: Error ensuring Resource Providers are registered.\nâ”‚ \nâ”‚ Terraform automatically attempts to register the Resource Providers it supports to\nâ”‚ ensure it's able to provision resources.\nâ”‚ \nâ”‚ If you don't have permission to register Resource Providers you may wish to use the\nâ”‚ \"skip_provider_registration\" flag in the Provider block to disable this functionality.\nâ”‚ \nâ”‚ Please note that if you opt out of Resource Provider Registration and Terraform tries\nâ”‚ to provision a resource from a Resource Provider which is unregistered, then the errors\nâ”‚ may appear misleading - for example:\nâ”‚ \nâ”‚ > API version 2019-XX-XX was not found for Microsoft.Foo\nâ”‚ \nâ”‚ Could indicate either that the Resource Provider \"Microsoft.Foo\" requires registration,\nâ”‚ but this could also indicate that this Azure Region doesn't support this API version.\nâ”‚ \nâ”‚ More information on the \"skip_provider_registration\" flag can be found here:\nâ”‚ https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs#skip_provider_registration\nâ”‚ \nâ”‚ Original Error: Cannnot register providers: Microsoft.StoragePool. Errors were: Cannot register provider Microsoft.StoragePool with Azure Resource Manager: resources.ProvidersClient#Register: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The client '************' with object id ''************'' does not have authorization to perform action 'Microsoft.StoragePool/register/action' over scope '/subscriptions/***' or the scope is invalid. If access was recently granted, please refresh your credentials.\".\nâ”‚ \nâ”‚   with provider[\"registry.terraform.io/hashicorp/azurerm\"],\nâ”‚   on environment.tf line 21, in provider \"azurerm\":\nâ”‚   21: provider \"azurerm\" {\n```\n\nThe account running terraform in my github actions pipeline is restricted to only have contributor over the resource group into which I'm deploying so it's unable to properly set up providers. Two things needed to fix it: \n\n  1. Tell terraform to not try to register providers \n  2. Register the providers manually \n\n  For 1 the provider block in the terraform file needs to be updated to look like \n\n```\n  provider \"azurerm\" {\n    features {\n    }\n    skip_provider_registration = true\n}\n```\n\nFor 2 it requires logging into the azure portal and registering the providers manually. Go to the subscription and select `Resource Providers` then search for the one you need, select it and hit `Register`. In my case the provider was already registered and the problem was just Terraform's attempt to register it without sufficient permission. \n\n![](/images/2022-08-11-register_providers.md/2022-08-11-07-07-29.png))\n\n\n```","categories":[],"tags":[]},{"title":"Debugging Azure Container Instance Startup","authorId":"simon_timms","slug":"debug-azure-container-instances-startup","date":"2022-08-04 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/debug-azure-container-instances-startup/","link":"","permalink":"https://westerndevs.com/_/debug-azure-container-instances-startup/","excerpt":"","raw":"---\ntitle:  Debugging Azure Container Instance Startup \nauthorId: simon_timms\ndate: 2022-08-04\noriginalurl: https://blog.simontimms.com/2022/08/04/debug_azure_container_instances_startup\nmode: public\n---\n\n\n\nI have some container instances which are failing to start up properly and the logs in the portal are blank. This makes debugging them kind of difficult. \n\nOn the command line running \n\n```bash\naz container logs -g <resource group name> -n <container group name> --container <container name>\n```\n\nJust gave me an output of `None`. Not useful either.\n\nFortunately, you can attach directly to the logs streams coming out of the container which will give you a better idea of what is going on.\n\n```bash\naz container attach -g <resource group name> -n <container group name> --container <container name>\n```\n\nThis was able to give me output like \n\n```\nStart streaming logs:\n/usr/local/lib/python3.9/site-packages/environ/environ.py:628: UserWarning: /ric-api/core/.env doesn't exist - if you're not configuring your environment separately, create one.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/site-packages/environ/environ.py\", line 273, in get_value\n    value = self.ENVIRON[var]\n  File \"/usr/local/lib/python3.9/os.py\", line 679, in __getitem__\n    raise KeyError(key) from None\nKeyError: 'DB_PORT'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/ric-api/manage.py\", line 22, in <module>\n    main()\n  File \"/ric-api/manage.py\", line 18, in main\n    execute_from_command_line(sys.argv)\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py\", line 419, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/__init__.py\", line 413, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/base.py\", line 354, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/commands/runserver.py\", line 61, in execute\n    super().execute(*args, **options)\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/base.py\", line 398, in execute\n    output = self.handle(*args, **options)\n  File \"/usr/local/lib/python3.9/site-packages/django/core/management/commands/runserver.py\", line 68, in handle\n    if not settings.DEBUG and not settings.ALLOWED_HOSTS:\n  File \"/usr/local/lib/python3.9/site-packages/django/conf/__init__.py\", line 82, in __getattr__\n    self._setup(name)\n  File \"/usr/local/lib/python3.9/site-packages/django/conf/__init__.py\", line 69, in _setup\n    self._wrapped = Settings(settings_module)\n  File \"/usr/local/lib/python3.9/site-packages/django/conf/__init__.py\", line 170, in __init__\n    mod = importlib.import_module(self.SETTINGS_MODULE)\n  File \"/usr/local/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n  File \"/ric-api/core/settings.py\", line 114, in <module>\n    'PORT': env('DB_PORT'),\n  File \"/usr/local/lib/python3.9/site-packages/environ/environ.py\", line 123, in __call__\n    return self.get_value(var, cast=cast, default=default, parse_default=parse_default)\n  File \"/usr/local/lib/python3.9/site-packages/environ/environ.py\", line 277, in get_value\n    raise ImproperlyConfigured(error_msg)\ndjango.core.exceptions.ImproperlyConfigured: Set the DB_PORT environment variable\n2022-07-14T14:37:17.6003172Z stderr F\n\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"threading.py\", line 932, in _bootstrap_inner\n  File \"threading.py\", line 870, in run\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure/cli/command_modules/container/custom.py\", line 837, in _stream_container_events_and_logs\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure/cli/command_modules/container/custom.py\", line 791, in _stream_logs\nAttributeError: 'NoneType' object has no attribute 'split'\n```\n\nLooks like I missed adding a `DB_PORT` to the environmental variables ","categories":[],"tags":[]},{"title":"Consuming SOAP Services in .NET Core","authorId":"simon_timms","slug":"consuming-wsdl","date":"2022-05-30 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/consuming-wsdl/","link":"","permalink":"https://westerndevs.com/_/consuming-wsdl/","excerpt":"","raw":"---\ntitle:  Consuming SOAP Services in .NET Core\n# Core WCF\nauthorId: simon_timms\ndate: 2022-05-30\noriginalurl: https://blog.simontimms.com/2022/05/30/consuming-wsdl\nmode: public\n---\n\n\n\nToday I ran into the need to consume an older SOAP web service in .NET Core. I was really fortunate in my timing because Core WCF was just released and it makes the whole process much easier. \n\nTaking a step back for you youngsters out there SOAP was the service communication technology that existed before REST showed up with its JSON and ate everybody's lunch. SOAP is really just the name for the transport mechanism but I think most of us would refer to the whole method of invoking remote procedures over the web as `SOAP Web Services`. SOAP, or Simple Object Access Protocol, is an XML-based standard for serializing objects from various different languages in a way that Java could talk to .NET could talk to Python. Unlike JSON it was a pretty well thought out protocol and had standard representations of things like dates which JSON just kind of ignores. \n\nWeb services were closer to a remote method invocation in that you would call something like `GetUserId` rather than the RESTful approach of hitting an endpoint like `/Users/7` to get a user with Id 7. The endpoints which were provided by a Web Service were usually written down in a big long XML document called a WSDL which stands for Web Service Definition Language. \n\nWeb services gained a reputation for being very enterprisy and complex. There were a large number of additional standards defined around it which are commonly known as ws-*. These include such things as WS-Discovery, WS-Security, WS-Policy and, my personal favorite, the memorably named Web Single Sign-On Metadata Exchange Protocol. \n\n## Core WCF\n\nIn the last month we've seen the 1.0 release of [Core WCF]( https://devblogs.microsoft.com/dotnet/corewcf-v1-released/) which I'm pretty certain I mocked at being a silly thing in which to invest resources. Tables have turned now I'm the one who needs it so thank to [Scott Hunter](https://twitter.com/coolcsh) or whoever it was that allocated resources to developing this. \n\nTo get started I needed to find the WSDLs for the services I wanted. This required a call to the support department of the company providing the services. The had a .NET library they pointed me to but it was compiled against .NET 4.5 so I wanted to refresh it. Fortunately the Core WCF release includes an updated `svcutil`. This tool will read a WSDL and generate service stubs in .NET for you. \n\nI started with a new console project \n\n```bash\ndotnet new console\n```\n\nThen installed the dotnet-svcutil tool globally (you only need to do this once) and generated a service reference\n\n```bash\ndotnet tool install --global dotnet-svcutil\ndotnet-svcutil --roll-forward LatestMajor https://energydataservices.ihsenergy.com/services/v2/searchservice.svc\n```\n\nThis updated my project's csproj file to include a whole whack of new library references \n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project Sdk=\"Microsoft.NET.Sdk\">\n  <PropertyGroup>\n    <OutputType>Exe</OutputType>\n    <TargetFramework>net6.0</TargetFramework>\n    <RootNamespace>wsdl_test</RootNamespace>\n    <ImplicitUsings>enable</ImplicitUsings>\n    <Nullable>enable</Nullable>\n  </PropertyGroup>\n  <ItemGroup>\n    <DotNetCliToolReference Include=\"dotnet-svcutil\" Version=\"1.0.*\" />\n  </ItemGroup>\n  <ItemGroup>\n    <PackageReference Include=\"CoreWCF.Http\" Version=\"1.0.0\" />\n    <PackageReference Include=\"CoreWCF.Primitives\" Version=\"1.0.0\" />\n    <Reference Include=\"System.ServiceModel\">\n      <HintPath>System.ServiceModel</HintPath>\n    </Reference>\n  </ItemGroup>\n  <ItemGroup>\n    <PackageReference Include=\"System.ServiceModel.Duplex\" Version=\"4.8.*\" />\n    <PackageReference Include=\"System.ServiceModel.Http\" Version=\"4.8.*\" />\n    <PackageReference Include=\"System.ServiceModel.NetTcp\" Version=\"4.8.*\" />\n    <PackageReference Include=\"System.ServiceModel.Security\" Version=\"4.8.*\" />\n  </ItemGroup>\n</Project>\n```\n\nIt also generated a 13 000 line long service reference file in the project. Wowzers. I'm glad I don't have to write that fellow myself. \n\nWith that all generated I'm now able to call methods in that service by just doing \n\n```csharp\nusing ServiceReference;\nvar client = new SearchServiceClient();\nvar result = await client.SomeMethodAsync();\n```\n\nThis example really only scratches the surface of what the new Core WCF brings to .NET Core. I certainly wouldn't want to develop new WCF services but for consuming existing ones or even updating existing ones then this library is going to be a great boost to productivity. ","categories":[],"tags":[]},{"title":"Azure Functions Provider Error","authorId":"simon_timms","slug":"azure-functions-error","date":"2022-04-07 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-functions-error/","link":"","permalink":"https://westerndevs.com/_/azure-functions-error/","excerpt":"","raw":"---\ntitle:  Azure Functions Provider Error\nauthorId: simon_timms\ndate: 2022-04-07\noriginalurl: https://blog.simontimms.com/2022/04/07/azure-functions-error\nmode: public\n---\n\n\n\nI started up a previously working Azure functions project today that I hadn't touched in a week. It failed to start with an error like this\n\n```\nA host error has occurred during startup operation 'b59ba8b8-f264-4274-a9eb-e17ba0e02ed8'.\napi: Could not load file or assembly 'Microsoft.Extensions.Options, Version=6.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60'. The system cannot find the file specified.\nValue cannot be null. (Parameter 'provider')\n```\n\nThis is the sort of error that terrifies me. Something is wrong but who knows what. No changes in git and an infinity of generic errors on google for `Could not load file or assembly`. Eventually after some digging it seems like I might be suffering from some corrupted tooling (some hints about that here: https://github.com/Azure/azure-functions-core-tools/issues/2232). I was able to fix mine by downloading the latest version of the tooling from https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=v4%2Cwindows%2Ccsharp%2Cportal%2Cbash","categories":[],"tags":[]},{"title":"Which SQL Hosting Option is Right for Me?","authorId":"simon_timms","slug":"which-sql-server-edition","date":"2022-04-07 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/which-sql-server-edition/","link":"","permalink":"https://westerndevs.com/_/which-sql-server-edition/","excerpt":"","raw":"---\ntitle:  Which SQL Hosting Option is Right for Me?\nauthorId: simon_timms\ndate: 2022-04-07\noriginalurl: https://blog.simontimms.com/2022/04/07/which_sql_server_edition\nmode: public\n---\n\n\n\nThere are a bunch of different ways to host SQL Server workloads on Azure. Answering some questions about how you use SQL server can help guide us to picking the right option for you.\n\nThe 3 options for hosting we're considering are \n\n1. SQL Azure - https://azure.microsoft.com/en-us/products/azure-sql/database/#overview\n2. Azure SQL Managed Instance - https://azure.microsoft.com/en-us/products/azure-sql/managed-instance/\n3. SQL Server on VM - https://azure.microsoft.com/en-us/services/virtual-machines/sql-server/#overview\n\nI've listed these in my order of preference. I'd rather push people to a more managed solution than a less managed one. There is a huge shortage of SQL server skills out there so if you can take a more managed approach then you're less likely to run into problems that require you finding an SQL expert. I frequently say to companies that they're not in the business of managing SQL server but in the business of building whatever widgets they build. Unless there is a real need don't waste company resources building custom solutions when you can buy a 90% solution off the shelf. \n\nWhen I talk with companies about migrating their existing workloads to the cloud from on premise SQL servers I find myself asking these questions:\n\n1. Does your solution use cross database joins?\n2. Does your solution make use of the SQL Agent to run jobs?\n3. Does your solution use FILESTREAM to access files on disk? \n4. Does your solution require fine tuning of availability groups?\n5. Does your solution require SQL logins from `CERTIFICATE`, `ASYMMETRIC KEY` or `SID`?\n6. Do you need to make use of a compatibility level below 100?\n7. Do you need to make use of database mirroring?\n8. Does your solution need to start and stop job scheduling?\n9. Are you making use of SQL Server Reporting Services (SSRS)?\n10. Are you using xp_cmdshell anywhere in your application (https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/xp-cmdshell-transact-sql?view=sql-server-ver15)\n\nIf the answer to any of the first 3 questions is `yes` then they can't easily use SQL Azure* and should set the baseline to a managed instance. If the answer to any of the rest of the questions is `yes` then they should set the baseline to a VM running a full on version of SQL Server. Only if the answer to all these questions is `no` is SQL Azure the best solution. \n\n* Cross database joins and SQL Agent can be replaced by Elastic Query and Elastic Jobs but neither one is an easy drop in replacement so I typically don't bother directing people to them for time constrained migrations. ","categories":[],"tags":[]},{"title":"Redis Cheat Sheet","authorId":"simon_timms","slug":"cheat-sheet","date":"2022-04-04 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/cheat-sheet/","link":"","permalink":"https://westerndevs.com/_/cheat-sheet/","excerpt":"","raw":"---\ntitle:  Redis Cheat Sheet\n# Running in Docker \n# Connection\n# Querying \nauthorId: simon_timms\ndate: 2022-04-04\noriginalurl: https://blog.simontimms.com/2022/04/04/cheat-sheet\nmode: public \n---\n\n\n\n## Running in Docker \n\nQuickly get started with \n\n```\ndocker run --name somename -p 6379:6379 redis\n```\n\n## Connection\n\nThe simplest connection string is to use `localhost` which just connects to the localhost on port 6379. \n\n## Querying \n\nRedis is a really simple server to which you can just telnet (or use the redis-cli) to run commands. \n\n**List All Keys**\n\nThis might not be a great idea against a prod server with lots of keys\n\n```\nkeys *\n```\n\n**Get key type**\n\nRedis supports a bunch of different data primatives like a simple key value, a list, a hash, a zset, ... to find the type of a key use `type`\n\n```\ntype HereForElizabethAnneSlovak\n+zset\n```\n\n**Set key value**\n```\nset blah 7\n```\nThis works for updates too\n\n**Get key value**\n\n```\nget blah\n```\n\n**Get everything in a zset**\n\n```\nZRANGE \"id\" 0 -1\n```\n\n**Count everything in a zset**\n```\nzcount HereForjohnrufuscolginjr. -inf +inf\n```\n\n**Get the first thing in a zset**\n\n```\nZRANGE \"id\" 0 0\n```\n\n**Get everything in a set**\n\n```\nSMEMBERS HereFeedHashTags\n```\n\n**Get the first member of a list**\n\n```\nLPOP somekey\n```\n\n**Get the last member of a list**\n\n```\nRPOP somekey\n```\n\n**Get the contents of a hash** \n\n```\nHGETALL somekey\n```\n\n**Clear out the whole database**\n\n```\nFLUSHALL\n```\n\n**Clear just the current db**\n\n```\nFLUSHDB\n```\n\n**Stats on keys**\n\n```\nINFO keyspace\n```\n\n**Get an idea of db size**\n\n```\nINFO Memory\n```","categories":[],"tags":[]},{"title":"Fixing VS Code Rubocop Issues","authorId":"simon_timms","slug":"rubocop-issue","date":"2022-03-18 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/rubocop-issue/","link":"","permalink":"https://westerndevs.com/_/rubocop-issue/","excerpt":"","raw":"---\ntitle:  Fixing VS Code Rubocop Issues \nauthorId: simon_timms\ndate: 2022-03-18\noriginalurl: https://blog.simontimms.com/2022/03/18/rubocop-issue\nmode: public\n---\n\n\n\nI run this rubocop extension in my VS Code\n\n![rubocop extension](/images/2022-03-18-rubocop-issue.md/2022-03-18-10-15-48.png))\n\nRecently the project I was on did a Ruby update and my rubocop stopped working with an error like this\n\n![Rubocop error](/images/2022-03-18-rubocop-issue.md/2022-03-18-10-16-43.png))\n\nThe issue here was that the rubocop in the project was newer than the globally installed rubocop so it was returning empty output. This extension doesn't look like it uses `rbenv` properly so I needed to globally update rubocop which I did with \n\n```bash\n/usr/local/bin/rubocop -v  -> 1.22.3\nsudo gem install rubocop\n/usr/local/bin/rubocop -v  -> 1.26\n```\n\nI still had some errors about missing rules and needed to also do \n\n```bash\nsudo gem install rubocop-rake\nsudo gem install rubocop-rails\nsudo gem install rubocop-performance\n```\n\nIdeally I'd like this extension to use the rbenv version of ruby but this gets me sorted for now. ","categories":[],"tags":[]},{"title":"Unsupported Architecture for fsevents with Oryx","authorId":"simon_timms","slug":"fsevents-optional-dependencies","date":"2022-03-16 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/fsevents-optional-dependencies/","link":"","permalink":"https://westerndevs.com/_/fsevents-optional-dependencies/","excerpt":"","raw":"---\ntitle:  Unsupported Architecture for fsevents with Oryx\nauthorId: simon_timms\ndate: 2022-03-16\noriginalurl: https://blog.simontimms.com/2022/03/16/fsevents-optional-dependencies\nmode: public\n---\n\n\n\nI updated the version of the lock file on a project the other day in the hopes it might restore a little bit more quickly. However for some steps in my build an older version of NPM was being used. This older version didn't have support for the new lock file version and while it is supposed to be compatible it seemed like optional dependencies like fsevents were causing a legit issue \n\n```\nnpm WARN read-shrinkwrap This version of npm is compatible with lockfileVersion@1, but package-lock.json was generated for lockfileVersion@2. I'll try to do my best with it!\nnpm ERR! code EBADPLATFORM\nnpm ERR! notsup Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\nnpm ERR! notsup Valid OS:    darwin\nnpm ERR! notsup Valid Arch:  any\nnpm ERR! notsup Actual OS:   linux\nnpm ERR! notsup Actual Arch: x64\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /root/.npm/_logs/2022-03-09T13_58_42_006Z-debug.log\n```\n\nIn theory these fsevent things should be warnings because they are optional dependencies. Updating the version of node used by Oryx, the build engine for Azure Static Web Apps, listens to the version of node defined in `package.config`. Adding this section to the `package.config` fixed everything\n\n```\n\"engines\": {\n    \"node\": \">=16.0\",\n    \"npm\": \">=8.0\"\n  },\n```","categories":[],"tags":[]},{"title":"Exclude node_modules from Backblaze Backups on Windows","authorId":"simon_timms","slug":"backblaze-exclude-node-modules","date":"2022-02-20 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/backblaze-exclude-node-modules/","link":"","permalink":"https://westerndevs.com/_/backblaze-exclude-node-modules/","excerpt":"","raw":"---\ntitle:  Exclude node_modules from Backblaze Backups on Windows\nauthorId: simon_timms\ndate: 2022-02-20\noriginalurl: https://blog.simontimms.com/2022/02/20/backblaze-exclude-node-modules\nmode: public\n---\n\n\n\nThere are [some](https://chrisblackwell.me/ignore-node_modules-and-git-folders-in-backblaze/) [articles](https://gist.github.com/nickcernis/bb4bd43a44efd73b87d857e29b1d5b96) out there about how to exclude `node_modules` from Backblaze backups on OSX but I couldn't find anything about windows. \n\nWhat you want to do is open up `C:\\ProgramData\\Backblaze\\bzdata\\bzexcluderules_editable.xml` and add a new rule.\n\n```xml\n<excludefname_rule plat=\"win\" osVers=\"*\"  ruleIsOptional=\"t\" skipFirstCharThenStartsWith=\"*\" contains_1=\"node_modules\" contains_2=\"*\" doesNotContain=\"*\" endsWith=\"*\" hasFileExtension=\"*\" />\n```\n\nIf you want to exclude .git folders too (they can also contain a lot of small files that are slow to backup) also add \n```xml\n<excludefname_rule plat=\"win\" osVers=\"*\"  ruleIsOptional=\"t\" skipFirstCharThenStartsWith=\"*\" contains_1=\".git\" contains_2=\"*\" doesNotContain=\"*\" endsWith=\"*\" hasFileExtension=\"*\" />\n```","categories":[],"tags":[]},{"title":"Parsing Vue Router Path Parameters","authorId":"simon_timms","slug":"parsing-vue-router-parameters","date":"2022-02-18 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/parsing-vue-router-parameters/","link":"","permalink":"https://westerndevs.com/_/parsing-vue-router-parameters/","excerpt":"","raw":"---\ntitle:  Parsing Vue Router Path Parameters\nauthorId: simon_timms\ndate: 2022-02-18\noriginalurl: https://blog.simontimms.com/2022/02/18/parsing-vue-router-parameters\nmode: public\n---\n\n\n\nIn the vue router you can set up path parameters that are bound into the rendered component. For instance you might have a route like this:\n\n```javascript\n{\n    path: '/reports/:reportId/:reportName/:favorite',\n    name: 'Reports',\n    component: ReportView,\n    props: true\n}\n```\n\nThis will bind the parameters `reportId`, `reportName` and `favorite` on the component. However when you drop into that component and look at the values passed in you will see that they are strings. Of course that makes sense, the router doesn't really know if the things you pass in are strings or something else. Consider the route `/reports/17/awesome report/false`. Here `reportId` and `favorite` are going to be strings. \n\nYou can work around that by giving props in the router a function rather than just a boolean. \n\n```javascript\n{\n    path: '/reports/:reportId/:reportName/:favorite',\n    name: 'Reports',\n    component: ReportView,\n    props: (route) => ({\n      ...route.params,\n      reportId: parseInt(route.params.reportId),\n      favorite: route.params.favorite === 'true',\n    })\n  }\n```","categories":[],"tags":[]},{"title":"Installing the AzFilesHybrid PowerShell Module","authorId":"simon_timms","slug":"install-azfileshybrid-powershell","date":"2022-02-16 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/install-azfileshybrid-powershell/","link":"","permalink":"https://westerndevs.com/_/install-azfileshybrid-powershell/","excerpt":"","raw":"---\ntitle:  Installing the AzFilesHybrid PowerShell Module\nauthorId: simon_timms\ndate: 2022-02-16\noriginalurl: https://blog.simontimms.com/2022/02/16/install-azfileshybrid-powershell\nmode: public\n---\n\n\n\nIf you don't do a lot of powershell then the instructions on how to install the AzFilesHybrid module can be lacking. Here is how to do it \n\n1. Download the module from https://github.com/Azure-Samples/azure-files-samples/releases\n2. Unzip the file downloaded in step 1\n3. Go into the folder and run the copy command \n\n```\n./CopyToPSPath.ps1\n```\n\n4. Install the module with \n```\nInstall-Module -Name AzFilesHybrid -Force\n```\n\nWith this module installed you can then run things like Join-AzStorageAccount to get a fileshare joined to the domain \n\n```\nJoin-AzStorageAccount -ResourceGroupName \"rg-azfileshare\" -StorageAccountName \"sa-azfileshare\" -DomainName \"somedomain.com\" -DomainUserName \"jane\" -DomainUserPassword \"password\"\n```","categories":[],"tags":[]},{"title":"Purge CDN in DevOps","authorId":"simon_timms","slug":"purge-cdn-in-devops","date":"2022-01-27 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/purge-cdn-in-devops/","link":"","permalink":"https://westerndevs.com/_/purge-cdn-in-devops/","excerpt":"","raw":"---\ntitle:  Purge CDN in DevOps\nauthorId: simon_timms\ndate: 2022-01-27\noriginalurl: https://blog.simontimms.com/2022/01/27/purge-cdn-in-devops\nmode: public\n---\n\n\n\nIn order to purge a cache in the build pipeline you can use some random task that some dude wrote or you can just use the Azure CLI.\n\nHere is an example of what it would look like to purge the entire CDN top to bottom\n\n\n```yml\n- task: AzureCLI@2\n  displayName: 'Invalidate CDN Cache'\n  inputs:\n    azureSubscription: 'Azure'\n    scriptType: 'batch'\n    scriptLocation: 'inlineScript'\n    inlineScript: 'az cdn endpoint purge --content-paths \"/*\"  -n devascacdnendpoint -g devasca-rg --no-wait --profile-name devascacdn'\n```","categories":[],"tags":[]},{"title":"Handling Nil in Nested Hashes in Ruby","authorId":"simon_timms","slug":"hash-nil","date":"2021-11-24 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/hash-nil/","link":"","permalink":"https://westerndevs.com/_/hash-nil/","excerpt":"","raw":"---\ntitle:  Handling Nil in Nested Hashes in Ruby\nauthorId: simon_timms\ndate: 2021-11-24\noriginalurl: https://blog.simontimms.com/2021/11/24/hash_nil\nmode: public\n---\n\n\n\nTony Hoare introduced Null in ALGOL W in 1965. He says that it was a billion dollar mistake. Fortunately, Ruby totally avoids this sticky problem by renaming `null` to `nil`. This is the same strategy that my kids use when I say \"next person to ask for popcorn is going to do pushups\" and they ask for \"corn that has been popped\". If you're navigating a nested hash, that is a hash that contains other hashes then you might get into a situation where you want to do something like \n\n```\nbest_dessert = meals[:dinner][:dessert].find{ | dessert | {name: dessert.name, rating: dessert.rating } }.sort_by{ |a,b| b[:rating] }.first\n```\n\nThere are a bunch of opportunities in there for nil to creep in. Maybe there are no dinners maybe no desserts... to handle these we can use a combination of `dig` (which searches a path in a hash and returns nil if the path doesn't exist) and the safe navigation `&.` operator.\n\n```\nbest_dessert = meals.dig(:dinner, :dessert)&.find{ | dessert | {name: dessert.name, rating: dessert.rating } }&.sort_by{ |a,b| b[:rating] }&.first\n```\n\nThis combination of dig and &. allows us to get a nil out if nil appears anywhere in the chain without ending up in an error. We are calling the combination of dig and &. the 811 operator after https://call811.com/","categories":[],"tags":[]},{"title":"Choosing Power BI","authorId":"simon_timms","slug":"choosing-power-bi","date":"2021-11-19 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/choosing-power-bi/","link":"","permalink":"https://westerndevs.com/_/choosing-power-bi/","excerpt":"","raw":"---\ntitle:  Choosing Power BI\n# What is it? \n# Cost \nauthorId: simon_timms\ndate: 2021-11-19\noriginalurl: https://blog.simontimms.com/2021/11/19/choosing-power-bi\nmode: public\n---\n\n\n\nIf you're a developer and you're looking for a solution for reporting in your application then Power BI might be the right choice for you. However, there are tradeoffs that are worth considering before you dig in too much. \n\n## What is it? \n\nThe GitHub copilot I have running while writing this wants me to say that \"Power BI is a platform for creating and managing data-driven reports. It's a powerful tool that allows you to create reports that are easy to understand and use.\" That sounds an awful lot like a sentence that an AI trained on Microsoft marketing data would say. It isn't entirely inaccurate. For me Power BI is a combination for two different reporting tools which are pretty different from one another. The first is the one you see in all the marketing literature: Dashboard reports. These are nifty looking reports that are largely driven by visuals like maps or charts. Users can easily drill into aspects of the data by clicking on the charts or tables to filter the data and drill in. These reports aren't great for people who like to see the underlying data and draw their own conclusions. \n\nThe second report type is Paginated Reports. These reports are basically a complete migration of the older SQL Server Reporting Services (SSRS) reports. There is limited support for cool graphics but if you need an Excel like view of the data then they are great. I've run into a few cases in the past where I've tried to convince users that they cannot possibly want a 90 page report and that they should use the dashboard report. But frequently users do legitimately want to print 90 pages and go at the report with a highlighter. One of my favorite sayings for these situations is that as a developer I make suggestions and users make decisions.\n\nThe desktop based tools for building these reports are all pretty good. The dashboard report tool in particular is excellent. It may be that your users don't need to use online reporting and that providing them with a database connection and a copy of Power BI Desktop will be a good solution. The added advantage there is that the desktop tools are free to use. If you have a read only replica of your database then letting users develop their own reports doesn't have much of a downside other than having to support people as they learn the system. A cool idea is to build projections or views inside your database to handle the complex joins in your data and remove that burden from the users.\n\nIf you want to embed the reports in your application there is facility for doing that through a JavaScript API. You do need to jump through some hoops to authenticate and generate tokens but that's a lot easier than developing your own HTML based reports. There aren't a whole lot of examples out there for how to do embeddeding and you will need to spend some time learning the security models. \n\nThe alternative to all this is to use one of the myriad of other reporting tools that are available. I've used Telerik Reporting quite a lot in the past and I can confidently say it is \"not terrible\". That's about as high of praise as you're going to get from me for a reporting tool. \n\n## Cost \n\nAs with anything Microsoft the pricing for all this is convoluted and contently changing. This is my current understanding of it but  you can likely get a better deal and understanding by talking to your sales representative. \n\n* Power BI Desktop: Free as in beer\n* Power BI Pro: Let's you run dashboard reports online and embed them in your application (note that this doesn't' let you embed paginated reports) $9.99/month a users\n* Power BI Premium per user: This lets you run dashboard reports online and embed them in your application and also run paginated reports (note I didn't say embed paginated reports) $20/month a user\n* Power BI Premium per capacity: Run and embed both report types. Open for as many users as you have. $4995/month. Yikes, that price sure jumps up\n\nBeing able to embed paginated reports was the killer feature for me that took the reporting cost from very reasonable to very expensive. \n","categories":[],"tags":[]},{"title":"App Services on VNET","authorId":"simon_timms","slug":"app-services-on-vnet","date":"2021-11-14 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/app-services-on-vnet/","link":"","permalink":"https://westerndevs.com/_/app-services-on-vnet/","excerpt":"","raw":"---\ntitle:  App Services on VNET\nauthorId: simon_timms\ndate: 2021-11-14\noriginalurl: https://blog.simontimms.com/2021/11/14/app-services-on-vnet\nmode: public\n---\n\n\n\nWhen setting up an app service, including azure functions,  you can have it reside on a vnet so it can access internal resources like a database. Often time though you'll run into some problems routing to the database, specifically because of DNS. There are some good tools for debugging the connection. \n\nFirst off you'll need to open a console to the app service. I do this using the kudu tools but I think the console exposed directly on the portal works too. The standard tools can't run in the restricted environment provided. However there are a couple of tools you can use in their place. \n\nNSLookup - > nameresolver.exe - run it with `nameresolver.exe blah.com` \nping -> tcpping.exe - run it with `tcpping.exe blah.com:80`\n\nIf you're seeing DNS issues you can override the DNS server with the variables `WEBSITE_DNS_SERVER` and `WEBSITE_DNS_ALT_SERVER`. These two are entered in the app service config settings\n\nOne of the most typical problems I've encountered is that the app service isn't routing requests properly unless you add the app setting `WEBSITE_VNET_ROUTE_ALL=1`. ","categories":[],"tags":[]},{"title":"Using ngrok for ASP.NET","authorId":"simon_timms","slug":"ngrok-aspnet","date":"2021-10-25 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/ngrok-aspnet/","link":"","permalink":"https://westerndevs.com/_/ngrok-aspnet/","excerpt":"","raw":"---\ntitle:  Using ngrok for ASP.NET\nauthorId: simon_timms\ndate: 2021-10-25\noriginalurl: https://blog.simontimms.com/2021/10/25/ngrok-aspnet\nmode: public\n---\n\n\n\nIf you try to just use ngrok like this \n\n```\nngrok http 1316\n```\n\nYou're likely going to run into an issue when you browse to the website via the ngrok supplied URL that there are invalid host headers. This is because the local server is expecting headers for `localhost` and instead it is getting them for something like `https://3fe1-198-53-125-218.ngrok.io`. This can be solved by running with \n\n```\nngrok http 1316 -host-header=\"localhost:1316\"\n```","categories":[],"tags":[]},{"title":"Reading a TSV file in Pandas","authorId":"simon_timms","slug":"pandas-read-tsv","date":"2021-10-25 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/pandas-read-tsv/","link":"","permalink":"https://westerndevs.com/_/pandas-read-tsv/","excerpt":"","raw":"---\ntitle:  Reading a TSV file in Pandas\nauthorId: simon_timms\ndate: 2021-10-25\noriginalurl: https://blog.simontimms.com/2021/10/25/pandas-read-tsv\nmode: public\n---\n\n\n\nPretty easy, just use the csv loader with a different record separator\n\n```\ndata = pd.read_csv('work/data.tsv', sep='\\t')\n```\n\nYou can tell it explicitly to use the first column as the header\n\n```\ndata = pd.read_csv('work/data.tsv', sep='\\t', header=0)\n```\n\nI also found that it interpreted my first column as an index which I didn't want (it offset all the columns by one)\n\n```\ndata = pd.read_csv('work/data.tsv', sep='\\t', header=0, index_col=False)\n```","categories":[],"tags":[]},{"title":"Using RLS in Power BI Embedded","authorId":"simon_timms","slug":"RLS-with-power-bi","date":"2021-10-22 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/RLS-with-power-bi/","link":"","permalink":"https://westerndevs.com/_/RLS-with-power-bi/","excerpt":"","raw":"---\ntitle:  Using RLS in Power BI Embedded\nauthorId: simon_timms\ndate: 2021-10-22\noriginalurl: https://blog.simontimms.com/2021/10/22/RLS_with_power_bi\nmode: public\n---\n\n\n\nPower BI is a funny animal. On some levels it is a logical successor to SSRS but on other levels it is a totally different beast. One of the ways it differs greatly from SSRS is in handling parameters, especially secure parameters. When embedding an SSRS report you could specify the parameter value in a secure fashion and then now show the selector to end users.\n\nIn many cases there is a need to use row level security (RLS) to restrict the data that a user can see in Power BI. There are a myriad of ways to do this wrong and I think I've explored most of them over the last few days. There is also at least one way that works. \n\nA tempting approach is to use a filter. These can be applied at render time in the browser by adding to the config when embedding the report. \n\n```javascript\nconst config = {\n      visualName: '',\n      type: 'report',\n      accessToken: token, \n      embedUrl: token.embedUrl,\n      tokenType: powerbi.models.TokenType.Embed,\n      permissions: permissions,\n      // ðŸ‘‡ filters\n      filters: [\n        {\n          $schema: \"http://powerbi.com/product/schema#basic\",\n          target: {\n              table: \"monthlyProducts\",\n              column: \"userId\"\n          },\n          operator: \"In\",\n          values: [\"stimms\"]\n        }\n      ],\n      // â˜ï¸\n      settings: {\n        panes: {\n          filters: {\n            visible: true,\n          },\n          pageNavigation: {\n            visible: true,\n          },\n        },\n      },\n    }\n```\n\nThis class of parameter is fine for providing filters that can be updated later by the user. However, it should not be used for parameters that require some degree of security like a user name. These parameters are easily changed and, unless your parameter are in some way cryptographically secure there is a good chance you're introducing a broken access control - [#1](https://owasp.org/Top10/A01_2021-Broken_Access_Control/) on the [OWASP top 10](https://owasp.org/www-project-top-ten/). \n\nInstead of this approach you can use the manage roles functionality in Power BI. \n\n![](/images/2021-10-22-RLS_with_power_bi.md/2021-10-22-15-55-31.png))\n\nThis functionality is designed to provide high level filters for data. A lot of the examples I see are for things like restricting a user in the `East` region from seeing the data of a user in the `West` region. This is done by assigning a role to that user when generating the embedding token. Then you'd set up a role for each region (see 1) and apply a filter expression to your tables so the only records with a region of `East` would show up.\n\n![](/images/2021-10-22-RLS_with_power_bi.md/2021-10-22-15-59-46.png))\n\nThis is a simplistic and somewhat tiresome approach to adding a role mapping. What if a new region like `SouthEast` is introduced? What isn't, perhaps, as clear is that DAX expression can contain dynamic functions like `UserName()` which make filtering more powerful. This `UserName()` will be bound to the effective identity passed in\n\n![](/images/2022-07-06-RLS_with_power_bi.md/2022-07-06-07-11-52.png))\n\nWhat I settled on for my filtering was to have a single role which I enforce at the embedded token generation level and then filter my data by the UserName() which I also set at the embedded token level. Because these are set at the embedded token generation time which occurs on the server I can be confident that I'm not providing a way for somebody to view data they shouldn't. \n\nThe code for generation looks like this:\n\n```csharp\nvar tokenRequest = new GenerateTokenRequestV2(\n                    reports: new List<GenerateTokenRequestV2Report>\n                    {\n                        new GenerateTokenRequestV2Report(reportId)\n                    },\n                    datasets: new List<GenerateTokenRequestV2Dataset> { new GenerateTokenRequestV2Dataset(report.DatasetId) },\n                    identities: new List<EffectiveIdentity> { new EffectiveIdentity(user.ContactId.ToString(),\n                                                                roles: new List<string> { \"Users\" },\n                                                                datasets: new List<String>{report.DatasetId })\n                    }\n                );\nvar embedToken = await client.EmbedToken.GenerateTokenAsync(tokenRequest);\n```\n\nIn this particular case the data I was returning from the database could be accessed by a number of different people depending on to which group they belonged. Initially I tried crossing the data against the user/group matrix but the cardinality of the resulting set was in the billions and totally overwhelmed both SQL Server and Power BI. Instead what I did was pull in the user/group matrix and the dataset that exposed the group id. In Power BI I did a merge of the datasets along with applying the row level filtering. This was necessary because, as far as I know, there is no way to pass the user name down to the actual query running against the server. \n\nWith all this in place I got good security and good performance. But, wow, did it take me a long time to get there. ","categories":[],"tags":[]},{"title":"Mathplotlib cheat sheet","authorId":"simon_timms","slug":"matplotlib-cheat-sheet","date":"2021-10-22 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/matplotlib-cheat-sheet/","link":"","permalink":"https://westerndevs.com/_/matplotlib-cheat-sheet/","excerpt":"","raw":"---\ntitle:  Mathplotlib cheat sheet\n# Include the library \n# Set the plot size to be larger\n# Set plot title\n# Plot a line chart \n# Add Legend\nauthorId: simon_timms\ndate: 2021-10-22\noriginalurl: https://blog.simontimms.com/2021/10/22/matplotlib-cheat-sheet\nmode: public\n---\n\n\n\nPart of an evolving cheat sheet\n\n## Include the library \n\n```\nimport matplotlib.pyplot as plt\n```\n\n## Set the plot size to be larger\n\n```\nplt.rcParams['figure.figsize'] = [30, 21]\n```\n\n## Set plot title\n\n```\nplt.title(\"some title)\n```\n\n## Plot a line chart \n\n```\n  plt.plot(filtered['datetime'], filtered['totalsales'], label=\"Sales Ingest\")\n  plt.show()\n```\n\n## Add Legend\n\n```\nplt.legend()\n```","categories":[],"tags":[]},{"title":"Pandas Cheat Sheet","authorId":"simon_timms","slug":"pandas-cheat-sheet","date":"2021-10-22 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/pandas-cheat-sheet/","link":"","permalink":"https://westerndevs.com/_/pandas-cheat-sheet/","excerpt":"","raw":"---\ntitle:  Pandas Cheat Sheet\n# Read a file \n# Get a summary of the data \n# Create a new column from combining with another \n# Create a new column from a function\n# Binning \n# Average a column \n# Find row at an index \n# Filter data rows \n# Sort \nauthorId: simon_timms\ndate: 2021-10-22\noriginalurl: https://blog.simontimms.com/2021/10/22/pandas-cheat-sheet\nmode: public\n---\n\n\n\nPart of an evolving cheat sheet\n\n## Read a file \n\nFor CSV use \n\n```\ndata = pd.read_csv('data.csv')\n```\n\nFor excel use \n\n```\ndf = pd.read_excel (r'./Detected as deleted from API data.xlsx')\n```\n\nYou might need to install a library for xlsx files \n\n```\npip install openpyxl\n```\n\n## Get a summary of the data \n\nNumerical summaries \n\n```\ndata.describe()\n```\n\nTypes of columns \n\n```\ndata.dtypes\n```\n\n## Create a new column from combining with another \n\nThis adds up values in two columns \n\n```\ndata[\"totalqueuelength\"] = data[\"Svc\"] + data[\"Svc Que\"]\n```\n\nThis converts a couple of columns that have the data and time to a single field and turns it into a date \n\n```\ndata[\"datetime\"] = pd.to_datetime(data[\"Date\"] + \" \" + data[\"Time\"], format='%m/%d/%Y %I:%M %p')\n```\nCheck date formats at https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n\n## Create a new column from a function\n\n```\ndf = df.assign(percent=lambda x: 100* x['deleted on date'] / x['total'])\n```\n\n## Binning \n\nYou can bin the data by adding a new bin column to the dataframe\n\n```\ndf['binning'] = pd.cut(df['percent'], 5) # 5 bins based on the percent column\n```\n\n## Average a column \n\n```\ndf['column'].mean()\n```\n\n## Find row at an index \n\n```\ndf.iloc[[int(totalRecords * .95)]] # Find the row at the 95th percentile\n```\n\n## Filter data rows \n\n```\ndata.loc[(data['datetime'] > '2021-10-16')]\n```\n\n## Sort \n\n```\ndf.sort_values(by=['percent'], ascending=False)\n```","categories":[],"tags":[]},{"title":"Running Serverless Offline with a Self-Signed SSL Certificate","authorId":"simon_timms","slug":"serverless-offline-https","date":"2021-10-12 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/serverless-offline-https/","link":"","permalink":"https://westerndevs.com/_/serverless-offline-https/","excerpt":"","raw":"---\ntitle:  Running Serverless Offline with a Self-Signed SSL Certificate\n# Generate a Cert Using OpenSSL\n# Running with the Certificate\n# Gotchas\nauthorId: simon_timms\ndate: 2021-10-12\noriginalurl: https://blog.simontimms.com/2021/10/12/serverless-offline-https\nmode: public\n---\n\n\n\nIf you find yourself in need of running serverless locally using [serverless offline](https://www.serverless.com/plugins/serverless-offline/) and you want an SSL certificate then fear not, it's not all that difficult. First you'll need an SSL certificate. For our purposes you we're going to use a self-signed certificate. This will cause browsers to complain but for local testing it isn't typically a big problem. \n\n## Generate a Cert Using OpenSSL\n\nYou should install OpenSSL (or one of the more secure alternatives like [LibreSSL](https://www.libressl.org/)) and then run \n\n```\nopenssl req -newkey rsa:2048 -nodes -keyout key.pem -x509 -days 365 -out cert.pem\n```\n\nThis will prompt you for a bunch of information about your organization. You can put anything you want in most of those fields but do pay attention to the `Common Name` field which needs to hold the value of `localhost`. \n\nThese are the answers I gave\n```\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:TX\nLocality Name (eg, city) []:Austin\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Inventive\nOrganizational Unit Name (eg, section) []:\nCommon Name (e.g. server FQDN or YOUR name) []:localhost\nEmail Address []:\n```\n\nYou should now have a `cert.perm` and a `key.pem` in your local directory. Copy these into a `cert` folder at the root of your serverless project. \n\n## Running with the Certificate\n\nNow you need to tell serverless where to find your certificate. You can either run with the flag\n\n```\n--httpsProtocol cert\n``` \n\nor update your `serverless.yml` to include the cert directory \n\n```\ncustom:\n  serverless-offline:\n    httpsProtocol: \"cert\"\n    ...\n```\n\n## Gotchas\n\nIf you're seeing a warning about an invalid certificate then check that you're accessing serverless via `localhost` and not `127.0.0.1` or `0.0.0.0`. SSL works with domain names so you need to use one, even if it is just `localhost`.","categories":[],"tags":[]},{"title":"Configuration in Azure Isolated Functions","authorId":"simon_timms","slug":"azure-isolated-functions-config","date":"2021-10-05 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-isolated-functions-config/","link":"","permalink":"https://westerndevs.com/_/azure-isolated-functions-config/","excerpt":"","raw":"---\ntitle:  Configuration in Azure Isolated Functions \nauthorId: simon_timms\ndate: 2021-10-05\noriginalurl: https://blog.simontimms.com/2021/10/05/azure-isolated-functions-config\nmode: public\n---\n\n\n\nThis is all done in the Program.cs. If you want to use the IOptions pattern which, let's face it, everybody does. Then you can start with creating your configuration classes (I like more than one so config for different parts of the app remain logical distinct). These are POCOs\n\n```csharp\npublic class AuthConfiguration\n{\n    \n    public string TenantId { get; set; }\n    public string ClientId { get; set; }\n    public string ClientSecret { get; set; }\n    public string RedirectUrl { get; set; }\n}\n```\n\nThen set this up in the host builder \n\n\n```csharp\nvar host = new HostBuilder()\n            .ConfigureFunctionsWorkerDefaults()\n            .ConfigureServices(container =>\n            {\n                container.AddOptions<AuthConfiguration>().Configure<IConfiguration>((settings, config) =>\n                {\n                    config.Bind(settings);\n                });\n            })\n            .Build();\n```\n\nIf this looks familiar it's because it totally is! All of this uses the generic .NET host so this same sort of pattern should work in most .NET apps now. ","categories":[],"tags":[]},{"title":"Closed form Fibonacci","authorId":"simon_timms","slug":"fibonacci","date":"2021-10-05 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/fibonacci/","link":"","permalink":"https://westerndevs.com/_/fibonacci/","excerpt":"","raw":"---\ntitle:  Closed form Fibonacci\nauthorId: simon_timms\ndate: 2021-10-05\noriginalurl: https://blog.simontimms.com/2021/10/05/fibonacci\nmode: public\n---\n\n\n\nA favorite programming test question is the Fibonacci sequence. This is defined as either `1 1 2 3 5...` or `0 1 1 2 3 5...` depending on what you feel fib of 0 is. In either case fibonacci is the sum of the two previous terms. So fib(10) = fib(9) + fib(8). The reason this is a programming test favorite is because it forces people to think about recursion and even memoization for performance. \n\nThere is however a shortcut for this and that is to use the closed form which uses the golden ratio. We have two interesting values called Phi and phi with the former being the golden ratio and the latter being a value observed in nature for leaf dispersions. \n\n```\nPhi = (1 + root(5))/2\nphi = (1-root(5))/2\n```\n\nWe can use this to create Binet's formula (Jacques Philippe Marie Binet was a French mathematician born in 1786, although this formula is named for him it was really discovered by fellow French mathematician Abraham do Moivre a century earlier)\n\n```\nfib(n) = (Phi^n - phi^n)/(Phi - phi)\n```\n\nIn code we can do the following to generate all the fib numbers up to n\n\n```csharp\nstatic double Phi = (1 + Math.Pow(5,.5))/2;\nstatic double phi = (1 - Math.Pow(5,.5))/2;\nstatic ulong[] generateFibonaccisClosed(int n){\n    ulong[] fib = new ulong[n];\n    for(int i = 0; i<n; i++)\n    {\n        fib[i] = (ulong)((Math.Pow(Phi, i)-Math.Pow(phi, i))/(Phi - phi));\n    }\n    return fib;\n}\n```\n\nThe advantage here is that the closed form is constant time, constant memory and uses only about 3 64-bit values (fewer if you calculate phi and Phi as you go). It's certainly a lot of fun to break out in a programming test. ","categories":[],"tags":[]},{"title":"Filtering Datatables","authorId":"simon_timms","slug":"datatable-filtering","date":"2021-09-17 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/datatable-filtering/","link":"","permalink":"https://westerndevs.com/_/datatable-filtering/","excerpt":"","raw":"---\ntitle:  Filtering Datatables\nauthorId: simon_timms\ndate: 2021-09-17\noriginalurl: https://blog.simontimms.com/2021/09/17/datatable-filtering\nmode: public\n---\n\n\n\nYears back there was this crazy way of dealing with data in .NET called a DataSet. DataSets contained DataTables which contained a combination of DataRows and DataColumns. It was all loosely typed and keyed with strings. Basically a database inside of your process. Even when they were cool I felt uncomfortable using them. Because I sometimes maintain legacy code I run into these monstrosities. \n\nToday's problem was that I needed to filter the contents of a table before bulk loading it. You can actually do simple filtering using a quasi-SQL like\n\n```\n   var dataRows = existingDataTable.Select(\"UserName = 'simon'\")\n```\n\nThis gives you back a collection of DataRows which I guess you could inset back into the table after clearing it of rows. To make this useful there is an extension method called `CopyToDataTable` in `System.Data.DataExtension` (be sure to include the dll for this). Using that you can get back a full data table\n\n```\nvar dataTable = existingDataTable.Select(\"UserName = 'simon'\").CopyToDataTable();\n```\n\nIn that same package, though, is a much better tool for filtering: converting the table to an IEnumerable. You still need to use some magic strings but that's somewhat unavoidable in data tables. \n\n```\nvar dt = existingDataTable.AsEnumerable()\n                    .Where(r => r.Field<String>(\"UserName\") == \"Simon\").CopyToDataTable();\n```\n\nYou can also do more advanced things like checking to see if something is a GUID\n\n```\nvar dt = existingDataTable.AsEnumerable()\n                    .Where(r => Guid.TryParse(r.Field<String>(\"Id\"), out var _)).CopyToDataTable();\n```\n\n","categories":[],"tags":[]},{"title":"Installing VSFTP from source","authorId":"simon_timms","slug":"vsftpd","date":"2021-09-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/vsftpd/","link":"","permalink":"https://westerndevs.com/_/vsftpd/","excerpt":"","raw":"---\ntitle:  Installing VSFTP from source\nauthorId: simon_timms\ndate: 2021-09-08\noriginalurl: https://blog.simontimms.com/2021/09/08/vsftpd\nmode: public\n---\n\n\n\nThe 3.0.3 version of VSFTP seems to have an exploit against it so you should update to 3.0.5. However at the current time that has not been released in Debian upstream so we get to compile it ourselves!\n\nFirst get and decompress the source\n```\nwget https://security.appspot.com/downloads/vsftpd-3.0.5.tar.gz\ntar xvf vsftpd-3.0.5.tar.gz\ncd vsftpd-3.0.5\n```\n\nNow you're going to need to edit the `builddefs.h` specifically you want to enable SSL with \n\n```\n #define VSF_BUILD_SSL\n```\n\nYou may need to install the open ssl headers\n\n```\nsudo apt install libssl-dev\n```\n\nNext run a standard make/make install\n\n```\nmake\nmake install\n```\n\nThat should be it. If you're replacing an apt sourced vsftpd then remember to uninstall that and you will have to update the init.d script to point at the new one which is in `/usr/local/sbin` instead of `/usr/sbin`. You could also change the install prefix but I like `local` better.","categories":[],"tags":[]},{"title":"SQL Mail","authorId":"simon_timms","slug":"enable-database-mail","date":"2021-08-31 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/enable-database-mail/","link":"","permalink":"https://westerndevs.com/_/enable-database-mail/","excerpt":"","raw":"---\ntitle:  SQL Mail\nauthorId: simon_timms\ndate: 2021-08-31\noriginalurl: https://blog.simontimms.com/2021/08/31/enable-database-mail\nmode: public\n---\n\n\n\nDid you know you can hook up your SQL server (or Managed SQL on Azure) to an SMTP server and use it to send email. Terrible idea? Yes, probably. I really encourage people not to build business logic that might require creating an email into stored procs.  Required for legacy code? Yes, certainly. \n\nYou first need to tell SQL server how to talk to the mail server. This is done using the `sysmail_add_Account_sp`\n\n```sql\nEXECUTE msdb.dbo.sysmail_add_account_sp\n    @account_name = 'Database Mail Account',\n    @description = 'SQL Server Notification Service',\n    @email_address = 'SQLServer@somedomain.com',\n    @replyto_address = 'SQLServer@somedomain.com',\n    @display_name = 'Database Mail Profile',\n    @mailserver_name = 'smtp.office365.com',\n    @port = 587,\n\t@username = 'SQLServer@somedomain.com',\n\t@password = 'totallynotourpassword',\n\t@enable_ssl = 1;\n```\n\nWith that in place you can set up a mail profile to use it\n\n```sql\nSELECT @sequence_number = COALESCE(MAX(profile_id),1) FROM msdb.dbo.sysmail_profile;\n\n-- Create a mail profile\nEXECUTE msdb.dbo.sysmail_add_profile_sp\n    @profile_name = 'Database Mail Profile',\n    @description = 'Sends email from the db';\n                              \n-- Add the account to the profile\nEXECUTE msdb.dbo.sysmail_add_profileaccount_sp\n    @profile_name = 'Database Mail Profile',\n    @account_name = 'Database Mail Account',\n    @sequence_number = @sequence_number;\n                              \n-- Grant access to the profile to the DBMailUsers role\nEXECUTE msdb.dbo.sysmail_add_principalprofile_sp\n    @profile_name = 'Database Mail Profile',\n    @principal_id = 0,\n    @is_default = 1 ;\n```\n\nYou can then make use of `sp_send_dbmail` to send email\n\n```sql\nEXEC msdb.dbo.[sp_send_dbmail]\n    @profile_name = 'Database Mail Profile',\n    @recipients = 'simon.timms@somedomain.com',\n    @subject = 'Testing db email',\n    @body = 'Hello friend, I''m testing the database mail'\n```\n\nYou can check the status of the sent email by querying \n\n```\nselect * from msdb.dbo.sysmail_allitems\n```\n\nIf things fail then checking the event log may be helpful \n\n```\nselect * from msdb.dbo.sysmail_event_log \n```","categories":[],"tags":[]},{"title":"Reauthenticate with Nuget","authorId":"simon_timms","slug":"reauth-nuget","date":"2021-08-31 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/reauth-nuget/","link":"","permalink":"https://westerndevs.com/_/reauth-nuget/","excerpt":"","raw":"---\ntitle:  Reauthenticate with Nuget\nauthorId: simon_timms\ndate: 2021-08-31\noriginalurl: https://blog.simontimms.com/2021/08/31/reauth-nuget\nmode: public\n---\n\n\n\nIf you have a private nuget feed authenticated with a password chances are your password will eventually expire or change. For some reason Visual Studio and perhaps nuget under the covers aggressively caches that password and doesn't prompt you when the password doesn't work anymore. To change the password the easiest approach I've found is to use the nuget.exe command line tool and run \n\n```\nc:\\temp\\nuget.exe sources update -Name \"Teamcity\" -Source \"https://private.nuget.feed.com/httpAuth/app/nuget/feed/_Root/SomeThing/v2\" -UserName \"simon.timms\" -Password \"Thisisactuallymypassword,no,really\"\nC:\\temp\\nuget.exe list -Source teamcity\n```","categories":[],"tags":[]},{"title":"Which Key to use for Managed Identity in Keyvault","authorId":"simon_timms","slug":"appliation-key-for-keyvault","date":"2021-08-13 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/appliation-key-for-keyvault/","link":"","permalink":"https://westerndevs.com/_/appliation-key-for-keyvault/","excerpt":"","raw":"---\ntitle:  Which Key to use for Managed Identity in Keyvault\nauthorId: simon_timms\ndate: 2021-08-13\noriginalurl: https://blog.simontimms.com/2021/08/13/appliation-key-for-keyvault\nmode: public\n---\n\n\n\nI have a terraform deployment which runs in azure pipeline. Azure pipelines is connected to Azure via a service connection. This service connection is registered as an application in the Azure AD of the Azure account. The problem I constantly run into is that I can't remember which id from the application should be granted keyvault access so the build pipeline can read and write to keyvault. \n\n```\nresource \"azurerm_key_vault_access_policy\" \"terraformaccess\" {\n  key_vault_id = azurerm_key_vault.keyvault.id\n\n  tenant_id = local.tenant_id\n  object_id = ???????????????????\n\n  key_permissions = [\n    \"Get\",\n    \"Create\",\n    \"List\",\n    \"Update\",\n    \"Verify\",\n    \"Delete\",\n    \"WrapKey\",\n    \"UnwrapKey\"\n  ]\n\n  secret_permissions = [\n    \"Get\",\n    \"List\",\n    \"Set\"\n  ]\n\n  storage_permissions = [\n    \"Get\",\n    \"List\",\n    \"Set\",\n    \"Update\"\n  ]\n}\n```\nMaybe the value is in the portal somewhere:\n\n![](/images/2021-08-12-appliation-key-for-keyvault.md/2021-08-12-21-21-51.png))\n\nNope. \n\nIt seems to be findable by doing either \n\n```powershell\nLogin-AzureRmAccount -SubscriptionId <your subscription id>;\n$spn=(Get-AzureRmADServicePrincipal -SPN <the application id>);\necho $spn.Id\n```\n\nor \n\n```bash\n az ad sp list --spn <the application id>\n```\nThen look for `ObjectId`","categories":[],"tags":[]},{"title":"Stop Terraform Managing State for a Resource","authorId":"simon_timms","slug":"stop-managing-resource","date":"2021-07-30 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/stop-managing-resource/","link":"","permalink":"https://westerndevs.com/_/stop-managing-resource/","excerpt":"","raw":"---\ntitle:  Stop Terraform Managing State for a Resource\nauthorId: simon_timms\ndate: 2021-07-30\noriginalurl: https://blog.simontimms.com/2021/07/30/stop-managing-resource\nmode: public\n---\n\n\n\nSay you want to keep a resource but you want to stop terraform from managing it. You can ask terraform to update its state to forget about it. In my case I want terraform to forget it managing my Azure Static Web App because Terraform doesn't support all the options I need and will clobber the app. \n\nI can run this \n\n```bash\nterraform state rm \"azurerm_static_site.agentportal\"\n```\n\nIf I decide to start managing the state again I can just run a terraform import to manage it again. ","categories":[],"tags":[]},{"title":"Storybook IFrame 404","authorId":"simon_timms","slug":"storybook-iframe-404","date":"2021-07-29 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/storybook-iframe-404/","link":"","permalink":"https://westerndevs.com/_/storybook-iframe-404/","excerpt":"","raw":"---\ntitle:  Storybook IFrame 404\nauthorId: simon_timms\ndate: 2021-07-29\noriginalurl: https://blog.simontimms.com/2021/07/29/storybook-iframe-404\nmode: public\n---\n\n\n\nIf you're running into a 404 on the IFrame on storybook the mostly likely cause is that your directory path to where storybook is contains some encoded characters. In this case we had a project called `Agent%20Portal` on disk. Renaming it to just `AgentPortal` fixed up storybook right away. ","categories":[],"tags":[]},{"title":"Building an SQL Azure Connection String using terraform","authorId":"simon_timms","slug":"build-sql-connection-string","date":"2021-07-26 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/build-sql-connection-string/","link":"","permalink":"https://westerndevs.com/_/build-sql-connection-string/","excerpt":"","raw":"---\ntitle:  Building an SQL Azure Connection String using terraform\nauthorId: simon_timms\ndate: 2021-07-26\noriginalurl: https://blog.simontimms.com/2021/07/26/build_sql_connection_string\nmode: public\n---\n\n\n\nIf you provision a database using terraform you often find that you need to get that connection string into app settings or key vault or something like that. To do that you first need to build it because the outputs from the database resource don't include it. \n\nFrom the database you want to export\n\n```json\noutput \"database_name\" {\n  value = azurerm_sql_database.database.name\n}\n\n```\n\nThen when you actually build the string you want something like this:\n\n```\ndatabase_connection_string    = \"Server=tcp:${module.database.name}.database.windows.net,1433;Initial Catalog=${module.database.database_name};Persist Security Info=False;User ID=${var.database_user};Password=${var.database_password};MultipleActiveResultSets=True;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\"\n```\n\nThen you can push this to KeyVault or an App Service directly. ","categories":[],"tags":[]},{"title":"Deploying App Settings to an Azure Static Web App","authorId":"simon_timms","slug":"static-app-settings","date":"2021-07-26 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/static-app-settings/","link":"","permalink":"https://westerndevs.com/_/static-app-settings/","excerpt":"","raw":"---\ntitle:  Deploying App Settings to an Azure Static Web App\nauthorId: simon_timms\ndate: 2021-07-26\noriginalurl: https://blog.simontimms.com/2021/07/26/static-app-settings\nmode: public\n---\n\n\n\nStatic web apps are pretty cool but certain parts of them feel like they are still a little raw. It is a newish product so I can understand that. I just wish the things that didn't get attention were something other that devops things. That's mostly because I'm so big on builds and repeatable processes. Being able to set app setting is one of the things I think falls through the cracks. \n\nThe [docs](https://docs.microsoft.com/en-us/azure/static-web-apps/application-settings#:~:text=%20Using%20the%20Azure%20portal%20%201%20Navigate,7%20Click%20OK.%208%20Click%20Save.%20More%20) for statics web apps suggests two different ways of setting app settings. First through the portal which we can ignore right out the gate because it is manual. The second is through an `az` command that actually just exercises a REST endpoint. No Arm support, no terraform support, no bicep support, no azure powershell support... a long way to go. \n\nThe az command takes in a specially formatted json file. My databse and connection string variables are set up as outputs from my terraform. Once I have them imported into my Azure DevOps build pipeline I use powershell to build the template file like so:\n\n```powershell\n$temp = gc api/api/local.settings.template.json | ConvertFrom-Json\n$temp.properties.STORAGE_CONNECTION_STRING = \"$(terraformOutput.storage_connection_string)\"\n$temp.properties.DATABASE_CONNECTION_STRING = \"$(terraformOutput.database_connection_string)\"\n$temp | ConvertTo-json > api/api/local.settings.template.json\n```\n\nNow this needs to be passed up to azure using the `AzureCLI` task \n\n```yaml\n  - task: AzureCLI@2\n    inputs:\n    azureSubscription: 'Azure Sub'\n    scriptType: 'pscore'\n    scriptLocation: 'inlineScript'\n    inlineScript: |\n        gc api/api/local.settings.template.json\n        az rest --method put --headers \"Content-Type=application/json\" --uri \"/subscriptions/6da8d6e6-41f1-xxxx-xxxx-xxxxxxxx/resourceGroups/dev-portal/providers/Microsoft.Web/staticSites/dev-portal/config/functionappsettings?api-version=2019-12-01-preview\" --body @api/api/local.settings.template.json\n```              ","categories":[],"tags":[]},{"title":"Vetur Warnings in Azure Static Web App","authorId":"simon_timms","slug":"vetur-cant-file-files","date":"2021-07-24 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/vetur-cant-file-files/","link":"","permalink":"https://westerndevs.com/_/vetur-cant-file-files/","excerpt":"","raw":"---\ntitle:  Vetur Warnings in Azure Static Web App\nauthorId: simon_timms\ndate: 2021-07-24\noriginalurl: https://blog.simontimms.com/2021/07/24/vetur_cant_file_files\nmode: public\n---\n\n\n\nI have a static web app that has directories for the front end written in Vue and the back end written in Azure functions. When I open it in VS Code I get warnings that Vetur can't find the `package.json` or `tsconfig.json`. This is because the Vue project isn't at the project root. This can be fixed by adding, at the root, a `vetur.config.js` containing a pointer to the web project. With my web project being in `web` (creative I know) the file looks like \n\n\n```javascript\n// vetur.config.js\n/** @type {import('vls').VeturConfig} */\nmodule.exports = {\n    // **optional** default: `{}`\n    // override vscode settings\n    // Notice: It only affects the settings used by Vetur.\n    settings: {\n      \"vetur.useWorkspaceDependencies\": true,\n      \"vetur.experimental.templateInterpolationService\": true\n    },\n    // **optional** default: `[{ root: './' }]`\n    // support monorepos\n    projects: [\n      './web'\n    ]\n  }\n```","categories":[],"tags":[]},{"title":"Enable TeamCity Symbol Server","authorId":"simon_timms","slug":"enable-symbol-server","date":"2021-07-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/enable-symbol-server/","link":"","permalink":"https://westerndevs.com/_/enable-symbol-server/","excerpt":"","raw":"---\ntitle:  Enable TeamCity Symbol Server\nauthorId: simon_timms\ndate: 2021-07-08\noriginalurl: https://blog.simontimms.com/2021/07/08/enable-symbol-server\nmode: public\n---\n\n\n\nFirst off a symbol server is a server which stores the symbols from a built binary so you don't have to ship out PDB files with your compiled code to be able to debug it. You can hook up visual studio to search a symbol server when you're debugging so that you can drop into code for something like a shared nuget package. Teamcity, as it turns out, has a plugin to support being a symbol server. Here is how you get started with it:\n\n1. Install the symbol server plugin by going to Administration > plugins > Browse plugins repository and search for `symbol`\n2. On your build agents install the windows debugging tools which are shipped as part of the Windows SDK. For windows 10 you can grab it here: https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk/ During the install you'll be prompted for which components you want to install and you can just turn off everything but the debugging tools.\n3. Remember to restart your build agents so they can register the debugging tools as being installed. You can check by going to the build agent in teamcity. Click on parameters\n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-29-31.png))\nIn there, at the bottom, you should find an entry for the debugger\n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-29-59.png))\n4. In the projects you want symbols for enable the symbol server feature\n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-31-20.png))\n5. In the build artifacts you need to ensure that both the PDB and the associated EXE or DLL are selected as artifacts. \n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-32-51.png))\n\nThat's pretty much it. In your build now you should see a few log messages to let you know that the symbol server indexing is working\n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-45-53.png))\n\nNow you can hook up Visual Studio to use this by going into settings and searching for `symbols` then paste the URL of the teamcity server with `/app/symbols` at the end of it into the box\n![](/images/2021-07-08-enable-symbol-server.md/2021-07-08-16-49-54.png))\n\nNow when you're debugging in visual studio you'll have access to the symbols. ","categories":[],"tags":[]},{"title":"Enable SSO for Snowflake using Azure AD","authorId":"simon_timms","slug":"enable-azure-ad-based-SSO-for-snowflake","date":"2021-07-06 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/enable-azure-ad-based-SSO-for-snowflake/","link":"","permalink":"https://westerndevs.com/_/enable-azure-ad-based-SSO-for-snowflake/","excerpt":"","raw":"---\ntitle:  Enable SSO for Snowflake using Azure AD\n# Azure Side\n# Snowflake Side\n# Automatic Provisioning \n# Adding Users to the Sync\n# Gotchas!\nauthorId: simon_timms\ndate: 2021-07-06\noriginalurl: https://blog.simontimms.com/2021/07/06/enable-azure-ad-based-SSO-for-snowflake\nmode: public\n---\n\n\n\nSo you want to enable single sign on for you AD users to Snowflake? There are a bunch of good reasons to do this: it makes managing users easier, deleting a user in AD deletes them in snowflake so you don't have a laundry list of places to delete a user when users leave. \n\nThe process is a 2 sided thing: setting up the Snowflake integration on the AD side and then letting Snowflake know where to authenticate its users. \n\n## Azure Side\n\n1. Go to azure AD and click on `Enterprise Applications` on the left hand side\n![](/images/2021-07-05-enable-azure-ad-based-SSO-for-snowflake.md/2021-07-05-12-25-20.png))\n2. Click `New Application` and search for Snowflake select it and create it\n3. In there set up the links to your Snowflake tenant for single sign on by selecting Single sign-on on the left\n4. Fill in the URLs for your snowflake instance. The only thing that you really need to pay attention to is that you're using the snowflake name on your already created snowflake instance.\n![](/images/2021-07-05-enable-azure-ad-based-SSO-for-snowflake.md/2021-07-05-12-28-33.png))\n5. Download the Base64 Certificate from the SAML Signing Certificate section\n6. Assign a test user to the snowflake integration by clicking on users and groups and adding an existing user\n\n## Snowflake Side\n\n1. Run this query in snowflake. It adds a saml identity provider and then set up single sign on\n\n```sql\nuse role accountadmin;\nalter account set saml_identity_provider = '{\n\"certificate\": \"<Paste the content of downloaded certificate from Azure portal>\",\n\"ssoUrl\":\"<Login URL value which you have copied from the Azure portal, something like https://login.microsoftonline.com/44xxxx25-xxxx-415b-bedc-xxxxxxxxxxxxxx/saml2>\",\n\"type\":\"custom\",\n\"label\":\"AzureAD\"\n}';\nalter account set sso_login_page = TRUE;\n```\n\n2. Hook up the user you created earlier in AD\n```sql\nCREATE USER simon_timms PASSWORD = '' LOGIN_NAME = 'user@somedomain.com' DISPLAY_NAME = 'Simon Timms';\n```\n\nYou should now be able to log in with your AD account. Open up an incognito tab and go to your snowflake instance. In there click on the SSO option and enter your AD credentials. \n\n## Automatic Provisioning \n\nObviously it sucks to provision the users manually in snowflake so you can have AD sync changes over to it. To do this start with snowflake. You'll need to create a user who can provision users.\n\n```sql\ncreate or replace role aad_provisioner;\ngrant create user on account to role aad_provisioner;\ngrant create role on account to role aad_provisioner;\ngrant role aad_provisioner to role accountadmin;\ncreate or replace security integration aad_provisioning\n    type = scim\n    scim_client = 'azure'\n    run_as_role = 'AAD_PROVISIONER';\nselect system$generate_scim_access_token('AAD_PROVISIONING');\n```\nThis should give you a long key which you should copy. \n\n![](/images/2021-07-06-enable-azure-ad-based-SSO-for-snowflake.md/2021-07-06-15-59-43.png))\n\nGo back to the AD app and click on Provisioning. In there change over to automatic provisioning. Enter the key in the `Secret Token` field and in the `Tenant Url` field enter your usual URL but this time with `/scim/v2` on the end of it. \n\nTest the connection and ensure that it can connect properly. With that done you'll need to turn provisioning status on\n\n![](/images/2021-07-06-enable-azure-ad-based-SSO-for-snowflake.md/2021-07-06-16-01-34.png))\n\n## Adding Users to the Sync\n\nIf you want to add a new user to the synchronizing then go back to the snowflake app under Enterprise Applications in Azure AD. In there click on `Users and groups`\n![](/images/2021-08-31-enable-azure-ad-based-SSO-for-snowflake.md/2021-08-31-13-17-47.png))\n\nThen on the add users and groups button. In there you can select your user and click `Assign`. That should be it. It may take a few minutes to sync. You can always check the status of the sync by going to the `Provisioning` item\n\n\n## Gotchas!\n\nThe biggest one here is that the snowflake key used in automatic provisioning only has a lifespan of 6 months. It is almost certainly going to break horribly at that time. You should mitigate this by having the sync job email you if it fails. This can be done in the settings page in Azure\n\n![](/images/2021-07-06-enable-azure-ad-based-SSO-for-snowflake.md/2021-07-06-16-06-05.png))\n\nTo get a new token you'll need to log into snowflake and run the following query\n\n```\nselect system$generate_scim_access_token('AAD_PROVISIONING');\n```\n\nThis will generate a new token and you'll need to copy it back into Azure. A gotcha inside a gotcha here is that running this command can only be done as ACCOUNTADMIN so you need to select that here: \n![](/images/2022-01-27-enable-azure-ad-based-SSO-for-snowflake.md/2022-01-27-09-33-56.png))","categories":[],"tags":[]},{"title":"I Can Teach Your Dog Quantum Physics","authorId":"david_wesst","slug":"i-can-teach-your-dog-quantum-physics","date":"2021-06-28 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"finding-inspiration/i-can-teach-your-dog-quantum-physics/","link":"","permalink":"https://westerndevs.com/finding-inspiration/i-can-teach-your-dog-quantum-physics/","excerpt":"It's true! I read a book about it and everything. Let me breakdown the highlights of this book that does a pretty good job of explaining the physics behind quantum computing.","raw":"---\ntitle: \"I Can Teach Your Dog Quantum Physics\"\ndate: \"2021-06-28T12:00:00\"\ncategories:\n- finding inspiration\ntags:\n- quantum computing\n- quantum physics\n- finding inspiration\n- book\n- Mark Russinovich\nexcerpt: \"It's true! I read a book about it and everything. Let me breakdown the highlights of this book that does a pretty good job of explaining the physics behind quantum computing.\"\noriginalurl: https://www.davidwesst.com/blog/i-can-teach-your-dog-quantum-physics/\nauthorId: david_wesst\n---\n\n![A hand holding the book How to teach physics to your dog by Chad Orzel](/images/2021-06-28-i-can-teach-your-dog-quantum-physics/physics_dog_resized.jpg)\n\nOkay fine. Maybe I can't teach your dog quantum physics, but this book taught _me_ something about quantum so that's something, right?\n\nI finished _How to Teach Physics to your Dog_ by Chad Orzel as I continue to dive deeper into my quest in understanding in quantum computing. This book was recommended by Mark Russonovich at the end of his Microsoft Ignite 2019 talked called _Quantum computing: Computing with a probabilistic universe with Mark Russinovich_ at Microsoft Ignite 2019. You can watch at this link at the Microsoft Ignite site: [LINK](https://myignite.microsoft.com/archives/IG19-BRK4011).\n\nIn short, it's pretty good.\n\nIt's solid way to understand some of the core of quantum mechanics that make quantum computing possible. It goes deeper just defining concepts like superposition, entanglement, and QED, but it also gets into a bit of the math and the history side of the concepts.\n\n# The Highlights\nNow, I'm no book scientist and as such, I'm not going to worry about trying to put together a fancy book review that rates my experience. Instead, I thought it would be a good idea to highlight the parts of the book that made it a good read, and you can figure out the rest on your own.\n\n## Dog Conversations\nOrzel jumps between explaining a concept traditionally to having a conversation with his dog (hense the title). \n\nThe style is kinda silly, which is something of a fresh take for explaining science, but it works surprisingly well. \n\nOutside of helping to simplify some of the more complicated or mind bending points of quantum mechanics, or lightens the mood and makes it less of a \"physics book\" and more of a \"story about physics\".\n\n## A story that teaches you about physics\nI think the best part about the book is how it builds on each topic, chapter by chapter. \n\nIt reads like a story, where each chapter prepares you for the next until you finally hit the big wrap up on entanglement and QED, or quantum electrodynamics, along with a chapter on what to look for when people are trying to abuse quantum physics to push their own non-scientific agenda of making a quick buck.\n\n## Mathy, but not too mathy\nPhysics is math. That's just how it is, and this book does a great job of introducing us to the math, but not dwelling or depending on it. I appreciated that, especially near the end of the book where ideas like QED start to really bend your mind.\n\nI appreciated how they didn't shy away from it and used the conversations with the dog to help bring it down to a \"not too mathy\" way. Keeps the reader in check and reminds them that this isn't just philosophy, but _real science_.\n\n# TL;DR; / Conclusion\n\nThis book set me up with a solid foundation on the physics that make quantum computing possible, which was the whole reason for reading it. It's \"learning through story and conversation\" approach that doesn't shy away from maths (but doesn't dwell on them) makes it an easier read than one might think.\n\nRecommended for those interesting in understanding the _science_ (not magic) of quantum physics that make quantum computing a real life thing.\n","categories":[{"name":"finding inspiration","slug":"finding-inspiration","permalink":"https://westerndevs.com/categories/finding-inspiration/"}],"tags":[{"name":"quantum computing","slug":"quantum-computing","permalink":"https://westerndevs.com/tags/quantum-computing/"},{"name":"quantum physics","slug":"quantum-physics","permalink":"https://westerndevs.com/tags/quantum-physics/"},{"name":"finding inspiration","slug":"finding-inspiration","permalink":"https://westerndevs.com/tags/finding-inspiration/"},{"name":"book","slug":"book","permalink":"https://westerndevs.com/tags/book/"},{"name":"Mark Russinovich","slug":"Mark-Russinovich","permalink":"https://westerndevs.com/tags/Mark-Russinovich/"}]},{"title":"Azure Automation","authorId":"simon_timms","slug":"azure-automation","date":"2021-06-24 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-automation/","link":"","permalink":"https://westerndevs.com/_/azure-automation/","excerpt":"","raw":"---\ntitle:  Azure Automation\n# Powershell Gotchas\n# Example Script\nauthorId: simon_timms\ndate: 2021-06-24\noriginalurl: https://blog.simontimms.com/2021/06/24/azure-automation\nmode: public\n---\n\n\n\nAzure Automation is a service that allows running small scripts to do automation of tasks inside azure. For instance if you want to scale a database up and down depending on the time of day this is an ideal place to do it. \n\nThere are basically 3 concepts in it\n\n1. Runbook - a script that you write and publish from within Azure Automation. The supported languages include Python (2 and 3!) and powershell. There is also a graphical builder which basically just run powershell commandlets\n![](/images/2021-06-24-azure-automation.md/2021-06-24-07-05-52.png))\n2. Jobs - executions of the runbook. These can take parameters and pass them off to a runbook. The job logs what it is doing but the logging is a bit sketchy. You should consider reviewing the json output to see exactly what went wrong with your job instead of relying on the UI. \n3. Schedule - You can kick off a job at any point in time using a schedule. Schedules allow passing parameters to the jobs.\n\n## Powershell Gotchas\n\nFor some reason, likely the typical Microsoft support of legacy software, the Azure modules included in powershell by default are the old AzureRM ones and not the newer, more awesome Az modules. You can go to the module gallery to install more modules \n![](/images/2021-06-24-azure-automation.md/2021-06-24-07-11-22.png))\nHowever, little problem with that is that the module installation process doesn't handle dependencies so if you want to install something like Az.Sql which relies on Az.Account then you need to go install Az.Account first. The installation takes way longer than you'd logically expect so I sure hope you don't need to install something like Az proper which has 40 dependencies.\n\n## Example Script\n\nThis script will scale a database to the desired level\n\n```powershell\n\n\nParam(\n [string]$ResourceGroupName,\n [string]$ServerName,\n [string]$DatabaseName,\n [string]$TargetEdition,\n [string]$TargetServiceObjective\n)\n\n$connectionName = \"AzureRunAsConnection\"\ntry\n{\n    # Get the connection \"AzureRunAsConnection \"\n    $servicePrincipalConnection=Get-AutomationConnection -Name $connectionName         \n\n    \"Logging in to Azure...\"\n    Connect-AzAccount `\n        -ServicePrincipal `\n        -TenantId $servicePrincipalConnection.TenantId `\n        -ApplicationId $servicePrincipalConnection.ApplicationId `\n        -CertificateThumbprint $servicePrincipalConnection.CertificateThumbprint \n}\ncatch {\n    if (!$servicePrincipalConnection)\n    {\n        $ErrorMessage = \"Connection $connectionName not found.\"\n        throw $ErrorMessage\n    } else{\n        Write-Error -Message $_.Exception\n        throw $_.Exception\n    }\n}\n\n\necho \"Scaling the database\"\nSet-AzSqlDatabase -ResourceGroupName $ResourceGroupName -DatabaseName $DatabaseName -ServerName $ServerName -Edition $TargetEdition -RequestedServiceObjectiveName $TargetServiceObjective\necho \"Scaling complete\"\n```","categories":[],"tags":[]},{"title":"What Value Does Making Content Bring to Me? (not a typo)","authorId":"david_wesst","slug":"what-value-does-making-content-bring-to-me","date":"2021-06-17 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"inner-monologue/what-value-does-making-content-bring-to-me/","link":"","permalink":"https://westerndevs.com/inner-monologue/what-value-does-making-content-bring-to-me/","excerpt":"I realize it's a selfish question, but if the post isn't valuable to me then why would it be valuable to you?","raw":"---\ntitle: \"What Value Does Making Content Bring to Me? (not a typo)\"\ndate: \"2021-06-17T12:00:00\"\ncategories:\n- inner monologue\ntags:\n- blog\n- defining value\n- content creation\n- self-retrospective\n- validation\nexcerpt: \"I realize it's a selfish question, but if the post isn't valuable to me then why would it be valuable to you?\"\noriginalurl: https://www.davidwesst.com/blog/what-value-does-making-content-bring-to-me/\nauthorId: david_wesst\n---\n\n![A gorilla with in a pose that denotes they're thinking](/images/2021-06-17-what-value-does-making-content-bring-to-me/rob-schreckhise-8zdEgWg5JAA-unsplash_altered.jpg)\n\nThat's not to say the content should provide value to the reader, also known as you, but it's still a very important question to answer. \n\nWith the reboot of my blog and getting back into social media, this has become the first question I ask myself before writing a post, or planning a livestream, or scripting a video, or even saving a link. It's selfish, but if the post isn't valuable to me, then why would it be valuable to you?\n\nIn my previous professional life, the content I created with exclusively for the readers and for Microsoft. I made content, they awarded me an MVP award with some great perks, an awesome community, but it lead to an unforeseen need to be validated and rewarded for my content,  which I wrote about recently [LINK](https://www.davidwesst.com/blog/my-secret-addiction-to-likes/). It was a good deal and made sense at the time. But that was then, and this is now. Now when I look at creating content, first person it needs to bring value to is me. \n\n## So why bother blogging at all?\n\nI didn't. \n\nAt least I didn't for a few years and just left my blog and website to be a development science experiment. Reason being, I didn't know the value the blog could bring me.\n\nI'll leave the numerous self-driven arguments and half baked reasons I tried to give myself out of it, but ultimately it didn't make a lot of sense to keep on blogging.\n\nUntil it did again.\n\n## Sharing my Self-Retrospectives\n\nEven though I stopped blogging and stopped doing my annual self retrospectives (like this one from 2014: [LINK](https://www.davidwesst.com/blog/highlight-reel-for-2014/) and this one from 2015 [LINK](https://www.davidwesst.com/blog/highlight-reel-for-2015/)) the self-reflective process never stopped. Rather, my the self-retrospectives evolved into smaller chunks of thought that I would or share with trusted friends or family to get opinions on the deep thoughts from this inner monologue I maintained.\n\nThen a pandemic started and I was unable to share, at least not with the frequency and ease that I used to.\n\n## The Absence of Validation\n\n![Black and white image of the word 'yes' drawn in sand](/images/2021-06-17-what-value-does-making-content-bring-to-me/drahomir-posteby-mach-__Hw50q04FI-unsplash_updated.jpg)\n\nIt's the inability to share the way I was used to got me thinking differently. I started to realize that sharing was my way to get approval and a pat on the head for an idea. I didn't need to take action with my website or my social media presence, because I already had a bunch of people tell me it was a good idea. Why bother doing it when I already got it validated?\n\nAnd so I tested my theory and stopped sharing my ideas on social media. Not long after that, I stopped sharing my ideas with my trusted friends virtually (unless I had something to show, which I never did) not because I didn't want to, but because I needed to learn to do this for myself.\n\n_For me._\n\nThis means I am the first validator of the idea, and ultimately gives me that first bit of validation to approve my time investment into it. Of course the catch is, if I want futher validation, I suppose I should validate my own feelings first to make sure I'm right.\n\n## If its valuable to me, then it'll be valuable to someone else (TL;DR;)\n\nTime is my most important asset. For that reason, the first question I ask the question: \"What value does <idea> it bring to me?\" as I need to decide if the idea is worth investing my time on it. If I don't see the value in it, the idea might not be bad, but it's just not for me. My time is better spent on an idea, post, or science experiment that I think will bring me value first.\n\nBecause if it brings me value, then there it will probably bring value to someone else out there on the internet.\n\nThanks for playing.\n\n~ DW\n\n----\n##### Photo Credit\nPhoto by <a href=\"https://unsplash.com/@robschreckhise?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Rob Schreckhise</a> on <a href=\"https://unsplash.com/s/photos/thinking?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n\nPhoto by <a href=\"https://unsplash.com/@postebymach?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">DrahomÃ­r Posteby-Mach</a> on <a href=\"https://unsplash.com/s/photos/approved?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n  \n\n\n\n\n\n\n\n\n\n\n","categories":[{"name":"inner monologue","slug":"inner-monologue","permalink":"https://westerndevs.com/categories/inner-monologue/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://westerndevs.com/tags/blog/"},{"name":"defining value","slug":"defining-value","permalink":"https://westerndevs.com/tags/defining-value/"},{"name":"content creation","slug":"content-creation","permalink":"https://westerndevs.com/tags/content-creation/"},{"name":"self-retrospective","slug":"self-retrospective","permalink":"https://westerndevs.com/tags/self-retrospective/"},{"name":"validation","slug":"validation","permalink":"https://westerndevs.com/tags/validation/"}]},{"title":"Getting started with Storybook and Vue","authorId":"simon_timms","slug":"storybook","date":"2021-06-17 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/storybook/","link":"","permalink":"https://westerndevs.com/_/storybook/","excerpt":"","raw":"---\ntitle:  Getting started with Storybook and Vue\n# Adding Vuetify\nauthorId: simon_timms\ndate: 2021-06-17\noriginalurl: https://blog.simontimms.com/2021/06/17/storybook\nmode: public\n---\n\n\n\n1. Starting with an empty folder you can run \n    ```\n    npx sb init\n    ```\n2. During the addition you'll be prompted for the template type - select vue\n3. If this is brand new then you'll need to install vue. The template assumes you have it installed already. \n    ```\n    npm install vue vue-template-compiler\n    ```\n4. Run storybook with \n\n    ```\n    npm run storybook\n    ```\nThis will get storybook running and you'll be presented with the browser interface for it \n![](/images/2021-04-27-storybook.md/2021-04-27-12-17-47.png))\n\n## Adding Vuetify\n\n1. In the project install vuetify\n   ```\n   npm install vuetify\n   ```\n2. In the `.storybook` folder add a `preview-head.html` file. This will be included in the project template. Set the content to \n\n    ```\n    <link href=\"https://cdn.jsdelivr.net/npm/@mdi/font@4.x/css/materialdesignicons.min.css\" rel=\"stylesheet\">\n    <link href=\"https://cdn.jsdelivr.net/npm/vuetify@2.x/dist/vuetify.min.css\" rel=\"stylesheet\">\n    ```\n\n3. Create a new file called `vuetify_storybook.js` and add to it \n\n```javascript\nimport Vue from 'vue';\nimport Vuetify from 'vuetify'; // loads all components\nimport 'vuetify/dist/vuetify.min.css'; // all the css for components\nimport en from 'vuetify/es5/locale/en';\n\nVue.use(Vuetify);\n\nexport default new Vuetify({\n    lang: {\n        locales: { en },\n        current: 'en'\n    }\n});\n```\n4. In the `.storybook` folder add to the `preview.js` and include \n\n    ```\n    import { addDecorator } from '@storybook/vue';\n    import vuetify from './vuetify_storybook';\n\n    addDecorator(() => ({\n    vuetify,\n    template: `\n        <v-app>\n        <v-main>\n            <v-container fluid >\n            <story/>\n            </v-container>\n        </v-main>\n        </v-app>\n        `,\n    }));\n    ```\n    This will add vuetify wrapping to the project. You can now just go ahead and us the components in your .vue files. Here is an example:\n    ```\n    <template>\n        <div>\n            <v-text-field dense label=\"User name\" hint=\"You can use your email\"></v-text-field>\n            <v-text-field dense label=\"Password\" hint=\"You need to use upper case and lower case\"></v-text-field>\n        </div>\n    </template>\n    <script>\n    module.exports = {\n        data: function () {\n            return {\n            userName: null,\n            password: null,\n            rememberMe: false,\n            };\n        },\n        computed: {\n            isValid: function () {\n            return true;\n            },\n        },\n    };\n    </script>\n    ```\n\n    ## Networking\n\n    If you're using a service layer then you an shim that in to prevent making network calls. However that might not be what you want to do so you can instead shim in something to intercept all network calls. This can be done using the mock service worker addon https://storybook.js.org/addons/msw-storybook-addon\n\n    To get it working install it \n    ```\n    npm i -D msw msw-storybook-addon\n    ```\n\n    Then to the preview.js file you can add a hook for it\n\n    ```\n    import { initializeWorker, mswDecorator } from 'msw-storybook-addon';\n\n    initializeWorker();\n    addDecorator(mswDecorator);\n    ```\n    ","categories":[],"tags":[]},{"title":"Quick Noda Time Conversions","authorId":"simon_timms","slug":"noda-time","date":"2021-06-16 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/noda-time/","link":"","permalink":"https://westerndevs.com/_/noda-time/","excerpt":"","raw":"---\ntitle:  Quick Noda Time Conversions\n# Convert a DateTime and TzDB Timezone to UTC\n# Convert from a UTC to a zoned DateTime\nauthorId: simon_timms\ndate: 2021-06-16\noriginalurl: https://blog.simontimms.com/2021/06/16/noda-time\nmode: public\n---\n\n\n\nNoda time makes working with timezones, well not a snap but better than dental surgery. \n\n## Convert a DateTime and TzDB Timezone to UTC\n\nA TzDB timezone is one that looks like `America/Edmonton` or, one might presume `Mars/OlympusMons`\n\n```\nDateTimeZone timezone = DateTimeZoneProviders.Tzdb.GetZoneOrNull(timezoneId);\nZoneLocalMappingResolver customResolver = Resolvers.CreateMappingResolver(Resolvers.ReturnLater, Resolvers.ReturnStartOfIntervalAfter);\nvar localDateTime = LocalDateTime.FromDateTime(dateTime);\nvar zonedDateTime = timezone.ResolveLocal(localDateTime, customResolver);\nreturn zonedDateTime.ToDateTimeUtc();\n```\n\n## Convert from a UTC to a zoned DateTime\n\n```\n var local = new LocalDateTime(dateTime.Year, dateTime.Month, dateTime.Day, dateTime.Hour, dateTime.Minute, dateTime.Second);\nvar tz = DateTimeZoneProviders.Tzdb[timeZoneID];\nreturn local.InZoneLeniently(tz);\n```\n\nBut be careful with this one because it might produce weird results around time change periods. If you want to avoid ambiguity or at least throw an exception for it consider `InZoneStrictly`","categories":[],"tags":[]},{"title":"Installing Fonts on Windows with Powershell","authorId":"simon_timms","slug":"installing-fonts","date":"2021-06-11 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/installing-fonts/","link":"","permalink":"https://westerndevs.com/_/installing-fonts/","excerpt":"","raw":"---\ntitle:  Installing Fonts on Windows with Powershell\nauthorId: simon_timms\ndate: 2021-06-11\noriginalurl: https://blog.simontimms.com/2021/06/11/installing-fonts\nmode: public\n---\n\n\n\nYou'd like to think that in 2021 installing a font would involve just copying it and some advanced AI system would notice it and install it on Windows. Again the future has failed us. \n\nLet's say you have a folder of TTF fonts you need installing. Just copying them to the `c:\\windows\\fonts` directory won't work. You need to copy them with a magic COM command that is probably left over from when file names in Windows looked like `PROGRA~1`. I've seen some scripts which add the font to the windows registry but I didn't have much luck getting them to work and they feel fragile should Microsoft ever update font handling (ha!). \n\nHere is a script that will copy over all the fonts in the current directory. \n\n```powershell\necho \"Install fonts\"\n$fonts = (New-Object -ComObject Shell.Application).Namespace(0x14)\nforeach ($file in gci *.ttf)\n{\n    $fileName = $file.Name\n    if (-not(Test-Path -Path \"C:\\Windows\\fonts\\$fileName\" )) {\n        echo $fileName\n        dir $file | %{ $fonts.CopyHere($_.fullname) }\n    }\n}\ncp *.ttf c:\\windows\\fonts\\\n```\n\nThe fonts don't seem to get installed using the same file name as they arrive with so that last `cp` line puts the original files in the fonts directory so you can run this script multiple times and it will just install the new fonts. If you wanted to get cool you could check for a checksum and install fonts where the checksum doesn't match. Don't both trying to use `CopyHere` with the flag `0x14` thinking it will overwrite fonts. That doesn't work for the font directory.\n\nIf you want to check and see which fonts are visible to .NET on the system then you can try \n\n```powershell\n[void] [System.Reflection.Assembly]::LoadWithPartialName(\"System.Drawing\")\n(New-Object System.Drawing.Text.InstalledFontCollection).Families\n```","categories":[],"tags":[]},{"title":"Transport for Azure Service Bus","authorId":"simon_timms","slug":"Azure-service-bus-transports","date":"2021-06-07 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Azure-service-bus-transports/","link":"","permalink":"https://westerndevs.com/_/Azure-service-bus-transports/","excerpt":"","raw":"---\ntitle:  Transport for Azure Service Bus\n# .NET Code\n# Azure Functions\nauthorId: simon_timms\ndate: 2021-06-07\noriginalurl: https://blog.simontimms.com/2021/06/07/Azure-service-bus-transports\nmode: public\n---\n\n\n\nThere are two transport mechanisms for service bus \n* AQMP\n* AQMP over web sockets\n\nThe default is to use plain AQMP but this uses port 5671. Often times this port may be blocked by firewalls. You can switch over to using the websocket based version which uses port 443 - much more commonly open already on firewalls. \n\n## .NET Code\n\nYou just need to update the `TransportType` in the service bus set up\n\n```\nvar client = new ServiceBusClient(Configuration[\"ServiceBusConnection\"], new ServiceBusClientOptions\n{\n    TransportType = ServiceBusTransportType.AmqpWebSockets\n});\n```\n\n## Azure Functions\n\nThe simplest way of getting websockets to work on functions is to update the connection string to mention it\n\n```\nEndpoint=sb://someendpoint.servicebus.windows.net/;SharedAccessKeyName=SenderPolicy;SharedAccessKey=asecretkey;TransportType=AmqpWebSockets\n```","categories":[],"tags":[]},{"title":"Add user to role in sql server","authorId":"simon_timms","slug":"add-user-to-role","date":"2021-06-07 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/add-user-to-role/","link":"","permalink":"https://westerndevs.com/_/add-user-to-role/","excerpt":"","raw":"---\ntitle:  Add user to role in sql server\nauthorId: simon_timms\ndate: 2021-06-07\noriginalurl: https://blog.simontimms.com/2021/06/07/add-user-to-role\nmode: public\n---\n\n\n\nThis can be done with \n\n```\nsp_addrolemember @rolename = 'role', @membername = 'security_account'\n```\n\nexample\n\n```\nsp_addrolemember @rolename = 'db_owner', @membername = 'evil_hacker_account'\n```\n\nanother example\n\n```\nsp_addrolemember @rolename = 'db_datareader', @membername = 'datafactory'\n```\n\nand another \n\n```\nsp_addrolemember @rolename = 'db_datawriter', @membername = 'asca_webapp'\n```\n\nBuilt in database roles are \n\n**db_owner** Members of the db_owner fixed database role can perform all configuration and maintenance activities on the database, and can also drop the database in SQL Server. (In SQL Database and Azure Synapse, some maintenance activities require server-level permissions and cannot be performed by db_owners.)\n**db_securityadmin** Members of the db_securityadmin fixed database role can modify role membership for custom roles only and manage permissions. Members of this role can potentially elevate their privileges and their actions should be monitored.\n**db_accessadmin** Members of the db_accessadmin fixed database role can add or remove access to the database for Windows logins, Windows groups, and SQL Server logins.\n**db_backupoperator** Members of the db_backupoperator fixed database role can back up the database.\n**db_ddladmin** Members of the db_ddladmin fixed database role can run any Data Definition Language (DDL) command in a database.\n**db_datawriter** Members of the db_datawriter fixed database role can add, delete, or change data in all user tables.\n**db_datareader** Members of the db_datareader fixed database role can read all data from all user tables and views. User objects can exist in any schema except sys and INFORMATION_SCHEMA.\n\n**db_denydatawriter** Members of the db_denydatawriter fixed database role cannot add, modify, or delete any data in the user tables within a database.\n\n**db_denydatareader** Members of the db_denydatareader fixed database role cannot read any data from the user tables and views within a database.","categories":[],"tags":[]},{"title":"My Secret Addiction to Likes","authorId":"david_wesst","slug":"my-secret-addiction-to-likes","date":"2021-06-03 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"inner-monologue/my-secret-addiction-to-likes/","link":"","permalink":"https://westerndevs.com/inner-monologue/my-secret-addiction-to-likes/","excerpt":"I decided to take a month off of contributing to social media. More specifically Twitter and YouTube, but Facebook and Instagram as well. That single month has turned into just over three months now, and I learned a few things about myself and how I think my return to some social media will be a good thing.","raw":"---\ntitle: \"My Secret Addiction to Likes\"\ndate: \"2021-06-03T12:00:00\"\ncategories:\n- inner monologue\ntags:\n- social media\n- twitter\n- youtube\n- analytics\n- acceptance\nexcerpt: I decided to take a month off of contributing to social media. More specifically Twitter and YouTube, but Facebook and Instagram as well. That single month has turned into just over three months now, and I learned a few things about myself and how I think my return to some social media will be a good thing.\nauthorId: david_wesst\noriginalurl: https://www.davidwesst.com/blog/my-secret-addiction-to-likes/\n---\n\nI decided to take a month off of contributing to social media. More specifically Twitter and YouTube, but Facebook and Instagram as well. That single month has turned into just over three months now, and I learned a few things about myself and how I think my return to _some_ social media will be a good thing.\n\n# Where it started...\nI'm a millennial. Which means, I've been doing social media since before it was called \"social media\". I got onto Twitter very early on, I needed a university email address to create my Facebook account, and YouTube...well, it wasn't owned by Google.\n\nThe point being is that social media has been a part of my entire adult life. I actually can't rememberof a time in my professional life that someone wasn't telling me that my social media presence or \"brand\" had the ability to propel my career forward, if I played my tweets just right.\n\nIt is probably that every job I had in tech was surrounded by marketing people, but I developed this bizarre obsession over \"my brand\" or \"persona\" that I portrayed to the online world. For years I have gone through exercises about how to build followers, read the analytics, all on a mission to appear professional.\n\nIt wasn't a fruitless venture. I'm pretty sure my blog and Twitter account secured my many MVP awards. What I wasn't expecting was the dependency this obsession created, which was this weird addiction to \"likes\". \n\n# \"Likes\" = Validation\nYou see, the MVP program was how I validated doing all the extra work on keeping up with technology. I love tech, I really really do, but had someone not pointed me in the direction of building a brand around JavaScript and/or Front-End Web Dev, I probably never would have gone down that road. As I built up that persona, th more likes and engagement I got, and the more likely the MVP Program would notice me. Eventually they did notice me and _TA-DA_, I became a Microsoft MVP.\n\nThe MVP Award was where I think this all started. It was a reward for being so...professional or knowledgable or hard-working in my B-time or whatever. I loved it and somehow rationalized that people get directly rewarded for their side efforts. I suppose that is sort of true, depending on how to define the term \"reward\", but in general I don't think it's as big of a perk as the MVP Award and all the benefits that come with it.\n\nAs the years went by, I started to expect that sort of reward for my effort and equated with validation and started to _need_ it in order feel like I was succeeding as a technology professional.\n\nThen, I decided to let go of the MVP Award and chase my dreams.\n\n# Validation Withdrawl\nThis is the part in the story where I started chase my game development hopes and dreams. It started out well, but not long after doing some game development streams on Mixer (yeah, remember Mixer?) and some blog posts, I started to feel uneasy about my ability and my \"success\" as a technology professional.\n\nBuilding an expertise takes time and effort. It takes even more time and effort when you day job doesn't care or need that expertise, and you have a new family to take care of. I kept getting caught up on how long it would take for me to \"become a professional\" or whatever. I kept checking my different social media analytics and started focusing a lot of time and energy on making game development content rather than actual games. \n\nI resarched marketing techniques, read social media management guides, and started learning how to promote my \"dream game\" before I had even really done anything other than a couple of game jams. I checked the \"likes\" multiple times a day and tried to figure out how to maximize the reach of my content, continuing to get in the way of building an actual commercial video game, but searching (somewhat desperately) for that acknowledgement through likes, thumbs up, post engagement, and views.\n\nIt kept coming and going, but it would always block my progress on whatever project I was working on. My game jams were about the content I produced, not the game itself. After a jam, I would share and talk about \"the next steps\" and all the planning I was doing instead of _actually doing_ something with the project. No matter how much time I spent, there wasn't enough to both \"share to the community\" and build a game.\n\nIt was an old habit that needed to go away, and so earlier this year I just stopped sharing on social media.\n\nThat break was supposed last about month. That was about three months ago.\n\n# Realization and Return\nIn my three month break, I looked inward and thought about what I've done with social media over...well, most of my professional life. I've decided that it's time to start figuring out how or if I should return to the social networks, but I'm taking it slow and flipping the script on my social media shares.\n\nRather than measuring my successwith likes and views, I'm looking at the social platforms as ways for me to grow personally and professionally. I'm asking myself two questions:\n\n1. What does my contribution do for the reader/viewer/you?\n2. What does my contribution do for me?\n\nDoes v7 of my website coincide with this? It sure does. \n\nI'll elaborate further another day, but just writing this post helps me reflect on my own story. It feels honest and healthy to write all this down and I'm creating content for both  _me_ and the readers. I share not only because I crave validation, but also think that my sharing my experience might help others learn something.\n\nThis is site and blog is the start. It has a purpose for both my personal and professional growth and so it is alive again.\n\n## Aren't you just going to obsess over the analytics again?\nI don't think so. I have analytics enabled on the site, but I have purpose for this: to learn. More specifically, I want to learn about what analytics can teach me about my audience. It's not just about views and the likes, a but what the readers (and players) are telling me through their engagement.\n\nPlus-- whether I like it or not, analytics plays a critical role in decision making these days. I see it in my day job, and I see it in game development. Either way, it's probably having some literacy around the different kinds of analytics out there can't hurt me. \n\nThe trick is not wrapping success around the metrics. \n\n# Next Steps\n\nI think it's going to be YouTube and other video content like livestreams. I really enjoy making my little movies, and with the pandemic in full swing, it's hampered my ability to practice my presentation skills at conferences with an audience. Between platforms like YouTube, LinkedIn, Twitch , and even Discord, there are some good opportunities to sharpen my video presentation skills.\n\nIf I'm being honest, I can't see Twitter or Facebook making a comeback in my day-to-day life. Possibly a place to echo posts or share activity, but I'm just not feeling the \"hot takes\" nature that comes with Twitter and Facebook. As for Instagram...I'm still undecided. I don't have a lot of pictures to share behind the scenes, but again-- never say never. \n\nRegardless of where I share content, the website will be hub and the question will the same: What does sharing do to help me grow personally and/or professionally?\n\nThanks for playing. ~ DW\n\n---\n\n_Photo Credit_\n\nPhoto by <a href=\"https://unsplash.com/@neonbrand?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">NeONBRAND</a> on <a href=\"https://unsplash.com/s/photos/likes?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n\n \n\n","categories":[{"name":"inner monologue","slug":"inner-monologue","permalink":"https://westerndevs.com/categories/inner-monologue/"}],"tags":[{"name":"social media","slug":"social-media","permalink":"https://westerndevs.com/tags/social-media/"},{"name":"twitter","slug":"twitter","permalink":"https://westerndevs.com/tags/twitter/"},{"name":"youtube","slug":"youtube","permalink":"https://westerndevs.com/tags/youtube/"},{"name":"analytics","slug":"analytics","permalink":"https://westerndevs.com/tags/analytics/"},{"name":"acceptance","slug":"acceptance","permalink":"https://westerndevs.com/tags/acceptance/"}]},{"title":"Sequences","authorId":"simon_timms","slug":"sequence","date":"2021-06-03 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/sequence/","link":"","permalink":"https://westerndevs.com/_/sequence/","excerpt":"","raw":"---\ntitle:  Sequences\nauthorId: simon_timms\ndate: 2021-06-03\noriginalurl: https://blog.simontimms.com/2021/06/03/sequence\nmode: public\n---\n\n\n\nSequences are a handy feature in SQL server which provide an increasing, unique number. You wouldn't typically use them directly but might use them under the covers in an `identity`. However from time to time they are useful when you need numbers but your primary key is a `uniqueidentifier` or you need two different ways of numbering records. I've been using them to associate records in a table into groups. \n\n```\ncreate SEQUENCE Seq_PermitNumber \n    start with 1 \n    increment by 1\n```\n\nYou can then use them like this\n\n```\nupdate tblManualPayment \n   set PermitNumber = next value for Seq_PermitNumber \n where PermitNumber is null\n```\n\nThis will give each record a unique permit number. ","categories":[],"tags":[]},{"title":"Lucky Number v7","authorId":"david_wesst","slug":"lucky-number-v7","date":"2021-05-24 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"technology/inner-monologue/lucky-number-v7/","link":"","permalink":"https://westerndevs.com/technology/inner-monologue/lucky-number-v7/","excerpt":"I fixed up my blog-website-thingy again. I'm calling this v7, although I'm pretty sure there are a few incarnations of the site that were lost early on in my years, but v7 seems about right for the content I'm sharing here. In any case-- I thought I'd share the \"why\" around the new website revamp and how I think a project like this is just what I need.","raw":"---\n\ntitle: \"Lucky Number v7\"\ndate: \"2021-05-24T12:00:00\"\ncategories:\n- technology\n- inner monologue\ntags:\n- blog\n- website\n- graphql\nexcerpt: I fixed up my blog-website-thingy again. I'm calling this v7, although I'm pretty sure there are a few incarnations of the site that were lost early on in my years, but v7 seems about right for the content I'm sharing here. In any case-- I thought I'd share the \"why\" around the new website revamp and how I think a project like this is just what I need.\nauthorId: david_wesst\noriginalurl: https://www.davidwesst.com/blog/lucky-number-v7/\n---\n\nI fixed up my blog-website-thingy again. I'm calling this v7, although I'm pretty sure there are a few incarnations of the site that were lost early on in my years, but v7 seems about right for the content I'm sharing here. In any case-- I thought I'd share the \"why\" around the new website revamp and how I think a project like this is just what I need.\n\n# Why did you stop blogging, anyway?\nI've been thinking a lot about that lately, and the reason I stopped was that I didn't have a reason to do it anymore. \n\nI started blogging because I was told that it was one of the things I _should be doing_ and had to _keep doing_ to get (and stay) in the Microsoft MVP program as a Front End Web Development MVP (formerly known as the Internet Explorer MVP program). Once I decided to leave the Front End Web Dev stuff behind to make way for my passion for video games and stuff, why should I bother blogging?\n\nAnd so that was that. I stopped blogging and made way for all the video game development effort I could muster! Sure, there was the occassional link to a YouTube video or a little tech problem that I've solved on the side, but for the most part, it was dead.\n\n0 commercial games later, and a few false restarts I'm back on the blog and the website...again. \n\n# So why bother starting it up again?\nWell, I stopped because I gave up my reason to do so. I am starting because I found a reason: I want to share and learn.\n\nLet's break that down a little:\n\n## \"I want to\"\nThat is probably the most important part of the whole reason. I want to make content. I'm feeling the need to do it. Maybe its habit or nostalgia for the days where creating content was part of keeping my professional status, either way it's something I like to do and the blog is a lightweight and easy way to do that.\n\n## \"share\"\nThinking back to when I blogged regularly as part of my MVP contribution, I didn't realize how much of a platform I had to share stories, advice, how-tos and whatever else. It really was quite the reach, and quite a privilege. My experience and work has changed a lot, but there is a still a lot to share for others. \n\nThe big difference this time around is that I'm not worried about whether or not it fits \"my career goals\" or my \"professional focus\".  It's really about me putting my thoughts together in a cohesive way that might help someone reflect and make a decision. In getting v7 of the website ready _I was that \"someone\"_ and looking over all my old posts confirmed that this is a good idea.\n\nBut, even if you (or me) don't find my posts helpful in the long or short term, that's okay. It feels good to get a post out there. There's a gratifying feeling that comes with putting a post together that helps me, and that makes it worth it too.\n\n## \"learn\"\nAs clichÃ© as it sounds, I'm a life long learner. Content creation and management is something that keeps coming up in my side projects, and yet I've never taken the time to properly learn and understand how to do it. It's not about the marketing side, but rather what it means to contribute, learn, and engage with your audience as a solo content creator. \n\nPlus, there are these weird \"little problems\" I've always had in my years of creating content with with MVP program. I'm hoping that with fresh eyes and new experiences with me, I can tackle these problems with a different perspective that I have in the past. \n\nUltimately, there is a lot to unpack here-- but assuming I keep this going, I'll continue to share what I learn, which will lead to more sharing and then more learning and...you see how this is good thing? :)\n\n## The Point\nScott Hanselman shared this idea [(reference)](https://www.hanselman.com/blog/do-they-deserve-the-gift-of-your-keystrokes) of valuing your effort in helping through your keystrokes (I'm greatly paraphrasing the idea). I spend time helping others one-on-one, but it's usually the same stories, ideas, thought patterns, and so on that people find helpful. This website and blog is my chance to share new things and old with a fresh perspective, one that is owned and driven by my values and ideas and not those of my employer or community.\n\nMaybe its selfish to think this is a good idea, but that's fine because it's my website, my blog, and my idea that I think is good. \n\nAnd if I think it's good, then that's a start.\n\nThanks for playing. ~ DW\n\n","categories":[{"name":"technology","slug":"technology","permalink":"https://westerndevs.com/categories/technology/"},{"name":"inner monologue","slug":"technology/inner-monologue","permalink":"https://westerndevs.com/categories/technology/inner-monologue/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://westerndevs.com/tags/blog/"},{"name":"website","slug":"website","permalink":"https://westerndevs.com/tags/website/"},{"name":"graphql","slug":"graphql","permalink":"https://westerndevs.com/tags/graphql/"}]},{"title":"Using Durable Entities","authorId":"simon_timms","slug":"durable-entities","date":"2021-05-20 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/durable-entities/","link":"","permalink":"https://westerndevs.com/_/durable-entities/","excerpt":"","raw":"---\ntitle:  Using Durable Entities\n# Gotchas\nauthorId: simon_timms\ndate: 2021-05-20\noriginalurl: https://blog.simontimms.com/2021/05/20/durable-entities\nmode: public\n---\n\n\n\nDurable entities are basically blobs of state that are stored somewhere (probably table storage). You can retrieve them and signal them with changes. They can be tied directly into standard Azure functions. \n\nYou build one as pretty much a POCO that looks like \n\n```csharp\n[JsonObject(MemberSerialization.OptIn)]\npublic class DuplicatePreventor\n{\n    [JsonProperty(\"value\")]\n    public int CurrentValue { get; set; };\n\n    public void Add(int amount) => this.CurrentValue += amount;\n\n    public void Reset() => this.CurrentValue = 0;\n\n    public int Get() => this.CurrentValue;\n\n    [FunctionName(nameof(DuplicatePreventor))]\n    public static Task Run([EntityTrigger] IDurableEntityContext ctx)\n        => ctx.DispatchAsync<DuplicatePreventor>();\n} \n```\n\nIn this example there is one piece of state: the CurrentValue. You can retrieve it using the Get() function. Add and Reset are other signals you can send to the state. \n\nUsing it in a function involves adding a client to the signature of the function like so \n\n```csharp\n[FunctionName(\"ShopifyPurchaseWebhook\")]\npublic static async Task<IActionResult> Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)] HttpRequest req,\n    [DurableClient] IDurableEntityClient client,\n    ILogger log)\n{\n        ...\n}\n```            \n\nOnce you have the client you can retrieve an existing state by specifying an entityId and then getting it from the client\n```csharp\nvar entityId = new EntityId(nameof(DuplicatePreventer), webhook.order_number.ToString());\nvar duplicationPreventionEntity = await client.ReadEntityStateAsync<DuplicatePreventer>(entityId);\n```\n\nThis gets you back a wrapper which includes properties like `EntityExists` and `EntityState`. \n\nYou can signal changes in the entity through an unfortunate interface that looks like \n\n```\nawait client.SignalEntityAsync(entityId, \"Add\", 1);\n```\n\nThat's right, strings are back in style. \n\n## Gotchas\n\nIf you create the durable entity in your function and then request it's value you at once you won't get the correct value - you just get null. I'd bet they are using some sort of outbox model that only sends data updates at the end of the function execution. ","categories":[],"tags":[]},{"title":"Advanced Web Application Firewall Rules in Azure with Terraform","authorId":"simon_timms","slug":"advanced-waf-rules-in-terraform","date":"2021-05-19 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/advanced-waf-rules-in-terraform/","link":"","permalink":"https://westerndevs.com/_/advanced-waf-rules-in-terraform/","excerpt":"","raw":"---\ntitle:  Advanced Web Application Firewall Rules in Azure with Terraform\n## Web application firewall settings\nauthorId: simon_timms\ndate: 2021-05-19\noriginalurl: https://blog.simontimms.com/2021/05/19/advanced-waf-rules-in-terraform\nmode: public\n---\n\n\n\nIf you're creating an Application Gateway in Terraform for Azure you're using this resource `azurerm_application_gateway`. This resource allows for some basic configuration of the Web Application Firewall through the `waf_configuration` block. However the configuration there is very limited and basically restricted to turning it off and on and choosing the base rule set. If you want a custom rule then you need to break off the rules into a separate `azurerm_web_application_firewall_policy`. This can then be referenced back in the `azurerm_application_gateway` through the `firewall_policy_id`\n\nYou can use the advanced rules to set up things like Geographic restrictions. For instance this set of rules will block everything but requests from Canada and the US.\n\n```\n### Web application firewall settings\nresource \"azurerm_web_application_firewall_policy\" \"appfirewall\" {\n  name                = local.basename\n  resource_group_name = var.resource_group_name\n  location            = var.resource_group_location\n\n  custom_rules {\n    name      = \"OnlyUSandCanada\"\n    priority  = 1\n    rule_type = \"MatchRule\"\n\n    match_conditions {\n      match_variables {\n        variable_name = \"RemoteAddr\"\n      }\n      operator           = \"GeoMatch\"\n      negation_condition = true\n      match_values       = [\"CA\", \"US\"]\n    }\n    action = \"Block\"\n  }\n\n  policy_settings {\n    enabled = true\n    mode    = \"Detection\"\n    # Global parameters\n    request_body_check          = true\n    max_request_body_size_in_kb = 128\n    file_upload_limit_in_mb     = 100\n  }\n}\n```","categories":[],"tags":[]},{"title":"Importing an Encrypted Backup into Azure Managed SQL","authorId":"simon_timms","slug":"importing-tde-encrypted-backup","date":"2021-05-18 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/importing-tde-encrypted-backup/","link":"","permalink":"https://westerndevs.com/_/importing-tde-encrypted-backup/","excerpt":"","raw":"---\ntitle:  Importing an Encrypted Backup into Azure Managed SQL\nauthorId: simon_timms\ndate: 2021-05-18\noriginalurl: https://blog.simontimms.com/2021/05/18/importing-tde-encrypted-backup\nmode: public\n---\n\n\n\nLet's say you're moving an encrypted backup into Azure. The encryption was set up like this \n\n```SQL\nCREATE CERTIFICATE BackupKey   \n   ENCRYPTION BY PASSWORD = 'a password that''s really strong here'  \n   WITH SUBJECT = 'test1backup',   \n   EXPIRY_DATE = '20220101';  \nGO  \n```\n\nNow we need to export this certificate which can be done with \n\n```sql\nBACKUP CERTIFICATE BackupKey TO FILE = 'c:\\temp\\backupkey.cer'\nWITH PRIVATE KEY (\n\tFILE = 'c:\\temp\\backupkey.pvk',\n\tDECRYPTION BY PASSWORD = 'a password that''s really strong here',\n\tENCRYPTION BY PASSWORD = 'A strong password for the certificate' )\n```\n\nNow we have two file which contain the public and private keys. We need to combine these into something that Azure Key Vault can understand and this something is a .pfx file. There is a tool called `pvk2pfx` which can be used for this task and it is found in the Windows Enterprise Driver Kit https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk. It is also installed as part of visual studio. On my machine it was in `C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x86\\pvk2pfx.exe`\n\nRun this command to combine them\n\n```powershell\n& \"C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x86\\pvk2pfx.exe\" -pvk C:\\temp\\backupkey.pvk -pi 'A strong password for the certificate' -spc C:\\temp\\backupkey.cer -pfx c:\\temp\\backupkey.pfx\n```\n\nNext up we need to import this key into azure keyvault. This can be done using the GUI or the command line tools. Everybody likes a pretty picture so let's use the Portal. Click into the key vault and then under certificates\n![](/images/2021-05-18-importing-tde-encrypted-backup.md/2021-05-18-14-00-09.png))\n\nThen click on `Generate/Import` and fill in the form there selecting the `.pfx` file created above.\n![](/images/2021-05-18-importing-tde-encrypted-backup.md/2021-05-18-12-55-32.png))\n\nThe password will be the same one you used when exporting from SQL server. Once the certificate is imported it should be available to anybody or any application with access to certificates in key vault.\n\nYou can open up SQL Server Management Studio and in there add a new certificate selecting the certificate from the Key Vault connection\n\n![](/images/2021-05-18-importing-tde-encrypted-backup.md/2021-05-18-13-58-27.png))","categories":[],"tags":[]},{"title":"JQ","authorId":"simon_timms","slug":"jq","date":"2021-05-11 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/jq/","link":"","permalink":"https://westerndevs.com/_/jq/","excerpt":"","raw":"---\ntitle:  JQ\nauthorId: simon_timms\ndate: 2021-05-11\noriginalurl: https://blog.simontimms.com/2021/05/11/jq\nmode: public\n---\n\n\n\nThis is a really nice tool for manipulating JSON on the command line. The syntax is, however, esoteric like you would not believe. Here are some cheats to help out\n\nIf you have an array and want to take just the object at a specific index\n\n```\n.[3]\n```\nwhich returns the 3rd element\n\nIf you want to extract a value from an array of objects then you can use\n```\n.[].LicensePlate\n```\n\nThis works for multiple levels too so if you have nested objects you can \n\n```\n.[].LicensePlate.Province\n```\n\nGiven an array where you want to filter it then you can use this\n```\n[ .[] | select( .LicensePlate | contains(\"PM184J\")) ] \n```\n\nTo select a single field you could then do \n\n```\n[ .[] | select( .LicensePlate | contains(\"PM184J\")) ] |  map( .LicensePlate)\n```\n\nIf you want multiple fields built back into an object do \n\n```\n{LicensePlate: .[].LicensePlate, EndTime: .[].EndTime}\n```\n\n","categories":[],"tags":[]},{"title":"Creating a Shortcut in Powershell","authorId":"simon_timms","slug":"creating-a-shortcut","date":"2021-05-10 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/creating-a-shortcut/","link":"","permalink":"https://westerndevs.com/_/creating-a-shortcut/","excerpt":"","raw":"---\ntitle:  Creating a Shortcut in Powershell\nauthorId: simon_timms\ndate: 2021-05-10\noriginalurl: https://blog.simontimms.com/2021/05/10/creating-a-shortcut\nmode: public\n---\n\n\n\nYou can't really create a shortcut in powershell directly but you can using the windows script host from powershell. For instance here is how you would create a new desktop icon to log the current user off. \n\n```powershell\n$WshShell = New-Object -comObject WScript.Shell\n$Shortcut = $WshShell.CreateShortcut(\"$home\\Desktop\\LogOff.lnk\")\n$Shortcut.TargetPath=\"C:\\Windows\\System32\\shutdown.exe\"\n$Shortcut.Arguments=\"/l\"\n$Shortcut.IconLocation=\"C:\\windows\\system32\\Shell32.dll,44\"\n$Shortcut.Save()\n```\n\nThe icon here is taken from the long list of icons in `Shell32.dll` in this case it is the little orange key icon. These icons are going to be refreshed soon so your mileage may vary on them. I found the right icon by just google image searching `shell32.dll icon` and found a picture of some of the index numbers. They were 1 indexed so I had to subtract 1\n\n![](/images/2021-05-10-creating-a-shortcut.md/2021-05-10-11-39-21.png))","categories":[],"tags":[]},{"title":"Setting Timezone from Powershell","authorId":"simon_timms","slug":"set-timezone","date":"2021-05-10 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/set-timezone/","link":"","permalink":"https://westerndevs.com/_/set-timezone/","excerpt":"","raw":"---\ntitle:  Setting Timezone from Powershell\nauthorId: simon_timms\ndate: 2021-05-10\noriginalurl: https://blog.simontimms.com/2021/05/10/set-timezone\nmode: public\n---\n\n\n\nThis is pretty easy. \n\n```powershell\nSet-Timezone -Id \"US Eastern Standard Time\"\n```\n\nYou need to know the id of the timezone and you can figure that out using \n\n```powershell\nGet-Timezone -ListAvailable\n```\n\n```\nId                         : Dateline Standard Time\nDisplayName                : (UTC-12:00) International Date Line West\nStandardName               : Dateline Standard Time\nDaylightName               : Dateline Daylight Time\nBaseUtcOffset              : -12:00:00\nSupportsDaylightSavingTime : False\n\nId                         : UTC-11\nDisplayName                : (UTC-11:00) Coordinated Universal Time-11\nStandardName               : UTC-11\nDaylightName               : UTC-11\nBaseUtcOffset              : -11:00:00\nSupportsDaylightSavingTime : False\n...\n```\n\nYou can also see the current timezone by running \n\n```powershell\nGet-Timezone\n```\n\n```\nId                         : Mountain Standard Time\nDisplayName                : (UTC-07:00) Mountain Time (US & Canada)\nStandardName               : Mountain Standard Time\nDaylightName               : Mountain Daylight Time\nBaseUtcOffset              : -07:00:00\nSupportsDaylightSavingTime : True\n```","categories":[],"tags":[]},{"title":"Create or Update Index","authorId":"simon_timms","slug":"drop-existing-index","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/drop-existing-index/","link":"","permalink":"https://westerndevs.com/_/drop-existing-index/","excerpt":"","raw":"---\ntitle:  Create or Update Index\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/drop-existing-index\nmode: public\n---\n\n\n\nOf course the SQL server syntax for this doesn't quite jive with what I want but you can use the clause ` WITH (DROP_EXISTING = ON)` to have SQL server handle updating an existing index keeping the old index live until the new version is ready. You use it like \n\n```sql\nCREATE NONCLUSTERED INDEX idxMonthlyParkers_vendor_expiry_issue\nON [dbo].[tblParkers] ([VendorId],[LotTimezoneExpiryDate],[LotTimezoneIssueDate])\nINCLUDE ([HangTagCode],[FirstName],[LastName])\n WITH (DROP_EXISTING = ON)\n```\n\nHowever that will throw an error if the index doesn't exist (of course) so you need to wrap it with an `if`\n\n```sql\nif exists (SELECT * \nFROM sys.indexes \nWHERE name='idxMonthlyParkers_vendor_expiry_issue' AND object_id = OBJECT_ID('dbo.tblMonthlyParker'))\nbegin\n    CREATE NONCLUSTERED INDEX idxMonthlyParkers_vendor_expiry_issue\n    ON [dbo].[tblParkers] ([VendorId],[LotTimezoneExpiryDate],[LotTimezoneIssueDate])\n    INCLUDE ([HangTagCode],[FirstName],[LastName])\n    WITH (DROP_EXISTING = ON)\nend\nelse \nbegin\n    CREATE NONCLUSTERED INDEX idxMonthlyParkers_vendor_expiry_issue\n    ON [dbo].[tblParkers] ([VendorId],[LotTimezoneExpiryDate],[LotTimezoneIssueDate])\n    INCLUDE ([HangTagCode],[FirstName],[LastName])\nend\n```\n\n","categories":[],"tags":[]},{"title":"Download a file in powershell","authorId":"simon_timms","slug":"download-file","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/download-file/","link":"","permalink":"https://westerndevs.com/_/download-file/","excerpt":"","raw":"---\ntitle:  Download a file in powershell\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/download-file\nmode: public\n---\n\n\n\nHere is a quick way to download a file in powershell:\n\n```\nInvoke-WebRequest -Uri <source> -OutFile <destination>\n```","categories":[],"tags":[]},{"title":"Logging in Functions","authorId":"simon_timms","slug":"function-appinsights","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/function-appinsights/","link":"","permalink":"https://westerndevs.com/_/function-appinsights/","excerpt":"","raw":"---\ntitle:  Logging in Functions\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/function-appinsights\nmode: public\n---\n\n\n\nLooks like by default functions log at the `info` level. To change the level you can use set the application setting `AzureFunctionsJobHost__logging__LogLevel__Default` to some other value like `Error` or `Info`. \n\nIf you want to disable adaptive sampling then that can be done in the host.json\n\n```json\n{\n  \"version\": \"2.0\",\n  \"extensions\": {\n    \"queues\": {\n      \"maxPollingInterval\": \"00:00:05\"\n    }\n  },\n  \"logging\": {\n    \"logLevel\": {\n      \"default\": \"Information\"\n    },\n    \"applicationInsights\": {\n      \"samplingSettings\": {\n        \"isEnabled\": false\n      }\n    }\n  },\n  \"functionTimeout\": \"00:10:00\"\n}\n```\nIn this example adaptive sampling is turned off so you get every log message.\n\nA thing to note is that if you crank down logging to Error you won't see the invocations at all in the portal but they're still running.","categories":[],"tags":[]},{"title":"Query BigTable Events","authorId":"simon_timms","slug":"query-collections-in-big-table","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/query-collections-in-big-table/","link":"","permalink":"https://westerndevs.com/_/query-collections-in-big-table/","excerpt":"","raw":"---\ntitle:  Query BigTable Events\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/query-collections-in-big-table\nmode: public\n---\n\n\n\nFirebase can feed its data to bigtable and then you can run queries there. The syntax is SQL like but not quite because they have internal record types. So for the data that is fed across from firebase you get a structure that looks like \n\n![](/images/2021-03-05-query-collections-in-big-table.md/2021-03-05-10-39-05.png))\n\nYou can see that event_params and user_properties are these kind of collection things. The easiest way to deal with them is to flatten the structure and internally join the table against itself\n\n```sql\nSELECT r.event_name, p.key, p.value FROM `pocketgeek-auto.analytics_258213689.events_intraday_20210305` r cross join unnest(r.event_params) as p where key = 'DealerName'\n```\n\nThis gets you a dataset like \n\n![](/images/2021-03-05-query-collections-in-big-table.md/2021-03-05-10-41-21.png))\n\n```SQL\nSELECT r.event_name, p.key, p.value FROM `pocketgeek-auto.analytics_258213689.events_intraday_20210305` r cross join unnest(r.event_params) as p where key = 'DealerName' and p.value.string_value <> 'none'\n```\nis probably even better with the filter","categories":[],"tags":[]},{"title":"Setting a persistent environment variable","authorId":"simon_timms","slug":"set-env-variable","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/set-env-variable/","link":"","permalink":"https://westerndevs.com/_/set-env-variable/","excerpt":"","raw":"---\ntitle:  Setting a persistent environment variable \nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/set-env-variable\nmode: public\n---\n\n\n\nIf you want to set a variable but you want it to live forever then you can use\n\n```powershell\n[System.Environment]::SetEnvironmentVariable(\"JAVA_HOME\", \"c:\\program files\\openjdk\\jdk-13.0.2\", \"Machine\")\n```\n\nThat last argument can take on the values {`Process`, `User`, `Machine`}","categories":[],"tags":[]},{"title":"Increase Terminal Buffer in VS Code","authorId":"simon_timms","slug":"terminal-buffer","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/terminal-buffer/","link":"","permalink":"https://westerndevs.com/_/terminal-buffer/","excerpt":"","raw":"---\ntitle:  Increase Terminal Buffer in VS Code\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/terminal-buffer\nmode: public\n---\n\n\n\nGot something in your terminal which is producing more output than you can scroll back through (I'm looking at you `terraform plan`)? You can adjust the setting in preferences:\n\n![](/images/2021-04-29-terminal-buffer.md/2021-04-29-15-04-52.png))","categories":[],"tags":[]},{"title":"Transforms","authorId":"simon_timms","slug":"xdt-transforms","date":"2021-05-08 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/xdt-transforms/","link":"","permalink":"https://westerndevs.com/_/xdt-transforms/","excerpt":"","raw":"---\ntitle:  Transforms\n# Testing\nauthorId: simon_timms\ndate: 2021-05-08\noriginalurl: https://blog.simontimms.com/2021/05/08/xdt-transforms\nmode: public\n---\n\n\n\nYou can apply little transforms by just writing XML transformation on configuration files. For instance here is one for adding a section to the `system.web` section of the configuration file\n\n```xml\n<?xml version=\"1.0\"?>\n<configuration xmlns:xdt=\"http://schemas.microsoft.com/XML-Document-Transform\">\n  <system.web>\n    <machineKey xdt:Transform=\"Insert\" decryptionKey=\"abc\" validationKey=\"def\" />\n  </system.web>\n</configuration>\n```\n\nHere is one for removing an attribute\n\n```xml\n<?xml version=\"1.0\"?>\n<configuration xmlns:xdt=\"http://schemas.microsoft.com/XML-Document-Transform\">\n  <system.web>\n    <compilation xdt:Transform=\"RemoveAttributes(debug)\" />\n  </system.web>\n</configuration>\n```\n\nHow about changing an attribute based on matching the key?\n\n```xml\n<?xml version=\"1.0\"?>\n<configuration xmlns:xdt=\"http://schemas.microsoft.com/XML-Document-Transform\">\n  <appSettings>\n    <add key\"MaxUsers\" value=\"3\" xdt:Transform=\"SetAttributes\" xdt:Locator=\"Match(key)\" />\n  </appSettings>\n</configuration>\n```\n\nIf you happen to be using Octopus Deploy they have a feature you can add to your IIS deployment task to run these transformations\n\n![](/images/2021-05-06-xdt-transforms.md/2021-05-06-13-34-59.png))\n\n## Testing\n\nThere is a great little online testing tool at https://elmah.io/tools/webconfig-transformation-tester/ where you can plug in random things until you get them working.","categories":[],"tags":[]},{"title":"Advent of Code 2020 - Day 3","authorId":"dylan_smith","slug":"Advent-Of-Code-Day03","date":"2020-12-04 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Advent-Of-Code-Day03/","link":"","permalink":"https://westerndevs.com/_/Advent-Of-Code-Day03/","excerpt":"Day 3 Solution. Some infinite char grids, int overflows, and a new Multiply() method;","raw":"---\nlayout: post\ntitle: Advent of Code 2020 - Day 3\nauthorId: dylan_smith\ndate: 2020-12-03 19:00\n---\n\nDay 3 Solution.  Some infinite char grids, int overflows, and a new Multiply() method;\n\n<!--more -->\n\n## Previous Posts\n- [Intro](https://www.westerndevs.com/_/Advent-Of-Code-Intro/)\n- [Day 1](https://www.westerndevs.com/_/Advent-Of-Code-Day01/)\n- [Day 2](https://www.westerndevs.com/_/Advent-Of-Code-Day02/)\n\nMy code can be found [here on GitHub](https://github.com/dylan-smith/AdventOfCode2020/blob/master/src/Days/Day03.cs)\n\nHere's quick walkthrough of my final solution:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ChP5A3RmSjA\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","categories":[],"tags":[]},{"title":"Advent of Code 2020 - Day 2","authorId":"dylan_smith","slug":"Advent-Of-Code-Day02","date":"2020-12-02 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Advent-Of-Code-Day02/","link":"","permalink":"https://westerndevs.com/_/Advent-Of-Code-Day02/","excerpt":"Day 2 solution with a new ParseLines() extension method, some Tuples, and the XOR operator","raw":"---\nlayout: post\ntitle: Advent of Code 2020 - Day 2\nauthorId: dylan_smith\ndate: 2020-12-02 17:00\n---\n\nDay 2 solution with a new ParseLines() extension method, some Tuples, and the XOR operator\n\n<!--more -->\n\nPrevious Posts:\n- [Intro](https://www.westerndevs.com/_/Advent-Of-Code-Intro/)\n- [Day 1](https://www.westerndevs.com/_/Advent-Of-Code-Day01/)\n\nMy code can be found [here on GitHub](https://github.com/dylan-smith/AdventOfCode2020/blob/master/src/Days/Day02.cs)\n\nHere's quick walkthrough of my final solution:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WXh7do8l54I\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","categories":[],"tags":[]},{"title":"Advent of Code 2020 - Day 1","authorId":"dylan_smith","slug":"Advent-Of-Code-Day01","date":"2020-12-01 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Advent-Of-Code-Day01/","link":"","permalink":"https://westerndevs.com/_/Advent-Of-Code-Day01/","excerpt":"There was a slight hiccup with the Advent Of Code site going down right at launch today. Within about 5-6 minutes they had solved the problem, and the site author gave a couple comments about it on reddit:","raw":"---\nlayout: post\ntitle: Advent of Code 2020 - Day 1\nauthorId: dylan_smith\ndate: 2020-12-01 00:00\n---\n\nThere was a slight hiccup with the Advent Of Code site going down right at launch today.  Within about 5-6 minutes they had solved the problem, and the site author gave a couple comments about it on reddit:\n\n<!--more -->\n\n![Postmortem](https://imgur.com/ei64QeP.png)\n![Postmortem comment](https://imgur.com/2fZOaJt.png)\n\nMy code can be found [here on GitHub](https://github.com/dylan-smith/AdventOfCode2020/blob/master/src/Days/Day01.cs)\n\nHere's quick walkthrough of my final solution:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CvMvjvUGz8Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","categories":[],"tags":[]},{"title":"Advent of Code 2020 - C# Framework","authorId":"dylan_smith","slug":"Advent-Of-Code-Intro","date":"2020-11-24 19:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Advent-Of-Code-Intro/","link":"","permalink":"https://westerndevs.com/_/Advent-Of-Code-Intro/","excerpt":"Advent Of Code 2020 is just around the corner. This year my hope is to create a short video after solving each day to show off my solution. Here's an intro video where I walk-through the framework/GUI I've created where I write and run my code.","raw":"---\nlayout: post\ntitle: Advent of Code 2020 - C# Framework\nauthorId: dylan_smith\ndate: 2020-11-24 14:00\n---\n\n[Advent Of Code 2020](https://adventofcode.com/) is just around the corner.  This year my hope is to create a short video after solving each day to show off my solution.  Here's an intro video where I walk-through the framework/GUI I've created where I write and run my code.\n\n<!--more -->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BDeR8KBJnv0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","categories":[],"tags":[]},{"title":"Allocating a Serverless Database in SQL Azure","authorId":"simon_timms","slug":"serverless-sql-azure-terraform","date":"2020-11-18 19:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/serverless-sql-azure-terraform/","link":"","permalink":"https://westerndevs.com/_/serverless-sql-azure-terraform/","excerpt":"I'm pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I'm provisioning a bunch of databases for a client and we're not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out.","raw":"---\nlayout: post\ntitle: Allocating a Serverless Database in SQL Azure\nauthorId: simon_timms\ndate: 2020-11-18 14:00\noriginalurl: https://blog.simontimms.com/2020/11/18/2020-11-18-serverless-sql-terraform/\n---\n\nI'm pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I'm provisioning a bunch of databases for a client and we're not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out. \n\n<!--more -->\n\nTraditionally we've been using the resource `azurerm_sql_database` for our databases but this provider is starting to be deprecated in favour of `azurerm_mssql_database` which has better support for some of the more modern concept in SQL Azure. The [documentation](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/mssql_database#state) is pretty good for it but while there was a `min_capacity` we couldn't find an equivalent `max_capacity`. Turns out you can set the max capacity using the SKU. So we had something like \n\n```\nresource \"azurerm_mssql_database\" \"database\" {\n  name                        = var.database_name\n  server_id                   = var.database_server_id\n  max_size_gb                 = var.database_max_size_gb\n  auto_pause_delay_in_minutes = -1\n  min_capacity                = 1\n  sku_name                    = \"GP_S_Gen5_6\"\n  tags = {\n    environment = var.prefix\n  }\n  short_term_retention_policy {\n    retention_days = 14\n  }\n}\n\n```\n\nThis allocates a database with a capacity of between 1 and 6 vCPU that has auto pause disabled. The S in the GP_S_Gen5_6 stands for serverless and the 6 denotes the maximum capacity. ","categories":[],"tags":[]},{"title":"From Travis CI to GitHub Actions (and GitHub Pages)","authorId":"david_wesst","slug":"from-travis-ci-to-github-actions","date":"2020-11-10 23:36:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devops/from-travis-ci-to-github-actions/","link":"","permalink":"https://westerndevs.com/devops/from-travis-ci-to-github-actions/","excerpt":"We recently migrated the continuous integration and deployment workflow for the Western Devs website from Travis CI to GitHub Actions. These are the steps I followed to get it done.","raw":"---\ntitle: \"From Travis CI to GitHub Actions (and GitHub Pages)\"\ndate: \"2020-11-10T18:36:00\"\nlayout: post\nauthorId: david_wesst\noriginalurl: https://www.davidwesst.com/blog/from-travis-ci-to-github-actions\ncategories:\n    - devops\ntags:\n    - github actions\n    - github workflow\n    - travis ci\n    - continuous integration\n    - continuous deployment\n---\n\nWe recently migrated the continuous integration and deployment workflow for the Western Devs website from Travis CI to GitHub Actions. These are the steps I followed to get it done.\n\n<!-- more -->\n\n[1]: https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing\n[2]: https://docs.github.com/en/free-pro-team@latest/actions\n[3]: https://westerndevs.com\n[4]: https://github.com/westerndevs/western-devs-website/blob/master/.github/workflows/ci-cd.yml\n[5]: https://www.davidwesst.com\n[6]: https://hexo.io/\n[7]: https://github.com/westerndevs/western-devs-website/blob/master/package.json\n[8]: https://pages.github.com/\n[9]: https://docs.github.com/en/free-pro-team@latest/actions/reference/context-and-expression-syntax-for-github-actions#about-contexts-and-expressions\n[10]: https://docs.github.com/en/free-pro-team@latest/actions/reference/encrypted-secrets\n[11]: https://github.com/marketplace?type=actions\n[12]: https://api.slack.com/messaging/webhooks\n\nTravis CI [announced a new pricing model][1] that _could_ have impact on open source projects that are using Travis for continuous integration and/or deployment. For static websites, like the [Western Devs website][3] or [personal website][5], this could result in getting some unforeseen costs. With that in mind, we decided to take the plunge an migrate away from Travis and over to [GitHub Actions][2] as they provide CI and CD workflows free for open source projects.\n\n## TL;DR; -- Just show me the code\nFine. [Here is is][4]. It is open source after all.\n\nBut just to be clear, this isn't a tutorial on how to code this up, rather its a walkthrough on what it took to get our [Hexo][6] based static site from Travis to GitHub Actions.\n\n## Start with Mapping Out Your Workflow\nAnd I mean _workflow_ and not just the build. \n\nFor the Western Devs, our workflow goes like this:\n\n1. Commit a change to the code (i.e. a new blog post)\n2. Build the website\n3. If master branch build is successful, deploy the build to production\n4. Notify the Western Devs of the build result in Slack\n\nGitHub workflow provides everything we need to do this, and I'll walk you through the code, which you can see for yourself in [here in our GitHub repo][4].\n\n### 1. Commit a change to the code (i.e. a new blog post)\nThis is our trigger to start the workflow. That is represented by the `on` section of the YAML. In our case, we want to trigger the workflow every time there is a pull request created for the master branch, a push to the master branch (i.e. a merge), or a push to any other feature (ft) or hotfix (hf) branches.\n\n```yaml\n\nname: CI/CD\non:\n  push:\n    branches:\n      - master\n      - ft/*\n      - hf/*\n  pull_request:\n    branches: [ master ]\n\n```\n\nNow we have a workflow that will trigger when we want to. Next, we need to actually build the website.\n\n### 2. Build the website\nOur build is exceptionally simple-- just generate the site, and if the generation is successful, the build was successful. To do this, we create a `build` job that handles the work.\n\n```yaml\n\njobs:\n  build:\n    name: Build and Deploy\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2 # checkout the source code\n    - uses: actions/setup-node@v1 # setup the environment\n      with:\n        node-version: 12\n    - run: npm install # setup dependencies\n    - run: npm run build # run the build command\n\n```\n\nThe first two steps are using GitHub Actions provided by GitHub themselves. This pulls our source code and the sets up the Node environment that we need to build the website. Once that is done, we  `run` steps to run shell commands to install our project specific dependendies and run the build script itself. \n\nThe scripts have been defined in our [project `package.json` file][7] and are used by the developers to build the site locally as well.\n\n### 3. If master branch build is successful, deploy the build to production\nIf we are talking about the master branch, we want to do a deployment if it is successful. For this step, we added a conditional expression using the `github` context that is provided to all actions. You can learn more about context and expressions for GitHub Actions in the [GitHub Docs here][9].\n\nYou might also see that were using an encrypted secret using the `secret.GITHUB_TOKEN` expression. All repositories have this feature in the settings section of the repo, and you can learn more about [creating encrypted secrets for a repository here][10] in the GitHub docs.\n\n```yaml\n    - name: Deploy to GitHub Pages\n      if: github.ref == 'refs/heads/master'\n      uses: peaceiris/actions-gh-pages@v3\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n        cname: westerndevs.com\n        commit_message: ${{ github.event.head_commit.message }}\n```\n\n#### BONUS: Free hosting with GitHub Pages\nIn our case, our deployment target is [GitHub Pages][8] which provides free hosting and SSL certificates for open source static sites sites like ours. \n\nWe decided to take this opportunity to consilate everything under the GitHub umbrella because it saved us a couple of bucks, and now everything we need to manage the site is in one spot rather than spread across multiple cloud services.\n\n### 4. Notify the Western Devs of the build result in Slack\n\nOriginally, we had forgotten this step and started to feel it right away. So an issue was created and I put a solution in place in about 15 minutes, thanks to someone else doing all the heavy lifting and publishing their work to the [GitHub Actions Marketplace][11].\n\nSlack supports incoming webhooks, even for for free workspaces. I set that up by following the [Slack documentation][12], created another secret in our repository and voila, we were back in business wih the notifications.\n\n```yaml\n    - name: Notify Slack\n      if: always()\n      uses: 8398a7/action-slack@v3\n      with:\n        status: ${{ job.status }}\n        fields: repo,message,commit,author,action,eventName,ref,workflow,job,took # selectable (default: repo,message)\n      env:\n        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} # required\n```\n\n## Conclusion\n\nThe combination of GitHub Actions and GitHub Pages provides every developer with the opportunity to get a taste of DevOps while actually producing something they can show off to their peers and community. Travis CI is, and will continue to be, a great CI/CD solution for developers...but if you're looking for a one-stop-shop for source control, workflow, and hosting. You can't really go wrong with GitHub.\n","categories":[{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/categories/devops/"}],"tags":[{"name":"github actions","slug":"github-actions","permalink":"https://westerndevs.com/tags/github-actions/"},{"name":"github workflow","slug":"github-workflow","permalink":"https://westerndevs.com/tags/github-workflow/"},{"name":"travis ci","slug":"travis-ci","permalink":"https://westerndevs.com/tags/travis-ci/"},{"name":"continuous integration","slug":"continuous-integration","permalink":"https://westerndevs.com/tags/continuous-integration/"},{"name":"continuous deployment","slug":"continuous-deployment","permalink":"https://westerndevs.com/tags/continuous-deployment/"}]},{"title":"Running Stored Procedures Across Databases in Azure","authorId":"simon_timms","slug":"cross-database-procs","date":"2020-11-09 19:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/cross-database-procs/","link":"","permalink":"https://westerndevs.com/_/cross-database-procs/","excerpt":"In a previous article I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn't entirely accurate. You can do it if you make use of stored procedures.","raw":"---\nlayout: post\ntitle: Running Stored Procedures Across Databases in Azure\nauthorId: simon_timms\ndate: 2020-11-09 14:00\noriginalurl: https://blog.simontimms.com/2020/11/09/2020-11-09-cross-database-procs/\n---\n\nIn a [previous article](https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/) I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn't entirely accurate. You can do it if you make use of stored procedures. \n\n<!-- more -->\n\nRunning a stored proc on a remote database is a little bit weird looking but once you get your head around that then it is perfectly usable. Let's go back to the same example we used before with a products database an an orders database. In the products database let's add a stored procedure to add a new product and return the count of products.\n\n```sql\ncreate procedure addProduct\n @item nvarchar(50)\nas\n\tinsert into Products(name) values(@item);\n\tselect count(*) cnt from products;\ngo\n```\n\nNow over in our orders database we can use our existing database connection to call this stored proc\n\n```sql\nsp_execute_remote ProductsSource, \n                  N'addProduct @item', \n                  @params = N'@item nvarchar(50)', \n                  @item = 'long sleeved shirts';\n```\n\nAt first glance this is a little confusing so let's break it down. \n\n```sql\nsp_execute_remote ProductsSource, \n```\nThis line instructs that we want to run a stored procedure and that it should use the ProductsSource data connection. \n\n```sql\nN'addProduct @item', \n```\nThis line lists the stored proc to run and the parameters to pass to it. You'll notice that it is a NVarchar string passed as a single parameter.\n\n```sql\n@params = N'@item nvarchar(50)', \n```\n\nThis line lists all the parameters to pass and their type. If you have multiple then you'd comma separate them here: `N'@item nvarchar(50), @price number(10,2)'`\n\n```\n@item = 'long sleeved shirts';\n```\n\nThis final line is an args-style array of the values for the parameters. Again if you had a second parameter you'd pass it in as separate item here `@item = 'long sleeved shirts', @price=10.99`\n\nRunning this command gets us something like \n\n```\ncnt\t  $ShardName\n6\t  [DataSource=testias.database.windows.net Database=testias]\n```\n\nYou'll notice that nifty ShardName colum which tells you about the source. This is because you can use a shard map to execute the stored procedure against lots of shards at once.","categories":[],"tags":[]},{"title":"New Artwork and a Complete Rewrite...ish | Out the Door Devlog","authorId":"david_wesst","slug":"out-the-door-post-jam-update","date":"2020-11-09 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devlog/out-the-door/out-the-door-post-jam-update/","link":"","permalink":"https://westerndevs.com/devlog/out-the-door/out-the-door-post-jam-update/","excerpt":"My effort continues on Out the Door with some new artwork, a rewrite (of sorts) to fix the build process, which has led to something of a self-driven code review.","raw":"---\ntitle: \"New Artwork and a Complete Rewrite...ish | Out the Door Devlog\"\ndate: \"2020-11-09T09:00:00\"\nlayout: post\nauthorId: david_wesst\noriginalurl: https://www.davidwesst.com/blog/out-the-door-post-jam-update\ncategories:\n    - devlog\n    - out the door\ntags:\n    - ludum dare\n    - ludum dare 47\n    - game development\n    - game design\n    - gamejam\n    - out the door\n---\n\nMy effort continues on Out the Door with some new artwork, a rewrite (of sorts) to fix the build process, which has led to something of a self-driven code review.\n\n<!-- more -->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LLXO-6Pretk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Wanna Play?\n\nSure you do! Head over to my [davidwesst.itch.io/out-the-door](https://davidwesst.itch.io/out-the-door) to give it a whirl in your browser (no install needed) or Windows! It's totally free, and feedback is always appreciated.\n\n","categories":[{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/categories/devlog/"},{"name":"out the door","slug":"devlog/out-the-door","permalink":"https://westerndevs.com/categories/devlog/out-the-door/"}],"tags":[{"name":"game development","slug":"game-development","permalink":"https://westerndevs.com/tags/game-development/"},{"name":"gamejam","slug":"gamejam","permalink":"https://westerndevs.com/tags/gamejam/"},{"name":"ludum dare","slug":"ludum-dare","permalink":"https://westerndevs.com/tags/ludum-dare/"},{"name":"ludum dare 47","slug":"ludum-dare-47","permalink":"https://westerndevs.com/tags/ludum-dare-47/"},{"name":"game design","slug":"game-design","permalink":"https://westerndevs.com/tags/game-design/"},{"name":"out the door","slug":"out-the-door","permalink":"https://westerndevs.com/tags/out-the-door/"}]},{"title":"Azure Processor Limits","authorId":"simon_timms","slug":"processor-limits","date":"2020-11-05 20:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/processor-limits/","link":"","permalink":"https://westerndevs.com/_/processor-limits/","excerpt":"","raw":"---\nlayout: post\ntitle: Azure Processor Limits\nauthorId: simon_timms\ndate: 2020-11-05 15:00\noriginalurl: https://blog.simontimms.com/2020/11/05/2020-11-05-processor-limits/\n---\n\nRan into a fun little quirk in Azure today. We wanted to allocate a pretty beefy machine, an M32ms. Problem was that for the region we were looking at it wasn't showing up on our list of VM sizes. We checked and there were certainly VMs of that size available in the region we just couldn't see them. So we ran the command \n\n```\naz vm list-usage --location \"westus\" --output table\n```\n\nAnd that returned a bunch of information about the quota limits we had in place. Sure enough in there we had \n\n```\nName                               Current Value   Limit\nStandard MS Family vCPUs           0               0\n```\n\nWe opened a support request to increase the quota on that CPU. We also had a weirdly low limit on CPUs in the region \n\n```\nTotal Regional vCPUs               0               10\n```\n\nWhich support fixed for us too and we were then able to create the VM we were looking for. ","categories":[],"tags":[]},{"title":"Querying Across Databases In SQL Azure","authorId":"simon_timms","slug":"elasticquery","date":"2020-11-05 19:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/elasticquery/","link":"","permalink":"https://westerndevs.com/_/elasticquery/","excerpt":"I seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the orders database from the customers database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that's one of the reasons.","raw":"---\nlayout: post\ntitle: Querying Across Databases In SQL Azure\nauthorId: simon_timms\ndate: 2020-11-05 14:00\noriginalurl: https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/\n---\n\nI seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the `orders` database from the `customers` database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that's one of the reasons. \n\n<!--more-->\n\nUnfortunately, whacking developers with a ruler and making them rewrite their business logic to observe proper domain boundaries isn't always on the cards. This is a problem when migrating them to SQL Azure because querying across databases, even ones on the same server, isn't permitted. \n\n![Broken query across databases](https://blog.simontimms.com/images/elasticquery/brokenQuery.png)\n\nThis is where the new [Elastic Query](https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-query-overview) comes in. I should warn at this point that the functionality is still in preview but it's been in preview for a couple of years so I think it is pretty stable. I feel a little bit disingenuous describing it as \"new\" now but it is new to me. To use it is pretty easy and doesn't even need you to use the Azure portal. \n\nLet's imagine that you have two databases one of which contains a collection of Products and a second database that contains a list of Orders which contain just the product id. Your mission is to query and get a list of orders and the product name. To start we can set up a couple of databases. I called mine `testias` and `testias2` and I had them both on the same instance of SQL Azure but you don't have to.\n\n![Two databases on the same server](https://blog.simontimms.com/images/elasticquery/setup.png)\n\n## Product Database\n\n```sql\ncreate table Products( \nid uniqueidentifier primary key default newid(),\nname nvarchar(50));\n\ninsert into Products(name) values('socks');\ninsert into Products(name) values('hats');\ninsert into Products(name) values('gloves');\n```\n\n## Orders Database\n\n```sql\ncreate table orders(id uniqueidentifier primary key default newid(),\ndate date);\n\ncreate table orderLineItems(id uniqueidentifier primary key default newid(),\norderId uniqueidentifier,\nproductId uniqueidentifier,\nquantity int,\nforeign key (orderId) references orders(id));\n\ndeclare @orderID uniqueidentifier = newid();\ninsert into orders(id, date)\nvalues(@orderID, '2020-11-01');\n \ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '3829A43D-FD2A-4B7C-9A09-23DBF030C1DC', 10);\ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '233BC430-BA3F-4F5C-B3EA-4B82867FC040', 1);\ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '95A20D82-EC26-4769-8840-804B88630A01', 2);\n\nset @orderId = newid();\ninsert into orders(id, date)\nvalues(@orderID, '2020-11-02');\n\ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '3829A43D-FD2A-4B7C-9A09-23DBF030C1DC', 16);\ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '233BC430-BA3F-4F5C-B3EA-4B82867FC040', 99);\ninsert into orderLineItems(orderId, productId, quantity) values(@orderID, '95A20D82-EC26-4769-8840-804B88630A01', 0);\n```\n\nNow we need to hook up the databases to be able to see each other. We're actually just going to make products visible from the orders database. It makes more sense to me to run these queries in the database which contains the most data to minimize how much data needs to cross the wire to the other database. \n\nSo first up we need to tell the Orders database about the credentials needed to access the remote database, products. To do this we need to use a SQL account on the products database. Windows accounts and integrated security doesn't currently work for this. \n\n```sql\ncreate master key encryption by password = 'monkeyNose!2';\ncreate database scoped credential ProductDatabaseCredentials \nwith identity = 'ProductsDBUser', \nsecret = 'wouNHk41l9fBBcqadwWiq3ert';\n```\n\nNext we set up an external data source for the products\n\n```sql\ncreate external data source ProductsSource with \n(type=RDBMS, location = 'testias.database.windows.net', \ndatabase_name = 'testias', credential = ProductDatabaseCredentials);\n```\n\n\nFinally we create a table definition in the Orders database that matches the remote table (without any defaults or constraints).\n\n```sql\ncreate external table Products( id uniqueidentifier,\nname nvarchar(50))\nwith ( data_source = ProductsSource)\n```\n\nWe now have a products table in the external tables section in the object explorer\n\n![Tables from both databases](https://blog.simontimms.com/images/elasticquery/testtableview.png)\n\nWe can query the external table and even cross it against the tables in this database\n\n```sql\nselect name, ol.quantity from orderLineItems ol inner join products p on ol.productId = p.id\n```\n\n```text\nsocks   16\nsocks   10\ngloves  1\ngloves  99\nhats    2\nhats    0\n```\n\nSo it is possible to run queries across databases in Azure but it takes a little set up and a little bit of thought about how to best set it up. \n\n# Possible Gotchas\n\n- I forgot to set up the database to be able to talk to Azure resources in the firewall so I had to go back and add that\n- Inserting to the external table isn't supported, which is good, make the changes directly in the source database","categories":[],"tags":[]},{"title":"A Solo Gamejam Experience | A Ludum Dare 47 Story","authorId":"david_wesst","slug":"a-solo-gamejam-experience-ld47","date":"2020-10-23 13:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devlog/a-solo-gamejam-experience-ld47/","link":"","permalink":"https://westerndevs.com/devlog/a-solo-gamejam-experience-ld47/","excerpt":"Wonder what it takes to be a solo amateur game developer in a global gamejam? DW summarizes his Ludum Dare 47 experience in this video.","raw":"---\nlayout: post\ntitle: A Solo Gamejam Experience | A Ludum Dare 47 Story\nauthorId: david_wesst\ndate: \"2020-10-23T09:00\"\ncategories:\n- devlog\ntags:\n- gamedev\n- game development\n- devlog\n- gamejam\n- ludum dare\n- github gameoff\noriginalurl: http://www.davidwesst.com/blog/a-solo-gamejam-experience/\nexcerpt: \"Wonder what it takes to be a solo amateur game developer in a global gamejam? DW summarizes his Ludum Dare 47 experience in this video.\"\n---\n\nI submitted a game to [Ludum Dare 47](https://ldjam.com/events/ludum-dare/47/out-the-door) I call [Out the Door (Play Now in your Browser)](https://davidwesst.itch.io/out-the-door) as a solo, amateur game developer with a non-gamedev day job, family responsibilities, and household to maintain. \n\n<!-- more -->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AFnGMS24qvg\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","categories":[{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/categories/devlog/"}],"tags":[{"name":"gamedev","slug":"gamedev","permalink":"https://westerndevs.com/tags/gamedev/"},{"name":"game development","slug":"game-development","permalink":"https://westerndevs.com/tags/game-development/"},{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/tags/devlog/"},{"name":"gamejam","slug":"gamejam","permalink":"https://westerndevs.com/tags/gamejam/"},{"name":"ludum dare","slug":"ludum-dare","permalink":"https://westerndevs.com/tags/ludum-dare/"},{"name":"github gameoff","slug":"github-gameoff","permalink":"https://westerndevs.com/tags/github-gameoff/"}]},{"title":"The trimStart rabbit hole","authorId":"simon_timms","slug":"typescript-definition","date":"2020-09-28 18:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/typescript-definition/","link":"","permalink":"https://westerndevs.com/_/typescript-definition/","excerpt":"If you're missing expected functions in your TypeScript app the problem might be an incorrect target","raw":"---\nlayout: post\ntitle: The trimStart rabbit hole \nauthorId: simon_timms\ndate: 2020-09-28 14:00\noriginalUrl: https://blog.simontimms.com/2020/09/28/2020-09-28-typescriptd-definition/\nexcerpt: If you're missing expected functions in your TypeScript app the problem might be an incorrect target\n---\n\nI was bragging to [David](https://westerndevs.com/bios/dave_paquette/) about a particularly impressive piece of TypeScript code I wrote last week\n\n```\nif (body.trim().startsWith('<')) { //100% infallible xml detection\n```\n\nHe, rightly, pointed out that `trimStart` would probably be more efficient. Of course it would! However when I went to make that change there was only `trim`, `trimLeft` and `trimRight` in my TypeScript auto-complete drop down. \n\n![TrimStart and TrimEnd are missing](https://blog.simontimms.com/images/trimStart/missing.png)\n\nOdd. This was for sure a real function because it appears in the [MDN docs](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/trimStart).\n\nA reasonable person would have used trimLeft and moved on but it was Monday and I was full of passion for programming. So I went down the rabbit hole. \n\nChecking out the TypeScript directory in my node_modules I found that there were quite a few definition files in there. These were the definition files that described the JavaScript language itself rather than any libraries. Included in that bunch was one called `lib.es2019.string.d.ts`. This file contained changes which were made to the language in es2019. \n\n```typescript\ninterface String {\n    /** Removes the trailing white space and line terminator characters from a string. */\n    trimEnd(): string;\n\n    /** Removes the leading white space and line terminator characters from a string. */\n    trimStart(): string;\n\n    /** Removes the leading white space and line terminator characters from a string. */\n    trimLeft(): string;\n\n    /** Removes the trailing white space and line terminator characters from a string. */\n    trimRight(): string;\n}\n```\n\nSo I must be targeting the wrong thing! Sure enough in my `tsconfig.js` I was targeting `es5` on this project. When we started this was using an older version of node on lambda that didn't have support for more recent versions of ES. I checked and the lambda was running node 12.18.3 and support for ES2020 landed in node 12.9 so I was good to move up to es2020 as a target.\n\nIncidentally you can check the running node version in JavaScript by running\n\n```\nconsole.log('Versions: ' + JSON.stringify(process.versions));\n```\n\nAfter updating my `tsconfig.js` and restarting the language server all was right in the world. \n\n![The missing functions appear](https://blog.simontimms.com/images/trimStart/there.png)","categories":[],"tags":[]},{"title":"Game Portfolio Self Evaluation (in prep for Ludum Dare 47)","authorId":"david_wesst","slug":"game-portfolio-review-2020","date":"2020-09-18 13:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devlog/game-portfolio-review-2020/","link":"","permalink":"https://westerndevs.com/devlog/game-portfolio-review-2020/","excerpt":"Ludum Dare 47, a weekend long global gamejam, is coming up in a few weeks. In order to prep for the event, I decided to take the time for review and reflect on my game portfolio to see what I learning objective and goals I can set for myself.","raw":"---\ntitle: \"Game Portfolio Self Evaluation (in prep for Ludum Dare 47)\"\ndate: \"2020-09-18T09:00:00\"\nexcerpt: \"Ludum Dare 47, a weekend long global gamejam, is coming up in a few weeks. In order to prep for the event, I decided to take the time for review and reflect on my game portfolio to see what I learning objective and goals I can set for myself.\"\ncategories:\n- devlog\ntags:\n- gamedev\n- game development\n- devlog\n- gamejam\n- ludum dare\n- github gameoff\nauthorId: david_wesst\noriginalUrl: https://www.davidwesst.com/blog/game-portfolio-evaluation-2020/\n---\n\nLudum Dare 47, a weekend long global gamejam, is coming up in a few weeks. In order to prep for the event, I decided to take the time for review and reflect on my game portfolio to see what I learning objective and goals I can set for myself.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V_zCHtZIsYw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## My Portfolio\n\nWhat was that? You wanted to know where to find and play my games?! Well then, if you're inclined to try some of my games (and hopefully leave some feedback), here they are:\n\n* [Vagabond Game](https://davidwesst.itch.io/vagabondgame) -> A top-down Unity based prototype where I learned the technical ropes of putting together a small, game experience, with narrative, graphics, and animation.\n* [Car Scientist](https://davidwesst.itch.io/leaps-and-bounds) -> My first gamejam submission and evolution of the Vagabond Game prototype, except this time built in Godot.\n* [Little Shop of Wall Street](https://davidwesst.itch.io/little-shop-of-wall-street) -> My Ludum Dare 46 submission, co-authored by [D'Arcy Lussier](https://westerndevs.com/bios/darcy_lussier/) where you trade stocks online in order to feed an interesting plant, before it feeds on YOU!\n","categories":[{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/categories/devlog/"}],"tags":[{"name":"gamedev","slug":"gamedev","permalink":"https://westerndevs.com/tags/gamedev/"},{"name":"game development","slug":"game-development","permalink":"https://westerndevs.com/tags/game-development/"},{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/tags/devlog/"},{"name":"gamejam","slug":"gamejam","permalink":"https://westerndevs.com/tags/gamejam/"},{"name":"ludum dare","slug":"ludum-dare","permalink":"https://westerndevs.com/tags/ludum-dare/"},{"name":"github gameoff","slug":"github-gameoff","permalink":"https://westerndevs.com/tags/github-gameoff/"}]},{"title":"Release Notes for Little Shop of Wall Street 0.1.0-beta","authorId":"david_wesst","slug":"little-shop-of-wall-street-01-release-notes","date":"2020-07-02 15:13:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devlog/little-shop-of-wall-street-01-release-notes/","link":"","permalink":"https://westerndevs.com/devlog/little-shop-of-wall-street-01-release-notes/","excerpt":"Little Shop of Wall Street has a 0.1-beta release!","raw":"---\ntitle: \"Release Notes for Little Shop of Wall Street 0.1.0-beta\"\ndate: \"2020-07-02T11:13:00\"\nexcerpt: \"Little Shop of Wall Street has a 0.1-beta release!\"\ncategories:\n- devlog\ntags:\n- gamedev\n- game development\n- devlog\n- little shop of wall street\n- godot\nauthorId: david_wesst\noriginalUrl: https://www.davidwesst.com/blog/little-shop-01-release-notes/\n---\nFinally! The 0.1 beta has arrived for Little Shop of Wall Street!\n\nIn this video, DW walks through the new features rolled out both in-game and behind the scenes for his LD46 game jam title.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/baMlNqGgiV4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nYou can play the game here on [Itch.io](https://davidwesst.itch.io/little-shop-of-wall-street).\n\n## Side Notes\n\nThis release is an important one for me. \n\nFirst, off it's the first \"beta\" release which I've categorized as a moderately stable release, and includes a \"complete gameplay loop\" on purpose. There are still plenty of bugs (as the video even showed) but it works and playable.\n\nSecond, this release is the original vision of what I pictured the gamejam submission to be when D'Arcy and I came up with the idea back in April. Many months later, I have that release which says a lot about my prototyping and experimenting process (i.e. I'm too slow).\n\nLastly, I have multiple versions of the game out there including Linux/X11 and Windows versions. There's still a lot more to learn and do with the while devops setup for my projects, but this is a great step forward and can be reused with all my Godot-based projects moving forward.\n\nUntil the next one-- thanks for playing.\n\n~ DW","categories":[{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/categories/devlog/"}],"tags":[{"name":"gamedev","slug":"gamedev","permalink":"https://westerndevs.com/tags/gamedev/"},{"name":"game development","slug":"game-development","permalink":"https://westerndevs.com/tags/game-development/"},{"name":"devlog","slug":"devlog","permalink":"https://westerndevs.com/tags/devlog/"},{"name":"little shop of wall street","slug":"little-shop-of-wall-street","permalink":"https://westerndevs.com/tags/little-shop-of-wall-street/"},{"name":"godot","slug":"godot","permalink":"https://westerndevs.com/tags/godot/"}]},{"title":"Scaling Azure Functions from Consumption Plan to Premium Plan (and back again)","authorId":"dave_paquette","slug":"scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan","date":"2020-05-23 16:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Azure/Azure-Functions/scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan/","link":"","permalink":"https://westerndevs.com/Azure/Azure-Functions/scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan/","excerpt":"In this post, we use the az cli to move an Azure Function app from a Consumption Plan to a Premium Plan (and back again).","raw":"---\nlayout: post\ntitle: Scaling Azure Functions from Consumption Plan to Premium Plan (and back again) \ntags:\n  - Azure\n  - Azure Functions\n  - Web Dev\n  - AZ CLI\ncategories:\n  - Azure\n  - Azure Functions\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2020/05/23/scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan.aspx'\ndate: 2020-05-23 12:30:00\nexcerpt: In this post, we use the az cli to move an Azure Function app from a Consumption Plan to a Premium Plan (and back again).\n---\nAzure Functions, when hosted on a consumption plan, are great for most scenarios. You pay per use which is great for keeping costs down but there are some downsides and limitations. One of those is the time it takes to cold start your function app. If your function app hasn't been triggered in some time, it can take a while for the a new instance to start up to run your app. Likewise, if a very sudden spike in load occurs, it can take some time for the consumption plan to start up enough instances to handle that load. In the meantime, you might have clients getting timeouts or failed requests.\n\nAzure Functions offers another hosting model called [Azure Functions Premium Plan](https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan). With premium plans, instead of paying per function execution, you pay for the underlying compute instances that are hosting your functions. This is often more expensive, but it also ensures there are always a pre-set number of warmed instances ready to execute your function.\n\nThat's great, but what if I only really need those pre-warmed instances for a short period of time when I'm expecting a lot of incoming traffic. The rest of the time, I would rather use a Consumption Plan to save on hosting costs.\n\nI thought the choice of hosting plan was something you needed to make up front but it turns out that you can actually move an Azure Function App from a consumption plan to a premium plan (and back again).\n\nThanks to [Simon Timms](https://twitter.com/stimms/) for starting this discussion on Twitter. We got very helpful responses from folks on the Azure Functions team:\n\n[Jeff Hollan](https://twitter.com/jeffhollan/) has a great [sample](https://github.com/Azure-Samples/functions-csharp-premium-scaler) using an Azure Durable Function to scale an Azure Function App to a premium plan for a specified amount of time, then automatically scale back down to a consumption plan.\n\n<blockquote class=\"twitter-tweet\"><p lang=\"und\" dir=\"ltr\"><a href=\"https://t.co/6C9l3PQDoZ\">https://t.co/6C9l3PQDoZ</a></p>&mdash; Jeff Hollan (@jeffhollan) <a href=\"https://twitter.com/jeffhollan/status/1245779682961674240?ref_src=twsrc%5Etfw\">April 2, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThis is a super cool sample. It uses the [Azure Resource Manager REST API](https://docs.microsoft.com/en-us/rest/api/resources/) to make changes to the target function app resources. For my project however, I didn't really want to spin up another Azure Function to manage my Azure Functions. I just wanted an easy way to scale my 12 function apps up to premium plans for a couple hours, then scale them back down to a consumption plan.\n\nI decided to try using the AZ CLI for this and it turned out really well. I was able to write a simple script to scale up and down.\n\n## Setting up the AZ CLI\nFirst up, [install the az cli](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).\n\nOnce installed, you'll need to login to your Azure Subscription.\n\n{% codeblock lang:bash %}\naz login\n{% endcodeblock %}\n\nA browser window will popup, prompting you to log in to your Azure account. Once you've logged in, the browser window will close and the az cli will display a list of subscriptions available in your account. If you have more than one subscription, make sure you select the one you want to use.\n\n{% codeblock lang:bash %}\naz account set --subscription YourSubscriptionId\n{% endcodeblock %}\n\n## Create a Resource Group\nYou will need a resource group for your Storage and CDN resources. If you don't already have one, create it here.\n{% codeblock lang:bash %}\naz group create --name DavesFunctionApps --location WestUS2\n{% endcodeblock %}\n\nMost commands will require you to pass in a `--resource-group` and `--location` parameters. These parameters are `-g` and `-l` for short, but you can save yourself even more keystrokes by setting defaults for `az`.\n\n{% codeblock lang:bash %}\naz configure -d group=DavesFunctionApps\naz configure -d location=WestUS2\n{% endcodeblock %}\n\n## Creating a (temporary) Premium Hosting Plan\nThere is a strange requirement with Azure Functions / App Service. As per Jeff Hollan's sample:\n\n{% blockquote Jeff Hollan https://github.com/Azure-Samples/functions-csharp-premium-scaler %}\nThe Azure Functions Premium plan is only available in a sub-set of infrastructure in each region. Internally we call these \"webspaces\" or \"stamps.\" You will only be able to move your function between plans if the webspace supports both consumption and premium. To make sure your consumption and premium functions land in an enabled webspace you should create a premium plan in a new resource group. Then create a consumption plan in the same resource group. You can then remove the premium plan. This will ensure the consumption function is in a premium-enabled webspace.\n{% endblockquote %}\n\nFirst, add an Azure Functions Premium plan to the resource group.\n{% codeblock lang:bash %}\naz functionapp plan create -n dave_temp_premium_plan --sku EP1 --min-instances 1\n{% endcodeblock %}\n\nYou can delete this premium plan using the command below _after_ you've deployed a function app to this resource group . **Don't forget to delete the premium plan. These cost $$$**\n\n{% codeblock lang:bash %}\naz functionapp plan delete -n dave_temp_premium_plan\n{% endcodeblock %}\n\n\n## Creating a Function App\n\nThere are many options for creating a new function app. I really like the `func` command line tool which I installed using npm. Check out the [Azure Functions Core Tools GitHub Repo](https://github.com/Azure/azure-functions-core-tools) for details on other options for installing the `func` tooling.\n\n{% codeblock lang:bash %}\nnpm i -g azure-functions-core-tools@3 --unsafe-perm true\n{% endcodeblock %}\n\nThe focus of this blog post is around scaling a function app. If you don't already have an app built, you can follow along with [this walkthrough](https://docs.microsoft.com/azure/azure-functions/functions-run-local?tabs=windows%2Ccsharp%2Cbash) to create a function app.\n\nA function app requires a Storage Account resource. An Application Insights resource is also highly recommended as this really simplifies monitoring your function app after it has been deployed. Let's go ahead and create those 2 resources.\n\n{% codeblock lang:bash %}\naz storage account create -n davefuncappstorage\naz extension add -n application-insights\naz monitor app-insights component create --app davefuncappinsights\n\n{% endcodeblock %}\n\nNow we can create our Azure Function App resource with a consumption plan, passing in the name of the storage account and app insights resources that we just created. In my case, I'm specifying the dotnet runtime on a Windows host.\n\n{% codeblock lang:bash %}\naz functionapp create --consumption-plan-location WestUS2 --name davefuncapp123 --os-type Windows --runtime dotnet --storage-account davefuncappstorage --app-insights davefuncappinsights --functions-version 3\n{% endcodeblock %}\n\nRemember to delete that temporary Premium Hosting Plan now!\n\n{% codeblock lang:bash %}\naz functionapp plan delete -n dave_temp_premium_plan\n{% endcodeblock %}\n\n### Deploying your Function App using the az cli\nThis is a bit outside the scope of this blog post but I like using the `az` cli to deploy my function apps because it's easy to incorporate that into my CI/CD pipelines. Since my app is using the dotnet runtime, I use the `dotnet publish` command to build the app.\n\n{% codeblock lang:bash %}\ndotnet publish -c release\n{% endcodeblock %}\n\nThen, zip the contents of the publish folder (`bin\\release\\netcoreapp3.1\\publish\\`).\n\nIn PowerShell:\n{% codeblock lang:bash %}\nCompress-Archive -Path .\\bin\\release\\netcoreapp3.1\\publish\\* -DestinationPath .\\bin\\release\\netcoreapp3.1\\package.zip\n{% endcodeblock %}\nor in Bash\n\n{% codeblock lang:bash %}\nzip -r ./bin/release/netcoreapp3.1/package.zip ./bin/release/netcoreapp3.1/publish/\n{% endcodeblock %}\n\nFinally, use the `az functionapp deployment` command to deploy the function app.\n\n{% codeblock lang:bash %}\naz functionapp deployment source config-zip  -n davefuncapp123 --src ./bin/release/netcoreapp3.1/package.zip\n{% endcodeblock %}\n\n\n## Scale up to a premium plan\nOkay, now that we have a functioning (pun intended) app deployed and running on a consumption plan, let's see what it takes to scale this thing up to a premium plan.\n\nFirst, create a new Premium Hosting Plan with the parameters that make sense for the load you are expecting. The `--sku` parameter refers to the size of the compute instance: EP1 is the smallest. The `--min-instancs` parameter is the number of pre-warmed instances that will always be running for this hosting plan. The `--max-burst` parameter is the upper bounds on the number of instances that the premium plan can elastically scale out if more instances are needed to handle load.\n\n{% codeblock lang:bash %}\naz functionapp plan create -n davefuncapp123_premium_plan --sku EP1 --min-instances 4 --max-burst 12\n{% endcodeblock %}\n\nNext, move the function app to that premium hosting plan.\n\n{% codeblock lang:bash %}\naz functionapp update --plan davefuncapp123_premium_plan -n davefuncapp123\n{% endcodeblock %}\n\nThat's it! All it took was those 2 command and your function app is now running on a premium plan!\n\n## Scale back down to a consumption plan\nOf course, that premium plan isn't cheap. You might only want your function app running on the premium plan for a short period of time. Scaling back down is equally easy.\n\nFirst, move the function app back to the consumption based plan. In my case, the name of the consumption plan is `WestUS2Plan`. You should see a consumption plan in your resource group. \n\n{% codeblock lang:bash %}\naz functionapp update --plan WestUS2Plan -n davefuncapp123\n{% endcodeblock %}\n\nNext, delete the premium hosting plan.\n\n{% codeblock lang:bash %}\naz functionapp plan delete -n davefuncapp123_premium_plan \n{% endcodeblock %}\n\n## Wrapping it up\nIn this post, we saw how easy it is to move a function app between Premium and Consumption plans. A couple very simple `az` commands can help you get the performance and features of the Premium plan _only_ when you need it while taking advantages of the simplicity and cost savings of a Consumption plan the rest of the time. ","categories":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/categories/Azure/"},{"name":"Azure Functions","slug":"Azure/Azure-Functions","permalink":"https://westerndevs.com/categories/Azure/Azure-Functions/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Web Dev","slug":"Web-Dev","permalink":"https://westerndevs.com/tags/Web-Dev/"},{"name":"AZ CLI","slug":"AZ-CLI","permalink":"https://westerndevs.com/tags/AZ-CLI/"},{"name":"Azure Functions","slug":"Azure-Functions","permalink":"https://westerndevs.com/tags/Azure-Functions/"}]},{"title":"Kubernetes - My Journey","authorId":"dave_white","slug":"kubernetes-my-journey","date":"2020-05-22 17:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey\nauthorId: dave_white\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\ndate: 2020-05-22 13:00\n---\n# The Journey\n\nThis is a series of articles chronicling my learning journey as I was asked to build an IdentityServer4-based authentication system for one of my clients. This story included details about my adoption of Kubernetes, Azure Kubernetes Service, and all the things that I had to do to stand-up this client's new IdentityServer4 authentication implementation.\n\nAs with most learning experiences, I've made mistakes, arrived at working applications, adjusted my implementations, and continued to grow. My hope is that this series will continue to grow and evolve and be a bit of a living series of documents. I'll actively change articles when I discover something new or a better way to describe things. I'll add new articles or maybe alternate paths through the series as new topics present themselves. And I'm certainly not an authority on all of these topics, so as I get feedback from friends and peers, I'll certainly be making adjustments.\n\nI have two primary goals with this series. Documenting the learning journey is the first one, but the second and almost as important goal was to provide someone (you) with a complete set of steps to get a Kubernetes cluster up and running with an IdentityServer4 implementation running inside of it. Once someone has this platform up and running, they can continue to learn and grow in the same manner that I will but hopefully they're path getting to this point was a little smoother than mine was.\n\nAs always, comments and feedback are encouraged and very welcome.\n\nNow, on to the articles!\n\n## Series Table of Contents\n\n  1. [Business problems](/kubernetes/kubernetes-my-journey-part-1)\n  2. [Initial Assumptions, Technologies, Learning resources](/kubernetes/kubernetes-my-journey-part-2)\n  3. [Tools in Use](/kubernetes/kubernetes-my-journey-part-3)\n  4. [Building an ASP.NET Core IdentityServer Implementation](/kubernetes/kubernetes-my-journey-part-4)\n  5. Getting Started with Kubernetes - Minikube\n      - [Part A - Getting Started with Kubernetes - Minikube](/kubernetes/kubernetes-my-journey-part-5a)\n      - [Part B - Getting Started with Kubernetes - Minikube](/kubernetes/kubernetes-my-journey-part-5b)\n  6. [Pause to reflect](/kubernetes/kubernetes-my-journey-part-6)\n  7. Moving to Azure Kubernetes Service\n       - [Part A - Moving to Azure Kubernetes Service](/kubernetes/kubernetes-my-journey-part-7a)\n       - [Part B - Moving to Azure Kubernetes Service](/kubernetes/kubernetes-my-journey-part-7b)\n  8. [Making Your Kubernetes Cluster Publicly Accessible](/kubernetes/kubernetes-my-journey-part-8)\n  9. [Adding Cluster Logging (fluentd)](/kubernetes/kubernetes-my-journey-part-9)\n  10. [Tuning resource usage](/kubernetes/kubernetes-my-journey-part-10)\n\n## Approach to this series\n\nI've decided to break this series into discrete posts to make this easier to write and consume. I'll have specific topics for a post that are aligned with the overall vision, and you'll be able to read the parts that are important to you.\n\nThis series is going to strive to demonstrate work that is completely done. Every bit of code in each post should work and be complete. I'll point out bits of knowledge and wisdom I've learned along the way, but the intention is to give you a working system that you can then alter/re-create and learn from that experience.\n\nAll the code, projects, manifests, etc. are (or will be) in [Github here](https://github.com/agileramblings/my-kubernetes-journey).\n\n**Enjoy**\n\n**Next up:**\n[Business problems](/kubernetes/kubernetes-my-journey-part-1)\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 1","authorId":"dave_white","slug":"kubernetes-my-journey-part-1","date":"2020-05-22 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-1/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-1/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 1\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 12:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n# The Business Problem\n\nMy client has been in business since 1993. As you can imagine, software has been an integral part of their ability to deliver services to their customers. About 10 years ago, they had a re-platforming initiative that was the start of their current monolithic, critical LOB system. It is an ASP.NET MVC 5.x application that has evolved over time and is certainly reaching the limits of what its architecture can provide. It isn't \"cloud-native\" and it basically continues to be enhanced/evolved based on assumptions that were made 10 years ago.\n\nFor this project, the important part to understand is that it used forms authentication and has a user store internally that manages all of the user accounts and authorization rules. It currently only uses cookies to store authenticated user details on the client and doesn't support any modern sort of token-based activities that are needed for modern applications.\n\nIt is also important to understand that the platform is growing as users and customers demand more modern and specific user experiences. New web applications (React), mobile devices (native), IoT devices, and system-to-system integrations are all being added to the platform and all these new applications need to participate in the authentication scheme. In some cases, new applications have been created but because the current LOB system doesn't support modern authentication techniques, they cloned the existing user store and used the data to implement the appropriate authentication techniques for their specific use case. This has led to a bunch of applications re-implementing their own authentication, with everyone sharing the user store (or a copy of it) of the core LOB system.\n\nSo, the current system has demonstrated a few key problems that the new system needs to address.\n\n### Security is hard and incredibly important\n\nOne of the plagues of modern business is bad actors attacking various systems trying to gain access and cause lots of troubles. This is an ever-present problem for most corporations. Security needs to be a first-class citizen in all projects. Using modern authentication platforms that are proven and community-reviewed is seen as a requirement.\n\nSpeaking of security, in this series, you will find a lot of usernames, passwords, and internal details about the systems that is being built. I would ask that you **please take a great deal of care when building your systems!** I'm not worried about presenting these details as I'm tearing all of this down all the time and it won't be left running on my Azure accounts. I will change passwords, usernames, and all kinds of other details to make it harder for anyone to break into a running system. I strongly encourage you to do the same.\n\n### Decentralized identity management sucks\n\nThe platform is currently a monolithic LOB system that provides **all** functional aspects for the various business groups. The platform is also growing a collection of loosely related applications that basically represent specific implementations that are tailored for each individual area of the business. All the systems are re-implementing authentication while using, in most cases, a copy of the LOB user store. This basically means that we have multiple user stores that all must be managed independently. There are SSIS workflows trying to keep the right data in sync while not clobbering specific customizations to the user store data.\n\nThis is all complicated, complex, and error prone. We generally don't have a lot of troubles with authentication, but it is expensive and time-consuming to setup and maintain and as new applications are added, it adds more things to manage and has more things that can go wrong.\n\n### New applications require modern Identity technologies\n\nIn 2020, business that used to be well-served by a monolithic web application, are finding that this is no longer the case. Native applications, new web applications based on new technologies (SPA), IoT devices and System-to-System integrations all require more modern authentication technologies and these applications are being added to platform ecosystems at an increasing pace.\n\n### Reduce the costs of running applications\n\nOne of the overall initiatives for this client is to become more cloud-native and reduce the running costs of applications. There is already an initiative to move all the platform servers to Azure, but we didn't want any new applications continuing the pattern of \"lift and shift\" used for the existing apps. There is a strong desire to leverage containers (docker) and orchestrators (Kubernetes) at an increasing rate for all the applications in order to reduce overall run costs.\n\n### Being able to scale up is important\n\nAs with all businesses, we expect growth. My client is heavily reliant on devices for their current business needs and will be adding customers, native apps, and IoT devices at an ever-increasing rate. This means that having the ability to scale vertically or horizontally needs to be present from the start. Growth was anticipated (pre-COVID19) to be > 30% year over year for the next 3-5 years so there were going to be lots of people and devices needing to use the system in the future.\n\n## Summary (TL;DR;)\n\nSecurity is hard! De-centralized (read: many) authentication systems are expensive. Old Application are ... well... old! Modern applications need support! Moving to the cloud is happening! And businesses grow!!\n\nI hope that this gives some context that helps you understand some of the decisions that I'll be sharing in the next section!\n\n**Next up:**\n[Initial Assumptions, Technologies, Learning resources](/kubernetes/kubernetes-my-journey-part-2)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    \n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 2","authorId":"dave_white","slug":"kubernetes-my-journey-part-2","date":"2020-05-22 15:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-2/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-2/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 2\ncategory: kubernetes\ntags: kubernetes, azure, **AKS**, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 11:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Business problems](/kubernetes/kubernetes-my-journey-part-1)\n\n# Initial Assumptions, Technologies, Learning Resources\n\nThere are some assumptions that were made before my involvement with this company that made some of this decision making easy. Those assumptions were driven by the business problems we discussed previously.\n\nSome of these decisions were made by me throughout this project. It was important for me to understand the organization's business, goals, and culture when making these decisions so they weren't as easy to make. The good thing is that I've got a great group of leaders and developers in this organization that helped me and trusted me to make better decisions on their behalf.\n\n## Pre-defined Assumptions\n\nSo, there are a bunch of things that we're decided before me arriving on the scene! These decisions governed my initial decision making to just get the project started.\n\n### We're going to the cloud\n\nI'm not sure this needs a lot of elaboration. Everyone is going to the cloud. Owning and operating data centres is hard. Companies like Microsoft, Amazon, and Google have commoditized hardware setup, management, and maintenance so much so that it almost doesn't makes sense to own your own data centre. So, anything I did should strive to be cloud-native.\n\n#### Azure\n\nAzure was chosen as the vendor for all our cloud services prior to my involvement. This certainly plays to my strength as a Microsoft-based technologist and an avid Azure user for all my own projects.\n\n#### ASP.NET Core (C#)\n\nThis company is a Microsoft shop. All the developers are C# developers who are familiar with Microsoft technologies, so it only makes sense that we are going to leverage the existing people and skills that are present in the organization. Using .NET Core and ASP.NET Core 3+ in all projects going forward was made a requirement (and is probably simple common sense) by the assumption of moving to the cloud.\n\nI couldn't build something that only I could maintain or would require the company to go and try to find new developers or acquire new skills in their current developers. There is going to be a lot of learning being done by everyone here, but we don't need to add to the pile of new stuff to learn.\n\n#### HTTPS\n\nOne of the things that seems obvious, but turns out isn't always obvious, is the fact that in today's world, we should be using HTTPS everywhere. It has become easier and easier to do this, and part of this project's mandate was to ensure that we accelerated the adoption of HTTPS throughout the deployed platform and make it _easier to own and maintain_. Simply doing an Identity implementation will drive this.\n\n## Decisions I made\n\nDuring the planning of this project, I had the flexibility to make a lot of decisions. I didn't make them in isolation, but I did get to make the final decisions.\n\n### Containers and Orchestration\n\nGoing to the cloud can be as simple as lift-and-shift. Pick up your server, virtualize it if necessary, and place it in the cloud. In some cases, we can make this even simpler by provisioning a VM with all the pre-installed parts we need and simply deploying our application into that new VM. This is certainly \"going to the cloud\" but it isn't considered _cloud-native_.\n\n#### Docker\n\nI'm not an expert on what _cloud-native_ entirely means, but I've discerned that one thing you need to do is containers. So today, that means **Docker**. Being a Microsoft shop, going to Azure, using Visual Studio and the az cli as tools, Docker was a no-brainer for a container engine with all the great tool support in Visual Studio, Azure DevOps Server, and the other tools, so I just rolled with it.\n\n#### Kubernetes\n\nWhen using Azure and running containers in the cloud, you have several options. Two easy ones are Azure Container Instances (server-less container hosting) and **Azure Kubernetes Services (aka AKS)**. Since our environment was going to be far more complicated than running a bunch of single containers in the cloud, we needed an orchestration platform. There are two options for that today: Kubernetes and DockerCompose and we chose Kubernetes. The community support for Kubernetes is incredible, the momentum it has in the marketplace can't be denied, and it is natively supported by Azure via the **AKS** product, so it was another easy decision.\n\n### Cloud-Hosted\n\nI sort of covered this one off, but there is an option to stand up your own Kubernetes cluster manually in Azure simply using Virtual Machines. **AKS** effectively took this option off the table. In my experience so far, **AKS** is a polished product, easy to use, and the control plane components are included for free. You simply pay for all the other Azure products that will make up your cluster. These include VMs, Storage, Networking, and any other products you leverage in your implementation.\n\n#### Azure Kubernetes Service (**AKS**)\n\n**AKS** also makes some of our *ability to scale* requirements much easier to address. You can scale vertically by dynamically increasing the size of the VMs in the cluster and we can scale horizontally by dynamically adding (or removing) nodes from the cluster.\n\n### Identity Implementation\n\nWhile this project has many objectives, they key objective for all this work was to deliver a new Identity management implementation to the platform that would enable a secure, modern way for all users, applications, and devices to authenticate with each other.\n\nOne of the underlying cultural desires at this client that I should state is a strong desire to minimize subscriptions and dependencies on 3rd party vendors. This desire precluded us from selecting any of the current Identity providers that exist on the marketplace today. AuthO and Okta are two such companies and while I'm sure they have great products, we\ndecided to build our own Identity system.\n\n#### IdentityServer4\n\nNow, we didn't want to build all the identity system from scratch! That doesn't make sense. We'd have to become experts in OAuth, OpenID Connect, flows, encryption, tokens, and all those technologies that are implemented by great communities out there already. This OSS frameworks are also scrutinized and vetted by the community, so you know that a lot of people care that they are done right. Basically, we wanted to take the best implementation of a security framework in the Microsoft open-source community, host it in our infrastructure, and be able to customize it to our specific use-cases. This led us to IdentityServer4, which is the core technical component of our Identity implementation.\n\n#### Skoruba IdentityServer4 Admin \n\nOne of the things about identity implementations is that there is a lot of configuration and user data to manage. To be honest, client configuration and user data is one of the biggest PITA about doing Identity work. Once you get it, it isn't so bad but so much of a successful roll-out with an identity platform hinges on getting all this data right. And to do that, it helps to have a good administrative platform.\n\nWhile I'd love to have had the time to write an application that gives a good user experience for the administration of this data, I didn't have that time. So, I went looking for something in the community that used IdentityServer4 as a foundational component and added the user experience I wanted. And after several pilots, I found and selected Skoruba IdentityServer4 Admin. This is an ASP.NET Core 3.x web application that provides the Security Token Service (STS), and Administrative Application, and an Administrative API, all in one easy to use package! I've been really happy that I found it.\n\n### Data Persistence\n\nThe Skoruba templates allows for the selection of one from three different built-in persistence providers via Entity Framework Core. SqlServer, MySql and Postgres.\n\n#### Postgres\n\nPostgres was the database provider the I selected. This reduced our licensing costs, provides the appropriate level of performance and functionality, and with us being in Azure, we always have the option of moving to **Azure Database for PostgreSQL** which has single instance SKU and an HA/Hyperscale SKU if that eventually becomes a needed capability. There is also a ton of community support around the product, which means libraries, documentation, and examples are plentiful.\n\n### Logging\n\nI cannot under-state how important logging is when developing, maintaining, and owning a multi-container environment, spread across multiple servers (nodes) that are hosted in the cloud. This isn't the first and hopefully won't be the last blog post to emphasize this point. The community building cloud-based applications already blogs about this, you've almost certainly read about it somewhere before here, but it still may not have sunk in that, perhaps, this needs to be the first thing you figure out in your new container-based platform. I paid some attention at the beginning, but not enough.\n\n#### Seq - Log Ingestion and Analytics\n\nWhen I first arrived at my client, they were still relying on file-based logging, spread over all the servers in the farm. They had a lot of logging in the application, but it wasn't easily accessible, and it wasn't easy at all to find out what happened across a workflow, sometimes broken, that traversed a bunch of servers.\n\nThe other thing that wasn't possible was querying and basic analytics of the log data that was being generated. It was inconsistent, unstructured, and just barely helping them solve problems.\n\nAt a previous engagement, I had used Splunk and really came to love it! The tool was great. Powerful queries, visualization, dashboard, alerts, integrations! Splunk has it all. There was only one problem. Splunk is expensive. That isn't a problem if your needs justify the spend and you have the required expertise in owning and operating Splunk, but that wasn't the case here, so I needed to find something easier to bring into the ecosystem. This was when I found Seq!\n\nSeq is a great entry-level tool (that keeps on getting better) for organizations that are just getting started on getting logs into a central product and using that log data to analyze problems and operational aspects of the platform. Seq is built by Datalust.co, who also makes Serilog the .NET library that we use for logging internally in the application. This combination has turned out to be a cost-effective, easy to learn, setup and maintain logging infrastructure for our apps, so it was quite easy for me to bring this into the Identity project and the **Kubernetes (k8s)** infrastructure.\n\n#### Fluentd\n\nIn addition to the identity applications that are in the **k8s** cluster that will be logging to Seq, the infrastructure applications in the cluster _itself_ will generate an incredible amount of information about its operations and problems. This is a non-trivial log ingestion problem that has been made very approachable with a product called **fluentd**. Built to live inside of **k8s** clusters, fluentd is basically an event processing pipeline implementation that takes events from log files that are written all over the **k8s** cluster. It takes care of finding the files, processing them as they get additional entries, and sending those events \"somewhere\". In this case, I used an fluentd docker image that emits logs to **graylog** consumers, which thankfully, Seq is with an addon. So, with Serilog in the applications, fluentd in the cluster using a graylog variant and Seq running in the cluster to ingest and aggregate all the log information, we have a very compact, useful logging strategy to get us started.\n\nI don't know how far this logging implementation will grow with the cluster into the future, but it is designed to have pieces replaced as we outgrow their capabilities.\n\n### Learning Resources\n\nThis adventure required an incredible amount of learning on my part. The way that I learn best is by reading/learning enough to get started, and then building like crazy, discovering deficiencies in what I know, resolving the deficiency, and then finding the next thing I don't know.\n\nThis series of blog posts will hopefully capture some of that learning experience, but I wanted to make sure I shared the places that I started.\n\n#### Pluralsight\n\nThere are a lot of courses on Pluralsight. I think I've enjoyed all them, with some courses being better than others for me at that point in my learning. For this project, I didn't need any help with C# or ASP.NET Core or any of that part. I needed to get familiar with Kubernetes and I needed to become familiar with authentication flows using OAuth2 and OpenID Connect.\n\nThe **k8s** courses were from Nigel Poulton were great places to get started with **k8s**. The ones that I watched are:\n\n- [Getting Started with Kubernetes](https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents)\n- [Docker and Kubernetes: The Big Picture](https://app.pluralsight.com/library/courses/docker-kubernetes-big-picture/table-of-contents)\n\nWhen getting more details on OAuth2 and OpenId Connect, Kevin Dockx and Micah Silverman had two nice courses to review.\n\n- [Securing ASP.NET Core 2 with OAuth2 and OpenID Connect](https://app.pluralsight.com/library/courses/securing-aspdotnet-core2-oauth2-openid-connect/table-of-contents)\n- [THAT Conference '19: OAuth 2.0 and OpenID Connect (In Plain English)](https://app.pluralsight.com/library/courses/that-conference-2019-session-07/table-of-contents)\n\n> There is a new version of Kevin Dockx course available using ASP.NET Core 3 [here](https://app.pluralsight.com/library/courses/securing-aspnet-core-3-oauth2-openid-connect/table-of-contents)\n\n#### Kubernetes.io\n\nBased on what and how I was learning **k8s** from Nigel on Pluralsight, Kubernetes.io became an invaluable resource for me to learn about **k8s** and its ecosystem. Nigel's courses encourage you to work with manifests, building your **k8s** cluster in a declarative manner, and kubernetes.io supported that approach well. I would highly recommend working with manifests and understanding them and how they work when learning **k8s**. We'll discover later that I don't work directly with manifests when provisioning my cluster resources, but you have to know manifests because all the examples in the communities are in manifests!\n\n#### Community blogs\n\nThis would have been much harder without the incredibly vibrant community of bloggers in the world who are sharing what they are doing, the problems, and how they are solving those problems. I'm hoping that my addition in writing this blog fills a gap and makes it a bit easier for you or someone else to put this all together, but this would have been incredibly difficult without this vibrant community.\n\n**Next up:**\n[Tools in Use](/kubernetes/kubernetes-my-journey-part-3)\n\n##### Links\n\n- [Azure](https://azure.microsoft.com/en-us/)\n- [Azure Kubernetes Services](https://azure.microsoft.com/en-us/-services/kubernetes-service/)\n- [Kubernetes](https://www.kubernetes.io)\n- [IdentityServer4](https://identityserver4.readthedocs.io/en/latest/)\n- [Skoruba IdentityServe4 Administration](https://github.com/skoruba/IdentityServer4.Admin)\n- [Pulumi](https://www.pulumi.com/)\n- [Seq Log Ingestion](https://datalust.co/seq)\n- [Fluentd](https://docs.fluentd.org/)\n- [Postgres](https://www.postgresql.org/)\n- [pgAdmin4](https://www.postgresql.org/)\n- [CertManager](https://github.com/jetstack/cert-manager)\n- [Nginx](https://www.nginx.com/)\n- [Traefix](https://containo.us/traefik/)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, **AKS**, identityserver, docker, containers","slug":"kubernetes-azure-AKS-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-AKS-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 3","authorId":"dave_white","slug":"kubernetes-my-journey-part-3","date":"2020-05-22 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-3/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-3/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 3\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 10:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Initial Assumptions, Technologies, Learning resources](/kubernetes/kubernetes-my-journey-part-2)\n\n# Tools in Use\n\nTooling is an incredibly important aspect of a developer's daily life. IDEs, CLIs, Automations, Visualizations; the list goes on and on. Sifting through the set of tools that are available and finding out if they work for you can take a lot of time. As a former Microsoft MVP (Dev Tools), I'm always interested in tooling because I know the importance it can make in developer productivity and the overall productivity and quality for an organization! I'm hoping that by sharing my base set of tools, and what I use them for, you'll get a bit of tool-curation time savings back to spend on learning other things.\n\nThese are the tools I use **every day** on this project.\n\n## Visual Studio\n\n| | |\n|------------------------------------|:--:|\n|[Visual Studio](https://visualstudio.microsoft.com/vs/) has come a long, long way since the first time I ever used is as Microsoft Visual Studio 97 (5.x). Wow! This IDE has been my constant companion for the entirety of my development career, and I have to say that the current VS 2019 version of the tool is a pleasure to work with. Given the complex nature of building projects and supporting all of the ecosystems that we build projects for, VS 2019 does a fantastic job of supporting _my_ needs.|<img width=\"200px\" src=\"https://s3.amazonaws.com/neowin/news/images/uploaded/2017/02/1486663278_visual-studio-97.jpg\" alt=\"Visual Studio 97\">|\n\nWith this project, I obviously appreciate all of the normal C# coding functions that are present, but the docker/docker-compose support is particularly important and the ability to debug applications running in docker containers is great.\n\nI still have [ReSharper](https://www.jetbrains.com/resharper/) and I still have many extensions (Thanks [Mads](https://marketplace.visualstudio.com/publishers/MadsKristensen)!) running in Visual Studio, but this IDE is the workhorse of my day-to-day activities when I'm working in C#.\n\n> I don't recommend that you download and try to use Microsoft Visual Studio 97!\n\n## VS Code\n\n[Visual Studio Code](https://code.visualstudio.com/) is a fantastic, light-weight text editor with an incredible extensibility feature that the community has taken full advantage of! Out of the box, it is very good at one thing. Editing text files. With all the extensions being written and placed on the marketplace, it has become some people's full-time IDE. The great thing about VS Code is that it runs on macOS, Linux, and Windows, so you take it wherever you go! And, it's free! It has mostly replaced all my other text editors, with the exception of **Notepad**. Still use that one from time to time.\n\nI use VS Code for editing all my manifests, **Pulumi** Typescript applications, and markdown files for documentation. This blog post was written in Markdown using VS Code! With an integrated terminal window using PS Core, I can do pretty much all my **k8s** deployment and resource work without leaving VS Code.\n\n## Pulumi\n\nI've mentioned that while I find manifests a great way to learn **k8s**, I've adopted a different approach for deploying **k8s**. That approach is from a company called [Pulumi](https://www.pulumi.com/)! What they've basically done is built a platform and multiple SDKs in various languages that allow us to do **Infrastructure as Code** in a programming language of our choice!! It's a great way to define and manage your cloud resources. Once you have written your application that knows what you want to do, you simply tell the **pulumi cli** to `up` or `destroy` your infrastructure! `pulumi up` and it will create or update your infrastructure resources as needed, and `pulumi destroy` tears it all down and cleans everything up!\n\n## Git\n\n[Git](https://git-scm.com/) is the most popular and arguably the defacto source control system in the world today. All my manifests and pulumi typescript files are version controlled in git, as are all the Identity application projects that we'll be building later on.\n\n## Azure DevOps Services (Server 2019)\n\n[Azure DevOps (Server)](https://azure.microsoft.com/en-ca/services/devops/) is our DevOps management software. Source control (Git and TFVC), Work Item Management, Builds, and Deployments are all provided by this service. It provides us a tremendous amount of automation, and it gives us a place to make our knowledge about how to build and deploy our applications concrete! If you haven't tried Azure DevOps recently, you really should give it another go. It's been a tremendously valuable addition to the development process.\n\n## Kubernetes Web UI (Dashboard)\n\n[Kubernetes Web UI (Dashboard)](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) is a great dashboard that is built into your **k8s** cluster that provides a User experience to help you visualize and manage your **k8s** cluster.\n\n## Octant\n\n[Octant](https://octant.dev/) is a dashboard for your **k8s** cluster, similar to the Kubernetes Dashboard, but it doesn't require you to port-forward to your local machine to get at the dashboard, it simply runs on your local development machine and uses the kubectl current context to access the cluster! This makes is quite easy to spin up. It provides port-forwarding capabilities in addition to many of the capabilities present in the Kubernetes Web UI. I find myself using them both quite often throughout the day.\n\n## K9s\n\n[K9s](https://k9scli.io/) is a console-based user experience for managing your **k8s** cluster! As with Octant, it uses kubectl and your current context to determine which **k8s** cluster it is managing. I quite like it! I also find it really interesting how I have three different tools for managing the **k8s** cluster and depending on what I'm doing or even my mood, I can pick from any of the three tools.\n\n## Honorable Mention - Docker Desktop for Windows\n\n[Docker Desktop for Windows](https://hub.docker.com/editions/community/docker-ce-desktop-windows) is the easiest way to get docker containers running on a windows desktop. You need a CPU that supports virtualization and you have to have Windows 10 Pro for the HyperV/WSL2 support. There are other options if you aren't running Windows 10 Pro and up, but I'll leave that as an exercise for you to figure out. But during initial development of the Skoruba project, it was immensely helpful to be able to build, deploy, and debug the application without having to deal with **k8s** or **minikube**. There are additional complexities involved with that that you will want to defer for a while.\n\n**Next up:**\n[Building an ASP.NET Core IdentityServer Implementation](/kubernetes/kubernetes-my-journey-part-4)\n\n### Links\n\n- [Visual Studio](https://visualstudio.microsoft.com/)\n- [VS Code](https://code.visualstudio.com/)\n- [Pulumi](https://www.pulumi.com/)\n- [Git](https://git-scm.com/)\n- [Github](https://www.github.com)\n  - [Git Simulator - Github](http://git-school.github.io/visualizing-git/)\n- [Azure DevOps (Server)](https://azure.microsoft.com/en-ca/services/devops/)\n- [Kubernetes Dashboard](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)\n- [Octant](https://octant.dev/) - [Github](https://github.com/vmware-tanzu/octant)\n- [K9s](https://k9scli.io/) - [Github](https://github.com/derailed/k9s)\n- [Useful Interactive Terminal And Graphical UI Tools For Kubernetes - Virtually Ghetto](https://www.virtuallyghetto.com/2020/04/useful-interactive-terminal-and-graphical-ui-tools-for-kubernetes.html)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 4","authorId":"dave_white","slug":"kubernetes-my-journey-part-4","date":"2020-05-22 13:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-4/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-4/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 4\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 09:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n [Tools in Use](/kubernetes/kubernetes-my-journey-part-3)\n\n# Building an ASP.NET Core IdentityServer Implementation\n\nThis is where the articles start to get a bit longer. I've broken a couple up into a-b parts. We won't be talking about tasks in **k8s** for a bit, so if you want to skip on to that, you can go [here](/kubernetes/kubernetes-my-journey-part-5).\n\nThere are, in my opinion, 2 parts that need to be considered when building up this IdentityServer4 implementation, and the 2nd part can arguably be split into 2 smaller parts, which is how this implementation is built.\n\nThe first part of the system needs to be the **Security Token Services (STS)** implementation. This part of the system is simply responsible for managing tokens. This means validating log in credentials, creating tokens, refreshing tokens, and sending along the appropriate metadata and claims. I would want this broken into a separate service because it will probably have different operational characteristics then the other parts of the application, and so we can scale is separately. If there are 1000s of request per minute to generate/validate/refresh tokens and send along metadata, that is different than administrating the data that the system relies on. Additionally, it makes it easier to secure the administrative access if it is a separate application.\n\nThe second part of the system is the **Administrative (Admin)** implementation. The application that provides a user experience that will help identity administrators do the right things.\n\nIn our implementation, which is based on the Skoruba project, there is actually a third part to the administrative side of the system and that is a **Admin WebAPI** that does all of the actual CRUD on the database. I'm happy with this setup and I'm glad the Skoruba project went in this direction.\n\nLet's get into the details of what I did.\n\n## Start Learning with the IdentityServer4 Quickstart\n\nIf you are new to IdentityServer4 or even newer to OAuth2 and OpenID Connect, your first stop needs to be the [IdentityServer4 Quickstarts](https://identityserver4.readthedocs.io/en/latest/quickstarts/0_overview.html). These quickstart modules will walk you through all of the steps required to incrementally build an IdentityServer4 server. They are all self-contained, building your knowledge from one to the next, and by the end of the quickstarts, you've built a functional IdentityServer4 implementation. I'm not going to re-produce any of these quickstarts or even how to get you started on them, other than providing the [link](https://identityserver4.readthedocs.io/en/latest/quickstarts/0_overview.html) and some encouragement. Go do the quickstarts, then come back here and continue reading!\n\n> It was during the quickstarts that I side-stepped and did the Pluralsight courses on OAuth2 and OpenID Connect.\n\n## Starting your project with Skoruba\n\nSo, you've done the quickstarts, or you're already experienced with IdentityServer4 and the real reason you are here is because the quickstarts left you wanting more! You wanted an Administrative application!! You wanted databases to persist all of this configuration and the quickstarts don't give you any of that! You're in the right place! I was in that position just a little while ago myself.\n\nAfter doing the quickstarts, I realized I had to build/find an administrative experience for our platform. It isn't hard to find a couple when you google _IdentityServer4 Administration_. There are a number of Github repos that you'll find, and perhaps a couple products that you can purchase. Again, because of the desire to build/own vs. buy/subscribe, we continued looking until we found the [Skoruba IdentityServer4 Admin](https://github.com/skoruba/IdentityServer4.Admin) project on Github. This is a great project and I've been very happy that I found it. It is put together well and is super easy to get started with.\n\nIn order to use the project, you'll need to have the latest ASP.NET Core 3.x SDK and development tools.\n\nThe first thing I did was get the awesome template that the Skoruba team build to get you started using their project. This command tells the dotnet tooling to go off to nuget.org and get the template.\n\n`dotnet new -i Skoruba.IdentityServer4.Admin.Templates::1.0.0-rc1-update2`\n\nOnce you have the template, you are ready to start creating your solution. You can start by using the template.\n\n```ps\ndotnet new skoruba.is4admin --name MyProject --title MyProject `\n--adminemail \"admin@codingwithdave.xyz\" --adminpassword \"P@ssw0rd!\" `\n--adminrole IdentityAdminRole --adminclientid MyClientId `\n--adminclientsecret MyClientSecret --dockersupport true\n```\n\nA couple notes about that command that make sense once you have done the ID4 Quickstarts:\n\n- `-adminrole IdentityAdminRole` is the User Role that will be used in the Admin application and the AdminAPI to know if a user is an administrator of the Identity system. This role will be assigned to your admin user account.\n- `-adminclientid MyClientId` is the ClientId (username of your application) for the Admin application in the STS. Your Admin application needs this id to be allowed to access the STS.\n- `-adminclientsecret MyClientSecret` is the ClientSecret (password of your application) for the Admin application in the STS. It is used with ClientId.\n\nSince we are planning to run this all in Docker locally and eventually deploying this to a **k8s** cluster, Docker support is required. All of the yaml examples in this post are in the docker-compose.yml file.\n\nOnce that command has run, you should have a project that looks something like this:\n\n<img src=\"/images/dwhite/Skoruba-projects-initial.png\" alt=\"Skoruba Initial Projects Setup\" height=\"250px\">\n\n### Select your DB Platform\n\nOne of the cool things about the Skoruba template is that it comes out of the box with 3 different database persistence mechanism already waiting for you. You can just select the one you want to use, and off it goes. I don't need to provide the ability to switch mechanisms, so I'm simply going to remove the templated **MySql** and **SqlServer** options.\n\nIf you are planning to use SqlServer in your **k8s** cluster, you can **skip** these instructions and the next Postgres/pgAdmin4 instructions. You could delete everything except for SqlServer implementation details. I understand why the template spits it out, but your running application probably won't need persistence choices.\n\n### So you selected PostgreSQL\n\n[PostgreSQL (Postgres) is an open-source database](https://www.postgresql.org/) that is more than sufficient for our use-cases. It is free to use/run and has great support in the community. In order to minimize costs, I choose to use PostgresSQL.\n\nThe first thing I did was delete the MySql and SqlServer projects. I wasn't planning to use them and as such, just deleting them is the right course. They can always be put back in if the need ever arose to migrate to a different database platform. This is going to immediately break your solution. You can just go and delete/fix all of the broken code. You're just removing `using` statements and adjusting/removing `if {}` blocks. I also deleted all of the database types/selection code and configuration settings.\n\n<img src=\"/images/dwhite/delete-switch-block.png\" alt=\"Deleting a Switch Block\" height=\"250px\">\n\nThat should leave you with a solution that builds and that only contains these projects.\n\n<img src=\"/images/dwhite/skoruba-projects-only-postgres.png\" alt=\"Skoruba Final Projects Setup\" height=\"250px\">\n\nYou probably also want to delete the `db` entry in the docker-compose.yml file. The SqlServer image is a large image and you don't necessarily need to download it.\n\n```yaml\n# remove this entry\n  db:\n    image: \"mcr.microsoft.com/mssql/server\"\n    ports:\n      - 1433:1433\n    container_name: skoruba-identityserver4-DB\n    environment:\n      SA_PASSWORD: \"${DB_PASSWORD:-Password_123}\"\n      ACCEPT_EULA: \"Y\"\n    volumes:\n      - dbdata:/var/opt/mssql\n```\n\n> In case you didn't know, Docker Desktop is the local image registry for all things Docker on your Windows workstation. All images will be downloaded and cached here. Docker Desktop is _not_ the local image registry for **k8s** (minikube).\n\n## Backend Containers - Postgres,  pgAdmin4 & Seq\n\nNow, your solution is building and ready to run, but our local infrastructure isn't quite there yet. We're going to need a Postgres database instance on our local machine to run this in Visual Studio, so we can do that in the docker-compose.yml file.\n\nTo bring a Postgres database container into Docker Desktop, add the following:\n\n```yaml\n  postgresdb:\n    image: postgres:alpine\n    hostname: postgres\n    ports:\n      - 5432:5432\n    container_name: postgresdb\n    environment:\n      - \"POSTGRES_USER=admin\"\n      - \"POSTGRES_PASSWORD=P@ssw0rd!\"\n      - \"POSTGRES_DB=identity\" # this is the DB name that will be generated by EFCore\n    volumes:\n      - postgresdata:/var/lib/postgresql/data\n    networks:\n      default:\n        aliases:\n          - postgres\n```\n\nWhat this does is get the latest version of the Postgres container image from DockerHub. It exposes the default Postgres port of **5432**, gives this container an alias in the DNS of **postgres**, and hooks the container up to a persistent volume, mapped to a volume on the local host, to store its identity data. We need to add that mapping near the bottom of the existing docker-compose.yml file. We will add a mapping for **pgAdmin4** while we are at it.\n\n```yaml\nvolumes:\n  postgresdata:\n    driver: local\n  pgdata:\n    driver: local\n```\n\n> There is a lot of YAML in **k8s** and **docker**. You are going to have to become familiar with it. I'm going to assume that you'll work through any yaml syntax errors that may come out of working through the articles.\n\nAfter adding in Postgre, we can now add in our pgAdmin4 container instance. pgAdmin4 is a database management tool built as a web application. If you have another Postgre management tool, you don't need to follow these steps.\n\n```yaml\n  pgAdmin4:\n    image: dpage/pgadmin4:4.20\n    ports:\n      - 5050:80\n    container_name: pgAdmin4\n    environment:\n      - \"PGADMIN_DEFAULT_EMAIL=admin@codingwithdave.xyz\"\n      - \"PGADMIN_DEFAULT_PASSWORD=P@ssw0rd!\"\n      - \"PGDATA=/mnt/data/pgdata\"\n    volumes:\n      - pgdata:/mnt/data/pgdata\n```\n\nThe pgAdmin4 container doesn't need an alias. We gave the Postgre instance a DNS alias so that all of the running containers in the cluster can simply reference the database (in the connection string) by the DNS entry of postgres. The IdentityServer4 apps and pgAdmin4 don't really need this kind of mapping so we'll leave them as accessible at the **http://127.0.0.1.xip.io:port** addresses.\n\nIf you want to explore more options in the pgAdmin4 configuration that are available to you, you can go to the [pgAdmin4 Container Configuration](https://www.pgadmin.org/docs/pgadmin4/latest/container_deployment.html) page for more details.\n\nWe're also going to add a Seq instance into our docker-compose.yml file.\n\n```yaml\n  seq:\n    container_name: seq\n    image: datalust/seq:preview\n    ports:\n      - \"5341:80\"\n    environment:\n      - \"ACCEPT_EULA=Y\"\n```\n\nWhen I added the **Seq** container, I did _not_ give it a persistent volume on the host to store log data. In the case of Seq, I'm using it for transient purposes in docker so I don't really care if the history of log entries goes away when the container is re-started. If you do want to keep your log files, you can give it a persistent volume on the host, in the same way as the databases.\n\n> Persistent Volumes do not survive a Docker Desktop data purge.\n\nWe are going to add _one_ more little container that I want to introduce now before it really appeared in my story, but I wish that I had found it earlier. It is a small little container from **Google** that helps me do DNS diagnostics and debugging in the containers/**k8s** network. You can quickly and easily add this container at just about any time, do your diagnostics, and then remove it from the cluster. It automatically stops after 1 hour as well.\n\n```yaml\n  dnsutil:\n    container_name: dnsutils\n    image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3\n    command: \"sleep 3600\"\n```\n\n## Configuration Changes\n\nWe aren't quite ready to run things yet. We need to make some more configuration adjustments.\n\n> You should be prepared to make a lot of configuration adjustments in your IdentityServer4 and **k8s** career!\n\nHere are the things I've done to make this easier/cleaner for running locally.\n\n1. Change all usernames to **admin**\n1. Change all admin passwords to **P@ssw0rd!**\n1. Change all Role entries to **IdentityAdminRole**\n1. Delete all of the unnecessary launchSettings profiles\n1. Tweak the URL configuration values\n1. Change all of the database connection strings to point at our local Postgres instance\n1. Tweak the code to always run migrations and seeding data on startup\n1. Configure Serilog to use Seq and the console\n\n### Change Credentials and Role\n\nYou will have lots of chances once you've explored and learned about all of these configurations to change usernames, potentially move to Azure Managed Identities, or use something like Azure KeyVault to store your credentials. For the time being, we want this to be quite easy to do, so we're going to simplify this and make all the admin accounts the same.\n\n- **Postgres** - If you revisit the Postgre yaml snippet, you see the default Postgre power user is **admin** and **P@ssword!**.\n- **pgAdmin4** - You'll see that the pgAdmin4 wants an email address, so in the yaml snippet we see the username **admin@codingwithdave.xyz** and **P@ssw0rd!**\n- **Seq** - currently requires no authentication\n\nThere are three .json files in a file in the root of the solution called **shared**. These files hold seed data for the applications. They are joined to the docker containers and are used by the running instances of the applications when they are in docker. You need to edit the copies of these files.\n\n<img src=\"/images/dwhite/Shared_seed_data_files_location.png\" alt=\"Seed data JSON files\" height=\"175px\">\n\nI added a **solution folder** to the solution and added these files to it.\n\n<img src=\"/images/dwhite/solution-add-existing-shared-files.png\" alt=\"Add shared files to solution\" height=\"175px\">\n\nI changed the following:\n\n- **identitydata.json**\n  - change the initial username credentials to **admin** and **P@ssw0rd!**\n  - change the **Roles** entry to **IdentityAdminRole**.\n\nIn the three projects, we need to make a few tweaks in the appsettings.json files. I did this for a little added clarity.\n\n- appsettings.json in MyProject.Admin\n- appsettings.json in MyProject.Admin.Api\n- appsettings.json in MyProject.STS.Identity\n  - change the **AdminApiConfiguration:AdministrationRole** to **IdentityAdminRole**\n\n<br/>\n\nHere is an example from the STS **appsettings.json** file.\n\n```json\n    \"AdminConfiguration\": {\n        \"PageTitle\": \"Skoruba IdentityServer4\",\n        \"HomePageLogoUri\": \"/images/skoruba-icon.png\",\n        \"FaviconUri\": \"/favicon.ico\",\n        \"IdentityAdminBaseUrl\": \"http://127.0.0.1.xip.io:9000\",\n        \"AdministrationRole\": \"IdentityAdminRole\"\n    },\n```\n\n### Delete Unnecessary LaunchSettings Profiles\n\nThis one is fairly simple and just a cleaning exercise. I can debug with Docker Desktop and the DockerCompose project, so good riddance to all the other clutter! I don't plan to run this in IIS or single Docker containers, so I can remove those profiles and settings. I will leave the Kestrel settings since I can envision _someone_ doing that. Otherwise, I'm only planning to run ecosystem this via DockerCompose or sending it to a **k8s** cluster.\n\n```json\n{\n  \"iisSettings\": {\n    \"windowsAuthentication\": false,\n    \"anonymousAuthentication\": true,\n    \"iisExpress\": {\n      \"applicationUrl\": \"http://127.0.0.1.xip.io:5000\",\n      \"sslPort\": 0\n    }\n  },\n  \"profiles\": {\n    \"IIS Express\": {\n      \"commandName\": \"IISExpress\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\"\n      }\n    },\n    \"MyProject.AspNetIdentity\": {\n      \"commandName\": \"Project\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\"\n      },\n      \"applicationUrl\": \"http://127.0.0.1.xip.io:5000\"\n    },\n    \"Docker\": {\n      \"commandName\": \"Docker\",\n      \"launchBrowser\": true,\n      \"launchUrl\": \"{Scheme}://{ServiceHost}:{ServicePort}\",\n      \"environmentVariables\": {},\n      \"httpPort\": 10000\n    }\n  }\n}\n```\n\nbecomes\n\n```json\n{\n  \"profiles\": {\n    \"MyProject.AspNetIdentity\": {\n      \"commandName\": \"Project\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\"\n      },\n      \"applicationUrl\": \"http://127.0.0.1.xip.io:5000\"\n    }\n  }\n}\n```\n\nRepeat for all of the launchSettings.json files.\n\n### The URL configuration values\n\nIdentity implementations require a lot of information about people and applications in order to work. One of those pieces of information is the URL of the caller. The Skoruba template spits out all of the URLs in the form of **127.0.0.1.xip.io**. These URLs are using the [xip.io](http://xip.io/) service provided by the makers of Basecamp. This DNS server basically takes the DNS entry request, pulls the IP address out of it, and send that IP address base as the resolved IP. You can basically get a public DNS to resolve to an IP address that is your local machine.\n\n<img src=\"/images/dwhite/xip-io-landingpage.png\" alt=\"Xip.io Landing Page\" height=\"200px\">\n\nOne thing I found while looking over the project was a blend of **localhost** configurations and **127.0.0.1.xip.io**. Because of the way that xip.io works, we can get rid of all of the localhost references. So, you can do a **Replace in Files** and replace all of the localhost with 127.0.0.1.xip.io.\n\n> We could also replace all of the 127.0.0.1.xip.io with **lvh.me** which is a public DNS entry that resolves to 127.0.0.1 as well. [This link](https://nickjanetakis.com/blog/ngrok-lvhme-nipio-a-trilogy-for-local-development-and-testing) describes lvh.me and some other good utilities.\n\nThese URLs are in the **launchSettings.json** file and the **appsettings.json**. Remember that in ASP.NET Core applications, by convention the **environmental variable** configurations are loaded last and overwrite anything in the appsettings.json files.\n\n> One thing about configuration in this adventure is making sure that your configuration supports anywhere you run. That is why you see this crazy overlap of appsettings.json files or environmental variables managed all over the place. You have to develop a good understanding of how the configuration systems work in the runtime environments and take advantage of them.\n\nWhile we are doing this work, I also adjusted the ports that everything lives at. There are some mistakes in the Skoruba template that state that the Admin API is at 5001. So in this exercise I've set:\n\n- STS to port 80 (no port)\n- Admin is at port 9000\n- Admin API is at port 5000\n\nThese go along with pgAdmin4 already being at port 5050 and Seq being at port 5341.\n\n### Database Connection strings\n\nThe Skoruba uses 5 different DbContext to do its work. This in theory allows you to maintain smaller, more specific entity sets and could also spread some of this persistence work across different servers, but in practice, we'll only use one server. You can replace all connection strings with this one connection string.\n\n```text\nServer=postgres; User Id=admin; Database=identity; Port=5432; Password=P@ssw0rd!; SSL Mode=Prefer; Trust Server Certificate=true;\n```\n\n> Notice that we can reference the Postgre DB at its DNS alias of **postgres**.\n\n### Tweak Code to Run Migrations Always\n\nYou can see that the Skoruba team already had this idea in mind when they created the template. You just need to make the switch in **Program.cs** around line **32** and the MyProject.Admin project.\n\n```csharp\n    // Uncomment this to seed upon startup, alternatively pass in `dotnet run /seed` to seed using CLI\n    await DbMigrationHelpers.EnsureSeedData<IdentityServerConfigurationDbContext,\n                                            AdminIdentityDbContext,\n                                            IdentityServerPersistedGrantDbContext,\n                                            AdminLogDbContext,\n                                            AdminAuditLogDbContext,\n                                            UserIdentity,\n                                            UserIdentityRole>(host);\n    //if (seed)\n    //{\n    //    await DbMigrationHelpers\n    //        .EnsureSeedData<IdentityServerConfigurationDbContext, AdminIdentityDbContext,\n    //            IdentityServerPersistedGrantDbContext, AdminLogDbContext, AdminAuditLogDbContext,\n    //            UserIdentity, UserIdentityRole>(host);\n    //}\n\n```\n\n### Configure Serilog to use Seq\n\nThis is going to require a little bit of work in Visual Studio. The Skoruba template already makes use of Serilog as the logging engine, but it doesn't have the Seq sink that will be used to send all of the structured log messages to Seq. We need to add that bit to the Identity applications.\n\n1. Add the `Serilog.Sinks.Seq` package to all three Identity projects\n  <img src=\"/images/dwhite/add-serilog-sink-seq-package.png\" alt=\"Add Serilog Sink Seq Package to Projects\" height=\"250px\">\n1. Change the configuration in the **/shared/serilog.json** file that will be used by the docker containers.\n    ```json\n    {\n      \"Serilog\": {\n        \"Using\": [ \"Serilog.Sinks.Console\" ],\n        \"MinimumLevel\": {\n          \"Default\": \"Debug\",\n          \"Override\": {\n            \"Microsoft\": \"Information\",\n            \"System\": \"Error\"\n          }\n        },\n        \"WriteTo\": [\n          {\n            \"Name\": \"Console\",\n            \"Args\": { \"outputTemplate\": \"[{Timestamp:o}][{Level:u4}][{SourceContext}] {Message}{NewLine}{Exception}\" }\n          },{\n            \"Name\": \"Seq\",\n            \"Args\": { \"serverUrl\": \"http://seq:5341\" }\n          }\n        ],\n        \"Enrich\": [ \"FromLogContext\", \"WithMachineName\" ],\n        \"Properties\": {\n          \"Product\": \"IdentityServer4\",\n          \"Platform\": \"Docker\"\n        }\n      }\n    }\n    ```\n1. **Optional** Change the individual **serilog.json** files in each project to approximately match. Remember that the docker containers all use the **shared/serilog.json** file, not the individual serilog.json file via the `volumes:` directive\n    ```yaml\n        volumes:\n          - \"./shared/serilog.json:/app/serilog.json\"\n          - \"./shared/identitydata.json:/app/identitydata.json\"\n          - \"./shared/identityserverdata.json:/app/identityserverdata.json\"\n    ```\n\n## Ready to run\n\nAlright! We've scaffolded the IdentityServer4/Skoruba applications, we've put all of our infrastructure in place, and we've adjusted all of our configuration! We should be ready to run this application!\n\n1. Select the docker-compose project in the solution explorer!\n  <img src=\"/images/dwhite/select-docker-compose-project.png\" alt=\"Select Docker Compose file\" height=\"200px\">\n1. You should see only the docker-compose option in the Visual Studio Run button\n  <img src=\"/images/dwhite/docker-compose-run-button.png\" alt=\"Docker Compose Run Button\" height=\"200px\">\n1. Run your docker-compose orchestration!\n  <img src=\"/images/dwhite/containers-running-in-docker.png\" alt=\"Container Explorer in Visual Studio\" height=\"200px\">\n\nNow we can start to explore what we've got running in the container network.\n\n1. pgAdmin4\n1. Seq\n1. STS\n1. Admin\n1. AdminApi\n\n### pgAdmin4\n\nYou should be able now to navigate to [http://127.0.0.1.xip.io:5050](http://127.0.0.1.xip.io:5050) in order to see the pgAdmin4 login screen.\n\n<img src=\"/images/dwhite/pgadmin4-login.png\" alt=\"pgAdmin4 Login Screen\" height=\"250px\">\n\nNow we enter our login credentials that we set in the pgAdmin4 section in the docker-compose.yml file.\n\n```yaml\n  pgAdmin4:\n    image: dpage/pgadmin4:4.20\n    ports:\n      - \"5050:80\"\n    container_name: pgAdmin4\n    environment:\n      - \"PGADMIN_DEFAULT_EMAIL=admin@codingwithdave.xyz\"\n      - \"PGADMIN_DEFAULT_PASSWORD=P@ssw0rd!\"\n      - \"PGDATA=/mnt/data/pgdata\"\n    volumes:\n      - pgdata:/mnt/data/pgdata\n```\n\n<img src=\"/images/dwhite/pgadmin4-login-details.png\" alt=\"pgAdmin4 Login Screen\" height=\"250px\">\n\nOnce we've logged in! We need to add a connection to the Postgre database into pgAdmin4.\n\n1. Create Server Listing Entry\n  <img src=\"/images/dwhite/pgadmin4-create-server-listing.png\" alt=\"pgAdmin4 Create Server Entry\" height=\"250px\">\n1. Enter server location (DNS)\n  <img src=\"/images/dwhite/pgadmin4-server-listing-name.png\" alt=\"pgAdmin4 Enter Server DNS location\" height=\"150px\">\n1. Enter Postgre server admin credentials\n    ```yaml\n      postgresdb:\n        image: postgres:alpine\n        hostname: postgres\n        ports:\n          - \"5432:5432\"\n        container_name: postgresdb\n        environment:\n          - \"POSTGRES_USER=admin\"\n          - \"POSTGRES_PASSWORD=P@ssw0rd!\"\n          - \"POSTGRES_DB=identity\" # this is the DB name that will be generated by EFCore\n    ```\n    <img src=\"/images/dwhite/pgadmin4-server-login-credentials.png\" alt=\"pgAdmin4 Postgres Admin Credentials\" height=\"250px\">\n\nAnd voila! We can administer our Postgre-based Identity store that is used by the 3 applications in the platform!\n\n<img src=\"/images/dwhite/pgadmin4-identity-database.png\" alt=\"pgAdmin4 Identity Database Structure\" height=\"250px\">\n\nWe can now run Postgre queries and sql commands to look at or manipulate our data.\n\n<img src=\"/images/dwhite/pgadmin4-identity-query-users-table.png\" alt=\"pgAdmin4 Identity Database Query\" height=\"250px\">\n\n### Seq\n\nLooking at Seq is a little easier! We just need to go to the [http://127.0.0.1.xip.io:5341](http://127.0.0.1.xip.io:5341) URL. Since we are in a single-user license and with no authentication turned on or hooked up, we'll simply land on the query screen!\n\n<img src=\"/images/dwhite/Seq-landing-page.png\" alt=\"Seq Landing Page\" height=\"250px\">\n\nNow, we can start to just get a taste of the benefit of a embedded log-ingestion application. We can see that the apps are all logging into the Seq platform.\n\n<img src=\"/images/dwhite/Seq-application-name-query.png\" alt=\"Seq distinct query\" height=\"250px\">\n\nAnd now we can start to look at basic loads across all three applications _in the same timeframe_.\n\n<img src=\"/images/dwhite/seq-query-visualization-all-apps.png\" alt=\"Seq load query for all applications\" height=\"250px\">\n\nI'll leave a much deeper exploration of what Seq can do for you as homework! I know your probably already at home (or work?!?!) but I'm not going to directly explore Seq's capabilities in this blog post! I'll save that for another series. But you should definitely go check out [Seq by DataLust.co](https://www.datalust.co/seq).\n\n> I am not affiliated with Datalust or Seq in anyway. I get no money for this. I just really like the product.\n\n### STS\n\nDatabase. Check!\nDatabase Admin. Check!\nLog Ingestion. Check!\n\nNow we get to see the applications (finally) that is the reason we are doing all of the rest of this work.\n\n> Ensure the **DockerCompose** project is running in Visual Studio.\n\nIf you navigate to [http://127.0.0.1.xip.io](http://127.0.0.1.xip.io) you will land on the STS login page.\n\n<img src=\"/images/dwhite/sts-landing-page.png\" alt=\"Security Token Service Landing Page\" height=\"250px\">\n\n> If you have another web server running and serving pages on port 80 you may have a conflict here.\n\nWe can log into this application using the account that was created for us! We saw the **admin** account in the pgAdmin4 query of the `Users` table!\n\n<img src=\"/images/dwhite/sts-login-page.png\" alt=\"Security Token Service Login Page\" height=\"250px\">\n\n```json\n{\n  \"IdentityData\": {\n    \"Roles\": [\n      {\n        \"Name\": \"IdentityAdminRole\"\n      }\n    ],\n    \"Users\": [\n      {\n        \"Username\": \"admin\",\n        \"Password\": \"P@ssw0rd!\",\n        \"Email\": \"admin@codingwithdave.xyz\",\n        \"Roles\": [\n          \"IdentityAdminRole\"\n        ],\n        \"Claims\": [\n          {\n            \"Type\": \"name\",\n            \"Value\": \"admin\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nOnce we are logged into the STS, we can see that users can manage _their_ details. They can look at the applications they've granted permissions to, they can look at their profile data. They can delete their personal data, turn on MFA, and change their password! Phew!\n\n<img src=\"/images/dwhite/sts-user-functions.png\" alt=\"Security Token Service User Functions\" height=\"250px\">\n\nAgain, I'll leave a deeper exploration of the STS for homework. You should have enough familiarity after working through the IdentityServer4 Quickstarts that most of this will seem familiar. Also, this is only the user profile information, so it isn't exciting.\n\n### Admin\n\nNext, we'll open a tab to the Admin application. If you are already logged into the STS, you won't be challenged for a username/password but have no doubt, you were authenticated!\n\nNavigate to [http://127.0.0.1.xip.io:9000](http://127.0.0.1.xip.io:9000) you will land on the Admin application landing page.\n\n<img src=\"/images/dwhite/admin-landing-page.png\" alt=\"Admin Landing Page\" height=\"250px\">\n\nIf you were not logged in, you would have been re-directed to the STS to enter your credentials, prove who you were, and then returned to the Admin page. Give it a try. Log out of the STS and try going to the admin URL again!\n\nThere is a lot to learn about administering an IdentityServer4 implementation. I'm going to cop out again and leave you to explore this as homework.\n\n### Admin Api\n\nLast but not least, the Swashbuckle Api Explorer application (swagger) that is embedded in the AdminApi project!\n\nNavigate to [http://127.0.0.1.xip.io:5000/swagger](http://127.0.0.1.xip.io:5000/swagger) you will land on the Admin WebApi ApiExplorer page.\n\n<img src=\"/images/dwhite/swashbuckle-api-explorer.png\" alt=\"Swashbuckle Api Explorer\" height=\"250px\">\n\nThe Api Explorer is already setup to leverage token-based authentication, so if you hit the Authorize button, you'll be directed to log in (authenticate) with the STS (if you haven't already). If you are already authenticated, you may be asked by the Api Explorer app for your consent to use your profile details, but you won't have to type in your username/password again. And if you allow your consent to be remembered, you won't be challenged for it again.\n\nIf you execute any of the API methods before you are authorized, you'll get the dreaded **401** Unauthorized HTTP status code!\n\n<img src=\"/images/dwhite/swashbuckle-api-explorer-unauthorized.png\" alt=\"Swashbuckle Api Explorer Unauthorized\" height=\"250px\">\n\nNow get authenticated!\n\n<img src=\"/images/dwhite/swashbuckle-api-explorer-authorize.png\" alt=\"Swashbuckle Api Explorer Authorize\" height=\"250px\">\n\nAnd this time, we are authorized and our API call succeeds!\n\n<img src=\"/images/dwhite/swashbuckle-api-explorer-authorized.png\" alt=\"Swashbuckle Api Explorer Authorized\" height=\"250px\">\n\n## Summary\n\nNow that is it! Wow! That was a lot of work! But we have been rewarded by a fully functioning IdentityServer4 running on our machine with a Postgres DB as persistence, pgAdmin4 for database administration, and Seq for log ingestion!\n\nOne thing I didn't do for demonstration purposes is do any sort of custom-branding on the Skoruba applications, but they are just ASP.NET Core 3.1 _web applications_ that you already know how to modify and enhance. The Skoruba template has a little bit of built-in branding configuration, but you can certainly charge ahead and make this look however you'd like.\n\nSo, there we go. We have a fully functional system running in Docker, but that isn't going to help us where we are going. Time for the next step and we move our platform to **Kubernetes**!!\n\n**Next up:**\n[Getting Started with Kubernetes - Minikube](/kubernetes/kubernetes-my-journey-part-5a)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 5a","authorId":"dave_white","slug":"kubernetes-my-journey-part-5a","date":"2020-05-22 12:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-5a/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-5a/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 5a\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 08:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Building an ASP.NET Core IdentityServer Implementation](/kubernetes/kubernetes-my-journey-part-4)\n\n# Getting Started with Kubernetes - Minikube - Part A\n\nWe're finally getting to the part of the series where we have a group of applications and we want to have them live in **Kubernetes (k8s)**. In order to do that, we need to have **k8s** on our local development machine and we are going to use **Minikube** to do that.\n\n## Important Caveat\n\nI'm going to make an assumption that at a minimum, you've reviewed various types of **k8s** resources on [kubernetes.io](https://kubernetes.io) and in a best case scenario, you've watched Nigel Poulton's Pluralsight course [Getting Started with Kubernetes](https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents). If you haven't, my discussions about these topics may be harder to understand because they do not cover these basics.\n\nAlso, my understanding of Kubernetes is certainly not as extensive as I'd like. I'm not sure I'd hazard calling myself an expert. I've got a working cluster and applications in that cluster but I am not going to make the statement that I've done it all right or with the current best practices in place. This is a learning exercise for me (and you) and while I want to get you with a cluster up and running as soon as possible, I expect you to learn/challenge/grow your **k8s** cluster knowledge as well.\n\n## Getting started\n\nThankfully, I don't have to create a huge blog post on this! There is already some great documentation at [kubernetes.io](https://kubernetes.io) that gets you setup with [a learning environment based on minikube](https://kubernetes.io/docs/setup/learning-environment/minikube/#installation) so I'll just let you go there and read the installation guide! I'm using v1.9.2 of minikube during the creation of this series of articles.\n\n### A Powershell Script\n\nIt is certainly easy enough to remember some commands when standing up your environment, but as we work through getting everything into minikube using the command-line and manifests, it can become quite a list of commands and so I'd recommend creating a powershell script that you can capture your commands in the order that you are likely to execute them.\n\nThe way that I've been structuring my source files is:\n\n```text\nproject_root\n      |- infra\n      |- manifests\n      |- src\n```\n\nThe **src** folder is where the ASP.NET Core applications are. You can ignore the **infra** directory for a while, but for this part of the journey, we'll be using the **manifests** folder to store all of our **k8s** declarative manifest files. This is also where I store my **stand-up.ps1** powershell script file!\n\nThe first line in your stand-up.ps1 file should probably be (if your on windows):\n\n```powershell\nminikube start --vm-driver=hyperv --cpus=4 --memory=16g --extra-config=apiserver.service-node-port-range=80-33000\n```\n\nThis is saying:\n\n- start minikube\n  - use the hyperv driver\n  - with 4 logical CPUS (4 of the _n_ you have in Task Manager - Performance Tab)\n  - with 16 gb of memory\n  - 20 gb of disk space (default value)\n  - and expanding the range of usable ports in minikube\n\nYou're going to want to adjust these numbers to what make sense for your workstation. I have a i9-9900k with 64 gbs of memory, so these numbers makes sense for me. You can certainly run all of this on 1 CPU and 4 gb of RAM with no problems. As you build **k8s** clusters with more hosted pods, you'll probably need to increase the values when starting minikube.\n\n> It is important to understand that this command creates a virtual machine, running linux, in hyperv, on your local workstation. If you want to change these parameters, you'll need to destroy your current minikube instance and re-create a new one.\n\nOut of the box, **k8s** (minikube) limits the port range of containers in the cluster to 30000-33000. I've expanded this range because I want to use the same port values that we used in docker. This expanded range allows **k8s** in minikube to use more ports, but it doesn't claim all of them.\n\nAfter the minikube VM has been created (which may take a few minutes), we can then invoke our next command to make sure it is up and running!\n\n```powershell\n# Start a tunnel from your local machine into minikube and the kubernetes dashboard\n# 127.0.0.1 should open in a browser window for you and it should be the Kubernetes Web UI in minikube\nStart-Process -NoNewWindow minikube dashboard\n```\n\nUsing this powershell command, the process creating the tunnel is started and control of the shell is returned to you, but the process remains running. When you close down this shell, the running process will also be closed and the tunnel will close.\n\nHopefully, the tunnel started, a browser window opened and you can see the Kubernetes Web UI for your new minikube **k8s** cluster! Let's leave this browser window open and we can keep our eyes on it.\n\n## Putting your backend into the cluster\n\nNow that **k8s** is running on your local machine, we can start to install our backend services into the cluster. We'll do this activity first, with easy to configure pods, to get used to working with manifests. Installing our IdentityServer4-based applications will require some additional resources and automation.\n\nIn this section, we will start the process of converting our docker-compose.yml into a bunch of **k8s** _manifest_ files. We could do this as a monolithic manifest file, but I prefer smaller manifest files. They are easier to think about and just as easy to use.\n\n### Postgres manifests\n\nIf we look at the postgres section of our docker-compose.yml file, we will see a couple of things.\n\n```yaml\n  postgresdb:\n    image: postgres:alpine\n    hostname: postgres\n    ports:\n      - \"5432:5432\"\n    container_name: postgresdb\n    environment:\n      - \"POSTGRES_USER=admin\"\n      - \"POSTGRES_PASSWORD=P@ssw0rd!\"\n      - \"POSTGRES_DB=identity\" # this is the db name that will be generated by EFCore\n    volumes:\n      - postgresdata:/var/lib/postgresql/data\n    networks:\n      default:\n        aliases:\n          - postgres\n```\n\n- an image from DockerHub\n- container name\n- ports that are exposed\n- environmental variable declarations\n- attached persistent volumes\n- network alias\n\nIn **k8s**, those concepts are separated into different types of resources that we will want to provision. Generally speaking, the container-based resources will be described in a **Deployment** manifest and the network-based resources will be described in a **Service** manifest.\n\n> I tend to clump container-based concerns into a _manifest_ file. So in this case, with postgres, I will also describe the **PersistentVolume** resources in with the Deployment resources.\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgres-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /var/lib/postgresql/data\n# divider to separate resource declarations in a yaml file\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-dep\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      volumes:\n        - name: postgres-pv-storage\n          persistentVolumeClaim:\n            claimName: postgres-pv-claim\n      containers:\n        - name: postgres\n          image: postgres:alpine\n          env:\n            - name: POSTGRES_USER\n              value: \"admin\"\n            - name: POSTGRES_PASSWORD\n              value: \"P@ssw0rd!\"\n            - name: POSTGRES_DB\n              value: \"identity\"\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - mountPath: \"/var/lib/postgresql/data\"\n              name: postgres-pv-storage\n```\n\nLet's decompose that Deployment manifest in more detail.\n\n#### PersistenVolume\n\nFrom _kubernetes.io_...\n\n_A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource._\n\nSo we need to have some disk space made available for us to use in the cluster.\n\n[PersistentVolumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgres-pv-volume\n  labels:\n    type: local\n    app: postgres\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /var/lib/postgresql/data\n```\n\nThe **TL;DR;** (too long; didn't read it) description of this resource description is:\n\n- Create a volume on the linux host (**k8s** _node_)\n  - at the path described\n  - make it 1Gi large\n  - only let _one_ node (VM/host) connect to it\n\n#### PersistentVolumeClaim\n\nFrom _kubernetes.io_...\n\n_A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory)._\n\nWe need to claim some of the disk resources from the cluster (the PersistentVolume) in the same way that we would request some CPU or memory capacity from the cluster. It is important to explore the relationship between PV and PVC. [This blog article](https://rancher.com/blog/2018/2018-09-20-unexpected-kubernetes-part-1/) provides some insights as does this [Stack Overflow Question/Answer](https://stackoverflow.com/questions/48956049/what-is-the-difference-between-persistent-volume-pv-and-persistent-volume-clai).\n\nAs I understand it, we will need to do this PV+PVC technique because we are using minikube, not AKS, and the storageClassName that we are using (manual) doesn't allow for _dynamic_ provisioning of the storage resource. This will change when we move to AKS.\n\n[PersistentVolumeClaims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pv-claim\n  labels:\n    app: postgres\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n```\n\n#### Deployment\n\nSo we have the persistent storage that we want for our database setup. Now we need to get postgres deployed into our cluster.\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-dep\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      volumes: #this pod is going to claim this PVC and call it postgres-pv-storage\n        - name: postgres-pv-storage\n          persistentVolumeClaim:\n            claimName: postgres-pv-claim\n      containers:\n        - name: postgres\n          image: postgres:alpine\n          env:\n            - name: POSTGRES_USER\n              value: \"admin\"\n            - name: POSTGRES_PASSWORD\n              value: \"P@ssw0rd!\"\n            - name: POSTGRES_DB\n              value: \"identity\"\n          ports:\n            - containerPort: 5432\n          volumeMounts: # this container is going to mount the volume that is the claimed PVC\n            - mountPath: \"/var/lib/postgresql/data\"\n              name: postgres-pv-storage\n```\n\nThere is a lot that goes into a Deployment manifest. You can [see more details here](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) in the event that you need a refresher.\n\nWe've created this manifest that will stand-up Postgres in the **k8s** cluster. Now we can use **kubectl** to run this manifest against our cluster.\n\nFirst, let's make sure our kubectl is configured to point at minikube.\n\n`kubectl config current-context` should report **minikube**.\n\nNext, let's run the postgre-dep.yml file through kubectl.\n\n`kubectl create -f .\\postgres-dep.yml`\n\n```bash\nPS D:\\temp\\testidentity\\MyProject\\manifests> kubectl create -f .\\postgres-dep.yml\n\npersistentvolume/postgres-pv-volume created\npersistentvolumeclaim/postgres-pv-claim created\ndeployment.apps/postgres-dep created\n\nPS D:\\temp\\testidentity\\MyProject\\manifests>\n```\n\nNow go to the browser window that is showing you the **Kubernetes Web UI** and you should see all of the new resources in your cluster!\n\n##### PersistentVolume In the Cluster\n\n<img src=\"/images/dwhite/postgres-deployment-persistentvolume.png\" alt=\"Postgres PersistentVolume Created\" height=\"250px\">\n\nNotice how this PV is a resource _in the cluster_ and not related to any particular pod.\n\n##### Postgres-based Resources\n\nIf you'd like to see only the Postgres resources,you can use the Search bar to filter everything else out.\n\n<img src=\"/images/dwhite/kubernetes-search-bar.png\" alt=\"Postgres Deployment Completed\" height=\"120px\">\n\nAnd now we can see all of the resources we just deployed, and explore them in more detail individually.\n\n<img src=\"/images/dwhite/postgres-deployment-completed.png\" alt=\"Postgres Deployment Completed\" height=\"250px\">\n\nSo that completes putting our Postgres database into the cluster! Now we have to expose it.\n\n#### Postgres Service Manifest\n\nFrom _kubernetes.io_...\n\n_An abstract way to expose an application running on a set of Pods as a network service._\n_With Kubernetes you donâ€™t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them._\n\n[Services](https://kubernetes.io/docs/concepts/services-networking/service/)\n\nGenerally, you want to present as little surface area for security reasons as possible, so I have no desire to expose the database outside of the cluster. We still need a service resource created so that the container gets a DNS name within the cluster, and as containers/pods come and go, the cluster will ensure that the DNS entry always points to the right place.\n\nSo in this service manifest, we are basically telling **k8s** to map the DNS entry **postgres-svc** to whatever pod spins up with the label **app:postgres**.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-svc\nspec:\n  selector:\n    app: postgres\n  ports:\n    - protocol: TCP\n      port: 5432\n```\n\n### pgAdmin4 Deployment manifests\n\nThe pgAdmin4 deployment manifest is very similar to the postgres-dep.yml file we've created already. So I'll just post it in here so you can take a look at it. The only things that are really different are the environmental variables and the names of things.\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pgadmin4-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /var/lib/pgadmin4/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pgadmin4-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pgadmin4-dep\n  labels:\n    app: pgadmin4\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: pgadmin4\n  template:\n    metadata:\n      labels:\n        app: pgadmin4\n    spec:\n      volumes:\n        - name: pgadmin4-pv-storage\n          persistentVolumeClaim:\n            claimName: pgadmin4-pv-claim\n      containers:\n        - name: pgadmin4\n          image: dpage/pgadmin4\n          env:\n            - name: PGADMIN_DEFAULT_EMAIL\n              value: \"admin@codingwithdave.xyz\"\n            - name: PGADMIN_DEFAULT_PASSWORD\n              value: \"P@ssw0rd!\"\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: \"/var/lib/pgadmin/data\"\n              name: pgadmin-pv-storage\n```\n\n### pgAdmin4 Service manifests\n\nSince I do want to be able to access the pgAdmin4 application from outside of the cluster, we need to make a service manifest that exposes the pgAdmin4 pod.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: pgadmin4-svc\nspec:\n  selector:\n    app: pgadmin4\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5050\n```\n\nSimply speaking, this manifest says to create a NodePort service resource for this pod and expose it on port 5050. Notice that the **spec: selector:** is saying that this service will be applied to pods with the same label of **pgadmin4**. Remember, in **k8s**, labels are usually very important. And because of the **type: NodePort** on this service, we will be able to access this pod from outside of the cluster.\n\n> If you want to expose your Postgres resource outside of the cluster, you can change that manifest to create a **type: NodePort** service exposing the postgres pod on port 5432.\n\nNow we need to use kubectl to create this resource in the cluster.\n\n`kubectl create -f .\\pgadmin4-svc.yml`\n\nYou should see the new pgAdmin4 resources in your cluster.\n\nWe can also now use another feature of minikube which allows you to expose services in your cluster to the outside world on minikube's IP address. In order to expose pgAdmin4, simply type:\n\n`minikube service pgadmin4-svc`\n\nMinikube should then create a tunnel into the cluster for you, and open a web browser to the URL that was generated!\n\nYou can also use `minikube service list` at any time to see a list of the exposed services in your cluster.\n\n```bash\nPS D:\\temp\\testidentity\\MyProject\\manifests> minikube service list  \n\n|----------------------|---------------------------|--------------|----------------------------|\n|      NAMESPACE       |           NAME            | TARGET PORT  |            URL             |\n|----------------------|---------------------------|--------------|----------------------------|\n| default              | kubernetes                | No node port |                            |\n| default              | pgadmin4-svc              |           80 | http://172.28.129.202:5050 |\n| default              | postgres-svc              | No node port |                            |\n| kube-system          | kube-dns                  | No node port |                            |\n| kubernetes-dashboard | dashboard-metrics-scraper | No node port |                            |\n| kubernetes-dashboard | kubernetes-dashboard      | No node port |                            |\n|----------------------|---------------------------|--------------|----------------------------|\n```\n\nYou should be able to log into pgAdmin4 with the credentials we've come to know and love (**user:** admin@codingwithdave.xyz **pwd:** P@ssw0rd!). Once in there, you will be able to re-create your server list entry, but this time, the location of the server is the name of the postgres service, as described in the metadata: element of the yaml.\n\n```yaml\nmetadata:\n  name: postgres-svc\n```\n\n<img src=\"/images/dwhite/minikube-postgres-server-location.png\" alt=\"Postgres Deployment Completed\" height=\"250px\">\n\n### Seq Deployment manifests\n\nThe Seq deployment manifest is very similar to the others we've created already. So take a look, and then put Seq into your minikube cluster.\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: seq-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 3Gi\n  hostPath:\n    path: /mnt/data/seqv6/\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: seq-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: seq-dep\n  labels:\n    app: seq\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: seq\n  template:\n    metadata:\n      labels:\n        app: seq \n    spec:\n      volumes:\n        - name: seq-pv-storage\n          persistentVolumeClaim:\n            claimName: seq-pv-claim\n      containers:\n      - name: seq \n        image: datalust/seq:preview\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: \"/data\"\n          name: seq-pv-storage\n        env:\n        - name: ACCEPT_EULA\n          value: \"Y\"\n```\n\n`kubectl create -f .\\seq-dep.yml` will get your resources created!\n\n### Seq Service manifests\n\nWe want to see Seq outside of the cluster, so we will also want to expose it with a **NodePort** service.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: seq-svc\nspec:\n  selector:\n    app: seq\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5341\n```\n\n`kubectl create -f .\\seq-dep.yml` will get your service resource created!\n\nThen expose your service via:\n\n`minikube service seq-svc`\n\nAnd voila! A browser window opens and you'll see Seq!! Nothing is logging there yet, but it's there! Woo hoo!!\n\n## Summary\n\nHopefully now, you've shifted all of the backend services required for our IdentityServer4 applications into minikube! They are all up and running, and you've been able to access pgAdmin4 (connected to the postgres db) and you've been able to access Seq, even if nothing is logging to it!\n\nThis has been really long post, so I'm going to take a break, let you have a break, and continue on to Part B of putting our system in Minikube!\n\n**Next up:**\n[Getting Started with Kubernetes - Minikube - Part B](/kubernetes/kubernetes-my-journey-part-5b)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 5b","authorId":"dave_white","slug":"kubernetes-my-journey-part-5b","date":"2020-05-22 11:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-5b/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-5b/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 5b\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 07:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Getting Started with Kubernetes - Minikube - Part A](/kubernetes/kubernetes-my-journey-part-5a)\n\n# Getting Started with Kubernetes - Minikube - Part B\n\nWe're finally getting to the part of the series where we have a group of applications and we want to have them live in **Kubernetes (k8s)**. In order to do that, we need to have **k8s** on our local development machine and we are going to use **Minikube** to do that.\n\n## Important Caveat\n\nI'm going to make an assumption that at a minimum, you've reviewed various types of **k8s** resources on kubernetes.io and in a best case scenario, you've watched Nigel Poulton's Pluralsight course [Getting Started with Kubernetes](https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents). If you haven't, some of the stuff I discuss will not have sufficient detail in this post to close the gap.\n\nAlso, my understanding of Kubernetes is certainly not as extensive as I'd like. I'm not sure I'd hazzard calling myself an expert. I've got a working cluster and applications in that cluster but I am not going to make the statement that I've done it all right or with the current best practices in place. This is a learning exercise for me (and you) and while I want to get you with a cluster up and running as soon as possible, I expect you to learn/challenge/grow your **k8s** cluster knowledge as well.\n\n## Continuing on... \n\nSo we've now deployed all of our backend services into minikube. There is a little detail that probably went un-noticed until now. Where did minikube get the container images from? In this case, it pulled the postgres, pgAdmin4, and Seq container images from [DockerHub](https://hub.docker.com/), a public container registry. So if you want to publish your custom-built Skoruba templates to DockerHub, everything will keep on working without any changes. But chances are, your going to want to publish those images to a private container registry such a DockerHub (private) or in my case, an Azure Container Registry hosted in our Azure subscription.\n\n### What about my local docker repository?\n\nYou may be asking, you've built your app locally and deployed it to Docker Desktop registry, why can't I just get it from there?\n\nMinikube (**k8s** in the VM) has it's own docker daemon instance running, so now you have two docker daemons running on your machine. One in Docker Desktop and one in minikube. And the one in minikube didn't build/deploy your applications, the Docker Desktop one did. But minikube doesn't use that one. It uses its own internal docker container registry or a public/private one from the internet. I've read that you can use minikube's docker daemon for builds, but I don't want to make any adjustments to your environment that might break the Docker Desktop experience so we'll just continue on with creating a private container registry. We need to get there eventually no matter what, so let's keep going!\n\n## Azure Container Registry (ACR)\n\nFor this part of the journey, we are going to leave Visual Studio and the comfort of C# and yaml and journey to the [Azure Subscription](https://portal.azure.com) that we want to host our private container registry in!\n\nAt this point, you'll need to have access to a Azure subscription. Microsoft has made it very easy to create them (they are free) and use them (there are lots of free services)! The ACR service is not free, but it is pretty inexpensive for the Basic SKU which holds up to 10gb of container images.\n\nIf an Azure ACR is not in the cards for you, you can continue (without my help for the moment) with a free DockerHub public registry, or a private container registry that you may already have access to.\n\nFor this part of the project, I'll be using my Depth Consulting Azure Subscription and Azure DevOps Service instance. My client, who has the identity management business problem, also has Azure DevOps Server 2019 and an Azure subscription that holds their private ACR.\n\n### Creating the ACR\n\nCreating a ACR is very easy.\n\n1. Log into the [Azure Portal](https://portal.azure.com)\n1. Create a new ACR Resource<br/>\n    <img src=\"/images/dwhite/azure-container-registry-create.png\" alt=\"Create an Azure Container Registry\" height=\"250px\">\n1. Follow the wizard and complete the creation of your ACR\n    - pick the **Basic** SKU. It is more than enough for now and you can upgrade later.\n    - The Basic ACR costs about $0.17 per day for 10GiB of storage.\n\nTada! ACR created!\n\n<img src=\"/images/dwhite/azure-container-registry-created.png\" alt=\"Azure Container Registry Created\" height=\"250px\">\n\n> I could create the ACR via Pulumi but it is a one-and-done kind of activity, so I didn't do that. It's just easier to create it in the portal.\n\n### Secrets! Secrets! Secrets!\n\nIn order to access a private container registry, you need credentials! And more specifically, your **k8s** _cluster_ needs the credentials for your private container registry. And it isn't just **k8s** that needs those credentials, your DevOps pipeline will need those credentials as well in order to publish container images into the registry.\n\nOur ACR credentials are available in the Azure Portal. You can find the ones we'll need there.\n\n<img src=\"/images/dwhite/azure-container-registry-credentials.png\" alt=\"Azure Container Registry Created\" height=\"250px\">\n\nRemember where they are. When we get back to manifest land, we'll need them.\n\n## Azure DevOps Service (Server 2019)\n\nNow that we have a private container registry, we need to build our IdentityServer4 applications and publish those container images to our ACR. If you do not have an Azure DevOps Service instance, you can [create one for free](https://azure.microsoft.com/en-ca/services/devops/).\n\n> I will try to create blog post sometime doing something like this from [GitHub](https://github.com/), using [GitHub Actions](https://github.com/features/actions) with a public [DockerHub](https://hub.docker.com/) Registry.\n\n### Assumptions\n\nI'm progressing on the assumption that your IdentityServer4 implementation projects have been checked into a git repository somewhere. If this is in an Azure DevOps Services instance, that's great, but it can be GitHub or any other repository that Azure DevOps Pipelines can monitor for changes.\n\n### Build and Publish in one Pipeline\n\nThe next step is to build and publish our applications. For this, we're going to leverage Azure DevOps Pipelines that contain a couple DockerCompose tasks. Thankfully, this is very easy to setup in thanks to the pipeline templates and the integration between Azure Portal and Azure DevOps Services.\n\n1. Go to your Pipelines in Azure DevOps Service instance\n1. Create a new Pipeline\n    <img src=\"/images/dwhite/azure-pipelines-create.png\" alt=\"Create new Azure Pipeline\" height=\"180px\">\n1. Connect the pipeline to your repository\n1. On the _Configure your pipeline_ wizard step, select **Docker - Build and Push an Image to Azure Container Registry**\n    <img src=\"/images/dwhite/azure-pipelines-configure-pipeline.png\" alt=\"Configure the pipeline\" height=\"250px\">\n1. Select your Azure subscription with the ACR in it\n    <img src=\"/images/dwhite/azure-pipelines-select-azure-subscription.png\" alt=\"Select your Azure Subscription\" height=\"250px\">\n1. Select your ACR that you want the images in\n    <img src=\"/images/dwhite/azure-pipelines-select-acr.png\" alt=\"Select your ACR in the subscription\" height=\"250px\">\n    - don't worry about the **image name** or **Dockerfile** parameters. They'll be deleted in a moment\n1. Create the new pipeline\n1. Delete the whole Docker@2 task\n    ```yaml\n    stages:\n    - stage: Build\n      displayName: Build and push stage\n      jobs:  \n      - job: Build\n        displayName: Build\n        pool:\n          vmImage: $(vmImageName)\n        steps:\n        - task: Docker@2 # <-- remove this whole task\n          displayName: Build and push an image to container registry\n          inputs:\n            command: buildAndPush\n            repository: $(imageRepository)\n            dockerfile: $(dockerfilePath)\n            containerRegistry: $(dockerRegistryServiceConnection)\n            tags: |\n              $(tag)\n    ```\n\n1. Delete the Docker@2 task variables\n\n    ```yaml\n    variables:\n      # Container registry service connection established during pipeline creation\n      dockerRegistryServiceConnection: '<your service connection Id>'\n      imageRepository: 'identityserver' # <-- delete\n      containerRegistry: 'depthconsulting.azurecr.io' # <-- delete\n      dockerfilePath: '$(Build.SourcesDirectory)/src/DepthConsulting.Identity.Admin.Api/Dockerfile' # <-- delete\n      tag: '$(Build.BuildId)'\n    ```\n\n1. With your cursor under the **steps** yaml entry, Add a Docker Compose task\n    - Work through the wizard and the Command is **build**\n1. With your cursor under the last DockerCompose@0 task, Add another Docker Compose task\n    - Work through the wizard again and the Command is **push**\n1. Save the pipeline and run it!\n    - You may need to give your pipeline permission to use a service principal to connect to the ACR. If you see that your pipeline is queued and waiting for persmission, go ahead and give it permission.\n\nYour final pipeline yaml should look something like this.\n\n```yaml\n# Docker\n# Build and push an image to Azure Container Registry\n# https://docs.microsoft.com/azure/devops/pipelines/languages/docker\n\ntrigger:\n- master\n\nresources:\n- repo: self\n\nvariables:\n  # Container registry service connection established during pipeline creation\n  dockerRegistryServiceConnection: '<your service connection guid here>'\n  tag: '$(Build.BuildId)'\n  \n  # Agent VM image name\n  vmImageName: 'ubuntu-latest'\n\nstages:\n- stage: Build\n  displayName: Build and push stage\n  jobs:  \n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: $(vmImageName)\n    steps:\n    - task: DockerCompose@0\n      inputs:\n        containerregistrytype: 'Azure Container Registry'\n        azureSubscription: '<your subscription id here>'\n        azureContainerRegistry: '{\"loginServer\":\"depthconsulting.azurecr.io\", \"id\" : \"/subscriptions/<your subscription id here>/resourceGroups/<your resource group here>/providers/Microsoft.ContainerRegistry/registries/depthconsulting\"}'\n        dockerComposeFile: '**/docker-compose.yml'\n        action: 'Run a Docker Compose command'\n        dockerComposeCommand: 'build'\n    - task: DockerCompose@0\n      inputs:\n        containerregistrytype: 'Azure Container Registry'\n        azureSubscription: '<your subscription id here>'\n        azureContainerRegistry: '{\"loginServer\":\"depthconsulting.azurecr.io\", \"id\" : \"/subscriptions/<your subscription id here>/resourceGroups/<your resource group here>/providers/Microsoft.ContainerRegistry/registries/depthconsulting\"}'\n        dockerComposeFile: '**/docker-compose.yml'\n        action: 'Run a Docker Compose command'\n        dockerComposeCommand: 'push'\n```\n\nAfter the build has run, we should now see our IdentityServer4 container images in the ACR!\n\n<img src=\"/images/dwhite/azure-pipelines-acr-success.png\" alt=\"Successfully pushed container images to ACR\" height=\"250px\">\n\nYou should navigate the ACR repositories and look at what images are in each repository. Also, all of our images were tagged as **latest** so every time the build runs, we'll have new a **latest** image that will be pulled down as needed by the **k8s** cluster. I'll leave container image tag management to another blog post. We're just trying to get to **k8s** today.\n\n## Adding our ACR Secrets to minikube\n\nNow that we have our IdentityServer4 applications published to a container registery, we need to give our **k8s** cluster the credentials that will allow it to pull those images down from the ACR into the cluster. To do that, we will create a **k8s** Secret resource.\n\n[Secrets](https://kubernetes.io/docs/concepts/configuration/secret/)\n\nFor this step in the process, the easiest way to do this is to use **kubectl** to create a secret in the cluster.\n\n```powershell\nkubectl --namespace default create secret docker-registry docker-credentials `\n--docker-server=<acr url> `\n--docker-username=<acr username> `\n--docker-password=<acr password> `\n--docker-email=<your@email.address.com>\n```\n\n> Put this command in your start-up.ps1 file for now\n\nThis will create a Secret resource in the **k8s** cluster that it's docker daemon will use to access your private container registry. You can look at the secret that was created using kubectl.\n\n`kubectl get secrets`\n\n```bash\nNAME                  TYPE                                  DATA   AGE\ndefault-token-7n4h4   kubernetes.io/service-account-token   3      26h\ndocker-credentials    kubernetes.io/dockerconfigjson        1      17s\n```\n\nAnd you can look at the specific secret in yaml with kubectl as well.\n\n`kubectl get secret docker-credentials -o yaml`\n\n```yaml\napiVersion: v1\ndata:\n  .dockerconfigjson: <snipped>\nkind: Secret\nmetadata:\n  creationTimestamp: \"2020-04-30T16:15:02Z\"\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:.dockerconfigjson: {}\n      f:type: {}\n    manager: kubectl.exe\n    operation: Update\n    time: \"2020-04-30T16:15:02Z\"\n  name: docker-credentials\n  namespace: default\n  resourceVersion: \"207328\"\n  selfLink: /api/v1/namespaces/default/secrets/docker-credentials\n  uid: 419c553b-bcaa-49e5-831e-1388c9fad5fe\ntype: kubernetes.io/dockerconfigjson\n```\n\nYou could take this yaml and create a `docker-secret.yml` manifest file and declaratively create your secret. I'll leave that to you as homework.\n\nNow our **k8s** cluster should be able to get the container images. We'll prove that in the next section!\n\n## Deploying our IdentityServer4 applications to minikube\n\nNow that we have our applications in our container registry, we can build up the manifest files for these applications and we can reasonablly expect our **k8s** to be able to fulfill the Deployments that we have in those manifest files.\n\n### STS\n\nHere is the deployment and service manifests for the STS.\n\n#### Deployment Manifest\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-sts-dep\n  labels:\n    app: identity-sts\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-sts\n  template:\n    metadata:\n      labels:\n        app: identity-sts\n    spec:\n      containers:\n      - name: identity-sts\n        image: depthconsulting.azurecr.io/depthconsulting.identity.sts:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: AdminConfiguration__IdentityAdminBaseUrl\n          value: http://127.0.0.1.xip.io:9000\n        - name: ASPNETCORE_URLS\n          value: \"http://+:80\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n```\n\nInteresting entries in this manifest are:\n\n- Labels are important. They match a Service to a Deployment\n- We are telling the deployment where to get the container images from\n- our Seq url is the DNS entry created for the Seq post in the cluster\n- we are telling **k8s** to use the **docker-credentials** secret when trying to pull down the container images from our ACR\n  - this allows us to pull images from multiple private repositories if needed\n\n#### Service Manifest\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: identity-sts-svc\nspec:\n  selector:\n    app: identity-sts\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 80\n```\n\nA simple Service manifest telling us that:\n\n- use the **type: NodePort** to expose the STS service to the external network on port 80\n- the label will hook this Service up with the pods with the matching label\n\n### Admin\n\n#### Deployment Manifest\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-admin-dep\n  labels:\n    app: identity-admin\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-admin\n  template:\n    metadata:\n      labels:\n        app: identity-admin\n    spec:\n      containers:\n      - name: identity-admin\n        image: depthconsulting.azurecr.io/depthconsulting.identity.admin:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: AdminConfiguration__IdentityServerBaseUrl\n          value: http://127.0.0.1.xip.io\n        - name: AdminConfiguration__IdentityAdminRedirectUri\n          value: http://127.0.0.1.xip.io:9000/signin-oidc\n        - name: ASPNETCORE_URLS\n          value: \"http://+:9000\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n```\n\n#### Service Manifest\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: identity-admin-svc\nspec:\n  selector:\n    app: identity-admin\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 9000\n```\n\n### Admin Api\n\n#### Deployment Manifest\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-admin-api-dep\n  labels:\n    app: identity-admin-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-admin-api\n  template:\n    metadata:\n      labels:\n        app: identity-admin-api\n    spec:\n      containers:\n      - name: identity-admin-api\n        image: depthconsulting.azurecr.io/depthconsulting.identity.adminapi:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: AdminApiConfiguration__IdentityServerBaseUrl\n          value: http://127.0.0.1.xip.io\n        - name: AdminApiConfiguration__ApiBaseUrl\n          value: http://127.0.0.1.xip.io:5000\n        - name: ASPNETCORE_URLS\n          value: \"http://+:80\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n```\n\n#### Service manifest\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name:  identity-admin-api-svc\nspec:\n  selector:\n    app:  identity-admin-api\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5002\n```\n\nNow we can kubectl all of these manifests into our **k8s** cluster and we should see them all appear in the Web UI.\n\n- `kubectl create -f .\\sts-dep.yml`\n- `kubectl create -f .\\sts-svc.yml`\n\n- `kubectl create -f .\\admin-dep.yml`\n- `kubectl create -f .\\admin-svc.yml`\n\n- `kubectl create -f .\\adminapi-dep.yml`\n- `kubectl create -f .\\adminapi-svc.yml`\n\n> Feel free to put put these into a single manifest file or any other combination that makes sense to you.\n\nNow expose all of the services from minikube.\n\n```powershell\n# expose the services from minikube\nminikube service identity-sts-svc\nminikube service identity-admin-svc\nminikube service identity-admin-api-svc\n```\n\n#### Inspect The Pods In Kubernetes\n\nNow is a good time, before we go try to use these applications, to see if the Pods are up and healthy in your **k8s** cluster in minikube. If you haven't yes, you can use the `minikube dashboard` command to get to the **Kubernetes Web UI** or you can go to a command prompt and type `octant` to start that tool if you've installed it.\n\n##### Octant\n\n<img src=\"/images/dwhite/octant-started.png\" alt=\"Start Octant\" height=\"200px\">\n\nWe're taking a look at the state of the pods now because with our approach to putting pod resources into **k8s**, we are taking an indirect path to getting actual pods running. Our approach is to:\n\n- create a `Deployment` resource\n  - which creates a `ReplicateSet` resource\n    - which creates `Pod` resource(s) as necessary\n\nIn our previous steps, the backend infrastructure resources that we wanted running came from DockerHub (a public repository) and they are pretty easy to get started, so we were unlikely to see any Pod start-up failures. With our own Skoruba-based applications in a private repository, there is a greater chance that our pods will fail to start up, and we'd like to see that now.\n\nHopefully, everything worked and what you see in Octant is this:\n\n<img src=\"/images/dwhite/octant-pods-running.png\" alt=\"Pods running in as seen in Octant\" height=\"250px\">\n\nWhat we see in this image is all the pods having fulfilled their ReplicaSet `replicas:` pod count and that they are all `running`.\n\nWhen I did this the first time, my **Pods** did _not_ all end up running. I encountered for the first time the dreaded **ImagePullBackoff** error condition! This is a condition where the **k8es** control plane is having troubles acquiring an image, for any of a number of reasons, and is now slowing down the rate that it tries to acquire the image.\n\n<img src=\"/images/dwhite/octant-image-pull-backoff-sts-pending.png\" alt=\"STS pod cannot run\" height=\"90px\">\n\n<img src=\"/images/dwhite/octant-image-pull-backoff.png\" alt=\"Image Pull Backoff Condition\" height=\"150px\">\n\nIn this case, I've simply changed the image that the Deployment is trying to pull to create this error condition. If the image name is incorrect, this error condition will occur.\n\nAnother reason that it occurred for me was because I had my _private container registry_ credentials wrong. If you are missing the docker-secret or you have the wrong username/password in that secret, you will also get an **ImagePullBackoff** error condition.\n\nI'd encourage you to give this a try! You only need to `kubectl apply -f .\\identity-sts-dep.yml` with an incorrect image name and you'll see this error. Change it back and apply again and you'll see it fixed. You could also remove the **docker-secret** to see how that affects things.\n\nIt should go without saying, that if you have this problem, you have to resolve this prior to continuing.\n\n## We need DNS entries for IdentityServer4\n\nThe combination of hosting our **k8s** cluster on minikube and running this all on our local machine and the fact that this is an bunch of Identity applications where the `hostname` is really important, we have to do a little extra work. You're going to want to put this into your **start-up.ps1** file.\n\n### Update your Windows host file\n\nWe need to be able to use hostnames for our web applications, so we are going to have to alter our **host** file in order to make that happen. Here is a powershell script (**that you should 100% review yourself!**) that will udpate your **host** file to make this work with the **127.0.0.1.xip.io** DNS/hostname. This should not damage your existing host file entries, but **please back-up your hosts file first.**\n\n```powershell\n# edit your workstations host file to add minikube ip and DNS alias\n$hostFileLocation = \"C:\\Windows\\System32\\drivers\\etc\\hosts\"\n$ip = minikube ip # get the minikube IP adddress put the address in a variable call $ip\n$localDnsEntry = \"127.0.0.1.xip.io\" # our fake DNS entry/hostname\n\nWrite-Host \"We are going to set the minikube IP address $ip to DNS entry of $localDnsEntry\"\n$dump =  Get-Content -Path $hostFileLocation\n$hostFile = [System.Collections.Generic.List[string]] $dump\n$entriesCount = $hostFile.Count\nWrite-host \"Found $entriesCount entries in the hosts file\"\nif($hostFile.Contains(\"# minikube section\"))\n{\n    Write-Host \"Found a minikube section... replacing it.\"\n    $hostFile.Remove(\"# minikube section\") | Out-Null\n    $hostFile.FindAll({param($line) $line.Contains($localDnsEntry)}) | % { $hostFile.Remove($_) | Out-Null }\n    $hostFile.Remove(\"# minikube end section\") | Out-Null\n}\n$hostFile.Add(\"# minikube section\")\n$hostFile.Add(\"$ip`t$localDnsEntry\")\n$hostFile.Add(\"# minikube end section\")\n\nif ($hostFile.Count -ge $entriesCount) # test to ensure we don't put fewer entries back in host file\n{\n  Write-Host \"Updating our hosts file with $($hostFile.Count) entries.\"\n  Set-Content -path C:\\Windows\\System32\\drivers\\etc\\hosts $hostFile\n}\nWrite-Host \"Done\"\n```\n\nThis powershell script **will** change the way that the DNS system works on your Windows workstation. The **hosts** file takes precedence over a DNS query to the internet, so once we do this, our Windows machine will resolve the IP address we've stated and not the one that would have been return by xip.io. This will break our ability to use DockerCompose. Comment out the entry to use DockerCompose again. The alternative is to create separate domains/configurations for each environment (Docker & minikube) on your local workstation. This is a bit cumbersome with the way seed data is handled today, but it is a problem that could be solved.\n\nYou should now be able to visit the following URLs and only be challenged for your username/password once and you will be asked for consent from each set that you visit.\n\n- [http://127.0.0.1.xip.io](http://127.0.0.1.xip.io) - STS Landing page\n- [http://127.0.0.1.xip.io:9000](http://127.0.0.1.xip.io:9000) - Admin Application\n- [http://127.0.0.1.xip.io:5000/swagger](http://127.0.0.1.xip.io:5000/swagger) - Admin API Swagger Api Explorer\n\nWith the Admin Application, if you have not already logged in, you should be re-directed to the STS login page, hence the single-sign on aspect of our authentication ecosystem!\n\n## Victory!!\n\nSo! I'm crossing my fingers (virtually and into perpetuity) that you've arrived at a place that you can test the IdentityServer4 applications (all three), running in a minikube-based **K8S** cluster! If you can't, you'll have to work at it a bit and if you really get stuck, try reaching out to me on Twitter! I'm usually paying attention to notifications there!\n\nI certainly encourage you to explore and experiment with **k8s** and IdentityServer4 in this environment. With these instructions, your manifests, and a little bit of luck, you should be able to always get back to a known working state, even if that means a little bit of lost data.\n\nYou can always start from scratch very easily from a minikube **k8s** cluster perspective. The following commands will tear down and re-build you minikube cluster. This destroys everything and gives you a totally clean slate.\n\n```powershell\nminikube stop\nminikube destroy\nminikube start --vm-driver=hyperv --cpus=4 --memory=16g --extra-config=apiserver.service-node-port-range=80-33000\n```\n\nOur next stop will be a bit of a reflection point, and then, on to AKS!!\n\n**Next up:**\n[Pause to reflect](/kubernetes/kubernetes-my-journey-part-6)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 6","authorId":"dave_white","slug":"kubernetes-my-journey-part-6","date":"2020-05-22 10:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-6/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-6/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 6\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 06:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Getting Started with Kubernetes - Minikube - Part B](/kubernetes/kubernetes-my-journey-part-5b)\n\n# Pause to reflect\n\nThere was a point in this development effort that things started to click in my head, and I wanted to put this chapter in here as a more concrete reflection point and not leave it to luck that you'll have yours as well. I also wanted to share my epiphany(s) so this spot makes as good a place as any.\n\nThere were a bunch of things that clicked for me after moving this from docker to **k8s**.\n\n## Kubernetes is not hard\n\nKubernetes, all by itself, is very approachable with the tools and the self-guided training I started. With **minikube**, [kubernetes.io](https://www.kubernetes.io), and [Pluralsight](https://www.pluralsight.com), I was able to stand-up a cluster and just do simple things like put resources into the cluster! Sure there are some things that turned out to be a bit of a pain in the butt that we'll discuss later on, but Kubernetes itself isn't hard.\n\n## Getting a running system in Kubernetes IS hard\n\nSo, as easy as kubernetes was, getting an actual working business solution running in kubernetes was much harder! This exercise has really adjusted my perspective about the scope of DevOps and it has adjusted my perspective of what \"full-stack\" could really mean. Let's step back for a second and look at what I had to create and stand-up in my **k8s** cluster for things to work.\n\n- a database (Postgres)\n  - needs Postgres-specific configuration\n  - needs networking configuration\n  - needs persistent volumes\n  - needs secrets\n- a database administration application (pgAdmin4)\n  - needs pgAdmin-specific configuration\n  - needs networking configuration\n  - needs persistent volumes\n  - needs secrets\n- log ingestion (Seq)\n  - needs Seq-specific configuration (minimal)\n  - needs networking configuration\n  - needs persistent volumes\n- raw log consumption (fluentd/graylog/sqelf)\n  - needs fluentd-specific configuration\n  - needs rule pipelines designed\n  - needs networking configuration\n- the identityserver4 group (STS, Admin, AdminAPI)\n  - build the applications (ASP.NET Core, Web Apps, etc)\n  - OAuth2, OpenID Connect is all about configuration!\n  - needs networking configuration\n- Ingress Controller (nginx and/or traefik)\n  - needs nginx-specific configuration\n  - needs traefik-specific configuration\n  - Azure-specific annotations\n- DNS rules (CoreDNS)\n  - forwarding and rewrite rule configuration\n- certificate management (CertManager)\n\nThe interesting thing about all of these applications is that they are all **completely independent** pieces of software/products that you need to become proficient, if not an expert, in configuring, running, and maintaining. Kubernetes makes it pretty easy to put all of these things in the cluster. Making it all work, in a manner than you can own and operate in an enterprise production setting, is non-trivial and I'll just come out and say it, Hard!\n\nI don't want it to be a doom and gloom story though. There is a light at the end of the tunnel. These are modern pieces of software that are really good to work with. But you still should plan to work with them if you are doing this yourself. If you are on a bigger team with lots of support, then make sure you delegate tasks to those with skills and or experience to get that part of it done. Then automate it, and share the knowledge you've learned.\n\n## Getting it running in the cloud is not bad\n\nThe good thing about getting it working in the cloud is, your probably going to focus on one cloud. Could you get it working in multiple clouds? You bet! The **k8s** resources are all the same, it would just be the cloud provisioning application(s) that would be different. Getting to be really familiar with what services your cloud provider offers and how to use those services will be key. But, again, you will need to be very proficient if not an expert in your cloud providers services to own and operate your **k8s** cluster in the cloud.\n\nThe Azure services that we are using and need to understand are:\n\n- Azure Kubernetes Service\n- Public IP Address\n- Load Balancer\n- Azure Storage Accounts\n  - azureFile\n  - azureDisk\n- Virtual Network\n- Virutal Machine Scale Set\n- Route Table\n- Network Security Group\n\nThankfully, programmatically approaching my infrastructure with [Pulumi](https://www.pulumi.com) has made this much easier, especially with intellisense support, so don't worry about this too much. I'll show you what I did shortly.\n\n## You do not need to master ALL runtime environments\n\nOne thing I discovered quickly is I don't need to keep my web applications running in:\n\n1. Windows Native - IIS\n1. Kestrel\n1. Docker\n1. **AKS**\n\nTrying to do this will lead to a lot of work that just isn't going to be needed. It is potentially a valuable learning experience so I'm not saying _don't_ do it, but once you decide on your runtime environment(s), do what is best to make sure it is configured and runnable there.\n\nIn this case, I felt I did want to keep it running in Docker for the local developer debugging experience. But my primary runtime target was going to be **AKS** so I didn't spend any time keeping this running in Kestrel or IIS via Visual Studio from a configuration perspective, and Docker did not get the full-fidelity experience that eventually **AKS** did. I do keep the DockerCompose experience working so I can debug the IdentityServer4 applications on my local machine.\n\n## Cluster Logging from the Start\n\nWhen I started this, I already knew I wanted to do logging in my applications, but I really didn't think about logging across the cluster. I knew it happened and I did solve some problems by looking at the logs in various pods, I should have explored and found **fluentd** sooner in my journey. Logs are so important in this environment because we have many applications and kubernetes itself spread across multiple nodes and who knows how many pods.\n\n## IaC and Automation are Awesome\n\nI am really glad that I started down the path of **Infrastructure as Code** right from the very start of the **AKS** journey, and I'm going to get my **Pulumi** IaC applications running against minikube as well. Being able to capture what you've learned about your infrastructure as application code is a fantastic benefit, in addition to the actual automation that you can invoke with a couple key strokes, the push of a button, or a commit/push. If I was going to give myself some advice, I'd tell myself I should really make sure I pay attention to the next post in the series.\n\n## Be Prepared to need a LOT of Permissions\n\nAt the beginning of your project, if you aren't in full control of the whole ecosystem, you should start the process of getting the required permissions. This whole series generally assumes you have full-control when we start working on the **AKS** components, but you'll also need access to DNS registrations, and you may need permissions to acquire and use some of the tooling.\n\n## Summary\n\nThis was intended to be a brief stop. A chance to pause, catch your breath, and reflect on what you might of learned based on what I did learn. Don't be discouraged if all of this is hard or tricky or taking longer than you expected. This has been an incredible amount of learning if you started from scratch like me with **k8s** and you should be proud of where you've gotten too.\n\n**Next up:**\n[Moving to Azure Kubernetes Service - Part A](/kubernetes/kubernetes-my-journey-part-7a)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 7a","authorId":"dave_white","slug":"kubernetes-my-journey-part-7a","date":"2020-05-22 09:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-7a/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-7a/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 7a\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 05:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Pause to reflect](/kubernetes/kubernetes-my-journey-part-6)\n\n# Moving to Azure Kubernetes Service - Part A\n\nIt would be great if we could get this all working in minikube and call it done, but we're not quite that lucky! We're probably going to need a platform with a bit more breathing room and additional capabilities to run our production workloads, so we'll have to figure out a way to move all of this into that platform. In our case, that platform is going to be **Azure** and the **Azure Kubernetes Services (AKS)**.\n\nWith the desire to move our resources into a new **k8s** cluster in the cloud, there are a lot of moving parts in the infrastructure as compared to what minikube has. Here is a picture of the basic resources we'll have in Azure after we stand up this **k8s** cluster.\n\n<img src=\"/images/dwhite/azure-basic-aks-resources.png\" alt=\"Basic Azure Kubernetes Services Resources\" height=\"400px\">\n\nI also had to consider managing the **k8s** resources (apps in manifests). I want that to be a part of any automation as well.\n\nWith all of this in mind, I knew I was going to want something more than a collection of PowerShell scripts to manage the **AKS** resources _and_ the **k8s** resources in our cluster. Thankfully, a new product called [Pulumi](https://www.pulumi.com) had recently joined the market that looked like it would fit the bill as far as ease of use, community support, and a full IaC ecosystem for me to work with.\n\nThis part of the series is mostly going to be about Pulumi, with side discussions about the specific Azure resources that we will instantiate with Pulumi.\n\n## Important Assumption\n\nNow that we are moving our activites off of our development machines and into the cloud, it is very important that you have all of the required permissions to act (or for Pulumi to act on your behalf) in your Azure subscription. We will be creating many resources in Azure and you **must** have permission to create these resources.\n\n## Pulumi - Getting Started\n\n[Pulumi](https://www.pulumi.com) is a platform that includes:\n\n1. A cloud-platform that stores data about your preferences, your settings for projects (stacks), and the results of your executions.\n1. Multiple language-specific SDKs (see languages below) that allow you to create a Pulumi application that will run and deploy your infrastructure. You can choose the language you are most comfortable with to write _your_ application.\n1. The Pulumi CLI tool that will allow you to manage your infrastructure and run your application to stand-up, tear down, or manage your project (stack).\n\nIn addition to the actual tooling, there is a tremendous amount of documentation and community support. I've generally been happy with the documentation even though I think it is still lacking in a couple places, but the community support has been really good. [Pulumi has a Slack](https://slack.pulumi.com/) that anyone can join; it has logical channels that will generally meet your needs, and the Pulumi team have been very responsive in this slack whenever I encountered a problem.\n\n> I don't know about you, but when I have a programming problem, I skip all of the \"conceptual\" stuff, dive in, and thrash around a bunch. But, if you are inclined to understand the core Pulumi architecture and concepts, you should [start reading here.](https://www.pulumi.com/docs/intro/concepts/)\n\n### Creating an Account\n\nPulumi is a platform and a part of that platform are cloud-based services, associated to an account, that stores your settings, secrets, and outcomes from deployments. Pulumi has 4 pricing tiers, the first of which is Community and is free! This is the one I'm currently using. In the Community edition, your user is basically mapped one to one with an _Organization_ and this organziation can have _stacks_ which are (sort of) the Pulumi term for a deployment target. These stacks are associated with deployment projects so a project can have _n_ stacks in it. The Community SKU of Pulumi is free and so far, it has been everything I needed.\n\n```text\nPulumi Organization\n  |- User Account(s)\n  |- Project A\n  |   |- Pulumi application\n  |   |- stack (dev)\n  |      |- config, history, etc\n  |   |- stack (prod)\n  |      |- config, history, etc\n  |\n  |-Project B\n      |_ Pulumi application\n      |- stack (dev)\n         |- config, history, etc\n      |- stack (prod)\n         |- config, history, etc\n```\n\nYou may already know that you need to have more than one person working on this or you may be concerned that you'll outgrow the Community edition, but you shouldn't be concerned. It looks like Pulumi has a seamless upgrade path (that I haven't used) and Pulumi also has a feature that allows you to transfer a stack to another account, so you aren't going to be stuck as you grow with the platform. Additionally, everything that you create to use Pulumi (apps and scripts) is **yours** and can be version controlled, shared, and re-used as you see fit. It would be quite easy to re-create a stack in a new organization as needed.\n\nSo, unless you know you are going to have multiple people involved in the IaC part of the project, you can just create a Community-based account and start deploying!\n\n### Install the Tools\n\nPulumi has a great set of tutorials [here](https://www.pulumi.com/docs/get-started/azure/) for getting started with Azure. I'm going to repeat some of it, but you should definitely check out their learning resources.\n\nNow that you've created an account, it is time to start building your application! First, you'll need to install the Pulumi CLI in your development environment and sign into your cloud account.\n\n#### Pulumi CLI\n\nYou have a couple choices to get the Pulumi CLI!\n\n```powershell\nchoco install pulumi # requires chocolatey\n# -or-\n# plain powershell\niex ((New-Object System.Net.WebClient).DownloadString('https://get.pulumi.com/install.ps1'))\n```\n\nOnce you have the CLI, you can login via a username/password redirect to a browser or you can use an access token that you've created in the web admin pages for your Pulumi account.\n\n<img src=\"/images/dwhite/pulumi-login-cli.png\" alt=\"Logging into Pulumi\" height=\"150px\">\n\nYou can use the `pulumi whoami` to see if you are currently logged in (or who you are logged in as) as well.\n\n#### Language-specific SDKs\n\nNext, you'll need to consider what [language](https://www.pulumi.com/docs/intro/languages/) you are going to use when creating your Pulumi application. There are many choices. **Typescript/JavaScript, Go, .NET Core (C#, F#, VB), and Python**. Pick whichever one your organization has the most skills in. I like Typescript so that is what I picked and what my code will be written in. If you want, you can write an SDK in your favorite language. This is all open-source.\n\n#### Project Structure\n\nOnce you've selected a language, you can use the Pulumi **CLI** to create your first **deployment project and stack**. I treat a stack as a deployment that I want to put in a specific environment. For example, I would have 2 stacks for my **k8s** infrastructure. One is the dev infrastructure in our Development Azure subscription, and the other is the production infrastructure which would be in the Production Azure subscription. These 2 stacks stand-up all of the Azure **AKS** resources. I would also have 2 stacks for the **k8s** resources that go into those **k8s** clusters.\n\nProjects are mostly containers for stacks (configuration, history) and an application. Stacks have specific configuration settings and histories that are important. The application that you run for the project can use each stack for specific deployment details.\n\n> A monolithic stack with a single app is a good way to learn and this is how most of the tutorials work. However, I found that it wasn't how I wanted to manage my deployments.\n\nI originally created a single monolithic project with a single stack and one application but have since changed this to two projects with a single application and a dev/prod stack each. The first project for the **AKS** infrastructure was less volatile and I didn't need to tear it all down all the time. It takes about 18 minutes to stand-up our cluster and 10 minutes to tear it all down. The second project for the **k8s** resources changed much more frequently and I would often want to clear out the **k8s** cluster and start from a clean slate. **k8s** resources can be added or removed from the cluster quickly and frequently. This series will only show the multi-project approach.\n\nFirst, using the Pulumi CLI, we are going to create a deployment application, in the Typescript language, for the Azure cloud, for only the AKS infrastructure. We'll do our **k8s** resource deployment application later.\n\nLet's create a folder in our infrastructure folder for the **AKS** deployment stack.\n\n<img src=\"/images/dwhite/pulumi-create-new-stack.png\" alt=\"Create a new Pulumi stack\" height=\"450px\">\n\nOnce that is done, we can use the Pulumi CLI to build our new project with its initial stack.\n\n`pulumi new azure-typescript --secrets-provider=passphrase`\n\nThis will kick off the workflow to acquire some details before it creates the stack. In my case, I answered the workflow questions with:\n\n1. project name (aks) <-- hit enter and accepted default\n1. project description: **Deploy our kubernetes infrastructure**\n1. stack name: (dev) <-- hit enter and accepted default\n1. Enter your passphrase to protect config/secrets: **P@ssw0rd!**\n1. azure:environment: (public) <-- hit enter and accept default\n1. azure:location: (WestUS) **WestUS**\n\nAfter answering those questions, the CLI will finish off by:\n\n- creating your project and first stack\n  - saving them in the cloud - this happens automatically\n- scaffolding out the initial application files locally\n  - pulling down all of the correct npm modules based on your cloud provider selection and language choices.\n\n<img src=\"/images/dwhite/pulumi-created-dev-stack.png\" alt=\"Create a new Pulumi stack\" height=\"300px\">\n\nWe can also look in the web portal for our Pulumi account and see the new stack is available there!\n\n<img src=\"/images/dwhite/pulumi-new-project-in-web.png\" alt=\"The new stack in the web portal\" height=\"300px\">\n\nYou can click the the stack to see what information has been published to the Pulumi cloud. There isn't much there yet, but there are some instructions on how to get more information there. We'll see that shortly.\n\n##### A Note about --secrets-provider\n\nYou should have noticed that I've used the `--secrets-provider` parameter in the pulumi CLI invocation. If you are going to be building a single stack like many of the pulumi examples you'll find, you will not use or see this parameter. By default, each stack as a unique secrets provider and stacks **cannot** read each other's secrets. I already plan to have multiple stacks that I want to be able to share secrets between so I need to use this parameter in order to create secrets providers that can reach each other's secrets.\n\nPassphrase is the simplest to use and get working so I'm using that for this article, but you can also use external 3rd party secrets providers. Support providers include:\n\n- awskms: AWS Key Management Service (KMS)\n- azurekeyvault: Azure Key Vault\n- gcpkms: Google Cloud Key Management Service (KMS)\n- hashivault: HashiCorp Vault Transit Secrets Engine\n\nMore details about how to use these encryption providers can be found [here -- Alternate Secrets Encryption.](https://www.pulumi.com/docs/intro/concepts/config/)\n\nWe will re-visit secrets in a little while. Now back to our new project.\n\n#### Scaffolded Files\n\nIf we inspect the scaffolded application in the aks folder, we'll see the following:\n\n| | |\n|:------------------------------------|:--|\n|**node_modules**    | This is where our SDK lives, we use NPM to add SDK components|\n|**.gitignore**      | Version controlled application development, just like you already do!|\n|**index.ts**        | the entry-point for our TypeScript-based IaC application|\n|**packages.json**   | The list of packages used in our application|\n|**Pulumi.dev.yaml** | stack-specific configuration values|\n|**Pulumi.yaml**     | project-specific values|\n|**tsconfig.json**   | TypeScript application configuration|\n\n## More Tooling - azure-cli\n\nSo, we have the Pulumi CLI, maybe git CLI, and now we need to make sure we have one more tool in place. We need the **azure-cli** command line tool. Pulumi will use the **azure-cli** to actually do all of the work in the correct subscription.\n\nYou'll need to install the **azure-cli** with [instructions here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest#install-or-update). I like the little PowerShell script that does it for you myself.\n\nOnce the azure-cli is installed, you'll need to log into your Azure subscription that you want to work with.\n\n`az login` will open a browser window and help you log into your subscription.\n\n`az account list` will list all of the available subscriptions (if there is more than one)\n\n`az account set <subscription name>` will set the current context to the desired subscription\n\n> If you have multiple subscriptions, you'll probably spend a bit of time switching back and forth. One thing I would suggest is to be careful when working with multiple subscriptions. Pulumi, via the current azure-cli context, will happily deploy or tear-down your infrastructure when asked. There are some safe-guards in place with regard to tear-down or changing, but I've found Pulumi is always happy to stand new things up into a subscription! I accidentally installed a minecraft server into my client's development subscription this way once! Ok, maybe twice!\n\nWith Pulumi ready and azure-cli ready, we should be ready to start coding! If you haven't done this already, it's time to open VS Code or your favorite text editor!\n\n## Your first Pulumi Application\n\nI like to open VS Code right away for a couple reasons. It is a nice text editor with excellent TypeScript support and it also has a built-in terminal window that I can set to use PowerShell Core and I can leave the directory set to the one that holds the files I'm working in.\n\nOpen your **index.ts** file and take a look at what the Pulumi CLI scaffolded.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\"; // Add the Pulumi core SDK module to your application\nimport * as azure from \"@pulumi/azure\";   // Add the Azure core SDK module to your application\n\n// Create an Azure Resource Group\nconst resourceGroup = new azure.core.ResourceGroup(\"resourceGroup\");\n\n// Create an Azure resource (Storage Account)\nconst account = new azure.storage.Account(\"storage\", {\n    // The location for the storage account will be derived automatically from the resource group.\n    resourceGroupName: resourceGroup.name,\n    accountTier: \"Standard\",\n    accountReplicationType: \"LRS\",\n});\n\n// Export the connection string for the storage account\nexport const connectionString = account.primaryConnectionString;\n```\n\nAt the top, you'll see that two modules have been added for you. The Pulumi Core SDK module and the Azure Core SDK module. Depending on what you need, you only add the modules to your application that you are actually using. If you need additional modules, we can use `npm install @pulumi/<module name>` to get those SDK modules.\n\nNext, we see code that is creating a new ResourceGroup in Azure to hold all of our new resources.\n\nThen, we see code that is creating a new Storage account.\n\nAnd finally, we have a snippet of code that is going to export the storage account's connection string for use later.\n\n> Anything that you `export` in your TypeScript will be published to the cloud for review or use by another Pulumi stack/application at a later date. If you don't want these properties publicly accessible, do not export them. We will demonstrate this later. This is important to understand when working with stacks that are dependent on other stacks.\n\nThe next step is deploying this stack! Go ahead! Type:\n\n`pulumi up`\n\nYou will see the Pulumi CLI kick off your IaC application. It builds the app, does some analysis of what it wants to do, and then asks you if you'd like to continue!\n\n> If your TypeScript application won't build, the process stops here, and you have to fix it.\n\n<img src=\"/images/dwhite/pulumi-first-up.png\" alt=\"Logging into Pulumi\" height=\"250px\">\n\nOnce you accept, it finishes doing what you've coded, and it deploys your new Azure infrastructure to your subscription.\n\n<img src=\"/images/dwhite/pulumi-first-up-completed.png\" alt=\"Logging into Pulumi\" height=\"250px\">\n\nAnd it also publishes details into your Pulumi cloud account for this project/stack. You can see the exported connnectionString. Also available in the cloud is a historical log of what has happened in this stack in the **Activity** tab.\n\n<img src=\"/images/dwhite/pulumi-first-up-completed-web.png\" alt=\"Logging into Pulumi\" height=\"600px\">\n\n<img src=\"/images/dwhite/pulumi-first-stack-history.png\" alt=\"Logging into Pulumi\" height=\"300px\">\n\nAnd here are the Azure resources that were created.\n\n<img src=\"/images/dwhite/pulumi-first-up-azure-storage-account.png\" alt=\"Azure Storage Account\" height=\"300px\">\n\n> Notice that Pulumi has appended a segment of characters on your resource names to try and ensure they are unique within the subscription. I haven't tried to alter that behaviour. You can create a resource directly in Azure and import it into your stack and Pulumi will respect the name it was given.\n\nThat's pretty cool! The only problem is, I don't want a lone storage account in my Azure subscription.\n\nSo, what do we do now? Tear it all down and let Pulumi clean up everything it created.\n\n`pulumi destroy`\n\nThis will ask you for confirmation, so you are protected that way. Just let the Pulumi CLI finish it's work and go look in your Azure subscriptions! It will be clean as a whistle!\n\n## Deploying an AKS\n\nI hope that was a good introduction to Pulumi, but what we really wanted to do was build an application that would deploy our **AKS** into our subscription. Let's get to that.\n\nBefore getting started, **delete all of the existing lines of code in your index.ts**. We will not be using anything created during the initial scaffolding.\n\n### Importing more Pulumi SDK modules\n\nStanding up an **AKS** service cluster is move involved that a simple storage account. We will need more SDK modules in our application in order to make that happen. Let's add some import statements into our Pulumi application.\n\n```typescript\nimport * as azure from \"@pulumi/azure\";\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as k8s from \"@pulumi/kubernetes\";\nimport * as azuread from \"@pulumi/azuread\";\n```\n\nThis is what the top of you **index.ts** should look like. If you are doing this in VS Code, you probably have some red squiggly lines under the bottom two imports. This is where we ask NPM to go get those modules for us!\n\n`npm install @pulumi/kubernetes` **Get kubernetes module of the SDK**\n`npm install @pulumi/azuread`  **Get Azure ActiveDirectory module of the SDK**\n\nOnce that is done, the red squiggly lines should go away and you'll see that you can use those SDK modules in your application now.\n\n### Initial Configuration Values\n\nThe next part of our app initializes and exports configuration variables that we'll need for the **AKS** provisioning. The names for these variables are intended to be informative, but they are names that I've chosen. The values are determined by the intended usage. Azure expects some of these values to be specific, such as **location** or **nodeSize**. The **nodeCount** variable needs to be an int. The string **const** values that I export are for consistency in the same way that you would have an **enum** or a class containing **consts** values in a C# application. I believe this initial list of variables are the bare minimum you need to create a cluster. You may eventually have many more in your application.\n\nThis is what the configuration section will look like _when it is complete_. We will add these lines of code into the application as we work though the configuration setup so that we can `pulumi up` multiple times and see the incremental changes.\n\n```typescript\n// Acquire stack configuration values and export application-defined configuration variables\nconst config = new pulumi.Config();\nconst password = config.requireSecret(\"password\");\nconst sshPublicKey = config.require(\"sshPublicKey\");\nexport const location = config.get(\"stackLocation\") || (config.get(\"azure:location\") || \"WestUS\");\nexport const nodeCount = config.getNumber(\"nodeCount\") || 2;\nexport const nodeSize = config.get(\"nodeSize\") || \"Standard_B2s\";\nexport const storageClassName = \"managed-premium\";\nexport const resourceGroupName = \"rg_identity_dev_zwus_aks\";\nexport const publicIpAddressName = \"pip_identity_dev_zwus_aks\";\nexport const k8sDnsName = \"identity-auth-dev\";\n\nconst clientConfig = azure.core.getClientConfig();\nexport const subscriptionId = clientConfig.subscriptionId;\n```\n\n#### Pulumi Config Object\n\n`const config = new pulumi.Config();`\n\nThe first thing we do is ask the Pulumi SDK to get our stacks configuration in the form of an object of type **pulumi.Config**. This object lets us get configuration values (secret/non-secret) for our application, specific to this stack, from the Pulumi cloud. You're probably wondering how they got there though?\n\nThe Pulumi CLI has a number of methods that allow us to manage our stack configuration values. In this case, we need to get 5 different values from the cloud.\n\n#### Passwords and Secrets\n\nHere we get to meet another part of the Pulumi cloud infrastructure. Stacks can contain plaintext configuration information, and they can also contain secret configuration information. We can acquire this configuration information from the cloud when our application runs in order to provision our cluster. Let's work through this for a moment.\n\nThis password will be used for our administrative user in our **AKS** cluster. We probably don't want this to be saved as plaintext anywhere, so we're going to use the **--secret** flag when we use the Pulumi CLI to set this configuration value in our stack.\n\n```bash\npulumi config set password --secret [your-cluster-password-here] # P@ssw0rd!\n```\n\nThis command tells the Pulumi CLI to set a property on our stack configuration called **password** to the value provided and make sure it is treated securely.\n\nSince we delete all of the text, let's `pulumi up` and get that value into the cloud to see what happens.\n\n<img src=\"/images/dwhite/pulumi-set-config-password.png\" alt=\"Set the admin password in the configuration\" height=\"450px\">\n\n> Other than the initial **pulumi new azure-typescript** CLI command, any changes we make to our local context wil not be available in the web portal until we use the `pulumi up` command.\n\nIn order to access this secret from the pulumi.Config object, we add this line of code to our application.\n\n`const password = config.require(\"password\");`\n\n> You can use config.requireSecret(\"password\") to mark a variable as secret and at that point it's safe to export because it will always be encrypted/masked (in the state as well as CLI and console)\n\n#### SSH Public Key\n\nIf you'd like to be able to SSH into your linux nodes (VMs) that are in the cluster, you'll need to provide an SSH key that is provisioned into your nodes. Using a tool called `ssh-keygen` we can create an SSH key and then we can put that key into our Pulumi stack config for use anytime we create the cluster.\n\n```bash\nssh-keygen -t rsa -f key.rsa\npulumi config set sshPublicKey < key.rsa.pub\n```\n\n`ssh-keygen` will walk you through the process of creating an SSH key.\n\nThen we will use the Pulumi config to set the sshPublicKey configuration variable on the stack. If you are running a PowerShell terminal, this won't work. PowerShell doesn't like the **<** operator. You can get around that by using this command.\n\n`cmd.exe /c \"pulumi config set sshPublicKey < key.rsa.pub\"`\n\nNow you can `pulumi up` and go take a look at your configuration in the web portal again.\n\nIn order to use this variable in our application, add this line of code to our application.\n\n`const sshPublicKey = config.require(\"sshPublicKey\");`\n\n<img src=\"/images/dwhite/pulumi-set-config-sshPublicKey.png\" alt=\"Set the SSH Public Key in the configuration\" height=\"225px\">\n\n#### Location, NodeCount, NodeSize\n\nAzure is going to want to know:\n\n1. What region to create your resources in\n1. How many nodes do we want in our cluster\n1. What VMs SKUs (size) do we want to use for our cluster\n\nThe location configuration value is interesting!\n\n```typescript\nexport const stackLocation = config.get(\"stackLocation\") || (config.get(\"azure:location\") || \"WestUS\");\n```\n\nIn this code, we look for a configuration value called **stackLocation** that we can set if we want. This supports DR/HA-specific stack scenarios that I'll discuss later. If it isn't present, we can use the the default location set in the **azure:location** configuration value that was set for us when we created the project. You can take a look again with the Pulumi CLI command `pulumi config get \"azure:location\"` or you can look in the web portal as well. In the event that there is no configuration values, we've provided a fallback value of **WestUS**.\n\nI love being able to use programmatic logic in my infrastructure deployments!!\n\n> **az account list-locations** will list supported regions for the current subscription. It will spit out a JSON blob of regions and you can use the name property. It seems that commands that take a region parameter name are case-insensitive.\n\nIn order to create a **k8s** cluster, we need VMs (nodes) in the cluster. We can configure how big we want cluster to be and store that data in the stack configuration. Our fallback value is **2**.\n\n`pulumi config set nodeCount 2`\n\n> A **k8** cluster only needs 1 node to operate. This is certainly sufficient for dev contexts, but you probably want 3+ nodes in production.\n\nAdd `export const nodeCount = config.getNumber(\"nodeCount\") || 2;` to the application.\n\nAzure also needs to know what SKU our nodes (VMs) should be.\n\n> `az vm list-skus` is the azure-cli command that will list out all of the SKUs you can pick from but it spews an enormous JSON blob that lists them all and their capabilities. You're probably better off visiting [here](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) to help you decide what SKU to use.\n\n`pulumi config set nodeSize Standard_B2s`\n\nAdd `export const nodeSize = config.get(\"nodeSize\") || \"Standard_B2s\";` to the application.\n\nAgain, we provided a fallback value in the application.\n\n`pulumi up` and look at the configuration values in the web portal.\n\n<img src=\"/images/dwhite/pulumi-set-config-done.png\" alt=\"All Configurations Set\" height=\"350px\">\n\n#### Exports From Your Application\n\nFinally, we want to provide some **const** values that will be available in the stack, displayed in the web portal, and also available to any other stack that belongs to the Pulumi organization.\n\n```typescript\nexport const storageClassName = \"managed-premium\";\nexport const resourceGroupName = \"rg_identity_dev_zncu_aks\";\nexport const publicIpAddressName = \"pip_identity_dev_zncu_aks\";\nexport const k8sDnsName = \"identity-auth-dev\";\nexport const acrSecretName = \"docker-credentials\";\n```\n\n`pulumi up` and you'll see all the rest of our configuration values in the web portal. Exports are shown in a different group in the web portal. The are considered **outputs** of the stack.\n\n<img src=\"/images/dwhite/pulumi-set-config-outputs.png\" alt=\"Outputs from the application\" height=\"400px\">\n\n> You should recognize when you export a value that you will get a value in the **outputs** section of the web portal and it also be in the **config**. You don't have to export `const` values if you want to avoid that confusion.\n\n#### Getting the Azure Subscription Id\n\nWe need one more value for our application and that is the **subscriptionId**. In this case, we could use the `pulumi config set` command to manually set it, but we can get the subscription from the Azure context that we are already connected to. This is exposed via an SDK component.\n\n```typescript\n// getClientConfig is an async call, so wrap in pulumi.output\nconst clientConfig = pulumi.output(azure.core.getClientConfig());\nexport const subscriptionId = clientConfig.subscriptionId;\n```\n\n## Add AKS resources to Azure\n\nNow that we have the basic configuration values that are required for our **AKS** cluster resources, we can start to add them into our Pulumi application.\n\n> After each section, you can `pulumi up` and see what happens. When you are done that increment, you can `pulumi destroy` to clean up the resources.\n\n#### A Pre-Defined ResourceGroup\n\nWhile Pulumi is quite capable of building a ResourceGroup from scratch, you may want to use one that already exists in your Azure subscription. Financial reporting, permissions, operational activities, etc may be leveraging ResourceGroups in this way. For this example, we are going to use a pre-existing ResourceGroup. This code is also the reason that we acquired the **subscriptionId** and set the **resourceGroupName** in our configuration section.\n\nYou will need to create this ResourceGroup in Azure via the **azure-cli** or in the Azure Portal. The `azure-cli command is\n\n```powershell\n# this resourceGroupName matches our const value in the config section\naz group create --location WestUs --name rg_identity_dev_zwus_aks\n```\n\nHere is the code to get the resourceGroup object for using in the application code.\n\n```typescript\n// get the Azure Resource Group\nvar resouceGroupId = pulumi.interpolate `/subscriptions/${subscriptionId}/resourceGroups/${resourceGroupName}`;\nconst resourceGroup = azure.core.ResourceGroup.get(resourceGroupName, resouceGroupId);\n```\n\n> TypeScript **string interpolation** doesn't work very good with Pulumi Output<T> objects. You need to use the **pulumi.interpolate** syntax to create strings from Output<T>\n\n#### Creating an Azure Service Principal\n\nYour new **AKS** service instance is going to need to be able to create a lot of Azure resources. It will do all of this for you, but in order to create these resources, it will need to log in as a Service Principal that you've created in your subscription. The first thing we'll do is get our application to create that Service Principal.\n\n```typescript\n// Original example: https://github.com/pulumi/examples/blob/master/azure-ts-aks-helm/README.md\n// Create an Azure AD Application\nconst adApp = new azuread.Application(\"aksSSO\");\nexport const adAppId = adApp.applicationId;\n\n// Create an Azure Service Principal for that application\nconst adSp = new azuread.ServicePrincipal(\"aksSSOSp\", { applicationId: adApp.applicationId });\nexport const adSpId = adSp.id;\n\n// Assign the password from our configuration values to the Service Principal\nconst adSpPassword:any = new azuread.ServicePrincipalPassword(\"aksSpPassword\", {\n    servicePrincipalId: adSpId,\n    value: password,\n    endDate: \"2099-01-01T00:00:00Z\"\n});\n```\n\nWe are not exporting any of these values. We won't need to see them in the Pulumi web portal or use them in any other project or stack. You can see this application and Service Principal in the AAD that your subscription is connected to.\n\n> Creating an AAD Service Principal sometimes takes a bit of time. Operations that depend on the SP being created (the AKS Service creation code) may fail until the SP is finished being created. If this happens, simply `pulumi up` again.\n\n#### Storage Account for Database backups\n\nOur business problem requires a database and a good thing to do once in a while is backup that database and put those backups somewhere. For this activity, we are creating an Azure Storage Account, in our **AKS** specific resource group, that we will use as a volume in the pgAdmin4 pod.\n\n```typescript\n// Create storage account for Azure Files\nconst storageAccountK8s = new azure.storage.Account(\"Identity\",{\n    resourceGroupName: resourceGroupName,\n    accountTier: \"Standard\",\n    accountReplicationType: \"LRS\",\n});\n\nexport const storageAccountName = storageAccountK8s.name;\nexport const storageAccountKeyPrimary = pulumi.secret(storageAccountK8s.primaryAccessKey);\nexport const storageAccountKeySecondary = pulumi.secret(storageAccountK8s.secondaryAccessKey);\nexport const storageAccountConnectionStringPrimary = pulumi.secret(storageAccountK8s.primaryConnectionString);\nexport const storageAccountConnectionStringSecondary = pulumi.secret(storageAccountK8s.secondaryConnectionString);\n\nconst fileShare = new azure.storage.Share(\"k8sFileShare\", {\n    name: \"k8s-file-share\",\n    storageAccountName: storageAccountK8s.name,\n    quota: 10\n});\n\nexport const fileShareName = fileShare.name;\nexport const fileShareUrl = fileShare.url;\n```\n\nYou'll notice that during the creation of the storage account, we get back the connection strings and keys which we'll need to use later on. We can use the `pulumi.secret()` method to ensure that these are treated as secrets by the Pulumi cloud infrastructure.\n\n<img src=\"/images/dwhite/pulumi-secrets-output.png\" alt=\"Outputs as secrets\" height=\"200px\">\n\n#### Azure Kubernetes Service\n\nWe are finally going to create our **Azure Kubernetes Service (AKS)** instance! w00 h00!! Or I should say, we're going to code it up in Pulumi and we'll let Pulumi take care of creating it!\n\n```typescript\n// Creates an AKS cluster.\nconst k8sCluster = new azure.containerservice.KubernetesCluster(\"aksCluster\", {\n    resourceGroupName: resourceGroupName,\n    kubernetesVersion: \"1.17.3\",\n    location: stackLocation,\n    defaultNodePool:{\n        name: \"aksagentpool\",\n        nodeCount: nodeCount,\n        enableAutoScaling: false,\n        vmSize: nodeSize\n    },\n    dnsPrefix: k8sDnsName,\n    linuxProfile: {\n        adminUsername: \"aksuser\",\n        sshKey: { keyData: sshPublicKey }\n    },\n    servicePrincipal: {\n        clientId: adAppId,\n        clientSecret: password,\n    },\n/* This is commented out because we do not want to do this. Please see my\n   blurb about LogAnalytics at the bottom of this post.\n    addonProfile: {\n        omsAgent: {\n            enabled: true,\n            logAnalyticsWorkspaceId: loganalytics.id,\n        },\n    } */\n});\n\nexport const nodeResourceGroup = k8sCluster.nodeResourceGroup;\n```\n\nThis is a pretty simple pulumi declaration given everything it took to get here. You'll notice the following parameters are mostly using our configuration. This is important when we add additional stacks (deployment target environments):\n\n- resourceGroupName\n- kubernetesVersion: \"1.17.3\"\n  - you can only use kubernetes versions supported by Azure. This is the most recent at the time of writing\n- location\n- nodeCount\n- vmSize\n- dnsPrefix\n- sshKey\n- servicePrincpal\n  - generated during creation\n\nOne of the things that does happen when Azure creates this **AKS** instance is that the cluster will use the Service Principal to create all of the resources and that all of the cluster resources will be placed in a in an auto-generated ResourceGroup. I haven't discovered to how to alter this behaviour. The name of the resource group that you create the **AKS** service in will be a component of this auto-generated ResourceGroup's name.\n\n> If you `pulumi up` after this section be aware that creating a cluster takes time and requires the Service Principal to exist as well. `pulumi destroy` also takes a few minutes to run when an **AKS** instance is involved.\n\n#### Interacting with Kubernetes in our Cluster\n\nNow that we have an **k8s** cluster running, we need to start interacting with the cluster and not Azure. In order to do that, we need a Pulumi object that is comparable to `kubectl`. In Pulumi, this is the **k8s.Provider** object.\n\n```typescript\n// Expose a k8s provider instance using our custom cluster instance.\nconst k8sProvider = new k8s.Provider(\"aksK8s\", {\n    kubeconfig: k8sCluster.kubeConfigRaw,\n});\n\n// put our new clusterName in Pulumi service\nexport const clusterName = k8sCluster.name;\n// put the az aks get-credentials command in Pulumi service\nexport const kubeCtrlCredentialsCommand = pulumi.interpolate `az aks get-credentials --resource-group ${resourceGroupName} --name ${clusterName} --context \"MyProject.Identity\"`;\n\n// Export the kubeConfig\nexport const kubeConfig = pulumi.secret(k8sCluster.kubeConfigRaw);\n```\n\nIn this code, you can see that we create a k8s.Provider instance using the kubeConfig that we can get from our **AKS** cluster instance. We also want to export that kubeConfig so that we can use it in our next Pulumi application that will create all of the **k8s** resources in the cluster. Remember, the kubeConfig is credentials to get into your cluster, so you should treat it as a very important **secret**.\n\nI've also exported the new clusterName and a helper output value of the `az aks get-credentials` command that will help you put your new **k8s** credentials in your local kubeConfig file.\n\nWe will use the **k8sProvider** object in the remainder of this script to interact with our **k8s** cluster.\n\n#### Kubernetes Secrets\n\nThe finish our basic **AKS** deployment, I am going to install a couple secrets that our **k8s** cluster will need to operate.\n\nFor our Azure Container Registry secrets, we need to get the ACR instance, ask it for it's secrets, and put them into **k8s**. This is the safest way to manage these secrets since we don't want them in our code base.\n\n> Please treat these and all other secrets with all due care.\n\n```typescript\nconst acrInstanceName = \"depthconsulting\";\nconst acrIdentifier:string =  \"/subscriptions/<your subscriptionId>/resourceGroups/<your resourceGroup>/providers/Microsoft.ContainerRegistry/registries/<your RegistryName>\";\nconst privateACRInstance = azure.containerservice.Registry.get(acrInstanceName, myAcrIdentifier);\n\nconst k8sDockerSecret = helpers.createImagePullSecret(\n    acrSecretName, // secret name\n    privateACRInstance.adminUsername,\n    privateACRInstance.adminPassword,\n    privateACRInstance.loginServer,\n    k8sProvider);\n```\n\nThere is a help function that we are using that hides the complexity of this.\n\n```typescript\nexport function createImagePullSecret(\n    secretName: string,\n    username: pulumi.Output<string>,\n    password: pulumi.Output<string>,\n    registry : pulumi.Output<string>,\n    k8sProvider : k8s.Provider): k8s.core.v1.Secret {\n\n    // Put the username password into dockerconfigjson format.\n    let base64JsonEncodedCredentials : pulumi.Output<string> = \n        pulumi.all([username, password, registry])\n        .apply(([username, password, registry]) => {\n            const base64Credentials = Buffer.from(username + ':' + password).toString('base64')\n            const json =  `{\"auths\":{\"${registry}\":{\"auth\":\"${base64Credentials}\"}}}`\n            console.log(json)\n            return Buffer.from(json).toString('base64')\n        })\n\n    return new k8s.core.v1.Secret(secretName, {\n        metadata: {\n            name: secretName,\n        },\n        type: 'kubernetes.io/dockerconfigjson',\n        data: {\n            \".dockerconfigjson\": base64JsonEncodedCredentials,\n        },\n    }, { provider: k8sProvider })\n};\n```\n\nWe will also need to create a way for our **k8s** cluster to connect to the various storage providers that are available to us in Azure. In this case, we want to enable **k8s** to connect to the file storage we are going to use for our database backups. This uses some of the variables that we captured when creating our Storage Account as well as a helper function.\n\n[Microsoft Documentation](https://docs.microsoft.com/en-us/azure/aks/azure-files-volume)\n\nThis is the **kubectl** command that would create this secret.\n\n`kubectl create secret generic azure-secret --from-literal=azurestorageaccountname=$AKS_PERS_STORAGE_ACCOUNT_NAME --from-literal=azurestorageaccountkey=$STORAGE_KEY`\n\nNow we convert that into a TypeScript function.\n\n```typescript\nconst azureStorageSecret = helpers.createAzureFileSecret(\n    azureStorageSecretName,\n    storageAccountName,\n    storageAccountKeyPrimary,\n    k8sProvider);\n```\n\nHere is a helper function that will create the secret properly for us.\n\n```typescript\nexport function createAzureFileSecret(\n    secretName: string,\n    storageAccountName: pulumi.Output<string>,\n    storageAccountKey: pulumi.Output<string>, \n    k8sProvider : k8s.Provider): k8s.core.v1.Secret {\n\n    let dataValue = pulumi\n        .all([storageAccountName, storageAccountKey])\n        .apply(([san,sak]) =>{\n            const b64SAN = Buffer.from(san).toString('base64');\n            const b64SAK = Buffer.from(sak).toString('base64');\n            return { azurestorageaccountname: b64SAN, azurestorageaccountkey: b64SAK };\n        }\n    );\n\n    return new k8s.core.v1.Secret(secretName, {\n        type: \"kubernetes.io/generic\",\n        metadata: {\n            name: secretName,\n            namespace: \"default\"\n        },\n        data: dataValue\n    },{provider: k8sProvider});\n};\n```\n\n> I'll eventually make this a more generic mechanism for creating secrets.\n\n### Our Full AKS Application\n\nWe have now completed our whole Pulumi application that will stand up a basic **AKS** cluster in Azure. Here is the entire script for completeness.\n\n```typescript\nimport * as azure from \"@pulumi/azure\";\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as k8s from \"@pulumi/kubernetes\";\nimport * as azuread from \"@pulumi/azuread\";\n\n// Import some stack configuration and export used configuration variables for the AKS stack.\nconst config = new pulumi.Config();\nconst password = config.requireSecret(\"password\");\nconst sshPublicKey = config.require(\"sshPublicKey\");\nexport const stackLocation = config.get(\"stackLocation\") || (config.get(\"azure:location\") || \"WestUS\");\nexport const nodeCount = config.getNumber(\"nodeCount\") || 2;\nexport const nodeSize = config.get(\"nodeSize\") || \"Standard_B2s\";\nexport const storageClassName = \"managed-premium\";\nexport const resourceGroupName = \"rg_identity_dev_zwus_aks\";\nexport const publicIpAddressName = \"pip_identity_dev_zwus_aks\";\nexport const k8sDnsName = \"identity-auth-dev\";\nexport const acrSecretName = \"docker-credentials\";\n\nconst clientConfig = pulumi.output(azure.core.getClientConfig());\nexport const subscriptionId = clientConfig.subscriptionId;\n\n// Get reference to pre-existing Azure ResourceGroup\n// get the Azure Resource Group\nvar resouceGroupId:pulumi.Output<string> = pulumi.interpolate `/subscriptions/${subscriptionId}/resourceGroups/${resourceGroupName}`;\nconst resourceGroup = azure.core.ResourceGroup.get(resourceGroupName, resouceGroupId);\n\n// Create AAD Application and Service Principal for AKS Cluster to use to create resources in the subscription\n// https://github.com/pulumi/examples/blob/master/azure-ts-aks-helm/README.md\n// Create the AD service principal for the k8s cluster.\nconst adApp = new azuread.Application(\"aksSSO\");\nexport const adAppId = adApp.applicationId;\n\nconst adSp = new azuread.ServicePrincipal(\"aksSSOSp\", { applicationId: adApp.applicationId });\nexport const adSpId = adSp.id;\n\nconst adSpPassword:any = new azuread.ServicePrincipalPassword(\"aksSpPassword\", {\n    servicePrincipalId: adSpId,\n    value: password,\n    endDate: \"2099-01-01T00:00:00Z\"\n});\n\n// Create storage account for Azure Files\nconst storageAccountK8s = new azure.storage.Account(\"Identity\",{\n    resourceGroupName: resourceGroupName,\n    accountTier: \"Standard\",\n    accountReplicationType: \"LRS\",\n});\n\nexport const storageAccountName = storageAccountK8s.name;\nexport const storageAccountKeyPrimary = pulumi.secret(storageAccountK8s.primaryAccessKey);\nexport const storageAccountKeySecondary = pulumi.secret(storageAccountK8s.secondaryAccessKey);\nexport const storageAccountConnectionStringPrimary = pulumi.secret(storageAccountK8s.primaryConnectionString);\nexport const storageAccountConnectionStringSecondary = pulumi.secret(storageAccountK8s.secondaryConnectionString);\n\nconst fileShare = new azure.storage.Share(\"k8sFileShare\", {\n    name: \"k8s-file-share\",\n    storageAccountName: storageAccountK8s.name,\n    quota: 10\n});\nexport const fileShareName = fileShare.name;\nexport const fileShareUrl = pulumi.secret(fileShare.url);\n\n// Create AKS Cluster\nconst k8sCluster = new azure.containerservice.KubernetesCluster(\"aksCluster\", {\n    resourceGroupName: resourceGroupName,\n    kubernetesVersion: \"1.17.3\",\n    location: stackLocation,\n    defaultNodePool:{\n        name: \"aksagentpool\",\n        nodeCount: nodeCount,\n        enableAutoScaling: false,\n        vmSize: nodeSize\n    },\n    dnsPrefix: k8sDnsName,\n    linuxProfile: {\n        adminUsername: \"aksuser\",\n        sshKey: { keyData: sshPublicKey }\n    },\n    servicePrincipal: {\n        clientId: adAppId,\n        clientSecret: password,\n    }\n});\n\n// Expose a k8s provider instance using our custom cluster instance.\nconst k8sProvider = new k8s.Provider(\"aksK8s\", {\n    kubeconfig: k8sCluster.kubeConfigRaw,\n});\n\n// put our new clusterName in Pulumi service\nexport const clusterName = k8sCluster.name;\n// put the az aks get-credentials command in Pulumi service\nexport const kubeCtrlCredentialsCommand = pulumi.interpolate `az aks get-credentials --resource-group ${resourceGroupName} --name ${clusterName} --context \"MyProject.Identity\"`;\n\n// Export the kubeConfig as a secret\nexport const kubeConfig = pulumi.secret(k8sCluster.kubeConfigRaw);\n\n// Create secrets in **k8s** cluster to allow certain operations\n\n// Docker Registry credentials\nconst acrInstanceName = \"<your ACR name here>\";\n//const acrIdentifier = config.requireSecret(\"acrIdentifier\");\nconst acrIdentifier:string =  \"/<your ACR ID (uri) here>\";\nlet myAcrIdentifier = pulumi.output(acrIdentifier);\nconst privateACRInstance = azure.containerservice.Registry.get(acrInstanceName, myAcrIdentifier);\n\nconst k8sDockerSecret = helpers.createImagePullSecret(\n    \"docker-credentials\",\n    privateACRInstance.adminUsername,\n    privateACRInstance.adminPassword,\n    privateACRInstance.loginServer,\n    k8sProvider);\n\n\n// Azure Storage Account Credentials\nexport const azureStorageSecretName = \"azure-storage-secret\";\nconst azureStorageSecret = new k8s.core.v1.Secret(azureStorageSecretName, {\n    type: \"kubernetes.io/generic\",\n    metadata: {\n        name: azureStorageSecretName,\n        namespace: \"default\"\n    },\n    data:{\n        azurestorageaccountname: storageAccountName,\n        azurestorageaccountkey: storageAccountK8s.primaryAccessKey\n    }\n},{provider: k8sProvider});\n\nexport function createImagePullSecret(\n    secretName: string,\n    username: pulumi.Output<string>,\n    password: pulumi.Output<string>, \n    registry : pulumi.Output<string>,\n    k8sProvider : k8s.Provider): k8s.core.v1.Secret {\n\n    // Put the username password into dockerconfigjson format.\n    let base64JsonEncodedCredentials : pulumi.Output<string> = \n        pulumi.all([username, password, registry])\n        .apply(([username, password, registry]) => {\n            const base64Credentials = Buffer.from(username + ':' + password).toString('base64');\n            const json =  `{\"auths\":{\"${registry}\":{\"auth\":\"${base64Credentials}\"}}}`;\n            console.log(json);\n            return Buffer.from(json).toString('base64');\n        });\n\n    return new k8s.core.v1.Secret(secretName, {\n        metadata: {\n            name: secretName,\n        },\n        type: 'kubernetes.io/dockerconfigjson',\n        data: {\n            \".dockerconfigjson\": base64JsonEncodedCredentials,\n        },\n    }, { provider: k8sProvider });\n};\n```\n\nA last `pulumi up` and you should have your **AKS** cluster completely provisioned in your Azure subscription.\n\n## Creating a Production Stack\n\nOne of the things about using a Pulumi application with multiple stacks is that each stack holds environment-specific configuration, so we can re-use our application for different environments via stacks. You will probably create your **dev** and **prod** stacks, but you could also manage your **Disaster Recover(DR)** or **High-Availability (HA)** (different region) as stacks as well.\n\nIn this case, we're going to use the Pulumi CLI [to create a prod stack](https://www.pulumi.com/docs/intro/concepts/stack/) for our **k8s** infrastructure. To create a new stack, the command is:\n\n`pulumi stack init prod`\n\nWe can list all available stacks in a project using:\n\n`pulumi stack ls` and selecting a different stack is `pulumi stack select <stack-name>`\n\nOnce our production stack is created, we can set all of the `pulumi config` variables that are required for the stack to operate and voila! We can create a **prod** deployment of our **AKS** infrastructure that should look exactly like our **dev** stack infrastructure.\n\n## Summary\n\nThis has been a very long post! I hope you've been able to successfully deploy your **AKS** instance. We will need it for the next article in this series where we put our resources into that **k8s** cluster!\n\n## A Note about Log Analytics\n\nIn your research about **AKS** you will probably come across examples that show attaching Log Analytics instances to your **AKS** cluster.\n\nI did this and a week or so later, I was looking at costs in my Azure subscription and I saw that our **AKS** resource group had cost way more than I expected! Digging into the reasons, I found that Log Analytics actually cost **more** than our **AKS** VM resources! I immediately turned it off. Log Analytics is too expensive for us at this point in time. Perhaps when you have a large Kubernetes installation it is worth it, but right now, our Log Analytics bill couldn't be justified!\n\nWhen I was turning it off, I wanted to make sure that we removed all traces of it from our subscription and the **AKS** cluster. First, I deleted the Log Analytics components in Azure. The cluster still worked! Great! Stop the billing and keep everything working. Then I wanted to clean out all of the `omsagents` from the cluster, but those pods/deployments/services are impossible to remove from the AKS Cluster. I'm guessing that something on the outside that is managing the cluster is watching them and putting them in there all the time. I had to do something else, which lead me to this document.\n\n[Link to Microsoft Documents](https://docs.microsoft.com/en-us/azure/azure-monitor/insights/container-insights-optout)\n\nFor the time being, because I have a lot of internal logging happening in the cluster and I don't want to spend more money on monitoring than the cluster itself, I'll just leave it off and recommend that you start without it.\n\nIf you do want Log Analytics in your **AKS** cluster, you can add this code into your application...\n\n```typescript\n// Setup log analytics for k8s\nconst loganalytics = new azure.operationalinsights.AnalyticsWorkspace(\"aksloganalytics\", {\n    resourceGroupName: resourceGroupName,\n    location: location,\n    sku: \"PerGB2018\",\n    retentionInDays: 30,\n});\n```\n\n... and un-comment out these JSON parameters in the **AKS** cluster creation method.\n\n```typescript\n/* This is commented out because we do not want to do this. Please see my\n   blurb about LogAnalytics at the bottom of this post.\n    addonProfile: {\n        omsAgent: {\n            enabled: true,\n            logAnalyticsWorkspaceId: loganalytics.id,\n        },\n    } */\n```\n\n**Next up:**\n[Moving to Azure Kubernetes Service - Part B](/kubernetes/kubernetes-my-journey-part-7b)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n\n    img {\n       margin: 25px 0px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 7b","authorId":"dave_white","slug":"kubernetes-my-journey-part-7b","date":"2020-05-22 08:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-7b/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-7b/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 7b\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 04:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Moving to Azure Kubernetes Service - Part A](/kubernetes/kubernetes-my-journey-part-7a)\n\n# Moving to Azure Kubernetes Service - Part B\n\nWe're really making some progress! We should have our **AKS** cluster running now and ready for use to start putting some resources into. If you don't, head on back to the previous article in the series and get your **k8s** standing up in Azure!\n\n## Continuing with Pulumi\n\nIn the prevoius article, we used Pulumi exclusively to describe what we wanted in our **AKS** infrastructure and we had the Pulumi CLI do all the hard work of provisioning our **k8s** cluster. Now we are going to ask Pulumi to do a little more work and help us get our **k8s** resources into the cluster.\n\nFirst, let's create a new Pulumi Project and Stack to hold our resource specific configuration values, application, and history.\n\nLet's create a folder in our infrastructure folder for the **k8s** deployment stack. Starting in your `infra` folder, run the command:\n\n`mkdir k8s && cd k8s`\n\nNow use the Pulumi CLI to build our new project with its initial stack.\n\n`pulumi new azure-typescript --secrets-provider=passphrase`\n\nThis will kick off the workflow to acquire some details before it creates the stack. In my case, I answered the workflow questions with:\n\n1. project name (k8s) <-- hit enter and accepted default\n1. project description: **Deploy our kubernetes infrastructure**\n1. stack name: (dev) <-- hit enter and accepted default\n1. Enter your passphrase to protect config/secrets: **P@ssw0rd!**\n1. azure:environment: (public) <-- hit enter and accept default\n1. azure:location: (WestUS) **WestUS**\n\nThis will scaffold our new project and submit the project and stack details to our Pulumi cloud. Let's open VS Code, open the **index.ts** and delete everything from the file.\n\nWe are going to need to add the Pulumi kubernetes SDK module to our project.\n\n`npm install @pulumi/kubernetes`\n\nNow, at the top of our empty **index.ts** file, we can add the following imports.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as k8s from \"@pulumi/kubernetes\";\n```\n\n### Getting Configuration Values\n\nIn our **AKS** Pulumi project/stack, we had a number of configuration values that we stored in our Pulumi service. One of those important configuration values was the kubeConfig file that contains the credentials required to connect to our **k8s** instance. We are now going to use the Pulumi SDK to get that kubeConfig value so that we can use it in this project.\n\n[Inter-Stack Dependencies](https://www.pulumi.com/docs/intro/concepts/organizing-stacks-projects/#inter-stack-dependencies)\n\n```typescript\n// setup config\nconst env = pulumi.getStack(); // reference to this stack\nconst stackId = `dave/aks/${env}`;\nconst aksStack = new pulumi.StackReference(stackId);\nconst kubeConfig = aksStack.getOutput(\"kubeConfig\");\nconst k8sProvider = new k8s.Provider(\"k8s\", { kubeconfig: kubeConfig  });\n\n// output kubeConfig for debugging purposes\nlet _ = aksStack.getOutput(\"kubeConfig\").apply(unwrapped => console.log(unwrapped));\n\n```\n\nIf we break down this fragment of Typescript, we see that:\n\n1. Get a reference to the current stack\n1. Ask Pulumi for an object reference to our **AKS** stack variables. We want the kubeConfig secret from it.\n1. Create a **k8s.Provider** using the acquired kubeConfig values\n    - (Option) - Output the kubeConfig to the console for debugging\n\n`pulumi up` to test your application. It should simply compile, access our **aks8** stack, and output the kubeConfig!\n\n### Labels\n\nAlmost everything in **k8s** uses labels to perform important actions. For example, Services expose Deployments that match the selector labels. Also, you can use labels to do queries via **kubectl** or as filters in **octant**, **k9s**, or the **Kubernetes Web UI**. You'll see labels used throughout the manifests and the Pulumi code.\n\nWe will define some common label groups that we'll use throughout our application, merging them with service/deployment specific sets of labels as required. You can add to these groups as required for your situation.\n\n```typescript\n// Common labels\nconst baseBackEndLabels = { tier: \"backend\", group: \"infrastructure\" };\nconst baseApplicationGroupLabels = {tier: \"frontend\", group:\"auth\"};\n```\n\n### Moving Postgres from Manifest to Pulumi\n\nNow that we have a **k8s.Provider** that we can use to send instructions to our **AKS** cluster, we need to create the Pulumi instructions to start putting resources into our **k8s** cluster. Starting with postgres, let's take a look at our manifest that we used when putting resources into **minikube**.\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgres-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 1Gi\n  hostPath:\n    path: /var/lib/postgresql/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres-dep\n  labels:\n    app: postgres\n    tier: backend\n    group: infrastructure\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      volumes:\n        - name: postgres-pv-storage\n          persistentVolumeClaim:\n            claimName: postgres-pv-claim\n      containers:\n        - name: postgres\n          image: postgres:alpine\n          env:\n            - name: POSTGRES_USER\n              value: \"admin\"\n            - name: POSTGRES_PASSWORD\n              value: \"P@ssw0rd!\"\n            - name: POSTGRES_DB\n              value: \"identity\"\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - mountPath: \"/var/lib/postgresql/data\"\n              name: postgres-pv-storage\n---\n# from postgres-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-svc\nspec:\n  selector:\n    app: postgres\n  ports:\n    - protocol: TCP\n      port: 5432\n```\n\nWe're going to ignore the first resource in the manifest. Because we are using **AKS**, we are able to take advantage of the _dynamic persistent volume_ mechanism that is provided. We simply need to create **PersistentVolumeClaim** with the correct **storageClass** and **Azure/AKS** will take care of provisioning an actual persistent volume for us and attaching it to the correct host/node.\n\nThe second resource is the PersistentVolumeClaim. The important differnce between the manifest and the Pulumi application is going to be the use of the **storageClassName** configuration value that is coming from our **aksStack** configuration. In the **AKS** stack, this value is set to **managed-premium**.\n\nFor our third resource, the actual Deployment, we mostly want to map the values from our manifest into the TypeScript/JSON notation.\n\nLast but not least, we map the Service manifest settings that will give the postgres-dep resource some network (internal to cluster) configuration into our Pulumi application.\n\n```typescript\n// Add Postgres to AKS Cluster\nconst postgresLabels = {...{ app: \"postgres\", role: \"db\" }, ...baseBackEndLabels};\n\nconst postgresPVClaimName = \"postgres-pv-claim\";\nconst postgresPersistentVolumeClaim = new k8s.core.v1.PersistentVolumeClaim(postgresPVClaimName,{\n    metadata:{ name: postgresPVClaimName},\n    spec: {\n        storageClassName: aksStack.getOutput(\"storageClassName\"),\n        accessModes: [\"ReadWriteOnce\"],\n        resources: {\n            requests: {\n                storage: \"32Gi\"\n            }\n        }\n    }\n}, {provider: k8sProvider});\n\nconst postgresDepName = \"postgres-dep\";\nconst postgresDeployment = new k8s.apps.v1.Deployment(postgresDepName, {\n    metadata: { \n        name: postgresDepName, \n        labels: postgresLabels\n    },\n    spec: {\n        selector: { matchLabels: postgresLabels },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: postgresLabels },\n            spec: {\n                containers: [{\n                    name: \"postgres\",\n                    image: \"postgres:alpine\",\n                    volumeMounts: [{\n                        mountPath: \"/var/lib/postgresql/data\",\n                        name: \"volume\"\n                    }],\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"100Mi\",\n                        },\n                    },\n                    ports: [{ containerPort: 5432 }],\n                    env: [\n                        {name: \"POSTGRES_USER\", value: \"admin\"},\n                        {name: \"POSTGRES_PASSWORD\", value: \"P@ssw0rd!\"},\n                        {name: \"POSTGRES_DB\", value: \"identity\"},\n                        {name: \"PGDATA\", value: \"/mnt/data/pgdata\"}\n                    ]\n                }],\n                volumes:[{\n                    name: \"volume\",\n                    persistentVolumeClaim: {\n                        claimName: postgresPVClaimName\n                    }\n                }]\n            },\n        },\n    },\n}, {provider: k8sProvider});\n\nconst postgresServiceName = \"postgres-svc\";\nconst postgresService = new k8s.core.v1.Service(postgresServiceName, \n    {\n        metadata: {\n            name: postgresServiceName,\n            labels: postgresLabels,\n        },\n        spec: {\n            ports: [{ port: 5432, protocol: \"TCP\"}],\n            selector: postgresLabels,\n        },\n    }, {provider: k8sProvider});\n```\n\n`pulumi up` to compile our application and deploy postgres into the **k8s** cluster!\n\n### Moving pgAdmin4 from Manifest to Pulumi\n\nOur next step is to migrate the pgAdmin4 manifest settings into Pulumi. This is going to be mostly the same as the postgres migration. I'll point out a few differences below.\n\nFor brevity, I've already removed the PersistentVolume manifest declaration.\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pgadmin4-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pgadmin4-dep\n  labels:\n    app: pgadmin4\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: pgadmin4\n  template:\n    metadata:\n      labels:\n        app: pgadmin4\n    spec:\n      volumes:\n        - name: pgadmin4-pv-storage\n          persistentVolumeClaim:\n            claimName: pgadmin4-pv-claim\n      containers:\n        - name: pgadmin4\n          image: dpage/pgadmin4\n          env:\n            - name: PGADMIN_DEFAULT_EMAIL\n              value: \"admin@codingwithdave.xyz\"\n            - name: PGADMIN_DEFAULT_PASSWORD\n              value: \"P@ssw0rd!\"\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: \"/var/lib/pgadmin/data\"\n              name: pgadmin4-pv-storage\n---\n# from pgadmin4-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: pgadmin4-svc\nspec:\n  selector:\n    app: pgadmin4\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5050\n```\n\nThere are a few items that are different as we move to Pulumi because we are using Pulumi for the Azure production (eventually) environment and the operationalization of our stack. We will be using pgAdmin4 (the container, if not the application) to do database backups. The pgAdmin4 container has all of the tooling and settings required to do backups. What we need though is somewhere to put the backups. In this case, we will leverage another feature of **AKS** called azureFile storage provider for **k8s**. This bit of configuration instructs **k8s**, via Azure and **AKS**, to use a storage account file share as a volume in our pod.\n\n```typescript\n  name: pgAdminAzureVolumeName, // <-- This is \"azure\"\n    azureFile: {\n      secretName: azureStorageSecretName,\n      shareName: azureFileShareName,\n      readOnly: false\n    }\n```\n\nRecall that these resources were created in the **AKS** project so we need these values from our other stack configuration.\n\n> Eventually, I expect we will move to a managed [Azure Database for Postgres](https://azure.microsoft.com/en-ca/services/postgresql/) SaaS offering, so all of this may eventually disappear. For now, we'll manage it all ourselves.\n\n```typescript\n// Add pgAdmin4 to AKS Cluster\nconst pgAdminLabels = {...{ app: \"pgAdmin4\", role: \"db\"}, ...baseBackEndLabels };\n\nconst pgadminServiceName = \"pgadmin-svc\";\nconst pgAdminService = new k8s.core.v1.Service(pgadminServiceName, {\n    metadata: {\n        name: pgadminServiceName,\n        labels: pgAdminLabels,\n    },\n    spec: {\n        ports: [{ port: 80, targetPort: 80, protocol: \"TCP\"}],\n        selector: pgAdminLabels,\n    },\n}, {provider: k8sProvider});\n\nconst pgAdminAzureVolumeName = \"azure\";\nconst pgAdminDepName = \"pgadmin-dep\";\nconst pgAdminDeployment = new k8s.apps.v1.Deployment(pgAdminDepName, {\n    metadata: { name: pgAdminDepName },\n    spec: {\n        selector: { matchLabels: pgAdminLabels },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: pgAdminLabels },\n            spec: {\n                containers: [{\n                    name: \"pgadmin\",\n                    image: \"dpage/pgadmin4\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"150Mi\",\n                        },\n                        limits: {\n                            memory: \"200Mi\"\n                        }\n                    },\n                    ports: [{ containerPort: 80 }],\n                    env: [\n                        {name: \"PGADMIN_DEFAULT_EMAIL\", value: \"admin@codingwithdave.xyz\"},\n                        {name: \"PGADMIN_DEFAULT_PASSWORD\", value: \"P@ssw0rd!\"},\n                        {name: \"POSTGRES_USER\", value: \"admin\"},\n                        {name: \"POSTGRES_PASSWORD\", value: \"P@ssw0rd!\"},\n                        {name: \"POSTGRES_DB\", value: \"identity\"}\n                    ],\n                    volumeMounts:[\n                        {name: pgAdminAzureVolumeName, mountPath: \"/var/lib/pgadmin/azure\"}\n                    ]\n                }],\n                volumes: [{\n                    name: pgAdminAzureVolumeName,\n                    azureFile: {\n                        secretName: azureStorageSecretName,\n                        shareName: azureFileShareName,\n                        readOnly: false\n                    }\n                }],\n            },\n        },\n    },\n}, {provider: k8sProvider});\n```\n\n`pulumi up` will put the pgAdmin4 application/images into our **k8s** cluster. With the given configuration, it will be able to connect to the postgres database.\n\n### Moving Seq from Manifest to Pulumi\n\nThe final step (for now) of moving all of our infrastructure/backend components into pulumi is the Seq instance. This will be similar to the postgres instance as it also uses a PersistentVolumeClaim to get an azureDisk attached to the VM for saving our log data.\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: seq-pv-volume\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 3Gi\n  hostPath:\n    path: /mnt/data/seqv6/\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: seq-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: seq-dep\n  labels:\n    app: seq\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: seq\n  template:\n    metadata:\n      labels:\n        app: seq \n    spec:\n      volumes:\n        - name: seq-pv-storage\n          persistentVolumeClaim:\n            claimName: seq-pv-claim\n      containers:\n      - name: seq \n        image: datalust/seq:preview\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: \"/data\"\n          name: seq-pv-storage\n        env:\n        - name: ACCEPT_EULA\n          value: \"Y\"\n---\n# from seq-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: seq-svc\nspec:\n  selector:\n    app: seq\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5341\n```\n\nThere are two notable differences in Pulumi Application code that were not in the manifests.\n\nFirst of all, we are adding a [side-car container](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to this pod now that we are moving to Azure. This means that there are two containers running in this Pod. They are separate images (applications) but they will share the same network space and be able to access each other at the DNS **localhost** with the appropriate port. The side-car container we are adding is a container image called [sqelf](https://hub.docker.com/r/datalust/sqelf) which is a product produced by [datalust.co](https://www.datalust.co) that allows use to ingest log events in the `graylog` message format and send them directly into Seq.\n\nThe second change is in the Service description. We need to expose the **sqelf** service on a UDP port so that eventually, **fluentd** will be able to send log event messages, in the **graylog** format, to the **sqelf** container. The **squelf** container then just sends it on to **Seq**. We'll discuss **fluentd** in another article.\n\n```typescript\n// Add Seq to AKS Cluster\nconst seqLabels = {...{ app: \"seq\", role: \"logingestion\"}, ...baseBackEndLabels};\nconst seqPersistentVolumeClaim = new k8s.core.v1.PersistentVolumeClaim(\"seq-pv-claim\",{\n    metadata:{ name: \"seq-pv-claim\"},\n    spec: {\n        storageClassName: aksStack.getOutput(\"storageClassName\"),\n        accessModes: [\"ReadWriteOnce\"],\n        resources: {\n            requests: {\n                storage: \"32Gi\"\n            }\n        }\n    }\n}, {provider: k8sProvider});\n\nconst seqServiceName = \"seq-svc\";\nconst seqService = new k8s.core.v1.Service(seqServiceName, {\n    metadata: {\n        name: seqServiceName,\n        labels: seqLabels,\n    },\n    spec: {\n        ports: [\n            { name: \"http-seq\", port: 80, targetPort: 80, protocol: \"TCP\"},\n            { name: \"udp-sqelf\", port: 12201, protocol: \"UDP\"}],\n        selector: seqLabels,\n    },\n},{provider: k8sProvider});\n\nconst seqDeploymentName = \"seq-dep\";\nconst seqDeployment = new k8s.apps.v1.Deployment(seqDeploymentName, {\n    metadata: { name: seqDeploymentName },\n    spec: {\n        selector: { matchLabels: seqLabels },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: seqLabels },\n            spec: {\n                volumes: [{\n                    name: \"seq-pv-storage\",\n                    persistentVolumeClaim: {\n                        claimName: \"seq-pv-claim\"\n                    }\n                }],\n                containers: [{\n                    name: \"seq\",\n                    image: \"datalust/seq:preview\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"500Mi\",\n                        },\n                        limits:{\n                            memory: \"1000Mi\"\n                        }\n                    },\n                    ports: [{ containerPort: 80, protocol: \"TCP\" }],\n                    volumeMounts: [{\n                        mountPath: \"/data\",\n                        name: \"seq-pv-storage\"\n                    }],\n                    env: [{name: \"ACCEPT_EULA\", value: \"Y\"}]\n                },\n                {\n                    name: \"sqelf\",\n                    image: \"datalust/sqelf\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"100Mi\",\n                        },\n                    },\n                    ports: [{containerPort: 12201, protocol: \"UDP\"}],\n                    env: [\n                        {name: \"ACCEPT_EULA\", value: \"Y\"},\n                        {name: \"SEQ_ADDRESS\", value: \"http://localhost:80\"}\n                    ]\n                }\n            ],\n            },\n        },\n    },\n}, {provider: k8sProvider});\n```\n\n`pulumi up` will get the Seq pod up and running, with 2 containers, in our **k8s** infrastructure.\n\n### Moving IdentityServer4 from Manifest to Pulumi\n\nThe final bit of work to get our platform moved from **minikube** to **AKS** is to move our applications. There is nothing terribly special or noteworthy from a container perspective in this mapping. None of these pods need durable storage. The only difference with these pods is that they indicate that they want to use **docker-credentials** secret to access the private ACR.\n\nThere is a particularly important difference for our application when it comes to networking and accessing our applications. Our **k8s** has a public IP address and all of our applications will eventually be accessed via a unique DNS entry, not a shared DNS entry with different ports. I imagine you could use the shared DNS/multiple ports approach, but I did not. I don't think it helps human beings understand what they are using or working on to hide the intention behind a port abstraction. So, we will give everything its own URL.\n\nThis has consequences on our initial database seed data. Once this is all deployed, we will have to go into pgAdmin4 and run a script to update our configuration. Otherwise, the authentication system won't work. The Admin needs to be able _and allowed_ to access the STS from its hostname location so we'll need to change the STS seed data. We're also going to change the configuration that is stored in the environmental variables which are appsettings.json overrides.\n\n#### STS\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-sts-dep\n  labels:\n    app: identity-sts\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-sts\n  template:\n    metadata:\n      labels:\n        app: identity-sts\n    spec:\n      containers:\n      - name: identity-sts\n        image: depthconsulting.azurecr.io/skoruba-identityserver4-sts-identity:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: ConnectionStrings__ConfigurationDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__PersistedGrantDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__IdentityDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: AdminConfiguration__IdentityAdminBaseUrl\n          value: http://127.0.0.1.xip.io:9000\n        - name: ASPNETCORE_URLS\n          value: \"http://+:80\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n---\n# from identity-sts-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: identity-sts-svc\nspec:\n  selector:\n    app: identity-sts\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 80\n```\n\nI'm so glad that Pulumi is a programatic IaC model. One thing I can do here before we get to far along is move my DB connection strings (and the list) into variables and then use them where needed.\n\n```typescript\nconst dbConnectionString = \"Server=postgres-svc; User Id=admin; Database=identity; Port=5432; Password=P@ssw0rd!; SSL Mode=Prefer; Trust Server Certificate=true;\";\nconst dbConnectionStringList = [\n    {name: \"ConnectionStrings__ConfigurationDbConnection\", value: dbConnectionString},\n    {name: \"ConnectionStrings__PersistedGrantDbConnection\", value: dbConnectionString},\n    {name: \"ConnectionStrings__IdentityDbConnection\", value: dbConnectionString},\n    {name: \"ConnectionStrings__AdminLogDbConnection\", value: dbConnectionString},\n    {name: \"ConnectionStrings__AdminAuditLogDbConnection\", value: dbConnectionString}\n];\n```\n\nNow to code the STS provisioning.\n\n```typescript\n// Add Identity STS Service to AKS Cluster\nconst stsLabels = {...{ app: \"identity-sts\", role: \"authentication\"}, ...baseApplicationGroupLabels};\nconst stsDepName = \"identity-sts-dep\";\nconst stsDeployment = new k8s.apps.v1.Deployment(stsDepName, {\n    metadata: { \n        name: stsDepName,\n        labels: stsLabels\n    },\n    spec: {\n        selector: { matchLabels: {app: \"identity-sts\"} },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: stsLabels },\n            spec: {\n                containers: [{\n                    name: \"identity-sts\",\n                    image: \"depthconsulting.azurecr.io/skoruba-identityserver4-sts-identity:latest\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"200Mi\",\n                        },\n                        limits:{\n                            memory: \"300Mi\"\n                        }\n                    },\n                    ports: [{ containerPort: 80, protocol: \"TCP\" }],\n                    env: [\n                        {name: \"ASPNETCORE_ENVIRONMENT\", value: \"Development\"},\n                        {name: \"SEQ_URL\", value: \"http://seq-svc:5341\"},\n                        {name: \"AdminConfiguration__IdentityAdminBaseUrl\", value: \"https://auth-admin.codingwithdave.xyz\"},\n                        {name: \"ASPNETCORE_URLS\", value: \"http://+:80\"}\n                    ].concat(dbConnectionStringList)\n                }],\n                imagePullSecrets: [{name: acrSecretName }]\n            },\n        },\n    },\n}, {provider: k8sProvider});\n\nconst stsServiceName = \"identity-sts-svc\";\nconst stsService = new k8s.core.v1.Service(stsServiceName, {\n    metadata: {\n        name: stsServiceName,\n        labels: {app: \"identity-sts\"},\n    },\n    spec: {\n        ports: [\n            { name: \"http\", port: 80, targetPort: 80, protocol: \"TCP\"}\n        ],\n        selector: {app: \"identity-sts\"},\n    },\n},{provider: k8sProvider});\n```\n\n#### IdentityServer4 Admin\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-admin-dep\n  labels:\n    app: identity-admin\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-admin\n  template:\n    metadata:\n      labels:\n        app: identity-admin\n    spec:\n      containers:\n      - name: identity-admin\n        image: depthconsulting.azurecr.io/skoruba-identityserver4-admin:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: ConnectionStrings__ConfigurationDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__PersistedGrantDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__IdentityDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__AdminLogDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__AdminAuditLogDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: AdminConfiguration__IdentityServerBaseUrl\n          value: http://127.0.0.1.xip.io\n        - name: AdminConfiguration__IdentityAdminRedirectUri\n          value: http://127.0.0.1.xip.io:9000/signin-oidc\n        - name: ASPNETCORE_URLS\n          value: \"http://+:80\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n---\n# from identity-admin-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: identity-admin-svc\nspec:\n  selector:\n    app: identity-admin\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 9000\n```\n\n```typescript\n// Add Auth Admin Application to AKS Cluster\nconst adminLabels = {...{ app: \"identity-admin\", role: \"authentication\"}, ...baseApplicationGroupLabels};\nconst adminDepName = \"identity-admin-dep\";\nconst adminDeployment = new k8s.apps.v1.Deployment(adminDepName, {\n    metadata: {\n        name: adminDepName,\n        labels: adminLabels\n    },\n    spec: {\n        selector: { matchLabels: {app: \"identity-admin\"} },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: adminLabels },\n            spec: {\n                containers: [{\n                    name: \"identity-admin\",\n                    image: \"depthconsulting.azurecr.io/skoruba-identityserver4-admin:latest\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"200Mi\",\n                        },\n                        limits:{\n                            memory: \"300Mi\"\n                        }\n                    },\n                    ports: [{ containerPort: 80, protocol: \"TCP\" }],\n                    env: [\n                        {name: \"ASPNETCORE_ENVIRONMENT\", value: \"Development\"},\n                        {name: \"SEQ_URL\", value: \"http://seq-svc:5341\"},\n                        {name: \"AdminConfiguration__IdentityServerBaseUrl\", value: \"https://auth.codingwithdave.xyz\"},\n                        {name: \"AdminConfiguration__IdentityAdminRedirectUri\", value: \"https://auth-admin.codingwithdave.xyz/signin-oidc\"},\n                        {name: \"AdminConfiguration__RequireHttpsMetadata\", value: \"true\"},\n                        {name: \"ASPNETCORE_URLS\", value: \"http://+:80\"}\n                    ].concat(dbConnectionStringList)\n                }],\n                imagePullSecrets: [{name: acrSecretName }]\n            },\n        },\n    },\n}, {provider: k8sProvider});\n\nconst adminServiceName = \"identity-admin-svc\";\nconst adminService = new k8s.core.v1.Service(adminServiceName, {\n    metadata: {\n        name: adminServiceName,\n        labels: {app: \"identity-admin\"},\n    },\n    spec: {\n        ports: [{ port: 80, targetPort: 80, protocol: \"TCP\"}],\n        selector: {app: \"identity-admin\"},\n    },\n},{provider: k8sProvider});\n```\n\n#### IdentityServer4 Admin API\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: identity-admin-api-dep\n  labels:\n    app: identity-admin-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: identity-admin-api\n  template:\n    metadata:\n      labels:\n        app: identity-admin-api\n    spec:\n      containers:\n      - name: identity-admin-api\n        image: depthconsulting.azurecr.io/skoruba-identityserver4-admin-api:latest\n        env:\n        - name: ASPNETCORE_ENVIRONMENT\n          value: \"Development\"\n        - name: SEQ_URL\n          value: \"http://seq-svc\"\n        - name: ConnectionStrings__ConfigurationDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__PersistedGrantDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__IdentityDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__AdminLogDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: ConnectionStrings__AdminAuditLogDbConnection\n          value: \"User ID=admin;Password=P@ssw0rd!;Host=postgres-svc;Port=5432;Database=identity;Pooling=true;\"\n        - name: AdminApiConfiguration__IdentityServerBaseUrl\n          value: http://127.0.0.1.xip.io\n        - name: AdminApiConfiguration__ApiBaseUrl\n          value: http://127.0.0.1.xip.io:5000\n        - name: ASPNETCORE_URLS\n          value: \"http://+:80\"\n        ports:\n        - containerPort: 80\n      imagePullSecrets:\n      - name: docker-credentials\n---\n# from identity-adminapi-svc.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name:  identity-admin-api-svc\nspec:\n  selector:\n    app:  identity-admin-api\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 5000\n```\n\n```typescript\n// Add Auth Admin API to AKS Cluster\nconst adminApiLabels = {...{ app: \"identity-admin-api\", role: \"authentication\"}, ...baseApplicationGroupLabels};\nconst adminApiDepName = \"identity-admin-api-dep\";\nconst adminApiDeployment = new k8s.apps.v1.Deployment(adminApiDepName, {\n    metadata: { \n      name: adminApiDepName,\n      labels: adminApiLabels\n    },\n    spec: {\n        selector: { matchLabels: {app: \"identity-admin-api\"} },\n        replicas: 1,\n        revisionHistoryLimit: 2,\n        template: {\n            metadata: { labels: adminApiLabels },\n            spec: {\n                containers: [{\n                    name: \"identity-admin-api\",\n                    image: \"depthconsulting.azurecr.io/skoruba-identityserver4-admin-api:latest\",\n                    resources: {\n                        requests: {\n                            cpu: \"100m\",\n                            memory: \"200Mi\",\n                        },\n                        limits:{\n                            memory: \"300Mi\"\n                        }\n                    },\n                    ports: [{ containerPort: 80, protocol: \"TCP\" }],\n                    env: [\n                        {name: \"ASPNETCORE_ENVIRONMENT\", value: \"Development\"},\n                        {name: \"SEQ_URL\", value: \"http://seq-svc:5341\"},\n                        {name: \"AdminApiConfiguration__IdentityServerBaseUrl\", value: \"https://auth.codingwithdave.xyz\"},\n                        {name: \"AdminApiConfiguration__ApiBaseUrl\", value: \"https://auth-admin-api.codingwithdave.xyz\"},\n                        {name: \"ASPNETCORE_URLS\", value: \"http://+:80\"}\n                    ].concat(dbConnectionStringList)\n                }],\n                imagePullSecrets: [{name: acrSecretName }]\n            },\n        },\n    },\n}, {provider: k8sProvider});\n\nconst adminApiServiceName = \"identity-admin-api-svc\";\nconst adminApiService = new k8s.core.v1.Service(adminApiServiceName, {\n    metadata: {\n        name: adminApiServiceName,\n        labels: {app: \"identity-admin-api\"},\n    },\n    spec: {\n        ports: [{ port: 80, targetPort: 80, protocol: \"TCP\"}],\n        selector: {app: \"identity-admin-api\"},\n    },\n},{provider: k8sProvider});\n```\n\n`pulumi up` will push all of your applications up into **AKS**.\n\n### Verify all the Pods are Up and Running\n\nWith our last `pulumi up` we should have all of our **AKS** infrastructure up and hosting our **k8s** cluster, and we should have all of our applications/services/resources from our second pulumi stack in the **k8s** cluster. We should now be able to go into a one of our tools and verify that the apps are running.\n\nIf you used the pulumi application in our previous post, you should have a **az aks get-credentials ...** in your pulumi output for that stack. It should look something like this:\n\n```powershell\naz aks get-credentials --resource-group rg_identity_dev_zwus_aks `\n--name aksclusterfbfa950e --context \"MyProject.Identity\"\n```\n\nYou can run that command, with your specific values, and have the **azure-cli** append this **k8s** context details into your kubeConfig file. Once that is done, we can type `octant` on the command line and look at our pods.\n\nUsing **octant** (v0.12.1), we can see that all of our pods are present in the cluster.\n\n<img src=\"/images/dwhite/octant-azure-pods-running.png\" alt=\"Pods running in Azure\" height=\"250px\">\n\nWe will click into the **pgAdmin4** pod as an example in the article, but you should eventually click into all of the pods to ensure they are functioning.\n\nLooking at the pgAdmin4 pod, we can see that it is initialized and ready! If we go down a bit further, we'll see the port forward functionality of **octant** waiting for us to press the button.\n\n<img src=\"/images/dwhite/octant-azure-pgadmin-portforward.png\" alt=\"PortForward function in Octant\" height=\"250px\">\n\nOnce we press the button, we'll see that we now have an option to navigate to the port-forward URL.\n\n<img src=\"/images/dwhite/octant-azure-pgadmin-portforward-started.png\" alt=\"PortForward to pgAdmin4 started\" height=\"100px\">\n\nWhen we click on that link, you'll see the pgAdmin4 log in screen and once you log in, you should be able to create a server entry in our list to our postgres pod. The **k8s** network DNS name for our postgres pod should be **postgres-svc** from our Service resource. Once the entry is connected, we should be able to see our postgres database!\n\n<img src=\"/images/dwhite/octant-azure-pgadmin-running.png\" alt=\"pgAdmin4 running\" height=\"250px\">\n\nYou should work your way through all of the pods. They should all be up and running. The whole authentication/authorization system won't be working yet. We'll wait until the next article to fix that up and test it out.\n\n#### Problems\n\nWhen you have problems in **k8s**, you have to start looking in two places. First, you have to look in the logs. **octant** has a nice screen for looking at the log files directly on a pod. Remember, we don't have log ingestion and tooling setup for the **k8s** cluster yet, so we have to go look in the pods themselves. Here you can see pgAdmin4 running just fine, but if it wasn't working, you'll find clues as to why here.\n\n<img src=\"/images/dwhite/octant-azure-pgadmin-log.png\" alt=\"pgAdmin4 logs in the pod\" height=\"250px\">\n\nOnce we have all of our log ingestion infrastructure in place, we will be able to go to Seq to look log entries across the cluster, but you should always be ready to go look directly in the pods for the log entries.\n\nAnd the other place you will probably want to inspect is the running pod itself. You can use the **terminal** tab in **octant** to look at various settings in your pod.\n\n<img src=\"/images/dwhite/octant-azure-pgadmin-terminal.png\" alt=\"pgAdmin4 terminal window\" height=\"250px\">\n\n### Not Quite Done Yet\n\nThis article is going to end in a bit of a \"not quite working as I'd like\" state. All of the pods are up and running, but our IdentityServer4 needs some configuration changes in order to work. And in order to do that, we need to be able to access our **k8s** publicly, give it a DNS entry (hostname), and then configure our IdentityServer4 system to allow those hostnames to interact with the STS.\n\nOur next stop will be adding an Ingress Controller to our **k8s** cluster and getting all of these services publicly available.\n\n**Next up:**\n[Making Your Kubernetes Cluster Publicly Accessible](/kubernetes/kubernetes-my-journey-part-8)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n\n    img {\n       margin: 25px 0px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 8","authorId":"dave_white","slug":"kubernetes-my-journey-part-8","date":"2020-05-22 07:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-8/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-8/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 8\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 03:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Moving to Azure Kubernetes Service - Part B](/kubernetes/kubernetes-my-journey-part-7b)\n\n# Making Your Kubernetes Cluster Publicly Accessible\n\nHopefully you've arrived at this article with two Pulumi applications that you can use to create an **Azure Kubernetes Service**-based Kubernetes (**k8s**) cluster with a collection of **k8s** resources (container images) inside of it that all function together to provide a **IdentityServer4** based authentication system. If not, you should probably [go back and re-read those articles](/kubernetes/kubernetes-my-journey) since we're going to build off of that effort in this article.\n\nIn this article, we're going to go through the process of making our **k8s** cluster publicly accessible with proper DNS entries (hostname) so that the authentication system can be configured and works properly.\n\n## Pre-requisites\n\nUp until now, I've done my best to ensure you can do as much as possible on your developer machine with minimal financial requirements. From this point forward, I will be assuming that you have a **custom domain** that you can use for your configuration. On my part, I will be using the **codingwithdave.xyz** domain for the remainder of the articles.\n\n## Daily Ritual\n\nI want to share a side-note about one of my processes.\n\nOne thing I really appreciate about my infrastructure automation while I'm doing all of this development and writing is the ability to stand up and tear down the **k8s** cluster every day. I start my day by doing a `pulumi up` command and it stands up all the cluster. I do another `pulumi up` and it puts all of the applications in the cluster. And at the end of the day, I reverse my start of day activities by doing a `pulumi destroy` on the resources and then a `pulumi destroy` on the cluster itself. I do this for two reasons. First, it saves money. This cluster only runs when I'm working on this article or doing some **k8s** research and then it goes away. This is a great benefit for a development process. Second, it forces me to make sure that my automation is working at the beginning and end of each day.\n\nStanding up the **k8s** cluster daily makes this `get-credentials` command a nice output that we capture in Pulumi. This is generated in the Pulumi application that stands up the **aks** services. All of my **k8s** development tooling uses this set of credentials.\n\n```powershell\naz aks get-credentials --resource-group rg_identity_dev_zwus_aks --name akscluster5b1237c2 --context \"MyProject.Identity\"\n```\n\nThis command changes daily, so having this change captured in standing up the infrastructure so I can just go to my Pulumi service to pull it out and use it, is certainly helpful. Hey **@Pulumi** if you are reading, I'd love a [copy-to-clipboard-button beside my Outputs in the service page for my stacks!](https://github.com/pulumi/pulumi/issues/4623)\n\n> Consider rate limited activities like Let's Encrypt when standing-up and tearing down resources.\n\n## Ingress and Ingress Controllers\n\nA **k8s** cluster, kind of by default, is a closed system. You can access it administratively which is certainly dangerous, but unless you make conscious decisions about how and when to expose _your_ services, only the cluster services themselves are exposed so that tools like **kubectl** can work. There are a number of mechanisms for exposing your services but in this article, we are going to discuss the Ingress resource and the IngressController resource as the means by which we expose services in our cluster to the public.\n\n## Ingress Controllers\n\nIn order to expose the IdentityServer4 services, my first step was to create an [IngressController resource](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/). An IngressController is something that you put into your **k8s** cluster that will watch for **Ingress** resources, interpret them,  create the appropriate external resources for your cluster to interact with the outside, and then route all traffic that comes in through your ingress point to the appropriate services inside of your cluster.\n\nThere are many IngressController implementations that you can use, and if you find that you don't like one after a bit of use, it isn't terribly hard to change to another one. There are certainly considerations to make when picking on, but for me, honestly, it was originally ease of getting it working, so I choose the [nginx IngressController](https://kubernetes.github.io/ingress-nginx/). There are many examples of how to put this controller into your **k8s** cluster and how to create the Ingress rules that **nginx** will follow.\n\nFor the IngressController, there is no manifest. I didn't put one into the minikube instance. So we'll only have Pulumi code for this part of the journey.\n\n### Deploying nginx Ingress Controller\n\nUp until now, we've mostly been dealing with single-container resources. A self-contained application that for the most part can provide it's own bit of functionality. Arguably, the IdentityServer4 group of containers should always be deployed together, but I haven't set that up yet. The way that you do that in **k8s** is by using [Helm](https://helm.sh/), basically a package manager for your **k8s** resources. Eventually, I will figure out how to author a Helm package, but for the time being, I'm just going to use them.\n\nInstalling Helm is very easy. In our case, on Windows, we can use **chocolatey** to install it.\n\n`choco install kubernetes-helm`\n\n[This documentation](https://helm.sh/docs/intro/quickstart/) has more detailed instructions.\n\nOnce you have Helm installed, Pulumi is capable of using it to push Helm charts into your **k8s** cluster.\n\nTo give you a sense of what a Helm chart will do to your cluster, you can see that when we ask Pulumi to put a Helm chart into the cluster, a lot of things actually happen.\n\n<img id=\"image\" src=\"/images/dwhite/helm-nginx-resources.png\" alt=\"All the nginx resources\">\n\nLet's take a look at the pulumi typescript code required to install this Helm chart.\n\n```typescript\n// Deploy ingress-controller using helm to AKS Cluster\nconst nginxIngress = new k8s.helm.v3.Chart(\"nginx\", {\n    chart: \"nginx-ingress-controller\",\n    namespace: \"kube-system\",\n    repo: \"bitnami\",\n    values: {\n      service: {\n        annotations: {\n            \"service.beta.kubernetes.io/azure-dns-label-name\": aksStack.getOutput(\"k8sDnsName\"),\n        },\n      },\n      resources: { requests : {memory: \"150Mi\", cpu: \"100m\"}},\n      serviceType: \"LoadBalancer\",\n      nodeCount: 1,\n    }\n}, {provider: k8sProvider });\n```\n\nMost of the parameters in the **ChartOpts** parameter of the object we created are easy to understand.\n\n- **chart** - The chart we want to install\n- **namespace** - The namespace that we are going to put this chart into\n- **repo** - The Helm repository to get the chart from\n\nAfter that, we get into some options parameters that are a bit harder to deduce and I had to dig a bit into how Helm works to understand them and what Pulumi needed in order to make this work.\n\nIn our options we have created a **values** object that has additional properties. The **values** object is how we pass configurable values into Helm. This is comparable to the **values.yaml** in the chart definition. There are [a number of different ways](https://helm.sh/docs/chart_template_guide/values_files/) to pass in these values when using Helm charts directly, but this is how Pulumi provides these values to Helm.\n\n> When trying to figure out what Helm chart options are available, it can be helpful to look at the default **values.yaml** in a chart's definition. [Here is the Nginx Ingress Controller Charts default values.yaml file](https://github.com/kubernetes/ingress-nginx/blob/master/charts/ingress-nginx/values.yaml).\n\nThere are a couple simple parameters in our values object.\n\n```typescript\n  resources: { requests : {memory: \"150Mi\", cpu: \"100m\"}},\n  serviceType: \"LoadBalancer\",\n  nodeCount: 1,\n```\n\nThis are parameters that are for the **kubernetes** resource for what will eventually be provisioned in **k8s**. They are resource requests for the Pod template in the Deployment, the service type for the nginx **Service** resource, and a nodeCount (replicas) for the Deployment resource.\n\nThe next part of these values is where we encounter some of the complicated aspects of IngressControllers. They are a resource in your **k8s** cluster that needs to work with things **outside** of the cluster. In this case, the **nginx IngressController** needs to be able to talk to Azure and create a **LoadBalancer** and **PublicIP** outside of the cluster. The same **nginx IngressController** would need to talk to AWS if this was deployed there. We can give these IngressControllers some information that helps them understand which external provider to work with and what that provider needs. We can add provider-specific values via **annotations** which could be provided in the **values.yaml** file but in our case, we will provide them via the **values:** object in the Pulumi ChartOpts class.\n\n```typescript\n  service: {\n    annotations: {\n      \"service.beta.kubernetes.io/azure-dns-label-name\": aksStack.getOutput(\"k8sDnsName\"),\n    },\n  }\n```\n\nIn this case, we are telling the IngressController to tell Azure that we want to use **k8sDnsName** as the DNS name label on the PublicIP that is created for our load balancer.\n\n> **NOTE** ~~At this time, this annotation is not working. I'll update as soon as I figure out what is going on.~~ I've updated the code snippet as I've figured out how this works now. Another blog post (short) will be written up.\n\nNow that the Pulumi script is working, we don't need to use this PowerShell but I will leave it here as an example of using the azure-cli with some PowerShell. _After_ our IngressController has been provisioned, we can run this PowerShell script against the **azure-cli** command to do this for us.\n\n```powershell\n# get the PublicIP object for our load balancer\n$pip = az network public-ip list --query \"[?tags.service=='kube-system/nginx-nginx-ingress-controller']\" | ConvertFrom-Json\n# update the --dns-name and refresh our object in PowerShell\n$pip = az network public-ip update -n $pip.name -g $pip.resourceGroup  --dns-name \"identity-auth-dev\" | ConvertFrom-Json\n# set the clusterFQDN in Pulumi\npulumi config set clusterFQDN $pip.dnsSettings.fqdn\n# verify that we can resolve our DNS entry\nnslookup $pip.dnsSettings.fqdn\n```\n\nThe important thing about this activity in regard to progressing toward a working IdentityServer4 implementation is that we now have a single public IP address that we can create DNS entries for. We also have a single DNS entry provided to us by Azure that we can use as a CNAME entry, but we still need individual DNS entries for each service that we are going to expose.\n\nWe aren't quite done yet though. We have 3-5 apps (depending on how you feel) that we need to expose as separate applications from our cluster. In order to do that, we need to a couple more things.\n\nFor the next steps, let's assume that:\n\n- our the FQDN is **identity-auth-dev.westus.cloudapp.azure.com**\n- our public IPv4 is **137.135.18.68**\n\n> I abandon IP address all the time. Please don't expect this one to work. After this effort, I wouldn't expect the FQDN to be around either. It might be, but don't count on it.\n\n### Configuring Public DNS Records\n\nIn order to make this authentication system work on your custom domain, you can simply take the following sub-domains and make a [CNAME entry](https://en.wikipedia.org/wiki/CNAME_record) in your DNS provider for each that points at our Azure-provided DNS entry.\n\n| | | | |\n|---|---|---|---|\n|Record Type|Name |Value | TTL|\n|CNAME|auth|identity-auth-dev.westus.cloudapp.azure.com| 1hr|\n|CNAME|auth-admin|identity-auth-dev.westus.cloudapp.azure.com| 1hr|\n|CNAME|auth-admin-api|identity-auth-dev.westus.cloudapp.azure.com| 1hr|\n\n> I'm no longer trying to keep this all on my local machine. There are no hosts file instructions going forward.\n\n#### Give it a try\n\nAfter a quick `pulumi up` with the IngressController code added to our application, we should have a couple new Azure resources and a bunch of new **k8s** resources.\n\nIn Azure, you should find:\n\n- A load balancer\n  `az network lb list`\n- A public IP\n  `az network public-ip list`\n\nIn **k8s** you should find a bunch of nginx-based resources in the **kube-system** namespace. This should show you some, but not all, of these resources.\n\n```powershell\nkubectl get deployments --namespace kube-system -l app=nginx-ingress-controller\nkubectl get pods --namespace kube-system -l app=nginx-ingress-controller\nkubectl get services --namespace kube-system -l app=nginx-ingress-controller\n```\n\nThe great thing about the **nginx IngressController** Helm chart is that it installs a default backend for all un-managed routes. We will add rules for managing routes in the **Ingress** resource next, but until then, if you go to any of the URLs we setup, they should all work, with all of the traffic getting routed to the nginx backend web server!\n\n<img id=\"image\" src=\"/images/dwhite/helm-nginx-default-backend.png\" alt=\"All the nginx resources\" height=\"258px\">\n\nNow, lets add certificates!\n\n### Certificates\n\n#### Big Caveat Here\n\nIf you are still trying to use the host-file approach to access your applications, you **won't be able** to follow all of my steps for adding certificates management or HTTPs to your cluster. You will need to step outside of these articles to figure out how to provide a certificate to cert-manager that you can use. My guidance uses [lets-encrypt](https://letsencrypt.org/) and [lets-encrypt](https://letsencrypt.org/) needs to be able to access your public DNS entries to ensure that you are who you say you are. At this point, you should probably start thinking about getting a custom domain to make all of this a bit easier.\n\n#### Adding cert-manager\n\nOne of the goals of the project was to ensure that everything has certificates, is using HTTPs, and it is easy to manage. With **k8s** that is very easy with [cert-manager](https://cert-manager.io/docs/). In this case, we simply need to install **cert-manager** into our cluster with some pulumi code.\n\n```typescript\n// setup cert-manager\nconst certManager = new k8s.yaml.ConfigFile(\"cert-manager\", {\n    file: \"https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml\",\n }, {provider: k8sProvider});\n\n```\n\nThis file, which you can [look at on github](https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml), will install a lot of resources into your **k8s** cluster. You don't need to know a lot about them to use the certificate acquisition properties. I'm using [lets-encrypt](https://letsencrypt.org/) so I need to understand any limitations or restrictions imposed by them which in this case means the [rate limits](https://letsencrypt.org/docs/rate-limits/) for acquiring certificates. These are important to understand so please read up and manage your certificate usage appropriately.\n\n### Adding a ClusterIssuer\n\nOnce **cert-manager** is installed into your cluster, it will be able to acquire/issue local cluster self-signed certificates. If you'd like to acquire certificates from Let's Encrypt or another supported issuer, you need to add an **Issuer** or a **ClusterIssuer** resource so that **cert-manager** can get certs. In our case, we are going to [add two ClusterIssuers](https://docs.cert-manager.io/en/release-0.11/reference/clusterissuers.html) so that we can get staging certificates from Let's Encrypt and we can get production certificates from Let's Encrypt as well.\n\nYou should also take into consideration that we are using **nginx** as our **IngressController**. Each IngressController will have [different configuration and classes](https://cert-manager.io/docs/tutorials/acme/ingress/) that you need to setup and use.\n\nCurrently, Pulumi doesn't have a API/SDK module for **cert-manager** so we need to create a .yaml file and then use Pulumi to read it and bring the resources into the cluster.\n\n```yaml\n# filename: ca-cluster-issuer.yaml\napiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory # <-- staging issuer, no certificate rate limits\n    email: dave@depthconsulting.ca # <-- email address required for Let's Encrypt\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n---\napiVersion: cert-manager.io/v1alpha2\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory # <-- prod issuer, strict certificate rate limits\n    email: dave@depthconsulting.ca\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n```\n\n```typescript\n// create certificate issuers for lets-encrypt\nconst caClusterIssuer = new k8s.yaml.ConfigFile(\"ca-cluster-issuer\", {\n    file: \"ca-cluster-issuer.yaml\",\n }, {provider: k8sProvider});\n```\n\nOnce you've created this .yaml file and TypeScript fragment, you can `pulumi up` to get these resources into your cluster.\n\n### Ingress Resource\n\nNow that we have **cert-manager** installed and issuers configured, we can add our [Ingress Resource](https://kubernetes.io/docs/concepts/services-networking/ingress/).\n\nAn **Ingress** resource tells an **IngressController** what rules to setup for accessing services in the cluster. You can switch **IngressControllers** if you need to and the **Ingress** resource may not have to change. The rules can stay the same as long as the controller supports all of the rules laid out in the **Ingress** resource.\n\n```typescript\nconst ingressName = \"identity-ingress\";\nconst ingressFrontEnd = new k8s.networking.v1beta1.Ingress(ingressName,{\n    metadata:{\n        name: ingressName,\n        annotations: {\n            \"kubernetes.io/ingress.class\": \"nginx\", // <-- we are using an nginx ingress controller\n            \"cert-manager.io/cluster-issuer\": \"letsencrypt-staging\" // <-- we are using test certificates\n        }\n    },\n    spec: {\n        tls:[{ // <-- we want certificates for all of these domains\n          hosts:[\n              \"auth.codingwithdave.xyz\",\n              \"auth-admin.codingwithdave.xyz\",\n              \"auth-admin-api.codingwithdave.xyz\"\n            ],\n          secretName: \"tls-secret-certificate\" // <-- store the cert in this secret\n        }],\n        rules: [\n            {\n                host: \"auth.codingwithdave.xyz\",\n                http:{\n                    paths: [{\n                        path: \"/\",\n                        backend: {\n                            serviceName: \"identity-sts-svc\",\n                            servicePort: 80\n                        }\n                    }]\n                }\n            },\n            {\n                host: \"auth-admin.codingwithdave.xyz\",\n                http:{\n                    paths: [{\n                        path: \"/\",\n                        backend: {\n                            serviceName: \"identity-admin-svc\",\n                            servicePort: 80\n                        }\n                    }]\n                }\n            },\n            {\n                host: \"auth-admin-api.codingwithdave.xyz\",\n                http:{\n                    paths: [{\n                        path: \"/\",\n                        backend: {\n                            serviceName: \"identity-admin-api-svc\",\n                            servicePort: 80\n                        }\n                    }]\n                }\n            }\n        ]\n    }\n},{provider: k8sProvider});\n```\n\nThe rules in this particular **Ingress** definition are all using host-based rules. When someone visits our IdentityServer4 sites, the host-name that they used to get there will be pulled out of the request, and the **nginx IngressController** will look for that host in the rules, and if it finds it, send the request to the appropriate service. If it doesn't find it, it goes to the default backend controller.\n\n> Internally, the **k8s** cluster only uses http. I'm sure there is a security reason to have HTTPs everywhere internally in the cluster, but I don't understand why, so I haven't implemented any of that.\n\nIn these rules, we can see we need to declare a host and the path. Once that is in place, we simply describe what service to go to, using the internal **k8s** DNS entry and the port.\n\nIf you are using **cert-manager**, you can see that there are some instructions in the ingress for **cert-manager**.\n\n- **\"cert-manager.io/cluster-issuer\": \"letsencrypt-staging\"** - this tells **cert-manager** to use letsencrypt-prod generates production certificates but also has important rate limits\n- **tls:[{ ... }]** - this parameter will be used by **cert-manager** to determine what domains to generate certificates for.\n\nAt this point, you should now be able to `pulumi up` and add **cert-manager** and the **Ingress** resource to your cluster. This will do a lot of things for you. If you are not generating production certificates, you should use the **\"cert-manager.io/cluster-issuer\": \"letsencrypt-staging\"** issuer in **cert-manager**. There are no rate limits on generating staging certificates. If you are generating production certificates, you'll need to switch to the **letsencrypt-prod** issuer and understand the [rate limits](https://letsencrypt.org/docs/rate-limits/).\n\nOnce that is done, you can start visiting your sites and you should hit something other than the default nginx backend.\n\n- [https://auth.codingwithdave.xyz](https://auth.codingwithdave.xyz) - STS landing page, should work\n- [https://auth-admin.codingwithdave.xyz](https://auth-admin.codingwithdave.xyz) - Admin landing page, not default backend, won't work\n- [https://auth-admin-api.codingwithdave.xyz/swagger](https://auth-admin-api.codingwithdave.xyz/swagger) - Swagger page, won't work, CORS error due to configuration\n\nIf you used the letsencrypt-staging **ClusterIssuer** then you are going to get certificate error, but you can inspect the certificate before you acknowledge and continue. The certificates and certificate path should look like the following:\n\n<img id=\"image\" src=\"/images/dwhite/cert-manager-letsencrypt-staging-cert.png\" alt=\"Let's Encrypt Staging Certificate\" height=\"500px\"><br/>\n\n<img id=\"image\" src=\"/images/dwhite/cert-manager-letsencrypt-staging-cert-path.png\" alt=\"ALet's Encrypt Staging Certificate Path\" height=\"500px\"><br/>\n\nIn the case of the **STS**, you should land on a functioning page. You should be able to login with admin/P@ssw0rd!\n\nFor the **Admin** site, you should not land on the default backend, but you will probably land on a developer exception page because the admin site configuration in the IdentityServer4 database does not have the correct URLs anymore. We'll fix that in a moment.\n\nFor the **AdminApi** Swagger page, you should see the Swashbuckle Api Explorer frame, but the app will complain that it has a CORS issue. This will also be fixed by configuration now that we know our host names.\n\nBut this is progress! We are now able to access our services in the cluster!\n\n### Updating our Client Configuration\n\nIn order to fix all of our sites and get all of this working, we need to make some configuration changes in the IdentityServer4 database and in the Environmental Variables for our ASP.NET Core sites. Let's do that quickly.\n\nFor now, we have not exposed pgAdmin4 via the **Ingress** rules so we are going to need to connect to our cluster, create a port-forward to our local machines, and then browse to the pgAdmin4 app to do our data configuration changes via sql. I'm going to do this via **octant** again. We did exactly this activity [back in this article](kubernetes/kubernetes-my-journey-part-7b/).\n\nOnce you have pgAdmin4 open and connected to our postgres database, we can run the SQL script below in pgAdmin4.\n\n> Remember to ensure the custom domain we are using is corrected in the scripts.\n>\n> I am using https for all of these examples.\n\n```sql\n-- ClientCorsOrigins\nUPDATE \"ClientCorsOrigins\"\nSET \"Origin\" = 'https://auth-admin.codingwithdave.xyz'\nWHERE \"Id\" = 1;  \n\n-- ClientPostLogoutRedirectUris\nUPDATE \"ClientPostLogoutRedirectUris\"\nSET \"PostLogoutRedirectUri\" = 'https://auth-admin.codingwithdave.xyz/signout-callback-oidc'\nWHERE \"Id\" = 1;  \n\n-- ClientRedirectUris\nUPDATE \"ClientRedirectUris\"\nSET \"RedirectUri\" = 'https://auth-admin-api.codingwithdave.xyz/swagger/oauth2-redirect.html'\nWHERE \"Id\" = 1;\n\nUPDATE \"ClientRedirectUris\"\nSET \"RedirectUri\" = 'https://auth-admin.codingwithdave.xyz/signin-oidc'\nWHERE \"Id\" = 2;\n\n-- ClientPostLogoutRedirectUris\nUPDATE \"Clients\"\nSET \"ClientUri\" = 'https://admin-auth.codingwithdave.xyz', \"FrontChannelLogoutUri\" = 'https://auth-admin.codingwithdave.xyz/signout-oidc'\nWHERE \"Id\" = 2;\n```\n\n<img id=\"image\" src=\"/images/dwhite/identity-configuration-sql-update.png\" alt=\"IdentityServer4 SQL Data Configuration changes\" height=\"400px\">\n\nWe also need to update the configuration for the environmental variables for our IdentityServer4 applications in our pulumi script. You will find these lines (approximately) in 3 Deployment resources in our scripts. You need to update them all.\n\n```typescript\n  // find these lines in deployment env: [{ }] blocks and make the appropriate custom domain name changes\n  {name: \"AdminApiConfiguration__IdentityServerBaseUrl\", value: \"https://auth.codingwithdave.xyz\"},\n  {name: \"AdminApiConfiguration__ApiBaseUrl\", value: \"https://auth-admin-api.codingwithdave.xyz\"},\n  {name: \"AdminConfiguration__IdentityAdminRedirectUri\", value: \"https://auth-admin.codingwithdave.xyz/signin-oidc\"},\n```\n\nOnce you've changed your pulumi application, a simple `pulumi up` will get everything setup for you.\n\n<img id=\"image\" src=\"/images/dwhite/identity-configuration-envvar-update.png\" alt=\"IdentityServer4 App Settings Configuration changes\" height=\"300px\">\n\nWe should now have functional (or mostly functional) set of applications! If you used the **letsencrypt-staging** certificates, the **Admin** site will generate SSL errors since it can't create a valid TLS connection yet.\n\n### nginx, IdentityServer4 and Aggravation\n\n> When this cluster and Pulumi application were originally written, I had this problem. When I started writing these articles and building the assets, it went away. I'm guessing that a default in nginx changed in one of the containers, but I'm not certain. I'm going to leave this here as a precaution.\n\nSo, configuration is changed, services are running, everything is accessible, and you log into the STS. This works! Check the Admin Api Swashbuckle API Explorer and our page loads! But you can't log into the Admin application! What is going on!?!?\n\nThis particular problem vexed me for several days. I had gotten to this point pretty early in my journey, and I've done some (not all) things in these articles so far to help you have a chance to not spend several days on this problem or a problem like this one.\n\nOne thing I've done is the Seq log ingestion. You should be able to port-forward to the Seq application and look at the logs for our IdentityServer4 applications. This will allow you to see all of the applications logs in one spot. When I was looking at the Seq logs, I could see that some traffic simply wasn't flowing between the STS and the Admin application when a use performed an action. Technically, when you try to log into the Admin application, you go to the Admin application first, it determines you are not logged in, and then it re-directs you to the STS application to log in. Via Seq, I saw that this was not happening.\n\nBut there were things that I couldn't see directly in Seq, which was the motivation for our next chapter, but for now, the way this project is currently configured, you will miss log entries from all of our other applications/resources in **k8s** like nginx, coredns, and kubernetes itself.\n\nIn this case, I went to the **nginx** log entries and saw some problems that indicated that _something_ was too small to handle the communications with STS.\n\n<img id=\"image\" src=\"/images/dwhite/nginx-pod-logs.png\" alt=\"nginx pod logs\" height=\"250px\">\n\nIn the logs, I found this error.\n\n`upstream sent too big header while reading response header from upstream`\n\nThat little error snippet ended up sending me down a path where it took a couple days to refine my google-foo enough to find the solution to this problem. I tried a lot of different tricks during those couple days, but amazingly, I eventually found this exact problem on another blog that I read a lot, but I hadn't read this article! Andrew Lock does a LOT of good writing for the .NET community and he had run into the exact same problem _2 years earlier_!\n\n[Andrew Lock - upstream-sent-too-big-header-error](https://andrewlock.net/fixing-nginx-upstream-sent-too-big-header-error-when-running-an-ingress-controller-in-kubernetes/)\n\nAndrew fully describes the problem in a really good way. I'm not going to repeat it. I had the exact same problem and his solution worked. What I am going to present is the Pulumi application code that creates a ConfigMap that nginx will find to make the adjustments.\n\nMorale of the story - You have to be ready to explore ALL of the logs in your cluster. If you have to do it manually, you should still be prepared to do it.\n\n```typescript\nconst nginxIngressControllerConfigMap = new k8s.core.v1.ConfigMap(\"nginx-nginx-ingress-controller\", {\n    metadata:{\n        annotations: {},\n        name: \"nginx-nginx-ingress-controller\",\n        labels: {\"k8s-app\": \"nginx-ingress-controller\"},\n        namespace:\"kube-system\"\n    },\n    data: {\n        \"proxy-buffer-size\": \"128k\",\n        \"proxy-buffers\": \"8 128k\"\n    }\n},{provider: k8sProvider, import: \"kube-system/nginx-nginx-ingress-controller\"});\n```\n\n## Summary\n\nWe have arrived! If everything is going the way I hoped if not necessarily the way I planned, you (and I) should have a functional, basic Azure Kubernetes Service-based **k8s** cluster running an IdentityServer4 implementation in a completely self-sustained manner. It is publicly accessible, our backend infrastructure is not exposed but is accessible, and you can begin integrating your application into this authentication system.\n\nMy hope was that you were going to share my learning journey so that you could have a hands-on platform to continue your learning journey. Mine certainly isn't done. The next step is to show you how I made my logging a lot more comprehensive in a very easy manner!\n\n**Next up:**\n[Adding Cluster Logging (fluentd)](/kubernetes/kubernetes-my-journey-part-9)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 9","authorId":"dave_white","slug":"kubernetes-my-journey-part-9","date":"2020-05-22 06:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-9/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-9/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 9\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 02:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Making Your Kubernetes Cluster Publicly Accessible](/kubernetes/kubernetes-my-journey-part-8)\n\n# Adding kubernetes logging (fluentd)\n\nIn this article, we are going to add some logging infrastructure to our **k8s** cluster to address the problems I had when an application doesn't log directly to the log ingestion platform I was using. (Seq).\n\nThis article is mostly Pulumi application code with a bit of an origin story, but to be honest, I haven't fully refined my understanding of **fluentd** for the article to be much more. I will add to and refine this article as I refine my **fluentd** implementation. This article will get your whole-cluster logging working though.\n\n## fluentd\n\nAs I mentioned before, [fluentd](https://www.fluentd.org/) is one of several **k8s** log ingestion solutions that is out there right now. It has a lot of community support, so it was really easy to get started with it.\n\n### Service Account and Roles\n\nIn order to get **fluentd** running in the cluster, there are a bunch of resources that need to be in place. Because **fluentd** _needs_ access to _all_ of the pods in the cluster to get at their logs, it needs a fairly empowered service account but we also want to make sure that service account only has the minimum amount of permissions as required to do it's job.\n\n```typescript\n// Fluentd Service Account and Role creation\nconst fluentdServiceAccount = new k8s.core.v1.ServiceAccount(\"fluentd-serviceaccount\", {\n    metadata: {name: \"fluentd-serviceaccount\", namespace: \"kube-system\"}\n}, {provider: k8sProvider});\n\nconst fluentdClusterRole = new k8s.rbac.v1beta1.ClusterRole(\"fluentd-clusterrole\",{\n    metadata: {\n        name: \"fluentd-clusterrole\",\n        namespace: \"kube-system\"\n    },\n    rules: [{apiGroups: [\"\"], resources: [\"pods\", \"namespaces\"], verbs: [\"get\", \"list\", \"watch\"]}]\n}, {provider: k8sProvider});\n\nconst fluentdClusterRoleBinding = new k8s.rbac.v1beta1.ClusterRoleBinding(\"fluentd-clusterrolebinding\", {\n    metadata:{\n        name: \"fluentd-clusterrolebinding\",\n        namespace: \"kube-system\"\n    },\n    roleRef: {kind: \"ClusterRole\", name:\"fluentd-clusterrole\",apiGroup:\"rbac.authorization.k8s.io\" },\n    subjects: [{kind: \"ServiceAccount\", name:\"fluentd-serviceaccount\", namespace: \"kube-system\"}]\n}, {provider: k8sProvider})\n```\n\n### fluentd Pods\n\nWe now have a properly configured service account to use with our **fluentd** pods. Remember, **fluentd** is just another application running in the cluster performing its duties. And applications run in pods/containers.\n\nIn this section, we are going to see code that creates our first [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) set of resources in the cluster. A DaemonSet is similar to a Deployment with the primary difference being that a DaemonSet is used when you want to make sure there is one pod, that is described in the DaemonSet spec, on each node in the cluster. As nodes are added, the DaemonSet will ensure that the pods in its spec are automatically added.\n\n**fluentd** is an application that is running on the node basically scraping log files. All of the log files are stored in the node file system, so in order to get all of the logs, we have to have a **fluentd** instance per node. DaemonSets are how we do that in **k8s**.\n\n```typescript\nconst gldsLabels = { \"k8s-app\": \"fluentd-logging\", version: \"v1\"};\nconst fluentdGraylogDaemonSet = new k8s.apps.v1.DaemonSet(\"fluentd-graylog-daemonset\", {\n    metadata: { name: \"fluentd-graylog-daemonset\",\n    namespace: \"kube-system\",\n    labels: gldsLabels },\n    spec: {\n        selector: {\n            matchLabels: gldsLabels\n        },\n        updateStrategy: {type: \"RollingUpdate\" },\n        template: {\n            metadata: { labels: gldsLabels },\n            spec: {\n                serviceAccount: \"fluentd-serviceaccount\",\n                serviceAccountName: \"fluentd-serviceaccount\",\n                containers: [\n                    {\n                        name: \"fluentd\",\n                        image: \"fluent/fluentd-kubernetes-daemonset:v1-debian-graylog\",\n                        imagePullPolicy: \"IfNotPresent\",\n                        env: [\n                            {name: \"FLUENT_GRAYLOG_HOST\", value: \"seq-svc.default.svc.cluster.local\"},\n                            {name: \"FLUENT_GRAYLOG_PORT\", value: \"12201\"}\n                        ],\n                        resources: {\n                            requests: {\n                                cpu: \"200m\",\n                                memory: \"0.5Gi\"\n                            },\n                            limits: {\n                                cpu: \"1000m\",\n                                memory: \"1Gi\"\n                            }\n                        },\n                        volumeMounts :[\n                            {name: \"varlog\", mountPath: \"/var/log\"},\n                            {name: \"varlibdockercontainers\", mountPath: \"/var/lib/docker/containers\", readOnly: true}\n                        ],\n                    }\n                ],\n                terminationGracePeriodSeconds: 30,\n                volumes: [\n                    {name: \"config-volume\", configMap: { name: \"fluentd-conf\"}},\n                    {name: \"varlog\", hostPath: {path: \"/var/log\"}},\n                    {name: \"varlibdockercontainers\", hostPath: {path: \"/var/lib/docker/containers\"}}\n                ]\n            }\n\n        }\n    }\n}, {provider: k8sProvider});\n```\n\nThere is a bunch of configurations in here that are standard, but there are two things that are unique to this Pulumi application.\n\nThe first unique item is the **flavour** of fluentd that we are using. There are [numerous container images](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.3) that are available from fluentd's repository and I was really fortunate to find the graylog-flavoured image. This image creates containers that natively talk using the graylog logging format.\n\n`fluent/fluentd-kubernetes-daemonset:v1-debian-graylog`\n\nThe second unique item is a derivative of the first. Because we are using graylog, we need to give the **fluentd** pod some environmental configuration telling it where to send the logs to. In this case, we will be sending them to `sqelf` which will in turn send them directly into `Seq`. We can finally see this pair of containers in action in the Seq-dep pod.\n\n```typescript\nenv: [\n    {name: \"FLUENT_GRAYLOG_HOST\", value: \"seq-svc.default.svc.cluster.local\"},\n    {name: \"FLUENT_GRAYLOG_PORT\", value: \"12201\"}\n],\n```\n\nYou should notice that we've used the FQDN of the seq-svc because the **fluentd** application is not in the same namespace in the **k8s** cluster as Seq/sqelf.\n\n### Next Steps for fluentd\n\nThat is most of what I've done so far with **fluentd**. I've got it working, and the local Seq instance is ingesting a lot of log entries. This is ok because this is all in the same cluster, so network traffic isn't a concern I am currently worried about. It is something I'm more aware of because the Seq logs are **full** of log entries and it makes it a bit harder to find what I'm looking for.\n\n**fluentd** has an approach to describe the **pipeline** that all log entries flow through. This is the kubernetes.conf file and you can replace the default configuration with one in a ConfigMap. This yaml resource file is an example:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-conf\n  namespace: kube-system\ndata:\n  kubernetes.conf: \"<match kubernetes.**>\\r\\n  @type null\\r\\n</match>\\r\\n<match fluent.**>\\r\\n\n    \\ @type null\\r\\n</match>\\r\\n<match **>\\r\\n  @type stdout\\r\\n</match>\\r\\n\"\n```\n\nAnd the **fluentd** pod is configured to use this ConfigMap here in the **volumes** section of the Pulumi declaration:\n\n```typescript\n{name: \"config-volume\", configMap: { name: \"fluentd-conf\"}},\n```\n\nMy next learning steps are refining this pipeline so that I can better manage how much data is flowing to `sqelf` and `Seq`. \n\nAnother thing I need to get a handle on is doing ConfigMap declarations in Pulumi. Currently, a YAML ConfigMap examples is a PITA to convert to a Pulumi declaration. That will be another chapter, eventually, in this series. I've also asked Pulumi to start providing better examples of ConfigMaps in their documentation. A recipe book for many common YAML ConfigMaps would be great.\n\n**Next up:**\n[Tuning resource usage](/kubernetes/kubernetes-my-journey-part-10)\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Kubernetes - My Journey - Part 10","authorId":"dave_white","slug":"kubernetes-my-journey-part-10","date":"2020-05-22 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"kubernetes/kubernetes-my-journey-part-10/","link":"","permalink":"https://westerndevs.com/kubernetes/kubernetes-my-journey-part-10/","excerpt":"","raw":"---\nlayout: post\ntitle: Kubernetes - My Journey - Part 10\ncategory: kubernetes\ntags: kubernetes, azure, aks, identityserver, docker, containers\nauthorId: dave_white\ndate: 2020-05-22 01:00\n---\n[Series Table of Contents](/kubernetes/kubernetes-my-journey)\n\n**Previously:**\n[Adding kubernetes logging (fluentd)](/kubernetes/kubernetes-my-journey-part-9)\n\n# Tuning resource usage\n\nWe now have a full-functional cluster, producing metrics, logging, running applications, the whole ball of wax! We are now in the place where we can start to tune the performance characteristics of our cluster.\n\nI have just started down this path myself, and as such, I don't have a lot of experience with the current cluster as it is running. It is just being rolled out into production now and so over the next days, weeks, and months, we will be learning how to monitor the performance of the cluster and making adjustments to the CPU and Memory requests that we can define in our pods.\n\nI will be adding to this article as I learn but I also wanted a place to publicly share some links that I find and like.\n\n## Basic Resource Requests\n\nGoing back one chapter to the **fluentd** pulumi declarations, we can see that in the **DaemonSet** resource, we are describing requests and limits for our pod resources.\n\n```typescript\nresources: {\n    requests: {\n        cpu: \"200m\",\n        memory: \"0.5Gi\"\n    },\n    limits: {\n        cpu: \"1000m\",\n        memory: \"1Gi\"\n    }\n},\n```\n\nYou can use these requests and limits in any pod **spec** and you can make adjustments as you learn. Just a reminder, pods are intended to be ephemeral and any changes will cause pods to go be deleted and new replacements are created. You can make changes in your pulumi application, do a quick `pulumi up` and see the difference in node resource consumption nearly immediately.\n\n### Monitoring Resource Usage\n\nIt's important to remember that pods use node resources, so if you are going to monitor resources, they would be at the cluster/node level and not any particular application or namespace.\n\nUsing Octant, we can navigate to a node to get a sense of how it is doing.\n\n<img id=\"image\" src=\"/images/dwhite/cluster-node-resources.png\" alt=\"Cluster Node Resources\" height=\"250px\">\n\nOn the same node details page, we can find a section called **Conditions** where **k8s** is telling you how it feels about the current resources available vs. allocated. I don't have any ideas or guidance yet on what these ratios should be yet.\n\n<img id=\"image\" src=\"/images/dwhite/cluster-node-conditions.png\" alt=\"Cluster Node Conditions\" height=\"150px\">\n\nI've actually found that I really like the Kubernetes Dashboard WebUI is a great place to also look at resource utilizations and configurations.\n\n<img id=\"image\" src=\"/images/dwhite/cluster-node-resources-webui.png\" alt=\"WebUI Cluster Node Resources\" height=\"250px\">\n\nClicking into a single node gives details about Conditions on that node.\n\n<img id=\"image\" src=\"/images/dwhite/cluster-node-conditions-webui.png\" alt=\"WebUI Cluster Node Conditions\" height=\"250px\">\n\n## Summary\n\nAs mentioned, this is certainly a work in progress. As we monitor the cluster and how it behaves, we'll make adjustments. I'm certain we'll also make adjustments as to how we monitor. I expect at least one [prometheus](https://prometheus.io/) article in the near future, so I'll just leave the **Next Up** link space at the bottom of the article as TBD. :D\n\n## Links to Articles\n\n- [Kubernetes Managing Resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)\n- [Making Sense of Kubernetes Cluster Metrics](https://www.replex.io/blog/kubernetes-in-production-the-ultimate-guide-to-monitoring-resource-metrics)\n\n**Next up:**\n\nTBD\n\n<div style=\"width:100%;height:0;padding-bottom:42%;position:relative;\"><iframe src=\"https://giphy.com/embed/xUPGcdeU3wvdNPa1Py\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div>\n\n<style>\n    h1, h2, h3, h4, h5, h6 {\n       margin-top: 25px;\n    }\n    figure.highlight{\n        background-color: #E8EEFE;\n    }\n    figure.highlight .gutter{\n        color: #0033CD;\n    }\n    figure.highlight pre {\n        font-family: 'Cascadia Code PL', monospace;\n    }\n    code {\n        font-family: 'Cascadia Code PL', sans-serif;\n        border-width: 0.1em;\n        border-color: #E8EEFE;\n        border-style: solid;\n        border-radius: 0.3em;\n        background-color: #E8EEFE;\n        color: #0033CD;\n        padding: 0em 0.4em;\n        white-space: nowrap;\n    }\n</style>\n<link  href=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css\" rel=\"stylesheet\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js\"></script>\n<script>\n// View an image\nconst gallery = new Viewer(document.getElementById('mainPostContent', {\n    \"navbar\": false,\n    \"toolbar\": false\n}));\n</script>\n","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://westerndevs.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes, azure, aks, identityserver, docker, containers","slug":"kubernetes-azure-aks-identityserver-docker-containers","permalink":"https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"}]},{"title":"Go just big enough for your needs or go home","authorId":"kyle_baley","slug":"Go-just-big-enough-for-your-needs-or-go-home","date":"2020-05-18 19:42:18+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"bash/Go-just-big-enough-for-your-needs-or-go-home/","link":"","permalink":"https://westerndevs.com/bash/Go-just-big-enough-for-your-needs-or-go-home/","excerpt":"I love crossword puzzles. This post is only peripherally about crosswords specifically but I say it up front to filter my readership to those who might have a personal kinship with me and thus, are less likely to write angry comments.","raw":"---\nlayout: post\ntitle: Go just big enough for your needs or go home\ntags:\n  - bash\n  - XML\n  - XSLT\ncategories:\n  - bash\ndate: 2020-05-18 15:42:18\nauthorId: kyle_baley\n---\n\nI love crossword puzzles. This post is only peripherally about crosswords specifically but I say it up front to filter my readership to those who might have a personal kinship with me and thus, are less likely to write angry comments.\n\n<!-- more -->\n\nFor the longest time, the most commonly-used app on my phone by at least an order of magnitude was [Crosswords Classic](https://apps.apple.com/us/app/crosswords-classic/id284036524) by Stand Alone, Inc. (They have a newer version that's probably better but every time I've tried it, there's a \"who moved my cheese\" factor I've never really cared to overcome.) When I had an Android device, it was [Shortyz Crosswords](https://play.google.com/store/apps/details?id=com.totsp.crossword.shortyz&hl=en). Both excellent and both worth whatever price I paid for them (I've long forgotten).\n\nHowever, in my advancing years, I've taken to the printed page more and more and in the last year or so, I've been printing crosswords and doing them in pen and paper instead. This was a good move for two reasons: 1) My screen time dropped by more than half, and 2) it opened up several crossword providers that, to my knowledge, aren't available in the apps. For reasons unknown to me, many crossword providers are not making their work available to these apps (e.g. USA Today, Washington Post, and New York Times). In fact, several of them have set up their own payment systems to subscribe to them (e.g. American Values, Matt Gaffney, Elizabeth Gorski).\n\nThe upside is that now I have a whole new world of crosswords in an arguably more leisure form. The downside is why you're reading this post: now I have to hunt and peck my way through half a dozen providers, using different delivery mechanisms, rather than have an app just go out and get them. My workflow for this used to be:\n\n1) As emails come in from email providers, save the PDFs in a special folder\n2) For others:\n    - Navigate to the website\n    - Navigate to each day\n    - Download the PDF\n3) Once I have everything collected, merge the crosswords into a single PDF and print\n4) Archive the ones I merged\n5) Repeat when I run out of crosswords\n\n> Fun fact: Step 3 isn't as onerous as you might think. I spent a good half hour building an Automator action in MacOS to do this until I found out the functionality is built into the right-click menu.\n\nAs you can imagine, this virtually reeks of needing automation. So being a good developer, I designed in my head a system that would meet my needs:\n\n- An ASP.NET Core-based app with a React front-end\n- Authentication with multiple providers (Google, Facebook, etc.)\n- Azure storage to store the PDFs\n- Azure Logic Apps to parse emails that I send to a special address\n- Azure function triggered nightly to download from non-email providers\n- Some (hopefully free) PDF library to merge the PDFs\n- A utility to parse XML into an image to be exported as a PDF\n\n(That last one is mostly for USA Today but basically any puzzle provided by uclick.com. They have a special XML format that describes crosswords.)\n\n> Let's ignore the cost-benefit of building all this vs. the 10 minutes each week I spend on it manually and chalk it up to: because it's there\n\nI'll skip ahead a bit so I can get to my point. I was a couple of days into this when I remembered [a Reddit post](https://www.reddit.com/r/crossword/comments/dqtnca/my_automatic_nyt_crossword_downloading_script/) I had read recently where someone had automated the download of New York Times crosswords (provided you had a subscription, of course) using a bash script. Then it occurred to me: Do I _need_ all this infrastructure? Azure storage, hosting, logic apps, functions? Could I expand on this bash script from this random, helpful stranger instead?\n\nSkipping ahead a bit further and the answers are, of course: [no I don't, and yes I can](https://github.com/kbaley/xword-downloader). The new stack:\n\n- bash\n- Zapier\n- Mailparser.io\n- C# console app\n- XML/XSLT/HTML/CSS Grid\n- Google Chrome (headless)\n- The built-in MacOS functionality I mentioned earlier\n\nI'm using Zapier/Mailparser.io to handle the email providers, C#/XML/XSLT for the uclick providers to transform the XML into HTML (there's very little C#), Chrome to print that HTML to PDF, built-in MacOS to merge the PDFs, and bash to tie it all together. \n\nHere's my new workflow:\n\n1) Run the bash script\n2) Print\n3) Solve\n4) Repeat when I run out\n\n... and the only reason I haven't automated #2 is because I haven't looked into the `lp` command yet and I like to spot-check the NYT puzzles before I print them.\n\nNow all this is not polished by any definition of the word and it is highly customized to my environment. But by gum, it works. And it still gave me the same rush I've always felt when I've been working at something and it eventually works. Figuring out Zapier's and MailParser's rule systems, re-learning XSLT, seeing HTML files disappearing and working PDFs taking their place. Getting into the minutiae of something that should be easy (like URI-decoding text in XSLT 1.0) but isn't. Not to say, \"How can I learn Azure?\" but to break a problem down, find a tool, and execute until it works. This is the same routine I used almost 40 years ago when I thought to myself, \"I wonder if I can make it say, 'TAKE THAT LOSERS!!!' whenever I score a touchdown\" on the old ASCII-based football game in my dad's office computer they let me sit at while waiting for a dentist appointment.\n\nIt's likely I'll be the only one who uses this thing, and to be honest, I hope I am because the idea of supporting it gives me the shakes. My original design was certainly more ambitious and might be easier to turn into a conference talk. But it's overkill. Maybe there's a market for applications that collect and manage crossword puzzle PDFs; I'm past the point where I want to build an application that meets that need first and my personal need second.\n\nKyle the Unsolveable","categories":[{"name":"bash","slug":"bash","permalink":"https://westerndevs.com/categories/bash/"}],"tags":[{"name":"bash","slug":"bash","permalink":"https://westerndevs.com/tags/bash/"},{"name":"XML","slug":"XML","permalink":"https://westerndevs.com/tags/XML/"},{"name":"XSLT","slug":"XSLT","permalink":"https://westerndevs.com/tags/XSLT/"}]},{"title":"Deploying a Static Site to Azure Using the az CLI","authorId":"dave_paquette","slug":"deploying-a-static-site-to-azure-using-the-az-cli","date":"2020-05-10 22:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Azure/deploying-a-static-site-to-azure-using-the-az-cli/","link":"","permalink":"https://westerndevs.com/Azure/deploying-a-static-site-to-azure-using-the-az-cli/","excerpt":"The az command line interface (cli) is a powerful tool for creating, modifying and deploying to Azure resources. Since it's a cli AND cross platform, it's also a great tool for automating your deployments. In this post, we'll use the az cli to deploy a static site to Azure.","raw":"---\nlayout: post\ntitle: Deploying a Static Site to Azure Using the az CLI\ntags:\n  - Azure\n  - Web Dev\n  - AZ CLI\ncategories:\n  - Azure\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2020/05/10/deploying-a-static-site-to-azure-using-the-az-cli.aspx'\ndate: 2020-05-10 18:30:00\nexcerpt: The az command line interface (cli) is a powerful tool for creating, modifying and deploying to Azure resources. Since it's a cli AND cross platform, it's also a great tool for automating your deployments. In this post, we'll use the az cli to deploy a static site to Azure.\n---\nI was recently working on a project where the frontend was built in React. The project was hosted on Azure and we wanted to use Azure CDN to host the React app. I have been looking at the az cli recently and decided to use it on this project to script the setup of resources and deployments to Azure.\n\n## Setting up the AZ CLI\nFirst up, [install the az cli](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest).\n\nOnce installed, you'll need to login to your Azure Subscription.\n\n```\naz login\n```\n\nA browser window will popup, prompting you to log in to your Azure account. Once you've logged in, the browser window will close and the az cli will display a list of subscriptions available in your account. If you have more than one subscription, make sure you select the one you want to use.\n\n```\naz account set --subscription YourSubscriptionId\n```\n\n## Create a Resource Group\nYou will need a resource group for your Storage and CDN resources. If you don't already have one, create it here.\n```\naz group create --name DavesFancyApp --location SouthCentralUs\n```\n\nMost commands will require you to pass in a `--resource-group` and `--location` parameters. These parameters are `-g` and `-l` for short, but you can save yourself even more keystrokes by setting defaults for `az`.\n\n```\naz configure -d group=DavesFancyApp\naz configure -d location=SouthCentralUs\n```\n\n## Create a Storage Account for Static Hosting\n\nFirst, create a storage account:\n\n```\naz storage account create --name davefancyapp123\n```\n\nThen, enable static site hosting for this account.\n\n```\naz storage blob service-properties update --account-name davefancyapp123 --static-website --404-document 404.html --index-document index.html\n```\n\nYour storage account will now have a blob container named `$web`. That contents of that container will be available on the URL _accountname_.z21.web.core.windows.net/. For example, https://davefancyapp123.z21.web.core.windows.net/.\n\n## Deploying your app\nTo deploy your app to the site, all you need to do is copy your app's static files to the `$web` container in the storage account you created above. For my react app, that means running `npm run build` and copying the build output to the `$web` container. \n\n```\naz storage blob upload-batch --account-name davefancyapp123 -s ./build -d '$web'\n```\nNow your site should be available via the static hosting URL above. That was easy!\n\n## Create a CDN Profile and Endpoint\nNext up, we are going to put a Content Delivery Network (CDN) endpoint in front of the blob storage account. We want to use a CDN for a couple reasons. First, it's going to provide much better performance overall. CDNs are optimized for delivering web content to user's devices and we should take advantage of that as much as possible. The second reason is that a CDN will allow us to configure SSL on a custom domain name.\n\nFirst, we will need to create a CDN Profile. There are a few different of CDNs offerings available in Azure. You can read about them [here](https://docs.microsoft.com/azure/cdn/cdn-features). In this example, we will us the Standard Microsoft CDN.\n\n```\naz cdn profile create -n davefancyapp123cdn --sku Standard_Microsoft\n```\n\nNext, we will create the CDN endpoint. Here we need to set the origin to the static hosting URL from the previous step. Note that we don't include the protocol portion of the URL.\n\n```\naz cdn endpoint create -n davefancyapp123cdnendpoint --profile-name davefancyapp123cdn --origin davefancyapp123.z21.web.core.windows.net --origin-host-header davefancyapp123.z21.web.core.windows.net --enable-compression\n```\n\nNote: See the [az cli docs](https://docs.microsoft.com/en-us/cli/azure/cdn/endpoint?view=azure-cli-latest#az-cdn-endpoint-create) for more information on the options available when creating a CDN endpoint.\n\nNow your site should be available from _endpointname_.azureedge.net. In my case https://davefancyapp123cdnendpoint.azureedge.net/. Note that the endpoint is created quickly but it can take some time for the actual content to propagate through the CDN. You might initially get a 404 when you visit the URL.\n\n### Create CDN Endpoint Rules\nThese 2 steps are optional. The first one is highly recommended. The second is optional depending on the type of app your deploying. \n\nFirst, create URL Redirect rule to redirect any HTTP requests to HTTPS.\n\n```\n az cdn endpoint rule add -n davefancyapp123cdnendpoint --profile-name davefancyapp123cdn --rule-name enforcehttps --order 1 --action-name \"UrlRedirect\"  --redirect-type Found --redirect-protocol HTTPS --match-variable RequestScheme --operator Equal --match-value HTTP\n```\n\nNext, if you're deploying a Single Page Application (SPA) built in your favourite JavaScript framework (e.g. Vue, React, Angular), you will want a URL Rewrite rule that returns the app's root `index.html` file for any request to a path that isn't an actual file. There are many variations on how to write this rule. I found this to be the simplest one that worked for me. Basically if the request path is not for a specific file with a file extension, rewrite to `index.html`. This allows users to directly navigate to a route in my SPA and still have the CDN serve the `index.html` that bootstraps the application.\n\n```\naz cdn endpoint rule add -n davefancyapp123cdnendpoint --profile-name davefancyapp123cdn --rule-name sparewrite --order 2 --action-name \"UrlRewrite\" --source-pattern '/' --destination /index.html --preserve-unmatched-path false --match-variable UrlFileExtension --operator LessThan --match-value 1\n```\n\n### Configuring a domain with an Azure Managed Certificate\nThe final step in configuring the CDN Endpoint is to configure a custom domain and enable HTTPS on that custom domain.\n\nYou will need access to update DNS records for the custom domain. Add a CNAME record for your subdomain that points to the CDN endpoint URL. For example, I created a CNAME record on my davepaquette.com domain:\n\n```\nCNAME    fancyapp   davefancyapp123cdnendpoint.azureedge.net\n```\n\nOnce the CNAME record has been created, create a custom domain for your endpoint. \n\n```\naz cdn custom-domain create --endpoint-name davefancyapp123cdnendpoint --profile-name davefancyapp123cdn -n fancyapp-domain --hostname fancyapp.davepaquette.com\n```\n\nAnd finally, enable HTTPs. Unfortunately, this step fails due to a [bug](https://github.com/Azure/azure-cli/issues/12152) in the AZ CLI. There's [a fix](https://github.com/Azure/azure-cli/pull/12648) on it's way for this but it hasn't been merged into the CLI tool yet.\n\n```\naz cdn custom-domain enable-https --endpoint-name davefancyapp123cdnendpoint --profile-name davefancyapp123cdn --name fancyapp-domain\n```\n\nDue to the bug, this command returns `InvalidResource - The resource format is invalid`.  For now, you can [do this step manually](https://docs.microsoft.com/en-us/azure/cdn/cdn-custom-ssl?tabs=option-1-default-enable-https-with-a-cdn-managed-certificate) in the Azure Portal. When using CDN Managed Certificates, the process is full automated. Azure will verify your domain using the CNAME record above, provision a certificate and configure the CDN endpoint to use that certificate. Certificates are fully managed by Azure. That includes generating new certificates so you don't need to worry about your certificate expiring.\n\n#### CDN Managed Certificates for Root Domain\nMy biggest frustration with Azure CDN Endpoints is that CDN managed certificates are not supported for the apex/root domain. You can still use HTTPS but you need to bring your own certificate. \n\nThe same limitation exists for managed certificates on App Service. If you share my frustration, [please upvote here](https://feedback.azure.com/forums/169385-web-apps/suggestions/38981932-add-naked-domain-support-to-app-service-managed-ce).\n\n\n## Deploying updates to your application\nThe CDN will cache your files. That's great for performance but can be a royal pain when trying to deploy updates to your application. For SPA apps, I have found that simply telling the CDN to purge `index.html` is enough to ensure updates are available very shortly after deploying a new version. This works because most JavaScript frameworks today use WebPack which does a good job of cache-busting your JavaScript and CSS assets. You just need to make sure the browser is able to get the latest version of `index.html` and updates flow through nicely.\n\nWhen you upload your latest files to blob storage, follow it with a purge command for `index.html` on the CDN endpoint.\n\n```\naz storage blob upload-batch --account-name davefancyapp123 -s ./build -d '$web'\naz cdn endpoint purge -n davefancyapp123cdnendpoint --profile-name davefancyapp123cdn --no-wait --content-paths '/' '/index.html'\n```\n\nThe purge command can take a while to complete. We pass the `--no-wait` option so the command returns immediately. \n\n## My thoughts on az\nAside from the bug I ran in to with enabling HTTPS on the CDN endpoint, I've really enjoyed my experience with the `az` cli. I was able to fully automate resource creation and deployments using the [GitHub Actions az cli action](https://github.com/marketplace/actions/azure-cli-action). I can see `az` becoming my preferred method of managing Azure resources.","categories":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/categories/Azure/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Web Dev","slug":"Web-Dev","permalink":"https://westerndevs.com/tags/Web-Dev/"},{"name":"AZ CLI","slug":"AZ-CLI","permalink":"https://westerndevs.com/tags/AZ-CLI/"}]},{"title":"Flutter unit testing with native channels","authorId":"simon_timms","slug":"flutter-tests-with-native","date":"2020-04-01 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/flutter-tests-with-native/","link":"","permalink":"https://westerndevs.com/_/flutter-tests-with-native/","excerpt":"Today I was digging through some unit tests in our flutter project that seemed to be failing on my machine but not necessarily in other places like our build pipeline. The problem was that we had some calls to async methods which were not being awaited properly. I fixed those up and they uncovered a bunch of more serious problems in our tests. We were calling out to validate a phone number with libphonenumber and now we were actually awaiting the call properly we saw this error","raw":"---\nlayout: post\ntitle: Flutter unit testing with native channels\nauthorId: simon_timms\ndate: 2020-04-01 10:00\noriginalurl: 'https://blog.simontimms.com/2020/04/01/2020-04-01-flutter-tests-with-native/'\n---\n\nToday I was digging through some unit tests in our flutter project that seemed to be failing on my machine but not necessarily in other places like our build pipeline. The problem was that we had some calls to async methods which were not being awaited properly. I fixed those up and they uncovered a bunch of more serious problems in our tests. We were calling out to validate a phone number with libphonenumber and now we were actually awaiting the call properly we saw this error\n\n<!-- more -->\n\n```bash\n[master â‰¡ +0 ~2 -0 !]> flutter test\n00:03 +27 -1: test\\unit\\providers\\create_account_provider_test.dart: Real mobile number - is valid [E]\n  MissingPluginException(No implementation found for method isValidPhoneNumber on channel codeheadlabs.com/libphonenumber)\n  package:blah/providers/create_account_provider.dart 101:9  CreateAccountProvider.setMobileNumber\n\n00:03 +28 -2: test\\unit\\providers\\create_account_provider_test.dart: Valid state if properties are valid [E]\n  MissingPluginException(No implementation found for method isValidPhoneNumber on channel codeheadlabs.com/libphonenumber)\n  package:blah/providers/create_account_provider.dart 101:9  CreateAccountProvider.setMobileNumber\n```\n\nAs it turns out libphonenumber is actually a native implementation wrapped up with flutter. To communicate with this native code isn't possible in a test environment so it needs to be mocked. This can be done by mocking the channel. \n\nIn the setUp() for the unit tests I added a call to setMockMethodCallHandler like so \n\n```dart\nconst _channel = const MethodChannel('codeheadlabs.com/libphonenumber');\nsetUp(() async {\n_channel.setMockMethodCallHandler((MethodCall methodCall) async {\n    return true;\n    });  \n});\n\ntearDown((){\n_channel.setMockMethodCallHandler(null);\n});\n```\n\nWith this call in place I was able to run the test without issue. ","categories":[],"tags":[]},{"title":"Solve WebForms Errors with PreCompilation","authorId":"simon_timms","slug":"find-webforms-errors-with-precompilation","date":"2020-04-01 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/find-webforms-errors-with-precompilation/","link":"","permalink":"https://westerndevs.com/_/find-webforms-errors-with-precompilation/","excerpt":"I have a webforms application that I help maintain. Today I made some change and managed to break one of the pages on the site. The error was unbelievably unhelpful. In older versions of ASP.NET it is nearly impossible to diagnose these sorts of errors. Was it something with the web.config? Did I mess up the dependency injection? I messed about a bit and found that if I deleted everything out of the .aspx file things worked. So it was the view. But what?","raw":"---\nlayout: post\ntitle: Solve WebForms Errors with PreCompilation\nauthorId: simon_timms\ndate: 2020-03-31 20:00\noriginalUrl: 'https://blog.simontimms.com/2020/03/31/2020-03-31-find-webforms-errors-with-precompilation/'\n---\n\nI have a webforms application that I help maintain. Today I made some change and managed to break one of the pages on the site. The error was unbelievably unhelpful.\n\n![Wut? 500 error with no useful details](https://blog.simontimms.com/images/precompilewebforms/500.png)\n\nIn older versions of ASP.NET it is nearly impossible to diagnose these sorts of errors. Was it something with the web.config? Did I mess up the dependency injection? I messed about a bit and found that if I deleted everything out of the `.aspx` file things worked. So it was the view. But what? \n\n<!-- more -->\n\nI don't know where the logs go for these sorts of errors but I couldn't find them. But knowing that the error was in the ASPX file I figured I could get the errors by compiling the files. You can actually precompile the ASPX files pretty easily in your post build step. It does take some time so I don't typically have it enabled for development but this is the command\n\n```\n%windir%\\Microsoft.NET\\Framework64\\v4.0.30319\\aspnet_compiler.exe -v / -p \"$(SolutionDir)$(ProjectName)\"\n```\n\nSure enough running that build gave me the error message I was looking for: \n\n```\nerror ASPPARSE: Literal content ('<!--') is not allowed within a 'Telerik.Web.UI.GridColumnCollection'.\n```\n\nThis is the second time in a week XML comments caught me. But the story here is that if you're running into unexpected 500 errors on a webform page try compiling the page using `aspnet_compiler`","categories":[],"tags":[]},{"title":"Enhancing Application Insights Request Telemetry","authorId":"dave_paquette","slug":"enhancing-application-insights-request-telemetry","date":"2020-03-08 00:24:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Application-Insights/enhancing-application-insights-request-telemetry/","link":"","permalink":"https://westerndevs.com/Application-Insights/enhancing-application-insights-request-telemetry/","excerpt":"A continuation in my series of love letters about Application Insights. Today I explore a method of enhancing the request telemetry that is automatically collected by the Application Insights SDK.","raw":"---\nlayout: post\ntitle: Enhancing Application Insights Request Telemetry\ntags:\n  - .NET\n  - .NET Core\n  - Application Insights\n  - Azure\ncategories:\n  - Application Insights\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2020/03/07/enhancing-application-insights-request-telemetry.aspx'\ndate: 2020-03-07 19:24:26\nexcerpt: A continuation in my series of love letters about Application Insights. Today I explore a method of enhancing the request telemetry that is automatically collected by the Application Insights SDK. \n---\nThis post is a continuation of my series about using [Application Insights in ASP.NET Core](https://www.davepaquette.com/archive/2020/01/20/getting-the-most-out-of-application-insights-for-net-core-apps.aspx). Today we will take a deeper dive into Request telemetry. \n\n# Request Telemetry\nFor an ASP.NET Core process, the Application Insights SDK will automatically collect data about every request that the server process receives. This specific type of telemetry is called [Request telemetry](https://docs.microsoft.com/azure/azure-monitor/app/data-model-request-telemetry) and it contains a ton of very useful data including: the request path, the HTTP verb, the response status code, the duration, the timestamp when the request was received.   \n\n![Sample Request Telemetry](https://www.davepaquette.com/images/app_insights/request_telemetry.png)\n\nThe default data is great, but I often find myself wanting more information. For example, in a multi-tenant application, it would be very useful to track the tenant id as part of the request telemetry. This would allow us to filter data more effectively in the Application Insights portal and craft some very useful log analytics queries. \n\n# Adding custom data to Request Telemetry\nAll types of telemetry in Application Insights provide an option to store custom properties. In the [previous post](https://www.davepaquette.com/archive/2020/02/05/setting-cloud-role-name-in-application-insights.aspx), we saw how to create an `ITelemetryInitializer` to set properties on a particular telemetry instance. We could easily add custom properties to our Request telemetry using a telemetry initializer.\n\n{% codeblock lang:cs %}\npublic class CustomPropertyTelemetryInitializer : ITelemetryInitializer\n{\n    public void Initialize(ITelemetry telemetry)\n    {\n      requestTelemetry.Properties[\"MyCustomProperty\"] = \"Some Useful Value\";\n    }\n}\n{% endcodeblock %}\n\nAny custom properties you add will be listed under Custom Properties in the Application Insights portal.\n\n![Sample Request Telemetry with Custom Properties](https://www.davepaquette.com/images/app_insights/request_telemetry_custom_property.png)\n\n\nBut telemetry initializers are singletons and often don't have access to the useful data that we want to add to request telemetry. Typically the data we want is related in some way to the current request and that data wouldn't be available in a singleton service. Fortunately, there is another easy way to get an instance of the request telemetry for the current request. \n\n{% codeblock lang:cs %}\nvar requestTelemetry = HttpContext.Features.Get<RequestTelemetry>();\nrequestTelemetry.Properties[\"TenantId\"] = \"ACME_CORP\"; \n{% endcodeblock %}\n\nYou can do it anywhere you have access to an HTTP Context. Some examples I have seen include: `Middleware`, `ActionFilters`, `Controller` action methods, `OnActionExecuting` in a base `Controller` class and `PageModel` classes in Razor Pages. \n\n# Filtering by Custom Properties in the Portal\nOnce you've added custom properties to Request Telemetry, you can use those custom properties to filter data in the Application Insights portal. For example, you might want to investigate failures that are occurring for a specific tenant or investigate performance for a particular tenant. \n\n![Filtering by Custom Property](https://www.davepaquette.com/images/app_insights/filer_by_custom_property.png)\n\nThis type of filtering can be applied almost anywhere in the portal and can help narrow things down when investigating problems.\n\n# Writing Useful Log Analytics Queries\nNow this is where things get really interesting for me. What if we had one particular tenant complaining about performance. Wouldn't it be interesting to plot out the average request duration for all tenants? We can easily accomplish this using a log analytics query.\n\n```\nrequests\n| summarize avg(duration) by tostring(customDimensions.TenantId), bin(timestamp, 15m)\n| render timechart\n```\n\nThis simple query will produce the following chart:\n\n![Log Analytics Query Summarize by Custom Property](https://www.davepaquette.com/images/app_insights/log_analytics_by_custom_property.png)\n\nSmall variations on this query can be extremely useful in comparing response times, failure rates, usage and pretty much anything else you can think of. \n\n\n# Wrapping it up\nTenantId is just an example of a custom property. The custom properties that are useful for a particular application tend to emerge naturally as you're investigating issues and sifting through telemetry in Application Insights. You will eventually find yourself saying \"I wish I knew what `xxx` was for this request`. When that happens, stop and add that as a custom property to the request telemetry. You'll thank yourself later.","categories":[{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/categories/Application-Insights/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/tags/Application-Insights/"}]},{"title":"How Github Makes Game Making Easier for Noncoders","authorId":"david_wesst","slug":"How-GitHub-Makes-Game-Making-Easier-For-NonCoders","date":"2020-02-19 20:48:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Game-Design/How-GitHub-Makes-Game-Making-Easier-For-NonCoders/","link":"","permalink":"https://westerndevs.com/Game-Design/How-GitHub-Makes-Game-Making-Easier-For-NonCoders/","excerpt":"DW goes through five things he's been using in Github while designing his latest prototype that require zero coding skills.","raw":"---\ntitle: \"How Github Makes Game Making Easier for Noncoders\"\ndate: 2020-02-19T09:48:36-06:00\nlayout: post\nauthorId: david_wesst\noriginalurl: http://www.davidwesst.com/blog/how-github-makes-game-making-easier-for-noncoders/\ncategories:\n    - Game Design\ntags:\n    - Game Design\n    - Game Development\n    - GitHub\n    - Video\n---\n\nDW goes through five things he's been using in Github while designing his latest prototype that require zero coding skills. \n\n<!-- more -->\n\nGitHub is a social development platform that will make your game development journey easier even if you're not a coder! We'll cover how GitHub can help keep your file history, through to how it can help organize your work, and even where to find cool projects to learn (and possibly contribute to). \n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rq0JDWnVt8I\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## References and Links\n\n### Github Desktop (Desktop Client)\nhttps://help.github.com/en/desktop\n\n### Links to interesting game development & design projects\n\nTiled | https://github.com/bjorn/tiled\nInky | https://github.com/inkle/inky\nOpenTDD | https://github.com/OpenTTD/OpenTTD\nMicropolisJS | https://github.com/graememcc/micropolisJS","categories":[{"name":"Game Design","slug":"Game-Design","permalink":"https://westerndevs.com/categories/Game-Design/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://westerndevs.com/tags/GitHub/"},{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/tags/Game-Development/"},{"name":"Game Design","slug":"Game-Design","permalink":"https://westerndevs.com/tags/Game-Design/"},{"name":"Video","slug":"Video","permalink":"https://westerndevs.com/tags/Video/"}]},{"title":"Setting Cloud Role Name in Application Insights","authorId":"dave_paquette","slug":"setting-cloud-role-name-in-application-insights","date":"2020-02-06 00:59:38+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Application-Insights/setting-cloud-role-name-in-application-insights/","link":"","permalink":"https://westerndevs.com/Application-Insights/setting-cloud-role-name-in-application-insights/","excerpt":"A continuation in my series of love letters about Application Insights. Today I dig into the importance of setting cloud role name.","raw":"---\nlayout: post\ntitle: Setting Cloud Role Name in Application Insights\ntags:\n  - .NET\n  - .NET Core\n  - Application Insights\n  - Azure\ncategories:\n  - Application Insights\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2020/02/05/setting-cloud-role-name-in-application-insights.aspx'\ndate: 2020-02-05 19:59:38\nexcerpt: A continuation in my series of love letters about Application Insights. Today I dig into the importance of setting cloud role name. \n---\nThis post is a continuation of my series about using [Application Insights in ASP.NET Core](https://www.davepaquette.com/archive/2020/01/20/getting-the-most-out-of-application-insights-for-net-core-apps.aspx). Today we will explore the concept of Cloud Role and why it's an important thing to get right for your application.\n\nIn any application that involves more than a single server process/service, the concept of _Cloud Role_ becomes really important in Application Insights. A Cloud Role roughly represents a process that runs somewhere on a server or possibly on a number of servers. A cloud role made up of 2 things: a _cloud role name_ and a _cloud role instance_. \n\n## Cloud Role Name\nThe cloud role name is a logical name for a particular process. For example, I might have a cloud role name of \"Front End\" for my front end web server and a name of \"Weather Service\" for a service that is responsible for providing weather data.\n\nWhen a cloud role name is set, it will appear as a node in the Application Map. Here is an example showing a Front End role and a Weather Service role.\n\n![Application Map when Cloud Role Name is set ](https://www.davepaquette.com/images/app_insights/example_application_map.png)\n\nHowever, when Cloud Role Name is not set, we end up with a misleading visual representation of how our services communicate. \n![Application Map when Cloud Role Name is not set ](https://www.davepaquette.com/images/app_insights/example_application_map_no_cloud_role_name.png)\n\nBy default, the application insights SDK attempts to set the cloud role name for you. For example, when you're running in Azure App Service, the name of the web app is used. However, when you are running in an on-premise VM, the cloud role name is often blank.\n\n## Cloud Role Instance\nThe cloud role instance tells us which specific server the cloud role is running on. This is important when scaling out your application. For example, if my Front End web server was running 2 instances behind a load balancer, I might have a cloud role instance of \"frontend_prod_1\" and another instance of \"frontend_prod_2\". \n\nThe application insights SDK sets the cloud role instance to the name of the server hosting the service. For example, the name of the VM or the name of the underlying compute instance hosting the app in App Service. In my experience, the SDK does a good job here and I don't usually need to override the cloud role instance.\n\n## Setting Cloud Role Name using a Telemetry Initializer\nTelemetry Initializers are a powerful mechanism for customizing the telemetry that is collected by the Application Insights SDK. By creating and registering a telemetry initializer, you can overwrite or extend the properties of any piece of telemetry collected by Application Insights.\n\nTo set the Cloud Role Name, create a class that implements `ITelemetryInitializer` and in the `Initialize` method set the `telemetry.Context.Cloud.RoleName` to the cloud role name for the current application. \n\n{% codeblock lang:cs %}\npublic class CloudRoleNameTelemetryInitializer : ITelemetryInitializer\n{\n    public void Initialize(ITelemetry telemetry)\n    {\n      // set custom role name here\n      telemetry.Context.Cloud.RoleName = \"Custom RoleName\";\n    }\n}\n{% endcodeblock %}\n\nNext, in the `Startup.ConfigureServices` method, register that telemetry initializer as a singleton.\n\n{% codeblock lang:cs %}\nservices.AddSingleton<ITelemetryInitializer, CloudRoleNameTelemetryInitializer>();\n{% endcodeblock %}\n\nFor those who learn by watching, I have recorded a video talking about using telemetry initializers to customize application insights.\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/1OAaYb_HL5g?list=PLFHLo5Y9d4JaGXNF80SzymGTkbmED6VoO\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Using a Nuget Package\nCreating a custom telemetry initializer to set the cloud role name is a simple enough, but it's something I've done so many times that I decided to publish a [Nuget package](https://www.nuget.org/packages/AspNetMonsters.ApplicationInsights.AspNetCore/) to simplify it even further.\n\nFirst, add the `AspNetMonsters.ApplicationInsights.AspNetCore` Nuget package:\n```\ndotnet add package AspNetMonsters.ApplicationInsights.AspNetCore\n```\n\nNext, in call `AddCloudRoleNameInitializer` in your application's `Startup.ConfigureServices` method:\n```\nservices.AddCloudRoleNameInitializer(\"WeatherService\");\n```\n\n## Filtering by Cloud Role \nSetting the Cloud Role Name / Instance is about a lot more than seeing your services laid out properly in the Application Map. It's also really important when you starting digging in to the performance and failures tabs in the Application Insights portal. In fact, on most of the sections of the portal, you'll see this Roles filter.\n\n![Roles pill](https://www.davepaquette.com/images/app_insights/roles_pill.png)\n\nThe default setting is _all_. When you click on it, you have the option to select any combination of your application's role names / instances. For example, maybe I'm only interested in the _FrontEnd_ service and _WeatherService_ that were running on the dave_yoga920 instance.\n\n![Roles filter](https://www.davepaquette.com/images/app_insights/roles_filter.png)\n\nThese filters are extremely useful when investigating performance or errors on a specific server or within a specific service. The more services your application is made up of, the more useful and essential this filtering become. These filters really help focus in on specific areas of an application within the Application Insights portal.\n\n## Next Steps\nIn this post, we saw how to customize telemetry data using telemetry initializers. Setting the cloud role name is a simple customization that can help you navigate the massive amount of telemetry that application insights collects. In the next post, we will explore a more in complex example of using telemetry initializers.","categories":[{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/categories/Application-Insights/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/tags/Application-Insights/"}]},{"title":"Jan 2020 Devlog","authorId":"david_wesst","slug":"Jan-2020-Devlog","date":"2020-02-04 21:03:24+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Devlog/Jan-2020-Devlog/","link":"","permalink":"https://westerndevs.com/Devlog/Jan-2020-Devlog/","excerpt":"Did you know that I really try and make video games and not just talk about them? Me neither, but this video will fix all that!","raw":"---\ntitle: \"Jan 2020 Devlog\"\ndate: 2020-02-04T10:03:24-06:00\n\nlayout: post\nauthorId: david_wesst\noriginalurl: http://www.davidwesst.com/blog/jan-2020-devlog\ncategories:\n    - Devlog\ntags:\n    - Devlog\n    - Game Development\n    - Video\n    - Car Scientist\n---\n\nDid you know that I really try and make video games and not just talk about them? Me neither, but this video will fix all that!\n\n<!-- more -->\n\nI tend to fall into the crutch of talking about technology rather than actually doing it. There is nothing wrong with the evangelism/marketing side of the tech community, but the whole point of me doing this is to share my journey and the tips along the way. Note the \"journey\" part means, I have to move the needle. An so, here we are with the first devlog of the year. \n\nIn this video I cover the bit of progres I've had on Car Scientist, and how it's become something of a game development sandbox to practice and (ideally) sharpen my gamedev skills. I also review the LARGE amount of planning I did and reflect a bit on how I sort through what ideas should be in this game versus the next one.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-sKlp_CUhB0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n","categories":[{"name":"Devlog","slug":"Devlog","permalink":"https://westerndevs.com/categories/Devlog/"}],"tags":[{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/tags/Game-Development/"},{"name":"Devlog","slug":"Devlog","permalink":"https://westerndevs.com/tags/Devlog/"},{"name":"Video","slug":"Video","permalink":"https://westerndevs.com/tags/Video/"},{"name":"Car Scientist","slug":"Car-Scientist","permalink":"https://westerndevs.com/tags/Car-Scientist/"}]},{"title":"Where to Find Your Gamedev Community","authorId":"david_wesst","slug":"Where-To-Find-Your-Gamedev-Community","date":"2020-01-21 11:09:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Game-Development/Where-To-Find-Your-Gamedev-Community/","link":"","permalink":"https://westerndevs.com/Game-Development/Where-To-Find-Your-Gamedev-Community/","excerpt":"Four places to find your new game development and/or design and/or technology community and figure out if it's even good!","raw":"---\ntitle: \"Where to Find Your Gamedev Community\"\nauthorId: david_wesst\nlayout: post\noriginalUrl: https://davidwesst.com/blog/where-to-find-your-gamedev-community/\ndate: 2020-01-21T00:09:00-06:00\ncategory:\n    - Game Development\ntags:\n    - Game Development\n    - Gamedev\n    - Game Design\n    - Community\n    - Professional Development\n---\n\nFour places to find your new game development and/or design and/or technology community and figure out if it's even good!\n\n<!-- more -->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rfxfLZTM7Fk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nDid you know there are people out there who will help you with your game because they WANT TO DO IT? I know, right? This magical place is called a â€œcommunityâ€ and they exist literally all over the world. You just need to find one. In this video, I talk about four different places to find technology and game development/design communities right from where your sitting. \n","categories":[{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/categories/Game-Development/"}],"tags":[{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/tags/Game-Development/"},{"name":"Community","slug":"Community","permalink":"https://westerndevs.com/tags/Community/"},{"name":"Game Design","slug":"Game-Design","permalink":"https://westerndevs.com/tags/Game-Design/"},{"name":"Gamedev","slug":"Gamedev","permalink":"https://westerndevs.com/tags/Gamedev/"},{"name":"Professional Development","slug":"Professional-Development","permalink":"https://westerndevs.com/tags/Professional-Development/"}]},{"title":"Getting the Most Out of Application Insights for .NET (Core) Apps","authorId":"dave_paquette","slug":"getting-the-most-out-of-application-insights-for-net-core-apps","date":"2020-01-20 16:50:18+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Application-Insights/getting-the-most-out-of-application-insights-for-net-core-apps/","link":"","permalink":"https://westerndevs.com/Application-Insights/getting-the-most-out-of-application-insights-for-net-core-apps/","excerpt":"If you've worked with me in the last couple years, you know that I've fallen in love with Application Insights. This is the first in a series of posts designed to help you get the most out of Application Insights for .NET Core applications.","raw":"---\nlayout: post\ntitle: Getting the Most Out of Application Insights for .NET (Core) Apps\ntags:\n  - .NET\n  - .NET Core\n  - Application Insights\n  - Azure\ncategories:\n  - Application Insights\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2020/01/20/getting-the-most-out-of-application-insights-for-net-core-apps.aspx'\ndate: 2020-01-20 11:50:18\nexcerpt: If you've worked with me in the last couple years, you know that I've fallen in love with Application Insights.  This is the first in a series of posts designed to help you get the most out of Application Insights for .NET Core applications.\n---\n[Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview) is a powerful and surprisingly flexible application performance monitoring (APM) service hosted in Azure. Every time I've used Application Insights on a project, it has opened the team's eyes to what is happening with our application in production. In fact, this might just be one of the best named Microsoft products ever. It literally provides **insights** into your **applications**.\n\n![Application Map provides a visual representation of your app's dependencies ](https://www.davepaquette.com/images/app_insights/example_application_map.png)\n\nApplication Insights has built-in support for .NET, Java, Node.js, Python, and Client-side JavaScript based applications. This blog post is specifically about .NET applications. If you're application is built in another language, head over to the [docs](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview) to learn more. \n\n## Codeless Monitoring vs Code-based Monitoring\nWith codeless monitoring, you can configure a monitoring tool to run on the server (or service) that is hosting your application. The monitoring tool will monitor running processes and collect whatever information is available for that particular platform. There is built in support for [Azure VM and scale sets](https://docs.microsoft.com/en-us/azure/azure-monitor/app/azure-vm-vmss-apps), [Azure App Service](https://docs.microsoft.com/en-us/azure/azure-monitor/app/azure-web-apps), [Azure Cloud Services](https://docs.microsoft.com/en-us/azure/azure-monitor/app/cloudservices), [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-monitoring), [Kubernetes applications](https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-performance-live-website-now) and [On-Premises VMs](https://docs.microsoft.com/en-us/azure/azure-monitor/app/status-monitor-v2-overview). Codeless monitoring is a good option if you want to collect information for applications that have already been built and deployed, but you are generally going to get more information using Code-based monitoring.\n\nWith code-based monitoring, you add the Application Insights SDK. The steps for adding the SDK are well document for [ASP.NET Core](https://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-core), [ASP.NET](https://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net), and [.NET Console](https://docs.microsoft.com/en-us/azure/azure-monitor/app/console) applications so I don't need to re-hash that here.\n\nIf you prefer, I have recorded a video showing how to add Application Insights to an existing ASP.NET Core application.\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/C4G1rRgY9OI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Telemetry\nOnce you've added the Application Insights SDK to your application, it will start collecting telemetry data at runtime and sending it to Application Insights. That telemetry data is what feeds the UI in the Application Insights portal. The SDK will automatically collection information about your dependencies calls to SQL Server, HTTP calls and calls to many popular Azure Services. It's the dependencies that often are the most insightful. In a complex system it's difficult to know exactly what dependencies your application calls in order to process an incoming request. With App Insights, you can see exactly what dependencies are called by drilling in to the End-to-End Transaction view.\n\n![End-to-end transaction view showing an excess number of calls to SQL Server](https://www.davepaquette.com/images/app_insights/end-to-end-transaction-view.png)\n\nIn addition to dependencies, the SDK will also collect requests, exceptions, traces, customEvents, and performanceCounters. If your application has a web front-end and you add the JavaScript client SDK, you'll also find pageViews and browserTimings. \n\n## Separate your Environments\nThe SDK decides which Application Insights instance to send the collected telemetry based on the configured Instrumentation Key.\n\nIn the ASP.NET Core SDK, this is done through app settings:\n\n{% codeblock lang:js %}\n{\n  \"ApplicationInsights\": {\n    \"InstrumentationKey\": \"ccbe3f84-0f5b-44e5-b40e-48f58df563e1\"\n  }\n}\n{% endcodeblock %}\n\nWhen you're diagnosing an issue in production or investigating performance in your production systems, you don't want any noise from your development or staging environments. I always recommend creating an Application Insights resource per environment. In the Azure Portal, you'll find the instrumentation key in the top section of the Overview page for your Application Insights resource. Just grab that instrumentation key and add it to your environment specific configuration.\n\n## Use a single instance for all your production services\nConsider a micro-services type architecture where your application is composed of a number of services, each hosted within it's own process. It might be tempting to have each service point to a separate instance of Application Insights.\n\nContrary to the guidance of separating your environments, you'll actually get the most value from Application Insights if you point all your related production services to a single Application Insights instance. The reason for this is that Application Insights automatically correlates telemetry so you can track a particular request across a series of separate services. That might sound a little like magic but it's [not actually as complicated as it sounds](https://docs.microsoft.com/en-us/azure/azure-monitor/app/correlation).\n\nIt's this correlation that allows the Application Map in App Insights to show exactly how all your services interact with each other.\n\n![Application Map showing multiple services ](https://www.davepaquette.com/images/app_insights/detailed_application_map.png)\n\nIt also enables the end-to-end transaction view to show a timeline of all the calls between your services when you are drilling in to a specific request.  \n\nThis is all contingent on all your services sending telemetry to the same Application Insights instance. The Application Insights UI in the Azure Portal has no ability to display this visualizations across multiple Application Insights instances. \n\n## You don't need to be on Azure\nI've often heard developers say \"I can't use Application Insights because we're not on Azure\". Well, you don't need to host your application on Azure to use Application Insights. Yes, you will need an Azure subscription for the Application Insights resource, but your application can be hosted anywhere. That includes your own on-premise services, AWS or any other public/private cloud.\n\n## Next Steps\nOut of the box, Application Insights provides a tremendous amount of value but I always find myself having to customize a few things to really get the most out of the telemetry. Fortunately, the SDK provides some useful extension points. My plan is to follow up this post with a few more posts that go over those customizations in detail. I also have started to create a [NuGet package](https://github.com/AspNetMonsters/ApplicationInsights) to simplify those customizations so stay tuned!\n\n## *Update\nOther posts in this series:\n[Setting Cloud Role Name](https://www.davepaquette.com/archive/2020/02/05/setting-cloud-role-name-in-application-insights.aspx)\n[Enhancing Application Insights Request Telemetry](https://www.davepaquette.com/archive/2020/03/07/enhancing-application-insights-request-telemetry.aspx)\n\n","categories":[{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/categories/Application-Insights/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/tags/Application-Insights/"}]},{"title":"No Cost Game Design Toolbox","authorId":"david_wesst","slug":"No-Cost-Game-Design-Toolbox","date":"2020-01-13 21:51:04+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/No-Cost-Game-Design-Toolbox/","link":"","permalink":"https://westerndevs.com/_/No-Cost-Game-Design-Toolbox/","excerpt":"Want to design games but have no money? Never fear, for my game design toolbox (and process) can help get you started!","raw":"---\ntitle: \"No Cost Game Design Toolbox\"\ndate: 2020-01-13T10:51:04-06:00\n\nlayout: post\nauthorId: david_wesst\noriginalurl: http://www.davidwesst.com/blog/no-cost-game-design-toolbox/\ntags:\n    - Game Design\n    - Game Development\n    - Toolbox\n    - OneNote\n    - Excel\n---\n\nWant to design games but have no money? Never fear, for my game design toolbox (and process) can help get you started!\n\n<!-- more -->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Wtt_a2hbF3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nI've spent the past week working on the the game design for #CarScientist. What I noticed was that I didn't open a code editor or development tool once this whole week, and considering that's my comfort zone when it comes to any form of software development, I was both surprised and pleased that I managed to wear my design hat so well!\n\nOn top of just being in design mode, I avoided my other \"excuse\" which is to want and research what you don't have, which is usually some tool or domain I think is clever. This whole toolbox not only keeps me focused, but it keeps me on budget as well as you can get all of it for nothing. Take a look and let me know in the comments your thoughts on my game design toolbox, along with what tools you use.","categories":[],"tags":[{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/tags/Game-Development/"},{"name":"Game Design","slug":"Game-Design","permalink":"https://westerndevs.com/tags/Game-Design/"},{"name":"Toolbox","slug":"Toolbox","permalink":"https://westerndevs.com/tags/Toolbox/"},{"name":"OneNote","slug":"OneNote","permalink":"https://westerndevs.com/tags/OneNote/"},{"name":"Excel","slug":"Excel","permalink":"https://westerndevs.com/tags/Excel/"}]},{"title":"Allow Azure DevOps Hosted Agents Through Firewall","authorId":"simon_timms","slug":"Allow-hosted-agent-through-firewall","date":"2020-01-10 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Allow-hosted-agent-through-firewall/","link":"","permalink":"https://westerndevs.com/_/Allow-hosted-agent-through-firewall/","excerpt":"I have an on-premise (well in a third party data center but close enough) database which I'd like to update via a build in a hosted agent on Azure. We've done this before in Jenkins by just allowing a specific IP address through the firewall. However we're in the process of moving to DevOps for this build. Unfortunately, the hosted build agents don't have entirely predictable IP addresses. Every week Microsoft publishes a list of all the IP addresses in Azure. It is a huge json document and for our region (Central Canada) there are about 40 IP addresses ranges the build agent could be in. We want an automated way to update our firewall rules based on this list.","raw":"---\nlayout: post\ntitle: Allow Azure DevOps Hosted Agents Through Firewall\nauthorId: simon_timms\ndate: 2020-01-10 9:00\noriginalurl: https://blog.simontimms.com/2020/01/10/2020-01-10-Allow-hosted-agents-through-firewall/\n---\n\nI have an on-premise (well in a third party data center but close enough) database which I'd like to update via a build in a hosted agent on Azure. We've done this before in Jenkins by just allowing a specific IP address through the firewall. However we're in the process of moving to DevOps for this build. Unfortunately, the hosted build agents don't have entirely predictable IP addresses. Every week Microsoft publishes a list of all the IP addresses in Azure. It is a huge json document and for our region (Central Canada) there are about 40 IP addresses ranges the build agent could be in. We want an automated way to update our firewall rules based on this list. \n\n<!-- more -->\nTo do so we make use of the Azure Powershell extensions. The commandlet Get-AzNetworkServiceTag is an API based way to get the IP ranges. You can then pass that directly into the firewall rules like so\n\n```powershell\n$addrs = ((Get-AzNetworkServiceTag -Location canadacentral).Values|Where-Object { $_.Name -eq \"AzureCloud.canadacentral\" }).Properties.AddressPrefixes\nSet-NetFirewallRule -DisplayName \"Allow SQL 1433 Inbound\" -RemoteAddress $addrs\n```\n\nRunning this once a week lets us keep the firewall up to date with the hosted agent ranges.\n\nBonus: If you want to see the remote addresses in your firewall rule currently then this will do it\n\n```\n Get-NetFirewallRule -Direction Inbound -Action Allow -Enabled True -Verbose | ? DisplayName -like \"Allow SQL 1433 Inbound\" | Get-NetFirewallAddressFilter |Select-Object RemoteAddress -ExpandProperty RemoteAddress\n```","categories":[],"tags":[]},{"title":"UNION vs. UNION ALL in SQL Server","authorId":"simon_timms","slug":"UNION-vs-UNION-ALL","date":"2019-12-30 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/UNION-vs-UNION-ALL/","link":"","permalink":"https://westerndevs.com/_/UNION-vs-UNION-ALL/","excerpt":"I really dislike database queries which are slow for no apparent reason. I ran into one of those today. It queries over a few thousands of well indexed rows and returned a handful, perhaps 3, records. Time to do this? 33 seconds. Well that's no good for anybody. Digging into the query I found that it actually used a UNION to join 3 sets of similar data together. I go by the rule of thumb that SQL operations which treat data as sets and do things with that in mind are efficient. I'm not sure where I read that but it has stuck with me over the years. What it suggests is that you should avoid doing things like looping over rows or calling functions on masses of data. As it turns out there are actually two different UNION operators in SQL Server: UNION and UNION ALL. They differ in how they handle duplicate entries. UNION will check each entry to ensure that it exists in the output only one time.","raw":"---\nlayout: post\ntitle: UNION vs. UNION ALL in SQL Server\nauthorId: simon_timms\ndate: 2019-12-30 9:00\noriginalurl: https://blog.simontimms.com/2019/12/30/2019-12-30-UNION-vs-UNION-ALL/\n\n---\n\nI really dislike database queries which are slow for no apparent reason. I ran into one of those today. It queries over a few thousands of well indexed rows and returned a handful, perhaps 3, records. Time to do this? 33 seconds. Well that's no good for anybody. Digging into the query I found that it actually used a `UNION` to join 3 sets of similar data together. I go by the rule of thumb that SQL operations which treat data as sets and do things with that in mind are efficient. I'm not sure where I read that but it has stuck with me over the years.  What it suggests is that you should avoid doing things like looping over rows or calling functions on masses of data. \n\nAs it turns out there are actually two different `UNION` operators in SQL Server: `UNION` and `UNION ALL`. They differ in how they handle duplicate entries. `UNION` will check each entry to ensure that it exists in the output only one time. \n\n<!--more-->\n\nSo if you had results like \n\n```sql\nselect * from a\n\nID     Name\n 1     Bob\n 2     Jane\n\nselect * from b\n\nID     Name\n 3     Sally\n 2     Jane\n\n```\n\nThe result of running\n\n```sql\nselect * from a\nunion \nselect * from b\n\nID     Name\n 1     Bob\n 3     Sally\n 2     Jane\n\n```\n\nHere the duplicate record 2 is only returned once. On the other hand `UNION ALL` will assume that the result sets are already unique and return whatever it is given\n\n```sql\nselect * from a\nunion all\nselect * from b\n\nID     Name\n 1     Bob\n 2     Jane\n 3     Sally\n 2     Jane\n\n```\n\n`UNION ALL` is, as a result of not doing this duplicate check, far faster than `UNION`. On the data sets I tried the savings were between 40% and 95%. It isn't always the right answer but is another tool on your toolbelt. ","categories":[],"tags":[]},{"title":"GitHub Game Off 2019 Game Jam featuring DW","authorId":"david_wesst","slug":"gameoff2019-gamejam-devlog","date":"2019-11-26 14:57:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Game-Development/gameoff2019-gamejam-devlog/","link":"","permalink":"https://westerndevs.com/Game-Development/gameoff2019-gamejam-devlog/","excerpt":"For the past month I have been participating in the GameOff 2019 Gamejam hosted by GitHub over on Itch.io and I've been devlogging about it over on my YouTube channel.","raw":"---\nlayout: post\ntitle:  \"GitHub Game Off 2019 Game Jam featuring DW\"\ndate:   2019-11-26 09:57:00\nauthorId: david_wesst\noriginalurl: http://www.davidwesst.com/blog/gameoff2019-gamejam-devlog/\ncategories:\n    - Game Development\ntags:\n    - gamedev\ncomments: true\n---\n\nFor the past month I have been participating in the GameOff 2019 Gamejam hosted by GitHub over on Itch.io and I've been devlogging about it over on my YouTube channel.\n\n<!--more-->\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/videoseries?list=PLbTA1UhK0wKjTEEc_wO1n0w_hPAUIeVRf\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\nFor the past year or so, I've been working on learning and getting into video game development (a.k.a. gamedev) as a sort of side thing to the whole enterprise cloud architect thing that I do during the day.\n\nIn order to focus my effort, I decided that I would participate in the GitHub hosted [GameOff 2019 Game Jam on Itch.io][1]. It's been a ride to say the least, but as I've been doing it, I've been doing a devlog series on YouTube about my progress and my thoughts throughout the competition.\n\n## Where Do I Find the Vlog\nYou're in the right spot, as I'm going to cross post my [Itch.io][2] project devlog posts here, but you can also follow along on my [YouTube channel][3] if you're into that sort of thing, or you can just [checkout the playlist here][4].\n\nThe game jam ends on December 1, and there is more work to be done, so stay tuned for more posts.\n\n---\n\nThanks for playing. ~ DW\n\n[1]: https://itch.io/jam/game-off-2019\n[2]: https://itch.io/\n[3]: https://youtube.com/davidwesst\n[4]: https://www.youtube.com/playlist?list=PLbTA1UhK0wKjTEEc_wO1n0w_hPAUIeVRf","categories":[{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/categories/Game-Development/"}],"tags":[{"name":"gamedev","slug":"gamedev","permalink":"https://westerndevs.com/tags/gamedev/"}]},{"title":"Azure AD B2C Web Testing","authorId":"tyler_doerksen","slug":"Azure-AD-B2C-Web-Testing","date":"2019-11-15 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Azure-AD-B2C-Web-Testing/","link":"","permalink":"https://westerndevs.com/_/Azure-AD-B2C-Web-Testing/","excerpt":"Login Azure AD B2C User with Postman Recently a customer asked how to load test a web application that uses Azure AD B2C (OpenIdConnect) for authentication. Even though there are lots of articles on calling Web APIs with OAuth tokens, I could not find much info on automating the OpenIdConnect authentication flow. I thought that if I could execute the correct requests in Postman I should be able to create an automated web/load test.","raw":"---\nlayout: post\ntitle: Azure AD B2C Web Testing\ndate: 2019-11-15\nauthorId: tyler_doerksen\ntags:\n  - Azure\n  - AzureAD\n  - Azure AD B2C\n  - Postman\n---\n\n## Login Azure AD B2C User with Postman\n\nRecently a customer asked how to load test a web application that uses Azure AD B2C (OpenIdConnect) for authentication. Even though there are lots of articles on calling Web APIs with OAuth tokens, I could not find much info on automating the [OpenIdConnect authentication flow](https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-protocols-oidc).\n\nI thought that if I could execute the correct requests in Postman I should be able to create an automated web/load test.\n\n<!-- more -->\n\n**Problem:** Azure AD B2C login pages rely on Javascript. Postman (and most load testing frameworks like JMeter) do not run client-side Javascript.\n\nIn order to get around this, you need to \"fake\" the functionality of the Javascript code to create the subsequent requests.\n\nLets break down the auth flow into 5 steps, and outline how to craft the request in Postman. Specifically, I want to test a \"username and password\" type B2C user, not a \"social login (MS Live, Google, Facebook)\" type user.\n\n> Note: This article is based on the [Azure AD B2C ASP.NET Core Web App Sample](https://github.com/Azure-Samples/active-directory-b2c-dotnetcore-webapp) on Github, as of Nov 2019. To setup the sample just clone the repo, execute `dotnet run`, and sign up a new user.\n\n### Step 1: Initial GET Request\n\nFirst, **turn off auto-redirect** and send an initial GET request to your site root or signin route.\n\n```\nGET http://localhost:5000/Session/SignIn\n```\n\nThe response should have a **Location** header with the full URL and query string for the Authorize request.\n\nSpecifically, we are interested in the **state** and **nonce** values, which will be different each time.\n\n### Step 2: AAD Authorize Request\n\nIf you grab the **Location** redirect header from the previous step it should look like this\n\n[![Postman screenshot of authorize request](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/1.png)](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/1.png)\n\nWhile some information may be different based on your application, the following should be standard for OpenIdConnect\n\n* response_type is \"code id_token\"\n* scope includes \"openid\"\n\nThe **response_mode** is important to note, this will setup the flow to send the code back to your application using either a query string or form post mechanism.\n\nWhen you execute this request you should get a **200 OK** response, even though the content will say that you need Javascript to continue.\n\n![Postman screenshot of authorize result viewing the preview tab](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/2.png)\n\nDon't panic. It did work as expected.\n\nFrom here we need to extract some information from the body of the response, which I will outline in the next step.\n\n### Step 3: Login Request\n\nTo build the Login request and pass the username+password, we need some information from the body of the Authorize request.\n\nScroll down in Postman until you find `var SETTINGS = { ...` in a **&lt;script&gt;** tag.\n\nCopy the **csrf** and **transId** values from the SETTINGS JSON object.\n\n[![Postman screenshot of SETTINGS variable highlighting csrf and transId values](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/3.png)](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/3.png)\n\nYou can add them to the Postman environment or global variables, so in the future I will refer to these values with **csrf** and **transId**\n\nCreate a new POST request with this url and header\n\n```\nPOST https://fabrikamb2c.b2clogin.com/fabrikamb2c.onmicrosoft.com/B2C_1_SUSI/SelfAsserted?tx={{transId}}&p=B2C_1_SUSI\nX-CSRF-TOKEN: {{csrf}}\n```\n\nNote that `B2C_1_SUSI` is the policy name defined in B2C for the \"sign-in sign up\" auth flow, this may be different for your application.\n\nIn the message body, set to **x-www-form-urlencoded** and enter the following info.\n\n* request_type RESPONSE\n* logonIdentifier {{username}}\n* password {{password}}\n\n![Postman screenshot of form body, request_type, loginIdentifier, and password](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/4.png)\n\nNote: The **logonIdentifier** key is configurable in the B2C policy. This may be a different key like \"signInName\" or \"emailAddress\" depending on your configuration. Fiddler is your friend here.\n\nOnce you execute this request you should receive a **200 OK** with the following response body:\n\n```\n{\n    \"status\": \"200\"\n}\n```\n\nThis response also set cookies in Postman which means that other requests are authenticated.  Which is important for the next step.\n\n### Step 4: Generate Auth code and ID Token\n\nThis is fairly straight forward. Now that the session is authenticated we need to request the **code** and **id_token**.\n\nCreate a new request with the following URL\n\n```\nhttps://fabrikamb2c.b2clogin.com/fabrikamb2c.onmicrosoft.com/B2C_1_SUSI/api/CombinedSigninAndSignup/confirmed?csrf_token={{csrf}}&tx={{tx}}&p=B2C_1_SUSI\n```\n\nNote: **CombinedSigninAndSignup** is the name of the configured flow in Azure AD B2C, if you are using a different flow you will need to change this url. Again, try it yourself, Fiddler is your friend here.\n\nIf in Step 2 you used **form_post** as the **response_mode** you should recieve a basic HTML site with a form and hidden fields **state**, **code**, **id_token**, and maybe a few others.\n\n![Postman screenshot of code and id_token response](https://tylerdevblog.blob.core.windows.net/content/2019-11-14-Azure-AD-B2C-Web-Testing/5.png)\n\nCopy the **state**, **code**, and **id_token** values.\n\n### Step 5: POST Request to site\n\nFinally, create a POST request to your site with the **state**, **code**, and **id_token** values in **x-www-form-urlencoded**\n\n```\nPOST http://localhost:5000/signin-oidc\n```\n\nAt this point you should be able to load your site as an authenticated user.\n\nIf you are getting **Correlation failed** errors, the **state** value does not match an existing OpenIdConnect cookie, you may need to restart the process with an initial Login request to reset the proper cookies, and use the new state though the authentication process.\n\nThe good news is that once you have the B2C auth cookie, the /authorize request will return the **state**, **code**, and **id_token** values in form post HTML.\n\nI hope this helps you run end-to-end web tests on your B2C site! \n\n**Mileage may vary:** As you can tell, B2C is a highly configurable solution, this article is based on the .NET Core B2C sample found [here](https://github.com/Azure-Samples/active-directory-b2c-dotnetcore-webapp). Your app will likely have a different configuration. Use Fiddler to capture a login flow and use that as a guide.","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"AzureAD","slug":"AzureAD","permalink":"https://westerndevs.com/tags/AzureAD/"},{"name":"Azure AD B2C","slug":"Azure-AD-B2C","permalink":"https://westerndevs.com/tags/Azure-AD-B2C/"},{"name":"Postman","slug":"Postman","permalink":"https://westerndevs.com/tags/Postman/"}]},{"title":"Bulk Load and Merge Pattern","authorId":"simon_timms","slug":"bulk-load-and-merge","date":"2019-10-12 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/bulk-load-and-merge/","link":"","permalink":"https://westerndevs.com/_/bulk-load-and-merge/","excerpt":"The more years you spend programming the more you run into situations you've run into before. Situations you now know, instinctively, how to address. I suppose this is &quot;experience&quot; and is what I'm paid the medium dollars for. One such problem I've solved at least a dozen times over the years is updating a bunch of data in a database from an external source. This, as it turns out, can be a great source of poor performance if you don't know how to address it. Let's dig into my approach.","raw":"---\nlayout: post\ntitle: Bulk Load and Merge Pattern\nauthorId: simon_timms\ndate: 2019-10-12\n\n---\n\nThe more years you spend programming the more you run into situations you've run into before. Situations you now know, instinctively, how to address. I suppose this is \"experience\" and is what I'm paid the medium dollars for. One such problem I've solved at least a dozen times over the years is updating a bunch of data in a database from an external source. This, as it turns out, can be a great source of poor performance if you don't know how to address it. Let's dig into my approach.\n\n<!-- more -->\n\nTo start with let's give some examples of the problem \n\n* update a database with the result of a web service call\n* load an Excel workbook or CSV file into the database\n* synchronize an external data source with your own cache in a database\n\nThe approach that I commonly see people take is to get the records to load into the database and loop over them, checking them against existing records. It looks something like \n\n```csharp\npublic void Update(DbContext context, IEnumerable<ExternalData> toLoad)\n{\n    foreach(var record in toLoad){\n        var dbRecord = context.Find(record.Id);\n        if(dbRecord != null)\n        {\n            dbRecord.Field1 = record.Field1;\n            ...\n            context.Save(dbRecord);\n        }\n        else {\n            context.Add(record);\n        }\n    }\n}\n```\n\nThis is a pretty logical approach. Any existing record is updated, any new record is inserted. Problem is that you're running 2 database operations for every record that comes in. Try to load 10k records and all of a sudden you're in for a world of hurt. It gets even scarier if you're running all this inside a transaction which might live a minute or two. Operations like this are likely to be subject to lock escalation up to table locks which is certainly not something you want.  \n\n# Bulk Loading\n\nWay back in my university days we had a database class which was so popular the professor taught 2 sessions back to back. This professor was famous for wearing brown sweaters no matter the time of year. Because we had two sessions every day I'd grab somebody from the previous class and ask what the class covered that day. \n\nOn this day the professor was talking about bulk loading, it was the subject of some of his research. I asked a fellow in the previous class what they covered \n\n\"Bulk loading and how much faster it is than regular inserts\"\n\n\"Oh yeah? How much faster?\"\n\n\"312 times, I think it was\"\n\nSo down into the basement I trudged, into the windowless class room. The class started and the professor asked \n\n\"How many times faster do you think bulk loading is?\"\n\nHe sat back, waiting for the ridiculous answers, comfortable in the knowledge that he'd spend the last month writing paper on exactly this subject. \n\n\"312 times, sir\" I answered\n\nHe was flabbergasted that somebody would know this. He'd just spent the last month figuring out that exact number. Eventually I let him off the hook and told him where I'd found out his precise number but not until I span him some story about how Donald Kunth was my uncle.\n\nAnyway the point of this story is that I'm a better person now and that bulk loading is way faster than doing individual inserts. When loading data into the database I like to load the data into a bulk loading table instead of directly into the destination table. That provides a staging area where changes can be made.\n\nIn C# the bulk loading API is a bit [comically dated](https://blogs.msdn.microsoft.com/nikhilsi/2008/06/11/bulk-insert-into-sql-from-c-app/) and relies on data tables. There are some nice wrappers for it including [dapper-plus](https://dapper-plus.net/bulk-insert). Using bulk copy speeds up loading substantially, perhaps not 312 times but I've certainly seen 50-100x. This reduces the chances that the transaction will run for a long time and having it run against a non-production table makes things even less likely to be problematic. \n\nWith the data loaded we can now merge it into the live data, for this we can make use of merge.\n\n# Merge Statement\n\nI have a long list of features that SQL server is, frustratingly, missing. On that list is a simple upsert statement where you can tell the database what to do if there is a conflict. Both [Postgresql](http://www.postgresqltutorial.com/postgresql-upsert/) and [MySQL](https://www.techbeamers.com/mysql-upsert/) have a nice syntax for upsert. On SQL Server you have to wade through the complex `MERGE` statement. The [documentation](https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=sql-server-ver15) for `MERGE` has on off the longest grammars for a statement I've ever seen; as you would expect for such a powerful a command.\n\n\nA very simple example looks like this\n```sql\nMERGE HotelRooms AS target  \n    USING (SELECT @roomNumber, @occupants from bulkLoadHotelRooms) AS source (roomNumber, occupants)  \n    ON (target.roomNumber = source.roomNumber)  \n    WHEN MATCHED THEN\n        UPDATE SET Name = source.occupants  \n    WHEN NOT MATCHED THEN  \n        INSERT (roomNumber, occupants)  \n        VALUES (source.roomNumber, source.occupants) \n```\n\nThis will insert records into the table `HotelRooms` from the bulk load table `BulkLoadHotelRooms` matching them on the room number (the `MATCHED` clause). If there is already a room number there then the occupants are updated (the `NOT MATCHED` clause). Not shown here there is also the ability to delete records which aren't in the target table. Also not shown are about 10 more clauses. The documentation is certainly worthwhile reading. \n\n# Wrapping Up\n\nBulk loading and merging is the best approach I've found so far to load data into a database. I've loaded millions of records on dozens of projects using this approach. If there is a better way that you've found, I'd love to hear about it. ","categories":[],"tags":[]},{"title":"Losing Data with Azure Blob Storage","authorId":"justin_self","slug":"another-time-disposing-is-dangerous","date":"2019-08-16 08:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"bugs/another-time-disposing-is-dangerous/","link":"","permalink":"https://westerndevs.com/bugs/another-time-disposing-is-dangerous/","excerpt":"We had a bug that caused us to write 0 byte documents to Azure blob storage. It was there for a while. We tried hard to find it. Eventually, we turned on blob snapshots which, instead of replacing a blob with a new blob on every write, makes a copy that you can promote at a later time. This week, we had a production issue where a blob had 0 bytes. We hadn't seen it in so long, we secretly hoped the problem was magically fixed by someone else.","raw":"---\nlayout: post\ntitle:  Losing Data with Azure Blob Storage\ndate: 2019-08-15T23:30:00-05:00\ncategories: bugs\ncomments: true\nauthorId: justin_self\n---\n\nWe had a bug that caused us to write 0 byte documents to Azure blob storage. It was there for a while. We tried hard to find it.\n\nEventually, we turned on blob snapshots which, instead of replacing a blob with a new blob on every write, makes a copy that you can promote at a later time. \n\nThis week, we had a production issue where a blob had 0 bytes. We hadn't seen it in so long, we secretly hoped the problem was magically fixed by someone else.\n\n<!--more-->\n\nAfter promoting the previous copy, which unblocked the issue, I stared in frustration at the code, not understanding how we were writing to a stream with 0 bytes.\n\nI probably spent an hour tracing through code and found no place where we were doing anything that would cause this issue. So I decided to take a walk... to the kitchen. There, I sat down with our CTO and described the situation. We started talking through scenarios of how this could happen. Maybe this was a bug in Azure blob or the SDK. Maybe it was our code. Maybe we were somehow purging the stream buffer.\n\nAfter 10 minutes of ideas, we went back to my machine and started to take a closer look at the issue. First, we noticed the timestamps. We audit a lot of things in our system and we had an audit that occurred just before the time we wrote the 0 byte document. We knew what time the write occurred because of the timestamp on the document from the Azure Portal.\n\nWorking our way backwards, I filtered the logs looking for errors that may have occurred before the timestamp or just after it. Then I saw a null reference exception in our logs just a little bit before our successful audit. The top of the stacktrace showed the null reference was actually coming from our IOC container attempting to inject a dependency. That was bizarre. We hold on to the containers for the lifetime of the service and that could be days. Even still, that shouldn't have had anything to do with writing a 0 byte document.\n\nHowever, in an attempt to squash any leads, we dug a little deeper into the surrounding code where the exception was thrown.\n\nA few layers above where the IOC call was eventually made, we see that we are attempting to get an instance of a class that helps us manage encryption. It was that class that was receiving the null reference exception and we happened to be doing that just before we write our data to the blob document.\n\nThat shouldn't have mattered because we had code like this:\n\n```csharp\n            using (var stream = await blob.OpenWriteAsync())\n            {\n                //get factory here which gets stuff from IOC\n\n                await stream.WriteAsync(data);\n            }\n```\n\nWe call `OpenWriteAsync` which returns a `CloudBlobStream` which inherits from `Stream`. We do some encryption stuff and then we write the data to blob. The \"do some encryption stuff\" is what was failing. Since this was wrapping in a `using` block, that means the exception was actually thrown in the compiler generated `try` block and then the `finally` block calls `Dispose` on the `CloudBlobStream` because it ultimately implements `IDisposable`.\n\nWe dug a little deeper into what `Dispose` was doing on the `CloudBlobStream`: it ends up calling `Commit` which, as you can guess, commits the data in the stream to blob. But, at this point we hadn't written any data. So it was actually committing an empty stream which created a 0 byte blob document.\n\nBut why were we through that exception to begin with? Well, we DO dispose the container when the Cloud Service instance is shutting down. So, that means we have to start shutting down a worker role (which is done via autoscaling or deployments) and begin processing a new message from our queue infrastructure within a very tight window. Then we will attempt to create a new encryption helper instance at just the right time before the role is down and that will lead to the disposed container which causes the exception. \n\nThat, in of itself, shouldn't be a big deal. Our message goes back into the queue because it couldn't finish since the machine shut down. However, and without going too much into detail, we need to read data in the blob in order to know how we need to modify it. That means when we try to reprocess the message, it fails again because we don't have any of the data in the document that we should.\n\nThere are a couple of immediate takeaways from this that we are working through. First, we shouldn't have been doing anything inside of the `using` block other than what was purely necessary. We didn't need to do the encryption stuff in the using. If we hadn't we wouldn't have had an exception in a place where ultimately a commit would be called.\n\nSecond, we are considering putting the writes to the document in a separate message that isn't dependent on reading the document first. This would have let us replay the message and work the second time around.\n\nTo get around the issue right now (before we break things apart), we removed the `using` block all together and simply call `Dispose` ourselves when we are done. We've also removed anything between getting the stream and using the stream.","categories":[{"name":"bugs","slug":"bugs","permalink":"https://westerndevs.com/categories/bugs/"}],"tags":[]},{"title":"Deploying Azure resources to multiple resource groups","authorId":"tyler_doerksen","slug":"Deploy-Azure-Multi-RG","date":"2019-08-07 18:49:25+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Azure/Deploy-Azure-Multi-RG/","link":"","permalink":"https://westerndevs.com/Azure/Deploy-Azure-Multi-RG/","excerpt":"Advanced ARM Template Development Azure Resource Manager (ARM) templates provide an excellent, built-in resource configuration and deployment solution. You can find a wealth of templates for deploying anything from a Wordpress site on Azure App Service, to a full HDInsight cluster on a private VNET.","raw":"---\nlayout: post\ntitle: Deploying Azure resources to multiple resource groups\ncategories:\n  - Azure\ndate: 2019-08-07 14:49:25\ntags:\n  - Azure\n  - ARM\n  - Configuration-as-Code\nauthorId: tyler_doerksen\n\n---\n\n## Advanced ARM Template Development\n\nAzure Resource Manager (ARM) templates provide an excellent, built-in resource configuration and deployment solution. You can find a wealth of templates for deploying anything from a [Wordpress site on Azure App Service](https://github.com/Azure/azure-quickstart-templates/tree/master/wordpress-app-service-mysql-inapp), to a full [HDInsight cluster on a private VNET](https://github.com/Azure/azure-quickstart-templates/tree/master/101-hdinsight-secure-vnet).\n\n<!-- more -->\n\nOften I work with customers that need to go beyond the basics of ARM Templates, deploying complex solutions across multiple Resource Groups, with different RBAC permissions.\n\nSo here I will share some tips-and-tricks you may find helpful when authoring complex templates.\n\n## Deploying to multiple Azure Resource Groups\n\nFirst, a very common question, and the title of this post, deploying Azure resources across multiple Resource Groups. You can accomplish this in 3 ways:\n\n1. Deploy multiple times using a script or deployment engine (Azure DevOps Pipeline)\n2. Deploy to a \"primary\" Resource Group [with nested templates deploying to other Resource Groups](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-cross-resource-group-deployment \"Deploy Azure resources to more than one subscription or resource group\")\n3. Use a [Subscription-level resource template](https://docs.microsoft.com/en-us/azure/azure-resource-manager/deploy-to-subscription \"Create resource groups and resources at the subscription level\") to define all Resource Groups and nested templates\n\n### Using a script (#1)\n\nThis is by far the simplest solution, however it is also the most error-prone. You will have to code features that the Azure deployment system would otherwise handle for you, like dependencies, failures, and ordering. Most likely need a script, however it is best to keep it as simple as possible, adding all of the configuration into the ARM Template.\n\n### Resource Group deploying other Resource Groups (#2)\n\nThis is accomplished using the `\"resourceGroup\"` property which you can set on the `\"Microsoft.Resources/deployments\"` type, otherwise known as a nested template. Overall this is a minimal change if you are already using nested templates.\n\nYou can also deploy to multiple subscriptions using the `\"subscriptionId\"` property.\n\nThere are a couple of gotchas here, one is that the child Resource Groups need to exist before the nested deployment (just like how you need to define an existing RG when executing a template deployment). You can either script the creation of all of the RGs before running the deployment on the \"primary\" RG, or use the `\"Microsoft.Resources/resourceGroups\"` resource type, with the `dependsOn` property on the nested template.\n\nHere is an example\n\n```json\n{\n    \"type\": \"Microsoft.Resources/resourceGroups\",\n    \"apiVersion\": \"2018-05-01\",\n    \"location\": \"[parameters('location')]\",\n    \"name\": \"[parameters('msiResourceGroup')]\",\n    \"properties\": {}\n},\n{\n    \"name\": \"msiDeployment\",\n    \"type\": \"Microsoft.Resources/deployments\",\n    \"apiVersion\": \"2017-05-10\",\n    \"resourceGroup\": \"[parameters('msiResourceGroup')]\",\n    \"dependsOn\": [\n        \"[resourceId('Microsoft.Resources/resourceGroups/', parameters('msiResourceGroup'))]\"\n    ],\n    \"properties\": { ... }\n}\n```\n\nAlso, depending on how you nest templates, the `resourceGroup()` function will behave differently. If you have an embedded template `\"template\": {}` the `resourceGroup()` function will refer to the parent RG. Alternatively, if you have a linked template `\"templateLink\": { \"uri\": \"...\"}` the `resourceGroup()` function will refer to the child RG. The same applies to the `subscription()` function.\n\n### Subscription-level Templates (#3)\n\nThis may be my preferred method of deploying complex, multi-RG solutions. Most of the concepts are the same as cross-RG deployments, however there is no \"primary\" RG. With this method you can deploy to a completely blank Subscription, which is why this is often used in combination with [Azure Blueprints](https://docs.microsoft.com/en-us/azure/governance/blueprints/overview \"Overview of the Azure Blueprints service\") as a \"Subscription Factory\" pattern.\n\nTo author Subscription Templates, you need to use a different template schema `https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#` and execute the deployment using the `New-AzDeployment` or `az deployment create` command.\n\nHere is an Azure docs article for the details: [Create resource groups and resources at the subscription level](https://docs.microsoft.com/en-us/azure/azure-resource-manager/deploy-to-subscription \"Create resource groups and resources at the subscription level\")\n\nThe Subscription template will be fairly light, with most of the heavy lifting in the nested templates. There are a few functions that are not available in the Subscription Template, like `resourceGroup()` which means you can't use `resourceGroup().location` as a default deployment location.\n\nYou will need to add a `\"location\"` parameter to the template, and use the value when creating the Resource Groups.\n\nHere is an example:\n\n```json\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.1\",\n    \"parameters\": { \n        \"hdiResourceGroup\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"DL-HDI\"\n        },\n        \"msiResourceGroup\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"DL-MSI\"\n        },\n        \"location\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"westus2\"\n        } ...\n    },\n    \"variables\": {...},\n    \"resources\": {\n        {\n            \"type\": \"Microsoft.Resources/resourceGroups\",\n            \"apiVersion\": \"2018-05-01\",\n            \"location\": \"[parameters('location')]\",\n            \"name\": \"[parameters('hdiResourceGroup')]\",\n            \"properties\": {}\n        },\n    }\n    ....\n}\n```\n\n## Extra Tip: Using the `templateLink.uri` property\n\nI am not a big fan of using additional parameters for Nested Template URLs and SAS Tokens. You may have seen them in examples with underscores in front, like `\"_sasToken\"` or `\"_templateRoot\"`\n\nWhen you create a deployment using a template link URL (on `raw.githubusercontent.com` or Azure Blob Storage) you have access to a `templateLink` property on [the Deployment model](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-template-functions-deployment#deployment \"Deployment functions for Azure Resource Manager templates\")\n\nIf you are using public urls, you can just use the `uri()` function for nested templates.\n\n`\"msiTemplate\": \"[uri(deployment().properties.templateLink.uri, 'dl-msi.json')]`\n\nIf you want to [secure the templates using Azure Blob Storage SAS Tokens](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-powershell-sas-token \"Deploy private Resource Manager template with SAS token and Azure PowerShell\"), you can use some String functions to pull the SAS token out of the TemplateLink property.\n\nFor example:\n\n```json\n\"variables\": {\n  \"templateRoot\":\"[deployment().properties.templateLink.uri]\",\n  \"hasToken\":\"[not(equals(indexOf(variables('templateRoot'),'?'), -1))]\",\n  \"sasToken\":\"[if(variables('hasToken'),substring(variables('templateRoot'),indexOf(variables('templateRoot'),'?')),'')]\",\n  \"msiTemplate\": \"[concat(uri(deployment().properties.templateLink.uri, 'dl-msi.json'), variables('sasToken'))]\",\n}\n\n```\n\nNote that this example supports both public and access token URLs, which adds complexity with conditional statements. I tried to keep it as simple as possible.\n\nThis practice assumes that you are deploying the templates before running any deployments. This does not work with local files or inline JSON deployments.","categories":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/categories/Azure/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"ARM","slug":"ARM","permalink":"https://westerndevs.com/tags/ARM/"},{"name":"Configuration-as-Code","slug":"Configuration-as-Code","permalink":"https://westerndevs.com/tags/Configuration-as-Code/"}]},{"title":"Debug PHP Inside A Container with VSCode","authorId":"simon_timms","slug":"Debugging-php-in-container","date":"2019-07-05 23:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Debugging-php-in-container/","link":"","permalink":"https://westerndevs.com/_/Debugging-php-in-container/","excerpt":"Sometimes good things happen to bad people and sometimes bad things happen to good people. I'll let you decide which one me developing a PHP application is. Maybe it is a bit of a mixture. This particular PHP app was a bit long in the tooth (what PHP app isn't) and ran on full VMs. My first operation was was to get it running inside a docker container because I couldn't be sure that my Windows development environment would be representative of production, then I wanted to be able to debug it. This is the story of how to do that.","raw":"---\nlayout: post\ntitle: Debug PHP Inside A Container with VSCode\nauthorId: simon_timms\ndate: 2019-07-05 19:00\n\n---\n\nSometimes good things happen to bad people and sometimes bad things happen to good people. I'll let you decide which one me developing a PHP application is. Maybe it is a bit of a mixture. This particular PHP app was a bit long in the tooth (what PHP app isn't) and ran on full VMs. My first operation was was to get it running inside a docker container because I couldn't be sure that my Windows development environment would be representative of production, then I wanted to be able to debug it. This is the story of how to do that.\n\n<!-- more -->\n\nFortunately, I had some instructions on how to set up the VMs based on CentOS. Ah the good old days when installation instructions were written in word documents. The setup was a pretty standard LAMP stack and translated okay to a docker container. \n\n![LAMP stack - Linux, Apache, MySQL and PHP/Perl/Python](https://blog.simontimms.com/images/phpincontainer/lamp_stack.jpg)\n\nRunning the application was one thing, but I decided that what I really needed was the ability to attach a debugger to my PHP process. The PHP process inside the container. We are living in an age were VS Code is pretty amazing and it didn't fail to fulfill that promise this time. A recently announced feature in VS Code is the ability to split the front and back end of the editor. The persistence and language engines can run on a different machine from the user interface. In effect you're attaching to a remote sharing session except that the other end is inside the container. \n\nMind blown.\n\nTo set it up for PHP took a little bit it work. The first thing I did was install the xdebug extension for PHP. This I did by adding some lines to my Dockerfile.\n\n```dockerfile\n##### debugging environment #####\n# install pecl\nRUN yum -y install php-pear git\n# specific version of xdebug for the ancient php we're running\nRUN pecl install xdebug-2.2.4\n# add module to loaded modules\nRUN echo 'zend_extension=/usr/lib64/php/modules/xdebug.so' >> /etc/php.d/xdebug.ini\n# set up the environment\nENV XDEBUG_CONFIG \"remote_host=localhost remote_port=9000 remote_enable=1\"\n```\n\nThis installs xdebug and tells it to connect to the development environment found on port 9000 which is going to be the VS Code back-end running in the container. You can connect to any host using this so if you needed to debug on some remote machine you could have xdebug connect to your local machine no problem. \n\nNext I set up a `launch.json` in VS Code's `.vscode` directory which allowed VS Code to start debugging by listening to port 9000.\n\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Listen for XDebug\",\n            \"type\": \"php\",\n            \"request\": \"launch\",\n            \"port\": 9000\n        }\n    ]\n}\n```\n\nFinally I created a `.devcontainer/devcontainer.json` file which gives the remote extensions in VS Code knowledge of how to start up a container for development.\n\n```json\n{\n    \"name\": \"Portal\",\n    \"image\": \"portal\",\n    \"appPort\": [\"8080:80\"],\n    \"extensions\": [\n        \"felixfbecker.php-debug\"\n    ],\n    \"postCreateCommand\": \"git config --global core.autocrlf true\"\n}\n```\n\nThe container is to forward port 80 to my localhost 8080 so that I can use the web browser to connect to the site. Inside the container I want the debugger extension installed so I list it under extensions. Any additional extensions like `bmewburn.vscode-intelephense-client` could also be listed here. Finally because I'm running on Windows and copying the source code over to a Linux image I run a command to change the line endings in git. \n\nThe last step is to install the Remote Development extensions which is what allows VS Code to be split between machines. Once it is installed you'll be prompted to reopen the folder inside a container. \n\n![The prompt when you open VS Code](https://blog.simontimms.com/images/phpincontainer/launch_in_container.png)\n\nIf you click reopen in container VS code restarts with the back end running in the container. You can tell by looking at the bottom left corner of the editor. \n\n![Showing you're in a container](https://blog.simontimms.com/images/phpincontainer/in_container.png)\n\n\nWith that all set up I was able to add break points and actually intercept calls to the PHP code. \n\n![Hitting a breakpoint](https://blog.simontimms.com/images/phpincontainer/at_breakpoint.png)\n\nSuper-cool!","categories":[],"tags":[]},{"title":"Avoid Death With C# Compiler Directives","authorId":"justin_self","slug":"avoid-death-with-directives","date":"2019-06-14 08:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"happiness/avoid-death-with-directives/","link":"","permalink":"https://westerndevs.com/happiness/avoid-death-with-directives/","excerpt":"Compiler directives in C#: they should be avoided. If they aren't, and you're using them to compile different code based on build modes (like DEBUG or RELEASE), then listen closely and do what I do... it'll change your life.","raw":"---\nlayout: post\ntitle:  Avoid Death With C# Compiler Directives\ndate: 2019-06-13T23:30:00-05:00\ncategories: happiness\ncomments: true\nauthorId: justin_self\n---\nCompiler directives in C#: they should be avoided. If they aren't, and you're using them to compile different code based on build modes (like DEBUG or RELEASE), then listen closely and do what I do... it'll change your life.\n\n<!--more-->\n\nSimple scenario: You talk to another service that uses Azure AD. For development, you want to use a stub service that returns things and does things similar to the real one in Azure. Doesn't matter what it is. \n\nSome people may use IoC to handle this. Sometimes, that requires config switches. However, what if you want to be 100% sure that code can never be accidentally turned on in production? You may consider using a compiler directive.\n\n{% codeblock lang:csharp %}\n#if DEBUG\nDoUnsafeThingThatShouldNeverBeInProduction();\n#elif RELEASE\nDoItTheSafeWay();\n#endif\n{% endcodeblock %}\n\nDepending on the build mode, one of those methods will not be make it to the assembly. This prevents any accidents that could do the unsafe thing. Yes, there are other ways of doing this and typically those ways require process and convention. No, this is not fail safe is technically someone could accidentally change the build mode to DEBUG for a production release (sure, whatever).\n\n{% codeblock lang:csharp %}\nclass Program\n{\n    static void Main(string[] args)\n    {\n        HttpClient client;\n        \n        //weeping and gnashing of teeth\n#if RELEASE\n        client = ClientFactory.UseClientWithAzureAD();\n#elif DEBUG\n        client = new HttpClient();\n#endif\n        \n    }\n}\n{% endcodeblock %}\n\nThere's are problems, though, that can arise from using these directives. First, if you have your environment set to DEBUG and this is the only spot you use the `DoItTheSafeWay` method, looking for any usages using your IDE will result in ZERO INSTANCES! NONE! You'll spend 45 minutes trying to figure out how this thing is done in production because, like any normal person, you're using the Find References in your tool. \n\nBut NO! You won't find it. Your IDE simply laughs at you while you struggle knowing it must be used somehow. The IDE knows what's going on. It knows what you want but it decides to continue to hide this from you. So, you end up doing a damn regex search among all the files. The IDE knows it has be caught red handed trying to sabotage you and surfaces the files for you while sheepishly blaming Resharper for performance problems. I call it BS2019 for a reason (not always, generally I like VS).\n\nThe other problem is your trusty IDE will tell you that certain using statments are not being used and you should delete them or it will remind you with grayed out text or a colored dash on the scroll bar. You delete them, commit, push, and then find out the build failed because, in release mode, they are being used...\n\nSo, use the following code (or something similar):\n\n{% codeblock lang:csharp %}\npublic class RunMode\n{\n\n#if !DEBUG\n    private static bool _isRelease = true;\n#elif DEBUG\n    private static bool _isRelease = false;\n#endif\n\n    // ReSharper disable once ConvertToAutoProperty\n    public static bool IsRelease => _isRelease;\n}\n{% endcodeblock %}\n\nAnd then use it like this:\n\n{% codeblock lang:csharp %}\nclass Program\n{\n    static void Main(string[] args)\n    {\n        HttpClient client;\n\n        //Children laughter and happiness\n        if (RunMode.IsRelease)\n        { \n            client = ClientFactory.UseClientWithAzureAD();\n        }\n        else\n        {\n            client = new HttpClient();\n        }\n    }\n}\n{% endcodeblock %}\n\nYour experiences will vary, but reports of using this code show it has saved marriages, increased gas mileage and prevented the death of at least 2 dozen water fowl.\n","categories":[{"name":"happiness","slug":"happiness","permalink":"https://westerndevs.com/categories/happiness/"}],"tags":[]},{"title":"Using NodaTime with Dapper","authorId":"dave_paquette","slug":"using-noda-time-with-dapper","date":"2019-03-28 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/using-noda-time-with-dapper/","link":"","permalink":"https://westerndevs.com/Dapper/using-noda-time-with-dapper/","excerpt":"After my recent misadventures attempting to use Noda Time with Entity Framework Core, I decided to see what it would take to use Dapper in a the same scenario.","raw":"---\nlayout: post\ntitle: Using NodaTime with Dapper\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\n  - Noda Time\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2019/03/27/using-noda-time-with-dapper.aspx'\ndate: 2019-03-27 20:00:00\nexcerpt: After my recent misadventures attempting to use Noda Time with Entity Framework Core, I decided to see what it would take to use Dapper in a the same scenario.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n\nAfter my recent misadventures attempting to use Noda Time with [Entity Framework Core](https://www.davepaquette.com/archive/2019/03/26/using-noda-time-with-ef-core.aspx), I decided to see what it would take to use Dapper in a the same scenario.\n\n## A quick recap\nIn my app, I needed to model an `Event` that occurs on a particular date. It might be initially tempting to store the date of the event as a DateTime in UTC, but that's not necessarily accurate unless the event happens to be held at the Royal Observatory Greenwich. I don't want to deal with time at all, I'm only interested in the date the event is being held. \n\nNodaTime provides a `LocalDate` type that is perfect for this scenario so I declared a `LocalDate` property named `Date` on my `Event` class.\n\n{% codeblock lang:csharp %}\npublic class Event\n{\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public string Description { get; set; }\n    public LocalDate Date {get; set;}\n}\n{% endcodeblock %}\n\n## Querying using Dapper\n\nI modified my app to query for the `Event` entities using Dapper:\n\n{% codeblock lang:csharp %}\nvar queryDate = new LocalDate(2019, 3, 26);\nusing (var connection = new SqlConnection(myConnectionString))\n{\n    await connection.OpenAsync();\n    Events = await connection.QueryAsync<Event>(@\"SELECT [e].[Id], [e].[Date], [e].[Description], [e].[Name]\nFROM [Events] AS[e]\");\n}\n{% endcodeblock %}\n\nThe app started up just fine, but gave me an error when I tried to query for events.\n\n> System.Data.DataException: Error parsing column 1 (Date=3/26/19 12:00:00 AM - DateTime) ---> System.InvalidCastException: Invalid cast from 'System.DateTime' to 'NodaTime.LocalDate'.\n\nLikewise, if I attempted to query for events using a `LocalDate` parameter, I got another error:\n\n{% codeblock lang:csharp %}\nvar queryDate = new LocalDate(2019, 3, 26);\nusing (var connection = new SqlConnection(\"myConnectionString\"))\n{\n    await connection.OpenAsync();\n\n    Events = await connection.QueryAsync<Event>(@\"SELECT [e].[Id], [e].[Date], [e].[Description], [e].[Name]\nFROM [Events] AS[e]\nWHERE [e].[Date] = @Date\", new { Date = queryDate });\n}\n{% endcodeblock %}\n\n> NotSupportedException: The member Date of type NodaTime.LocalDate cannot be used as a parameter value\n\nFortunately, both these problems can be solved by implementing a simple `TypeHandler`.\n\n## Implementing a Custom Type Handler\n\nOut of the box, Dapper already knows how to map to the standard .NET types like Int32, Int64, string and DateTime. The problem we are running into here is that Dapper doesn't know anything about the `LocalDate` type. If you want to map to a type that Dapper doesn't know about, you can implement a custom type handler. To implement a type handler, create a class that inherits from `TypeHandler<T>`, where `T` is the type that you want to map to. In your type handler class, implement the `Parse` and `SetValue` methods. These methods will be used by Dapper when mapping to and from properties that are of type `T`. \n\nHere is an example of a type handler for `LocalDate`.\n\n{% codeblock lang:csharp %}\npublic class LocalDateTypeHandler : TypeHandler<LocalDate>\n{\n    public override LocalDate Parse(object value)\n    {\n        if (value is DateTime)\n        {\n            return LocalDate.FromDateTime((DateTime)value);\n        }\n\n        throw new DataException($\"Unable to convert {value} to LocalDate\");\n    }\n\n    public override void SetValue(IDbDataParameter parameter, LocalDate value)\n    {\n        parameter.Value = value.ToDateTimeUnspecified();\n    }\n}\n{% endcodeblock %}\n\nFinally, you need to tell Dapper about your new custom type handler. To do that, register the type handler somewhere in your application's startup class by calling `Dapper.SqlMapper.AddTypeHandler`.\n\n{% codeblock lang:csharp %}\nDapper.SqlMapper.AddTypeHandler(new LocalDateTypeHandler());\n{% endcodeblock %}\n\n## There's a NuGet for that\n\nAs it turns out, someone has already created a helpful NuGet package containing TypeHandlers for many of the NodaTime types so you probably don't need to write these yourself. Use the [Dapper.NodaTime package](http://mj1856.github.io/Dapper-NodaTime/) instead.\n\n## Wrapping it up\nTypeHandlers are a simple extension point that allows for Dapper to handle types that are not already handled by Dapper. You can write your own type handlers but you might also want to check if someone has already published a NuGet package that handles your types. ","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"},{"name":"Noda Time","slug":"Noda-Time","permalink":"https://westerndevs.com/tags/Noda-Time/"}]},{"title":"Using Noda Time with Entity Framework Core","authorId":"dave_paquette","slug":"using-noda-time-with-ef-core","date":"2019-03-26 11:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Entity-Framework-Core/using-noda-time-with-ef-core/","link":"","permalink":"https://westerndevs.com/Entity-Framework-Core/using-noda-time-with-ef-core/","excerpt":"Noda Time is a fantastic date/time library for .NET. I started using it recently and it really simplified the logic around handling dates. Unfortunately, I ran in to some problems with using Noda Time together with Entity Framework Core.","raw":"---\nlayout: post\ntitle: Using Noda Time with Entity Framework Core\ntags:\n  - Entity Framework\n  - Noda Time\ncategories:\n  - Entity Framework Core\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2019/03/26/using-noda-time-with-ef-core.aspx'\ndate: 2019-03-26 07:00:00\nexcerpt: Noda Time is a fantastic date/time library for .NET. I started using it recently and it really simplified the logic around handling dates. Unfortunately, I ran in to some problems with using Noda Time together with Entity Framework Core.\n---\nIf you have ever dealt dates/times in an environment that crosses time zones, you know who difficult it can be to handle all scenarios properly. This situation isn't made any better by .NET's somewhat limited representation of date and time values through the one `DateTime` class. For example, how to I represent a date in .NET when I don't care about the time. There is no type that represents a `Date` on it's own. That's why the [Noda Time](https://nodatime.org/) library was created, billing itself as a better date and time API for .NET.\n\n> Noda Time is an alternative date and time API for .NET. It helps you to think about your data more clearly, and express operations on that data more precisely.\n\n## An example using NodaTime\n\nIn my app, I needed to model an `Event` that occurs on a particular date. It might be initially tempting to store the date of the event as a DateTime in UTC, but that's not necessarily accurate unless the event happens to be held at the Royal Observatory Greenwich. I don't want to deal with time at all, I'm only interested in the date the event is being held. \n\nNodaTime provides a `LocalDate` type that is perfect for this scenario so I declared a `LocalDate` property named `Date` on my `Event` class.\n\n{% codeblock lang:csharp %}\npublic class Event\n{\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public string Description { get; set; }\n    public LocalDate Date {get; set;}\n}\n{% endcodeblock %}\n\n## Using Entity Framework\n\nThis app was using Entity Framework Core and there was a `DbSet` for the `Event` class.\n\n{% codeblock lang:csharp %}\npublic class EventContext : DbContext\n{\n    public EventContext(DbContextOptions<EventContext> options) : base(options)\n    {\n    }\n\n    public DbSet<Event> Events { get; set; }\n}\n{% endcodeblock %}\n\nThis is where I ran into my first problem. Attempting to run the app, I was greeted with a friendly `InvalidOperationException`:\n\n> InvalidOperationException: The property 'Event.Date' could not be mapped, because it is of type 'LocalDate' which is not a supported primitive type or a valid entity type. Either explicitly map this property, or ignore it using the '[NotMapped]' attribute or by using 'EntityTypeBuilder.Ignore' in 'OnModelCreating'.\n\nThis first problem was actually easy enough to solve using a [`ValueConverter`](https://docs.microsoft.com/ef/core/modeling/value-conversions). By adding the following `OnModelCreating` code to my `EventContext`, I was able to tell Entity Framework Core to store the `Date` property as a `DateTime` with the Kind set to [`DateTimeKind.Unspecified`](https://docs.microsoft.com/dotnet/api/system.datetimekind#System_DateTimeKind_Unspecified). This has the effect of avoiding any unwanted shifts in the date time based on the local time of the running process. \n\n{% codeblock lang:csharp %}\npublic class EventContext : DbContext\n{\n    public EventContext(DbContextOptions<EventContext> options) : base(options)\n    {\n    }\n\n    public DbSet<Event> Events { get; set; }\n\n    protected override void OnModelCreating(ModelBuilder modelBuilder)\n    {\n        base.OnModelCreating(modelBuilder);\n        var localDateConverter = \n            new ValueConverter<LocalDate, DateTime>(v =>  \n                v.ToDateTimeUnspecified(), \n                v => LocalDate.FromDateTime(v));\n\n        modelBuilder.Entity<Event>()\n            .Property(e => e.Date)\n            .HasConversion(localDateConverter);\n    }\n}\n{% endcodeblock %}\n\nWith that small change, my application now worked as expected. The value conversions all happen behind the scenes so I can just use the `Event` entity and deal strictly with the `LocalDate` type.\n\n## But what about queries\nI actually had this application running in a test environment for a week before I noticed a serious problem in the log files.\n\nIn my app, I was executing a simple query to retrieve the list of events for a particular date.\n\n{% codeblock lang:csharp %}\nvar queryDate = new LocalDate(2019, 3, 25);\nEvents = await context.Events.Where(e => e.Date == queryDate).ToListAsync();\n{% endcodeblock %}\n\nIn the app's log file, I noticed the following warning:\n\n> Microsoft.EntityFrameworkCore.Query:Warning: The LINQ expression 'where ([e].Date == __queryDate_0)' could not be translated and will be evaluated locally.\n\nUh oh, that sounds bad. I did a little more investigation and confirmed that the query was in fact executing SQL without a `WHERE` clause.\n\n{% codeblock lang:sql %}\nSELECT [e].[Id], [e].[Date], [e].[Description], [e].[Name]\nFROM [Events] AS [e]\n{% endcodeblock %}\n\nSo my app was retrieving **EVERY ROW** from `Events` table, then applying the where filter in the .NET process. That's really not what I intended to do and would most certainly cause me some performance troubles when I get to production.\n\nSo, the first thing I did was modified my EF Core configuration to throw an error when a client side evaluation like this occurs. I don't want this kind of thing accidently creeping in to this app again. Over in `Startup.ConfigureServices`, I added the following option to `ConfigureWarnings`. \n\n{% codeblock lang:csharp %}\nservices.AddDbContext<EventContext>(options =>\n        options.UseSqlServer(myConnectionString)\n        .ConfigureWarnings(warnings => \n            warnings.Throw(RelationalEventId.QueryClientEvaluationWarning)));\n{% endcodeblock %}\n\nThrowing an error by default is the correct behavior here and this is actually something that will be fixed in Entity Framework Core 3.0. The default behavior in EF Core 3 will be to throw an error any time a LINQ expression results in client side evaluation. You will then have the option to allow those client side evaluations. \n\n## Fixing the query\nNow that I had the app throwing an error for this query, I needed to find a way for EF Core to properly translate my simple `e.Date == queryDate` expression to SQL. After carefully re-reading the EF Core documentation related for value converters, I noticed a bullet point under Limitations:\n\n> Use of value conversions may impact the ability of EF Core to translate expressions to SQL. A warning will be logged for such cases. Removal of these limitations is being considered for a future release.\n\nWell that just plain sucks. It turns out that when you use a value converter for a property, Entity Framework Core just gives up trying to convert any LINQ expression that references that property. The only solution I found was to query for my entities using SQL.\n\n{% codeblock lang:csharp %}\nvar queryDate = new LocalDate(2019, 3, 25);\nEvents = await context.Events.\n    FromSql(@\"SELECT [e].[Id], [e].[Date], [e].[Description], [e].[Name]\nFROM[Events] AS[e]\nWHERE [e].[Date] = {0}\", queryDate.ToDateTimeUnspecified()).ToListAsync();\n{% endcodeblock %}\n\n## Wrapping it up\nNodaTime is a fantastic date and time library for .NET and you should definitely consider using it in your app. Unfortunately, Entity Framework Core has some serious limitations when it comes to using value converters so you will need to be careful. I almost got myself into some problems with it. While there are work-arounds, writing custom SQL for any query that references a NodaTime type is less than ideal. Hopefully those will be addressed in Entity Framework Core 3.  ","categories":[{"name":"Entity Framework Core","slug":"Entity-Framework-Core","permalink":"https://westerndevs.com/categories/Entity-Framework-Core/"}],"tags":[{"name":"Entity Framework","slug":"Entity-Framework","permalink":"https://westerndevs.com/tags/Entity-Framework/"},{"name":"Noda Time","slug":"Noda-Time","permalink":"https://westerndevs.com/tags/Noda-Time/"}]},{"title":"Optimistic Concurrency Tracking with Dapper and SQL Server","authorId":"dave_paquette","slug":"optimistic-concurrency-tracking-with-dapper-and-sql-server","date":"2019-03-20 13:45:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/optimistic-concurrency-tracking-with-dapper-and-sql-server/","link":"","permalink":"https://westerndevs.com/Dapper/optimistic-concurrency-tracking-with-dapper-and-sql-server/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we explore optimistic checks to ensure 2 users can't accidentally overwrite each other's updates to a particular row of data.","raw":"---\nlayout: post\ntitle: Optimistic Concurrency Tracking with Dapper and SQL Server\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2019/03/20/optimistic-concurrency-tracking-with-dapper-and-sql-server.aspx'\ndate: 2019-03-20 09:45:36\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we explore optimistic checks to ensure 2 users can't accidentally overwrite each other's updates to a particular row of data.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n  \nIn today's post, we explore a pattern to prevent multiple users (or processes) from accidentally overwriting each other's change. Given our [current implementation for updating the `Aircraft` record](https://www.davepaquette.com/archive/2019/02/04/basic-insert-update-delete-with-dapper.aspx), there is potential for data loss if there are multiple active sessions are attempting to update the same `Aircraft` record at the same time. In the example shown below, Bob accidentally overwrites Jane's changes without even knowing that Jane made changes to the same `Aircraft` record\n\n![Concurrent Updates](https://www.davepaquette.com/images/dapper/concurrency.png)\n\nThe pattern we will use here is [Optimistic Offline Lock](https://martinfowler.com/eaaCatalog/optimisticOfflineLock.html), which is often also referred to as Optimistic Concurrency Control.\n\n## Modifying the Database and Entities\nTo implement this approach, we will use a [rowversion column in SQL Server](https://docs.microsoft.com/sql/t-sql/data-types/rowversion-transact-sql). Essentially, this is a column that automatically version stamps a row in a table. Any time a row is modified, the `rowversion` column will is automatically incremented for that row. We will start by adding the column to our `Aircraft` table.\n\n{% codeblock lang:sql %}\nALTER TABLE Aircraft ADD RowVer rowversion\n{% endcodeblock %}\n\nNext, we add a `RowVer` property to the `Aircraft` table. The property is a `byte` array. When we read the `RowVer` column from the database, we will get an array of 8 bytes.  \n\n{% codeblock lang:csharp %}\npublic class Aircraft \n{\n    public int Id { get; set; }\n    public string Manufacturer {get; set;}\n    public string Model {get; set;}\n    public string RegistrationNumber {get; set;}\n    public int FirstClassCapacity {get; set;}\n    public int RegularClassCapacity {get; set;}\n    public int CrewCapacity {get; set;}\n    public DateTime ManufactureDate {get; set; }\n    public int NumberOfEngines {get; set;}\n    public int EmptyWeight {get; set;}\n    public int MaxTakeoffWeight {get; set;}\n    public byte[] RowVer { get; set; }\n}   \n{% endcodeblock %}\n\nFinally, we will modify the query used to load `Aircraft` entities so it returns the `RowVer` column. We don't need to change any of the Dapper code here.\n\n{% codeblock lang:csharp %}\npublic async Task<Aircraft> Get(int id)\n{ \n    Aircraft aircraft;\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var query = @\"\nSELECT \nId\n,Manufacturer\n,Model\n,RegistrationNumber\n,FirstClassCapacity\n,RegularClassCapacity\n,CrewCapacity\n,ManufactureDate\n,NumberOfEngines\n,EmptyWeight\n,MaxTakeoffWeight\n,RowVer\nFROM Aircraft WHERE Id = @Id\";\n        aircraft = await connection.QuerySingleAsync<Aircraft>(query, new {Id = id});\n    }\n    return aircraft;\n}\n{% endcodeblock %}\n\n## Adding the Concurrency Checks\nNow that we have the row version loaded in to our model, we need to add the checks to ensure that one user doesn't accidentally overwrite another users changes. To do this, we simply need to add the `RowVer` to the `WHERE` clause on the `UPDATE` statement. By adding this constraint to the `WHERE` clause, we we ensure that the updates will only be applied if the `RowVer` has not changed since this user originally loaded the `Aircraft` entity.\n\n{% codeblock lang:csharp %}\npublic async Task<IActionResult> Put(int id, [FromBody] Aircraft model)\n{\n    if (id != model.Id) \n    {\n        return BadRequest();\n    }\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var query = @\"\nUPDATE Aircraft \nSET  Manufacturer = @Manufacturer\n  ,Model = @Model\n  ,RegistrationNumber = @RegistrationNumber \n  ,FirstClassCapacity = @FirstClassCapacity\n  ,RegularClassCapacity = @RegularClassCapacity\n  ,CrewCapacity = @CrewCapacity\n  ,ManufactureDate = @ManufactureDate\n  ,NumberOfEngines = @NumberOfEngines\n  ,EmptyWeight = @EmptyWeight\n  ,MaxTakeoffWeight = @MaxTakeoffWeight\nWHERE Id = @Id\n      AND RowVer = @RowVer\";\n      \n      await connection.ExecuteAsync(query, model);\n    }\n\n    return Ok();\n}\n{% endcodeblock %}\n\nSo, the `WHERE` clause stops the update from happening, but how do we know if the update was applied successfully? We need to let the user know that the update was not applied due to a concurrency conflict. To do that, we add `OUTPUT inserted.RowVer` to the `UPDATE` statement. The effect of this is that the query will return the new value for the `RowVer` column if the update was applied. If not, it will return null. \n\n{% codeblock lang:csharp %}\npublic async Task<IActionResult> Put(int id, [FromBody] Aircraft model)\n{\n    byte[] rowVersion;\n    if (id != model.Id) \n    {\n        return BadRequest();\n    }\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var query = @\"\nUPDATE Aircraft \nSET  Manufacturer = @Manufacturer\n  ,Model = @Model\n  ,RegistrationNumber = @RegistrationNumber \n  ,FirstClassCapacity = @FirstClassCapacity\n  ,RegularClassCapacity = @RegularClassCapacity\n  ,CrewCapacity = @CrewCapacity\n  ,ManufactureDate = @ManufactureDate\n  ,NumberOfEngines = @NumberOfEngines\n  ,EmptyWeight = @EmptyWeight\n  ,MaxTakeoffWeight = @MaxTakeoffWeight\n  OUTPUT inserted.RowVer\nWHERE Id = @Id\n      AND RowVer = @RowVer\";\n        rowVersion = await connection.ExecuteScalarAsync<byte[]>(query, model);\n    }\n\n    if (rowVersion == null) {\n        throw new DBConcurrencyException(\"The entity you were trying to edit has changed. Reload the entity and try again.\"); \n    }\n    return Ok(rowVersion);\n}\n{% endcodeblock %}\n\nInstead of calling `ExecuteAsync`, we call `ExecuteScalarAsync<byte[]>`. Then we can check if the returned value is `null` and raise a `DBConcurrencyException` if it is null. If it is not null, we can return the new `RowVer` value. \n\n\n# Wrapping it up\nUsing SQL Server's `rowversion` column type makes it easy to implement optimistic concurrency checks in a .NET app that uses Dapper.\n\nIf you are building as REST api, you should really use the ETag header to represent the current RowVer for your entity. You can read more about this pattern [here](https://sookocheff.com/post/api/optimistic-locking-in-a-rest-api/).\n\n","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Durable Functions Analyzer","authorId":"simon_timms","slug":"durable_functions_analyzer","date":"2019-02-18 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"DurableFunctions/durable_functions_analyzer/","link":"","permalink":"https://westerndevs.com/DurableFunctions/durable_functions_analyzer/","excerpt":"When it was announced the Roslyn would become the default compiler for C# in Visual Studio I was super excited. I felt like it would generate all sorts of domain specific languages, custom flavors of C#, tons of custom error providers. So here we are 5 years later and almost none of it has come to pass. Why not?","raw":"---\nlayout: post\ntitle: Durable Functions Analyzer\nauthorId: simon_timms\ndate: 2019-02-17 19:00\ncategories:\n  - DurableFunctions\noriginalurl: https://blog.simontimms.com/2019/02/17/2019-02-17-durable_functions_analyzer/\n---\n\nWhen it was announced the Roslyn would become the default compiler for C# in Visual Studio I was [super excited](https://blog.simontimms.com/2014/04/04/roslyn-changes-everything/). I felt like it would generate all sorts of domain specific languages, custom flavors of C#, tons of custom error providers. So here we are 5 years later and almost none of it has come to pass. Why not?\n<!--more-->\n\nWell turns out the compiler stuff is kind of hard. It is just a bridge too far for people to do any of the cool things I thought they would do. I guess we can add this to the long list of things that I'm wrong about. \n\nBut a few weeks ago I broke some code in a durable function because I was returning the wrong shaped data. I didn't find out until the code was deployed which is obviously later than I wanted. Because of the way that Durable Functions were constructed favoring magic strings and Objects it tends to be more susceptible to bugs which you wouldn't normally see in a statically typed language. \n\nFor instance consider this code \n\n```csharp\n[FunctionName(\"HireEmployee\")]\npublic static async Task<Application> RunOrchestrator(\n    [OrchestrationTrigger] DurableOrchestrationContext context,\n    ILogger log)\n{\n    var applications = context.GetInput<List<Application>>();\n    var approvals = await context.CallActivityAsync<List<Application>>(\"ApplicationsFiltered\", Guid.NewGuid());\n    log.LogInformation($\"Approval received. {approvals.Count} applicants approved\");\n    return approvals.OrderByDescending(x => x.Score).First();\n}\n```\n\nThere are two magic strings in this code. The first is the name of the function in the annotation before the function declaration. The second is in the `CallActivityAsync` where a function called `ApplicationsFiltered` is called. If there is a typo in either of these strings the orchestration will fail at runtime.\n\nYou'll note too that we pass a Guid into that function. The function definition simply has an Object as the second argument so there is no type checking. Instead of passing in the Guid which is required there would be no compiler issues if we instead passed in a `Frog` or a `Puppy` or even an `int`.\n\nI started working on a Roslyn analyzer which could solve some, or all of these short comings. I won't get into the technical parts of how to build an analyzer here (although I did just submit a conference talk on that). \n\nIt produces warnings (for now) when your functions aren't used correctly. Right now it will detect \n\n* Incorrectly named functions\n* Incorrect return types from functions\n* Incorrect argument for functions\n* Orchestration annotations misapplied to arguments\n\nHere are some screenshots of it in action.\n\n![A misnamed function and a suggestion for what it should be called.](https://blog.simontimms.com/images/roslynanalyzer/poc.png)\nA misnamed function and a suggestion for what it should be called.\n\n![An incorrect argument being detected](https://blog.simontimms.com/images/roslynanalyzer/poc2.png)\nAn incorrect argument being detected\n\n\n![An incorrect return type being detected](https://blog.simontimms.com/images/roslynanalyzer/poc3.png)\nAn incorrect return type being detected\n\n![Orchestration trigger on the wrong data type](https://blog.simontimms.com/images/roslynanalyzer/poc4.png)\nOrchestration trigger on the wrong data type\n\nIf you want to try this out on your own project it is as easy as installing a [nuget package](https://www.nuget.org/packages/DurableFunctionsAnalyzer/). \n\nI'm looking for suggestions for new features or bugs in existing features. My tests are limited so any bug people can contribute will improve the product. Open an issue on [github](https://github.com/stimms/DurableFunctionsAnalyzer)\n\nNow I know how to build these analyzers I think I'll try to build more for internal applications. ","categories":[{"name":"DurableFunctions","slug":"DurableFunctions","permalink":"https://westerndevs.com/categories/DurableFunctions/"}],"tags":[]},{"title":"Durable Azure Functions vs. NServiceBus Sagas","authorId":"simon_timms","slug":"durablefunctions_sagas","date":"2019-02-15 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"DurableFunctions/durablefunctions_sagas/","link":"","permalink":"https://westerndevs.com/DurableFunctions/durablefunctions_sagas/","excerpt":"I've been on a bit of an Azure Functions kick over the last little while. I've blogged a bunch on Durable Functions and deployed a bunch more. When you're as old as me then you tend to draw comparisons between new technologies and existing ones. For instance I'm constantly telling people about how web pages are a lot like the cave paintings I use to do in my youth. The Twitter Exchange The technology that draws the closest comparison I've seen to Durable Functions are NServiceBus Sagas. A few weeks ago I tweeted out wondering if any body had done a comparison. The good folks at Particular stepped up and answered.","raw":"---\nlayout: post\ntitle: Durable Azure Functions vs. NServiceBus Sagas\nauthorId: simon_timms\ndate: 2019-02-14 19:00\ncategories:\n  - DurableFunctions\noriginalurl: https://blog.simontimms.com/2019/02/14/2019-02-14-durablefunctions_sagas/#more\n---\n\nI've been on a bit of an Azure Functions kick over the last little while. I've blogged a bunch on Durable Functions and deployed a bunch more. When you're as old as me then you tend to draw comparisons between new technologies and existing ones. For instance I'm constantly telling people about how web pages are a lot like the cave paintings I use to do in my youth. \n\n# The Twitter Exchange\n\nThe technology that draws the closest comparison I've seen to Durable Functions are NServiceBus Sagas. A few weeks ago I tweeted out wondering if any body had done a comparison. The good folks at Particular stepped up and answered. \n\n<!--more-->\n\n<blockquote class=\"twitter-tweet\" data-partner=\"tweetdeck\"><p lang=\"en\" dir=\"ltr\">Recently <a href=\"https://twitter.com/stimms?ref_src=twsrc%5Etfw\">@stimms</a> asked about comparisons between Azure Durable Functions and NServiceBus sagas. <a href=\"https://t.co/7sB5t9KMtH\">https://t.co/7sB5t9KMtH</a></p>&mdash; Particular Software (@ParticularSW) <a href=\"https://twitter.com/ParticularSW/status/1085212877034844160?ref_src=twsrc%5Etfw\">January 15, 2019</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nI'll reproduce the thread here so you don't have to click through and I'll add some comments.\n\n<div style=\"background: #f3f3f3\">\nBefore continuing, a quick disclaimer: Durable Functions are extremely new. We haven't fully assessed them yet. This is just a few engineers throwing thoughts against the wall. Our position is guaranteed to evolve over time. Now, on with the showâ€¦\n\nThere's definitely some similarities between durable functions and sagas. After all, an NServiceBus saga is essentially an NServiceBus message handler with durable state, and a durable function is an Azure Function with durable state.\n\nOne big difference is that a durable function is a non-message-bound orchestrator for other functions or processes. That means continuation of the durable function is driven by awaits, not by additional messages.\n</div>\n\nThis isn't entirely accurate. You can trigger durable function continuations using external events [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-external-events](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-external-events) but the mechanism is certainly less seamless than with NSB. You have to define a message handler and then raise the event. There is no nice way to find the running function like NSB does with Saga finding. [https://docs.particular.net/nservicebus/sagas/saga-finding](https://docs.particular.net/nservicebus/sagas/saga-finding)\n\n<div style=\"background: #f3f3f3\">\nA very cool part about durable functions is that they automatically checkpoint their progress whenever the function awaits. Local state is never lost if the process recycles or the VM reboots.\n\nBut this behavior is not free and does have consequences! The orchestrator stores the history of its past executions in Azure Storage, then executes the ENTIRE function again from the BEGINNING, skipping awaits already finished in previous runs.\n</div>\n\nYeah this is certainly something to be aware of. You need to be careful about how long orchestrations run for. There is a concept of eternal orchestrations but basically they throw away all the history and start over again. If you have a complex orchestration then you can break it up into sub-orchestrations. But again, this isn't as nice as NSB. \n\nOne thing it does afford is the ability to rewind an orchestration and play it again. \n\n<div style=\"background: #f3f3f3\">\nSo with durable functions you need to be careful that your orchestrator's implementation is deterministic. DateTime.Now, random numbers, Guids, etc. will get you into trouble! [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints)\n\nA Saga will typically focus on the interaction of one or two messages at a time. When thinking about a large process flowchart, one Saga will typically govern just the interactions at one point on the flowchart, not the entire process.\n\nOn the other hand, with a durable function, it's easy to have an entire process represented as one durable orchestrator function that can call other functions. This is both a strength (easily seeing the big picture) and a weakness (lots of coupling, can grow very complex).\n\nAdditionally, if you're trying to divide your system along service boundary lines (the vertical slices with independent databases mentioned in @MikhailShilkov's article) a giant orchestrator function covering an entire process flowchart makes this really difficult.\n</div>\nThis seems like another place where sub-orchestrations could come into play. Probably a better approach is to have a Durable Function simply drop messages on a queue which will trigger other functions which may or may not be durable. This provides the logical separation similar to what you'd get with a series of sagas in NSB.\n\n<div style=\"background: #f3f3f3\">\nA Saga on the other hand is a kind of \"policy\" object which statefully handles a few interrelated messages. It is completely isolated in its own vertical slice, and can communicate with other vertical slices in a lightly-coupled manner by publishing events.\n\nThe dangers of an all-encompassing durable function could be mitigated somewhat by doing all the real work in normal azure functions which are called from the orchestrator, and having the durable function be very lightweight, but this is a bit of a slippery slope.\n\nSpeaking of calling other functions, to do that you use the DurableOrchestrationContext's `CallActivityAsync<TResult>(\"FunctionName\")`. This requires the use of \"magic strings\" for the function names, definitely not refactoring-friendly. [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence)\n</div>\n\nThis is a limitation I've called out on twitter and in my durable functions talks. There are a few hacks you can use like leaning on `nameof` but they're not satisfactory. I've been working on a Roslyn Analyzer which will detect incorrect names, wrong parameters and broken return types. This is an addon nuget package, though, and not something that comes out of the box (unless I can convince the team to include it in the template). You can find it at [https://www.nuget.org/packages/DurableFunctionsAnalyzer](https://www.nuget.org/packages/DurableFunctionsAnalyzer) and eventually I'll get around to blogging the details.\n\n\n<div style=\"background: #f3f3f3\">\nThe capabilities of durable functions are impressive, but with an orchestrator function spanning multiple service boundaries and calling multiple other functions identified by magic strings in a command/control style, it would be VERY easy to end up with a distributed monolith.\n\nAlso, chaining functions together is itself a form of coupling, and this raises versioning challenges when changes to a system need to be made. [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-versioning](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-versioning)\n\nIn NServiceBus a Saga is a stateful object with some Handle(message) methods on it where that state is persisted to a database. Even timeouts are represented as messages.\n</div>\nThere is some really quite good unit testing advice at \n[https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-unit-testing](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-unit-testing) for Durable Functions but it certainly isn't as elegant as NSB's implementations. For complex orchestrations I think the tests would be quite hacky. \n<div style=\"background: #f3f3f3\">\nThe messages are strongly-typed classes. No magic strings. Message comes in, stored state is read, make some decisions, message(s) come out. Easy to test, easy to refactor. [https://docs.particular.net/nservicebus/testing/#testing-a-saga](https://docs.particular.net/nservicebus/testing/#testing-a-saga)\n\nAnd if anything goes wrong, the message rolls back to the queue and automatic retries kick in. This makes it easy for your system to recover from transient outages of databases or 3rd party APIs: [https://docs.particular.net/tutorials/quickstart/#transient-failures](https://docs.particular.net/tutorials/quickstart/#transient-failures)\n\nAlso, when reproducing a bug in your system, having the actual messages which caused the failure in an error queue is a huge help, not to mention being able to \"replay\" them in production to complete the business process for your customer: [https://docs.particular.net/tutorials/message-replay/](https://docs.particular.net/tutorials/message-replay/)\n</div>\n\nThis I can't argue with. I've reprocessed thousands of messages in NSB and the ability to do so has saved the companies for whom I've worked thousands of dollars and avoided angry customer.\n\nYou can rewind durable orchestrations after correcting the problem. However the lack of an error queue is really tricky. The restful APIs to list orchestrations and their status and potentially rewind them aren't scalable or generally usable. There is a serious lack of tooling here (start up idea, alert!). \n\n<div style=\"background: #f3f3f3\">\nOf course, this wouldn't be complete without our real-time performance monitoring capabilities. Keep an eye on message processing time, retries, and queues backing up - resolving issues before they hurt the business: [https://particular.net/real-time-monitoring](https://particular.net/real-time-monitoring)\n</div>\n\nI am pretty confident that you could build the equivalent of Service Pulse and Service Control on top of durable functions. If you were building a serious production system on top of Durable Functions I think you'd have to build some tooling in this space. That should be a cost consideration when you do your analysis of technologies to use.\n\n<div style=\"background: #f3f3f3\">\nBut it's naÃ¯ve to think that a system would have to be ALL sagas or ALL durable functions. Don't fall for the golden hammer fallacy, use each where it makes sense!\n\nAzure Functions can be really great integration glue to bridge from various Azure Services like Blobs, Tables, Event Grid, CosmosDB, etc. to NServiceBus sagas and from there to your core business logic.\n\nAzure functions are also great for small bits of infrastructure - we even recommend using them with NServiceBus to clean up data bus entries: [https://docs.particular.net/samples/azure/blob-storage-databus-cleanup-function/](https://docs.particular.net/samples/azure/blob-storage-databus-cleanup-function/)\n\nNo matter what, remember that Azure Durable Functions are in their infancy. Going \"all in\" may not be the best bet. Start small.\n</div>\n\nThis is totally accurate. I first used NSB in something like 2008 and even then it was a pretty mature product. Particular have built up a first class organization around NSB and you won't get better support anywhere. \n\n<div style=\"background: #f3f3f3\">\nAlso keep \"credit-card-driven development\" in mind. Azure Functions billing is all about actual usage. @troyhunt runs the haveibeenpwned API for pennies because the function is ridiculously efficient. You have to design for that, or you might get a bill you didn't expect.\n\nWeâ€™re still assessing Azure Functions, so if you are actively using them in production or even thinking about using them some day, weâ€™d love to get your input! Leave your comments here: [https://discuss.particular.net/t/azure-functions/872](https://discuss.particular.net/t/azure-functions/872)\n</div>\n\n# Other Aspects\n\nNo brief tweet thread can't quite capture the entirety of the comparison. Let me go through a few other thoughts I had about the comparison. \n\n## Platform Independence \n\nDurable Functions are highly coupled with Azure. This can be a good thing or a bad thing depending on your point of view. It speeds up development to have a lot of the decisions made for you already. Using existing Azure services together takes away from concerns about scalability and maintenance. However, if you ever need to migrate off Azure it would be a significant engineering effort. \n\nNSB on the other hand is an embarrassment of configuration options. You can use SQL transport, MSMQ, SQS, Service Bus and so on and so forth. You can host on windows hardware, on VMs, docker images on K8S on Linux or Windows. No matter your environment be it cloud or infrastructure based there is an NSB configuration which will work for you. There is an added burden of figuring out what the right solution is for your application. \n\n## Support\n\nI don't think the support aspect of NSB should be ignored either. Certainly there is a support system in place for Functions but it isn't going to be as good as the support you'll get from Particular. On multiple occasions I've been able to chat with the actual developers of a feature of NSB at some length. I think that's something that any users of NSB could get. I've had some discussions with the Functions team but I think that that is really through the benefit of being an MVP and being kind of vocal about Functions. \n\nCost is a factor too. I'm hesitant to bring this up because any cost comparison between free (which is Functions) and a paid product is bound to be one sided. You're going to have to pay for running your code one way or another. There isn't likely to be a huge price difference between running 10k messages through a cloud hosted NSB and Durable Functions. If you're a startup then there is free licensing available from Particular for NSB. I could certainly see an advantage for people in companies where buying licenses is harder than buying compute time on the cloud.  On the other hand I think that the advantages of using a mature framework such as NSB offer huge advantages in reducing bugs and maintaining a production environment. \n\n## Code Structure\n\nThe message based paradigm of NSB is another advantage. You could write your Durable functions in a similar way to use concrete messages but boy is it tempting not to. I've found myself using a lot of tuples to chuck messages around which really I shouldn't. The constraints that NSB puts around messages are helpful in establishing good engineering practices. You can follow the same practices in Durable Functions but you'd need a strong code review culture and coding guidelines to keep you honest. There is an opportunity here for some Roslyn Analyzers or even a framework on top of Durable Functions. \n\nOn the surface the await/async model of Durable Functions is really cool. The .NET Framework provides the syntactic framework for doing orchestrations so why not lean on it? Well because it is complicated looking. You end up defining your entire workflow in a single function and being able to trace though it in your mind is limiting. Message handlers let you focus on smaller parts of the system at a time which is helpful to those of us without giant brains. Any reasonably complex orchestration is liable to spread over 50 lines and to mix business logic and the plumbing of doing things like running fan-outs and chaining. \n\n## The Golden Hammer\n\nFunctions can be trigged though a lot of different mechanisms. One of those is HTTP which means you can run your entire web application inside of Functions. In the same project as your web API you can also put in place Durable Functions and they will just work. This provides a low friction way of doing background tasks or multi-step processes with checkpoints. But it also mixes a lot of code together and removes the nice separation of code you can get with deploying a bunch of NSB processes. At the moment I'm leaning towards the ease of being able to stand up Durable Functions inside your API code as outweighing the increased maintenance cost of mixing code up. However it is really something which we'll have to watch evolve as Durable Function products mature. \n\n## Scaling\n\nFinally I wanted to talk about the ease of doing things like scaling out. Particular have done a lot of work around ethereal instance as of late but for the longest time the handler processes were treated much more like pets than cattle. That mentality is totally different in the land of Functions where you don't even know what hardware is being used. Being able to transparently scale up and down is really nice. \n\n# Shut Up Already and Tell me What to Use\n\nI wish decisions were that easy. There are advantages to both systems and I'm sure I've missed a number of key aspects worth examining here. Durable Functions are great and I'm happy to see some competition for Particular who tend to fall very much on the slow and stable side of the spectrum as opposed to the move fast and break stuff side. Durable Functions have some growing up to do. The tooling around them isn't there yet and I don't expect that tooling is going to come out of Microsoft so much as it will come out of a vendor. \n\nIf such tooling does emerge then you'll be back in the same boat as having to pay Particular for their product which does have an impact on the cost equation. \n\nIf I were that ThoughtWorks Tech Radar thing then I would probably put Durable Functions in the `evaluate` quadrant while NSB would remain in the `adopt` quadrant. ","categories":[{"name":"DurableFunctions","slug":"DurableFunctions","permalink":"https://westerndevs.com/categories/DurableFunctions/"}],"tags":[]},{"title":"Accessing B2C Claims in an Azure Function","authorId":"simon_timms","slug":"Getting-b2c-claims-in-an-azure-function","date":"2019-02-14 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"B2C/Getting-b2c-claims-in-an-azure-function/","link":"","permalink":"https://westerndevs.com/B2C/Getting-b2c-claims-in-an-azure-function/","excerpt":"In a previous article I talked about how to authenticate your function application against Azure Active Directory Business to Consumer (which we're going to call B2C for the sake of my fingers). Chances are in your function you're going to want to get some of the information which is available as a claim from the bearer token. Here is how to do it.","raw":"---\nlayout: post\ntitle: Accessing B2C Claims in an Azure Function\nauthorId: simon_timms\ndate: 2019-02-13 19:00\ncategories:\n  - B2C\noriginalurl: https://blog.simontimms.com/2019/02/13/2019-02-12-Getting-b2c-claims-in-an-azure-function/\n---\n\nIn a [previous article](2019-01-30-Functions-aad-authentication) I talked about how to authenticate your function application against Azure Active Directory Business to Consumer (which we're going to call B2C for the sake of my fingers). Chances are in your function you're going to want to get some of the information which is available as a claim from the bearer token. Here is how to do it.\n\n<!--more-->\n\nOn the surface this seems like a really simple problem. After all you can take the bearer token you got and paste it into [jwt.io](https://jwt.io/) and get back all the information you want. However we should probably take some effort to validate that what's coming back is what was originally derived from the B2C login. We can do this by making use of the System.IdentityModel.Tokens.Jwt NugGet package along with a bunch of other cryptographic stuff from the framework.\n\nIn my case I'm most interested in the `emails` claim so I can send the user an e-mail. The first thing we need to do is get the Issuer Signing key from Azure B2C. The easiest way to do this is to use the json metadata endpoint your service provided. \n\n![The metadata URL for your B2C](https://blog.simontimms.com/images/functions-claims/metadata.png)\n\nIf you click on that link you'll get a document like \n\n```javascript\n{\n\tissuer: \"https://yourtenant.b2clogin.com/a04a23ad-1e6a-4114-a91b-b8f8f7ac9660/v2.0/\",\n\tauthorization_endpoint: \"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/authorize?p=b2c_1_siupin\",\n\ttoken_endpoint: \"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/token?p=b2c_1_siupin\",\n\tend_session_endpoint: \"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/logout?p=b2c_1_siupin\",\n\tjwks_uri: \"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/discovery/v2.0/keys?p=b2c_1_siupin\",\n\tresponse_modes_supported: [\n\t\t\"query\",\n\t\t\"fragment\",\n\t\t\"form_post\"\n\t],\n\tresponse_types_supported: [\n\t\t\"code\",\n\t\t\"code id_token\",\n\t\t\"code token\",\n\t\t\"code id_token token\",\n\t\t\"id_token\",\n\t\t\"id_token token\",\n\t\t\"token\",\n\t\t\"token id_token\"\n\t],\n\tscopes_supported: [\n\t\t\"openid\"\n\t],\n\tsubject_types_supported: [\n\t\t\"pairwise\"\n\t],\n\tid_token_signing_alg_values_supported: [\n\t\t\"RS256\"\n\t],\n\ttoken_endpoint_auth_methods_supported: [\n\t\t\"client_secret_post\",\n\t\t\"client_secret_basic\"\n\t],\n\tclaims_supported: [\n\t\t\"emails\",\n\t\t\"idp\",\n\t\t\"name\",\n\t\t\"sub\",\n\t\t\"tfp\"\n\t]\n}\n```\n\nIn this document is the `jwks_uri` which, if you click on it will give you something like this (keys altered to protect the innocent)\n\n```javascript\n\n{\n\tkeys: [{\n\t\tkid: \"X5eXjf93njNFum1kl2Ytv8dlNP4-c57dO6QGTVBwaNk\",\n\t\tnbf: 1493764781,\n\t\tuse: \"sig\",\n\t\tkty: \"RSA\",\n\t\te: \"AQAB\",\n\t\tn: \"tVKUtcx_n9rt5afY_2WFNvU6PlFMggCatsZ3l4RjKxH0jgdLqab345de3ZGXYbPzXvmmLiWZizpb-h0qup5jznOvOr-Dhw9908584BSgC83YacjWNqEK3urxhyE2jWjwRm2N95WGgb5mzE5XmZIvkvyXnn7X8dvgFPF5QwIngGsDG8LyHuJWlaDhr_EPLMW4wHvH0zZCuRMARIJmmqiMy3VD4ftq4nS5s8vJL0pVSrkuNojtokp84AtkADCDU_BUhrc2sIgfnvZ03koCQRoZmWiHu86SuJZYkDFstVTVSR0hiXudFlfQ2rOhPlpObmku68lXw-7V-P7jwrQRFfQVXw\"\n\t}]\n}\n```\n\nThe information in here is what's needed to decode the JWT and validate it. The keys here can rotate so if you're going to cache the information you'll want to make sure it expires with some frequency. I've seen an hour recommended but your mileage may vary. I followed the very helpful post [here](https://stackoverflow.com/a/47390593/361) for how to consume this information. \n\nIn my function I grabbed the bearer token out of the request and passed it \n\n```csharp\nvar bearerToken = request.Headers[\"Authorization\"].ToString().Split(' ').Last();\nvar json = await client.GetStringAsync(configService.TokenMetadataEndpoint()); //client is a static HTTP Client\nJwtSecurityTokenHandler handler = new JwtSecurityTokenHandler();\nvar claims = handler.ValidateToken(bearerToken,\n    new TokenValidationParameters\n    {\n        ValidateAudience = true,\n        ValidateIssuer = true,\n        ValidateLifetime = true,\n        ValidIssuer =  configService.TokenIssuer(),\n        ValidAudience = configService.TokenAudience(),\n        IssuerSigningKeys = GetSecurityKeys(JsonConvert.DeserializeObject<JsonWebKeySet>(json))\n    },\n    out var validatedToken).Claims;\n```\n\nThe token audience is the ID of the API application in Azure B2C. The token issuer I had trouble figuring out, in the end the only place I could find it was in a known JWT that I decoded. For me it ended up being `https://yourtenant.b2clogin.com/a04a23ad-1e6a-4114-a91b-b8f8f7ac9660/v2.0/`. I'm not sure if that GUID in there changes from tenant to tenant but certainly the host name in the URL will change to yours. We want to make sure to validate that the JWT hasn't expired (lifetime) that it was issued by our B2C instance (issuer) and that it was intended for this application (audience).\n\nThe claims object here will contain all of the claims for the JWT. I'm interested in `emails` so I run\n\n```csharp\nvar email = claims.Single(x => x.Type == \"emails\");\n```\n\nGetting the security keys is done via this handy function which deserializes the keys into a list of SecurityKeys. \n\n```csharp\nprivate static List<SecurityKey> GetSecurityKeys(JsonWebKeySet jsonWebKeySet)\n    {\n        var keys = new List<SecurityKey>();\n\n        foreach (var key in jsonWebKeySet.Keys)\n        {\n            if (key.Kty == \"RSA\")\n            {\n                if (key.X5C != null && key.X5C.Length > 0)\n                {\n                    string certificateString = key.X5C[0];\n                    var certificate = new X509Certificate2(Convert.FromBase64String(certificateString));\n\n                    var x509SecurityKey = new X509SecurityKey(certificate)\n                    {\n                        KeyId = key.Kid\n                    };\n\n                    keys.Add(x509SecurityKey);\n                }\n                else if (!string.IsNullOrWhiteSpace(key.E) && !string.IsNullOrWhiteSpace(key.N))\n                {\n                    byte[] exponent = Base64UrlDecode(key.E);\n                    byte[] modulus = Base64UrlDecode(key.N);\n\n                    var rsaParameters = new RSAParameters\n                    {\n                        Exponent = exponent,\n                        Modulus = modulus\n                    };\n\n                    var rsaSecurityKey = new RsaSecurityKey(rsaParameters)\n                    {\n                        KeyId = key.Kid\n                    };\n\n                    keys.Add(rsaSecurityKey);\n                }\n                else\n                {\n                    throw new Exception(\"JWK data is missing in token validation\");\n                }\n            }\n            else\n            {\n                throw new NotImplementedException(\"Only RSA key type is implemented for token validation\");\n            }\n        }\n\n        return keys;\n    }\n}\n```\n\nThe final pieces are the models\n\n```csharp\n//Model the JSON Web Key Set\n    public class JsonWebKeySet\n    {\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"keys\", Required = Required.Default)]\n        public JsonWebKey[] Keys { get; set; }\n    }\n\n\n    //Model the JSON Web Key object\n    public class JsonWebKey\n    {\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"kty\", Required = Required.Default)]\n        public string Kty { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"use\", Required = Required.Default)]\n        public string Use { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"kid\", Required = Required.Default)]\n        public string Kid { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"x5t\", Required = Required.Default)]\n        public string X5T { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"e\", Required = Required.Default)]\n        public string E { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"n\", Required = Required.Default)]\n        public string N { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"x5c\", Required = Required.Default)]\n        public string[] X5C { get; set; }\n\n        [JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = \"alg\", Required = Required.Default)]\n        public string Alg { get; set; }\n    }\n```\n\nand the Bas64Decoder\n\n```csharp\nstatic byte[] Base64UrlDecode(string arg)\n{\n    string s = arg;\n    s = s.Replace('-', '+'); // 62nd char of encoding\n    s = s.Replace('_', '/'); // 63rd char of encoding\n    switch (s.Length % 4) // Pad with trailing '='s\n    {\n        case 0: break; // No pad chars in this case\n        case 2: s += \"==\"; break; // Two pad chars\n        case 3: s += \"=\"; break; // One pad char\n        default:\n            throw new System.Exception(\n        \"Illegal base64url string!\");\n    }\n    return Convert.FromBase64String(s); // Standard base64 decoder\n}\n```\n\nAs with all things security related this stuff is confusing and convoluted. Hopefully this post will help out somebody in the future.","categories":[{"name":"B2C","slug":"B2C","permalink":"https://westerndevs.com/categories/B2C/"}],"tags":[]},{"title":"Managing Database Transactions in Dapper","authorId":"dave_paquette","slug":"managing-transactions-in-dapper","date":"2019-02-06 12:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/managing-transactions-in-dapper/","link":"","permalink":"https://westerndevs.com/Dapper/managing-transactions-in-dapper/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we explore a more complex write operation that requires us to manage a database transaction.","raw":"---\nlayout: post\ntitle: Managing Database Transactions in Dapper\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2019/02/06/managing-transactions-in-dapper.aspx'\ndate: 2019-02-06 7:00:00\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we explore a more complex write operation that requires us to manage a database transaction.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n  \nIn today's post, we explore a more complex scenario that involves executing multiple _write_ operations. In order to ensure consistency at the database level, these operations should all succeed / fail together as a single transaction. In this example, we will be inserting a new `ScheduledFlight` entity along with an associated set of `Flight` entities.\n\n As a quick reminder, a `Flight` represents a particular occurrence of a `ScheduledFlight` on a particular day. That is, it has a reference to the `ScheduledFlight` along with some properties indicating the scheduled arrival and departure times. \n\n\n{% codeblock lang:csharp %}\npublic class Flight \n{\n    public int Id {get; set;}\n    public int ScheduledFlightId {get; set;}\n    public ScheduledFlight ScheduledFlight { get; set;}\n    public DateTime Day {get; set;}\n    public DateTime ScheduledDeparture {get; set;}\n    public DateTime ScheduledArrival {get; set;}\n}\n{% endcodeblock %}\n\n{% codeblock lang:csharp %}\npublic class ScheduledFlight \n{\n    public int Id {get; set;}\n    public string FlightNumber {get; set;}\n\n    public int DepartureAirportId {get; set;}\n    public Airport DepartureAirport {get; set;}\n    public int DepartureHour {get; set;}\n    public int DepartureMinute {get; set;}\n\n    public int ArrivalAirportId {get; set;}\n    public Airport ArrivalAirport {get; set;}        \n    public int ArrivalHour {get; set;}\n    public int ArrivalMinute {get; set;}\n\n    public bool IsSundayFlight {get; set;}\n    public bool IsMondayFlight {get; set;}\n    // Some other properties\n}\n{% endcodeblock %}\n\n## Inserting the ScheduledFlight\nInserting the `ScheduledFlight` and retrieving the database generated id is easy enough. We can use the same approach we used in the [previous blog post](https://www.davepaquette.com/archive/2019/02/04/basic-insert-update-delete-with-dapper.aspx).\n\n{% codeblock lang:csharp %}\n// POST api/scheduledflight\n[HttpPost()]\npublic async Task<IActionResult> Post([FromBody] ScheduledFlight model)\n{\n    int newScheduledFlightId;\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var insertScheduledFlightSql = @\"\nINSERT INTO [dbo].[ScheduledFlight]\n    ([FlightNumber]\n    ,[DepartureAirportId]\n    ,[DepartureHour]\n    ,[DepartureMinute]\n    ,[ArrivalAirportId]\n    ,[ArrivalHour]\n    ,[ArrivalMinute]\n    ,[IsSundayFlight]\n    ,[IsMondayFlight]\n    ,[IsTuesdayFlight]\n    ,[IsWednesdayFlight]\n    ,[IsThursdayFlight]\n    ,[IsFridayFlight]\n    ,[IsSaturdayFlight])\nVALUES\n    (@FlightNumber\n    ,@DepartureAirportId\n    ,@DepartureHour\n    ,@DepartureMinute\n    ,@ArrivalAirportId\n    ,@ArrivalHour\n    ,@ArrivalMinute\n    ,@IsSundayFlight\n    ,@IsMondayFlight\n    ,@IsTuesdayFlight\n    ,@IsWednesdayFlight\n    ,@IsThursdayFlight\n    ,@IsFridayFlight\n    ,@IsSaturdayFlight);\nSELECT CAST(SCOPE_IDENTITY() as int)\";\n        newScheduledFlightId = await connection.ExecuteScalarAsync<int>(insertScheduledFlightSql, model);\n    }\n    return Ok(newScheduledFlightId);\n}\n\n{% endcodeblock %}\n\nAccording to the bosses at Air Paquette, whenever we create a new `ScheduledFlight` entity, we also want to generate the `Flight` entities for the next 12 months of that `ScheduledFlight`. We can add a method to the `ScheduledFlight` class to generate the flight entities.\n\n**NOTE:** Let's just ignore the obvious bugs related to timezones and to flights that take off and land on a different day.\n\n{% codeblock lang:csharp %}\npublic IEnumerable<Flight> GenerateFlights(DateTime startDate, DateTime endDate)\n{\n    var flights = new List<Flight>();\n    var currentDate = startDate;\n\n    while (currentDate <= endDate)\n    {\n        if (IsOnDayOfWeek(currentDate.DayOfWeek))\n        {\n            var departureTime = new DateTime(currentDate.Year, currentDate.Month, currentDate.Day, DepartureHour, DepartureMinute, 0);\n            var arrivalTime = new DateTime(currentDate.Year, currentDate.Month, currentDate.Day, ArrivalHour, ArrivalMinute, 0);\n            var flight = new Flight\n            {\n                ScheduledFlightId = Id,\n                ScheduledDeparture = departureTime,\n                ScheduledArrival = arrivalTime,\n                Day = currentDate.Date\n            };\n            flights.Add(flight);\n        }\n        currentDate = currentDate.AddDays(1);\n    }\n    return flights;\n}\npublic bool IsOnDayOfWeek(DayOfWeek dayOfWeek)\n{\n    return     (dayOfWeek == DayOfWeek.Sunday && IsSundayFlight)\n            || (dayOfWeek == DayOfWeek.Monday && IsMondayFlight)\n            || (dayOfWeek == DayOfWeek.Tuesday && IsTuesdayFlight)\n            || (dayOfWeek == DayOfWeek.Wednesday && IsWednesdayFlight)\n            || (dayOfWeek == DayOfWeek.Thursday && IsThursdayFlight)\n            || (dayOfWeek == DayOfWeek.Friday && IsFridayFlight)\n            || (dayOfWeek == DayOfWeek.Saturday && IsSaturdayFlight);\n}\n{% endcodeblock %}\n\nNow in the controller, we can add some logic to call the `GenerateFlight` method and then insert those `Flight` entities using Dapper.\n\n{% codeblock lang:csharp %}\n// POST api/scheduledflight\n[HttpPost()]\npublic async Task<IActionResult> Post([FromBody] ScheduledFlight model)\n{\n    int newScheduledFlightId;\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var insertScheduledFlightSql = @\"\nINSERT INTO [dbo].[ScheduledFlight]\n    ([FlightNumber]\n    ,[DepartureAirportId]\n    ,[DepartureHour]\n    ,[DepartureMinute]\n    ,[ArrivalAirportId]\n    ,[ArrivalHour]\n    ,[ArrivalMinute]\n    ,[IsSundayFlight]\n    ,[IsMondayFlight]\n    ,[IsTuesdayFlight]\n    ,[IsWednesdayFlight]\n    ,[IsThursdayFlight]\n    ,[IsFridayFlight]\n    ,[IsSaturdayFlight])\nVALUES\n    (@FlightNumber\n    ,@DepartureAirportId\n    ,@DepartureHour\n    ,@DepartureMinute\n    ,@ArrivalAirportId\n    ,@ArrivalHour\n    ,@ArrivalMinute\n    ,@IsSundayFlight\n    ,@IsMondayFlight\n    ,@IsTuesdayFlight\n    ,@IsWednesdayFlight\n    ,@IsThursdayFlight\n    ,@IsFridayFlight\n    ,@IsSaturdayFlight);\nSELECT CAST(SCOPE_IDENTITY() as int)\";\n        newScheduledFlightId = await connection.ExecuteScalarAsync<int>(insertScheduledFlightSql, model);\n\n        model.Id = newScheduledFlightId;\n        var flights = model.GenerateFlights(DateTime.Now, DateTime.Now.AddMonths(12));\n\n    var insertFlightsSql = @\"INSERT INTO [dbo].[Flight]\n    ([ScheduledFlightId]\n    ,[Day]\n    ,[ScheduledDeparture]\n    ,[ActualDeparture]\n    ,[ScheduledArrival]\n    ,[ActualArrival])\nVALUES\n    (@ScheduledFlightId\n    ,@Day\n    ,@ScheduledDeparture\n    ,@ActualDeparture\n    ,@ScheduledArrival\n    ,@ActualArrival)\";\n\n        await connection.ExecuteAsync(insertFlightsSql, flights);\n\n    }\n    return Ok(newScheduledFlightId);\n}\n\n{% endcodeblock %}\n\nNote that we passed in an `IEnumerable<Flight>` as the second argument to the `ExecuteAsync` method. This is a handy shortcut in Dapper for executing a query multiple times. Instead of writing a loop and calling `ExecuteAsync` for each flight entity, we can pass in a list of flights and Dapper will execute the query once for each item in the list.\n\n## Explicitly managing a transaction\nSo far, we have code that first inserts a `ScheduledFlight`, next generates a set of `Flight` entities and finally inserting all of those `Flight` entities. That's the happy path, but what happens if something goes wrong along the way. Typically when we execute a set of related write operations (inserts, updates and deletes), we want those operations to all succeed or fail together. In the database world, we have transactions to help us with this.\n\nThe nice thing about using Dapper is that it uses standard .NET database connections and transactions. There is no need to re-invent the wheel here, we can simply use the transaction patterns that have been around in .NET since for nearly 2 decades now.\n\nAfter opening the connection, we call `connection.BeginTransaction()` to start a new transaction. Whenever we call `ExecuteAsync` (or any other Dapper extension method), we need to pass in that transaction. At the end of all that work, we call `transaction.Commit()`. Finally, we wrap the logic in a `try / catch` block. If any exception is raised, we call `transaction.Rollback()` to ensure that none of those write operations are committed to the database.\n\n{% codeblock lang:csharp %}\n[HttpPost()]\npublic async Task<IActionResult> Post([FromBody] ScheduledFlight model)\n{\n    int? newScheduledFlightId = null;\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var transaction = connection.BeginTransaction();\n\n        try\n        {\n            var insertScheduledFlightSql = @\"\nINSERT INTO [dbo].[ScheduledFlight]\n    ([FlightNumber]\n    ,[DepartureAirportId]\n    ,[DepartureHour]\n    ,[DepartureMinute]\n    ,[ArrivalAirportId]\n    ,[ArrivalHour]\n    ,[ArrivalMinute]\n    ,[IsSundayFlight]\n    ,[IsMondayFlight]\n    ,[IsTuesdayFlight]\n    ,[IsWednesdayFlight]\n    ,[IsThursdayFlight]\n    ,[IsFridayFlight]\n    ,[IsSaturdayFlight])\nVALUES\n    (@FlightNumber\n    ,@DepartureAirportId\n    ,@DepartureHour\n    ,@DepartureMinute\n    ,@ArrivalAirportId\n    ,@ArrivalHour\n    ,@ArrivalMinute\n    ,@IsSundayFlight\n    ,@IsMondayFlight\n    ,@IsTuesdayFlight\n    ,@IsWednesdayFlight\n    ,@IsThursdayFlight\n    ,@IsFridayFlight\n    ,@IsSaturdayFlight);\nSELECT CAST(SCOPE_IDENTITY() as int)\";\n            newScheduledFlightId = await connection.ExecuteScalarAsync<int>(insertScheduledFlightSql, model, transaction);\n\n            model.Id = newScheduledFlightId.Value;\n            var flights = model.GenerateFlights(DateTime.Now, DateTime.Now.AddMonths(12));\n\n            var insertFlightsSql = @\"INSERT INTO [dbo].[Flight]\n    ([ScheduledFlightId]\n    ,[Day]\n    ,[ScheduledDeparture]\n    ,[ActualDeparture]\n    ,[ScheduledArrival]\n    ,[ActualArrival])\nVALUES\n    (@ScheduledFlightId\n    ,@Day\n    ,@ScheduledDeparture\n    ,@ActualDeparture\n    ,@ScheduledArrival\n    ,@ActualArrival)\";\n\n            await connection.ExecuteAsync(insertFlightsSql, flights, transaction);\n            transaction.Commit();\n        }\n        catch (Exception ex)\n        { \n            //Log the exception (ex)\n            try\n            {\n                transaction.Rollback();\n            }\n            catch (Exception ex2)\n            {\n                // Handle any errors that may have occurred\n                // on the server that would cause the rollback to fail, such as\n                // a closed connection.\n                // Log the exception ex2\n            }\n            return StatusCode(500);\n        }\n    }\n    return Ok(newScheduledFlightId);\n}\n\n{% endcodeblock %}\n\nManaging database transactions in .NET is a deep but well understood topic. We covered the basic pattern above and showed how Dapper can easily participate in a transaction. To learn more about managing database transactions in .NET, check out these docs:\n- [SqlConnection.BeginTransaction](https://docs.microsoft.com/dotnet/api/system.data.sqlclient.sqlconnection.begintransaction)  \n- [Transactions in ADO.NET](https://docs.microsoft.com/dotnet/framework/data/adonet/transactions-and-concurrency)\n\n\n## Wrapping it up\nUsing transactions with Dapper is fairly straight forward process. We just need to tell Dapper what transaction to use when executing queries. Now that we know how to use transactions, we can look at some more advanced scenarios like adding concurrency checks to update operations to ensure users aren't overwriting each other's changes. ","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Basic Insert Update and Delete with Dapper","authorId":"dave_paquette","slug":"basic-insert-update-delete-with-dapper","date":"2019-02-04 12:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/basic-insert-update-delete-with-dapper/","link":"","permalink":"https://westerndevs.com/Dapper/basic-insert-update-delete-with-dapper/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we explore how easy it is to perform basic Insert, Update and Delete operations.","raw":"---\nlayout: post\ntitle: Basic Insert Update and Delete with Dapper\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2019/02/04/basic-insert-update-delete-with-dapper.aspx'\ndate: 2019-02-04 07:00:00\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we explore how easy it is to perform basic Insert, Update and Delete operations.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n  \nIn today's post, we explore how easy it is to perform basic Insert, Update and Delete operations using the same `Aircraft` entity that we used in [the first post in this series](https://www.davepaquette.com/archive/2018/01/22/loading-an-object-graph-with-dapper.aspx). Basically, instead of using Dapper's `QueryAsync` extension method that we used to retrieve data, we will use the `ExecuteAsync` method.\n\nAs a quick reminder, here is the `Aircraft` class:\n\n{% codeblock lang:csharp %}\npublic class Aircraft \n{\n    public int Id { get; set; }\n\n    public string Manufacturer {get; set;}\n\n    public string Model {get; set;}\n\n    public string RegistrationNumber {get; set;}\n\n    public int FirstClassCapacity {get; set;}\n\n    public int RegularClassCapacity {get; set;}\n\n    public int CrewCapacity {get; set;}\n\n    public DateTime ManufactureDate {get; set; }\n\n    public int NumberOfEngines {get; set;}\n}   \n{% endcodeblock %}\n\n**NOTE:** In these examples, I am ignoring some important aspects like validation. I want to focus specifically on the Dapper bits here but validation is really important. In a real-world scenario, you should be validating any data that is passed in to the server. I recommend using [Fluent Validation](https://fluentvalidation.net/).\n\n## Insert\nInserting a single new record is really easy. All we need to do is write an `INSERT` statement with parameters for each column that we want to set. \n\n{% codeblock lang:csharp %}\n[HttpPost()]\npublic async Task<IActionResult> Post([FromBody] Aircraft model)\n{\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var sqlStatement = @\"\nINSERT INTO Aircraft \n(Manufacturer\n,Model\n,RegistrationNumber\n,FirstClassCapacity\n,RegularClassCapacity\n,CrewCapacity\n,ManufactureDate\n,NumberOfEngines\n,EmptyWeight\n,MaxTakeoffWeight)\nVALUES (@Manufacturer\n,@Model\n,@RegistrationNumber\n,@FirstClassCapacity\n,@RegularClassCapacity\n,@CrewCapacity\n,@ManufactureDate\n,@NumberOfEngines\n,@EmptyWeight\n,@MaxTakeoffWeight)\";\n        await connection.ExecuteAsync(sqlStatement, model);\n    }\n    return Ok();\n}\n{% endcodeblock %}\n\nThe version of the `ExecuteAsync` method we used here accepts two parameters: a string containing the SQL statement to execute and an object containing the parameter values to bind to the statement. In this case, it is an instance of the `Aircraft` class which has properties with names matching the parameters defined in the `INSERT` statement.\n\nOur `Aircraft` table's `Id` column is an auto-incremented identity column. That means the primary key is generated by the database when the row is inserted. We will likely need to pass that value back to whoever called the API so they know how to retrieve the newly inserted `Aircraft`.\n\nAn easy way to get the generated `Id` is to add `SELECT CAST(SCOPE_IDENTITY() as int)` after the `INSERT` statement. The [`SCOPE_IDENTITY()`](https://docs.microsoft.com/en-us/sql/t-sql/functions/scope-identity-transact-sql?view=sql-server-2017) function returns the last identity value that was generated in any table in the current session and current scope. \n\nNow, since the SQL statement we are executing will be returning a single value (the generated id), we need to call `ExecuteScalarAsync<int>`. The `ExecuteScalarAsync` method executes a SQL statement that returns a single value whereas the `ExecuteAsync` method executes a SQL statement that does not return a value.\n\n{% codeblock lang:csharp %}\n[HttpPost()]\npublic async Task<IActionResult> Post([FromBody] Aircraft model)\n{\n    int newAircraftId;\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var sqlStatement = @\"\nINSERT INTO Aircraft \n(Manufacturer\n,Model\n,RegistrationNumber\n,FirstClassCapacity\n,RegularClassCapacity\n,CrewCapacity\n,ManufactureDate\n,NumberOfEngines\n,EmptyWeight\n,MaxTakeoffWeight)\nVALUES (@Manufacturer\n,@Model\n,@RegistrationNumber\n,@FirstClassCapacity\n,@RegularClassCapacity\n,@CrewCapacity\n,@ManufactureDate\n,@NumberOfEngines\n,@EmptyWeight\n,@MaxTakeoffWeight);\n\nSELECT CAST(SCOPE_IDENTITY() as int)\";\n        newAircraftId = await connection.ExecuteScalarAsync<int>(sqlStatement, model);\n    }\n    return Ok(newAircraftId);\n}\n{% endcodeblock %}\n\n## Update\nUpdating an existing entity is similar to inserting. All we need is a SQL statement containing an `UPDATE` statement that sets the appropriate columns. We also want to make sure we include a `WHERE` clause limiting the update only to the row with the specified `Id`.\n\nAgain, the parameters in the SQL statement match the names of the properties in our `Aircraft` class. All we need to do is call the `ExecuteAsync` method passing in the SQL statement and the `Aircraft` entity.\n\n{% codeblock lang:csharp %}\n// PUT api/aircraft/id\n[HttpPut(\"{id}\")]\npublic async Task<IActionResult> Put(int id, [FromBody] Aircraft model)\n{\n    if (id != model.Id) \n    {\n        return BadRequest();\n    }\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var sqlStatement = @\"\nUPDATE Aircraft \nSET  Manufacturer = @Manufacturer\n,Model = @Model\n,RegistrationNumber = @RegistrationNumber \n,FirstClassCapacity = @FirstClassCapacity\n,RegularClassCapacity = @RegularClassCapacity\n,CrewCapacity = @CrewCapacity\n,ManufactureDate = @ManufactureDate\n,NumberOfEngines = @NumberOfEngines\n,EmptyWeight = @EmptyWeight\n,MaxTakeoffWeight = @MaxTakeoffWeight\nWHERE Id = @Id\";\n        await connection.ExecuteAsync(sqlStatement, model);\n    }\n    return Ok();\n}\n{% endcodeblock %}\n\n## Delete\n\nDeleting an entity is the easiest of the three operations since it only requires a single parameter: the unique Id to identify the entity being deleted. The SQL statement is a simple `DELETE` with a `WHERE` clause on the `Id` column. To execute the delete, call the `ExecuteAsync` method passing in the SQL statement and an anonymous object containing the `Id` to delete.\n\n{% codeblock lang:csharp %}\n// DELETE api/aircraft/id\n[HttpDelete(\"{id}\")]\npublic async Task<IActionResult> Delete(int id)\n{\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var sqlStatement = \"DELETE Aircraft WHERE Id = @Id\";\n        await connection.ExecuteAsync(sqlStatement, new {Id = id});\n    }\n    return Ok();\n}\n{% endcodeblock %}\n\nI really appreciate how simple delete is using Dapper. When using Entity Framework, delete requires you to first fetch the existing entity, then delete it. That requires 2 round trips to the database while the approach we used here only requires a single round trip.\n\n## Wrapping it up\nBasic insert, update and delete operations are easy to implement using Dapper. Real world scenarios are often a little more complex and we will dig into some of those scenarios in future posts:\n- Bulk inserts, updates and deletes\n- Managing transactions\n- Optimistic concurrency checks\n","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Running a single instance of a durable function","authorId":"simon_timms","slug":"Running-a-single-durable-function","date":"2019-01-31 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Running-a-single-durable-function/","link":"","permalink":"https://westerndevs.com/_/Running-a-single-durable-function/","excerpt":"I have a durable functions project which orchestrates several thousand function calls the purpose of which is to scrape and load a bunch of data. It is scheduled to run once a day but one of my concerns was that I didn't want to accidentally run to functions at the same time. They would duplicate a bunch of the data loading and, at least until the function ran again, chaos would reign. I'm not a huge fan of chaos reigning so I set out to find a way around this.","raw":"layout: post\ntitle: Running a single instance of a durable function\nauthorId: simon_timms\ndate: 2019-01-31 9:00\noriginalurl: https://blog.simontimms.com/2019/01/31/2019-01-31-Running-a-single-durable-function/\n---\n\nI have a durable functions project which orchestrates several thousand function calls the purpose of which is to scrape and load a bunch of data. It is scheduled to run once a day but one of my concerns was that I didn't want to accidentally run to functions at the same time. They would duplicate a bunch of the data loading and, at least until the function ran again, chaos would reign. I'm not a huge fan of chaos reigning so I set out to find a way around this. \n\n<!--More-->\n\nIf you've ever launched a durable function from the default HTTP triggered template you might have noticed that the JSON returned contains a number of interesting looking urls.\n\n```javascript\n\n{\n\t\"id\": \"ad6b8019-b356-4099-b70c-a2b503168db3\",\n\t\"statusQueryGetUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3?taskHub=DataIngest&connection=Storage&code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA==\",\n\t\"sendEventPostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/raiseEvent/{eventName}?taskHub=DataIngest&connection=Storage&code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA==\",\n\t\"terminatePostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/terminate?reason={text}&taskHub=DataIngest&connection=Storage&code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA==\",\n\t\"rewindPostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/rewind?reason={text}&taskHub=DataIngest&connection=Storage&code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA==\"\n}\n```\n\nThe one we're interested in there is the `statusQueryGetUri`. Poking at that gets you something like \n\n```javascript\n{\n\t\"instanceId\": \"ad6b8019-b356-4099-b70c-a2b503168db3\",\n\t\"runtimeStatus\": \"Completed\",\n\t\"input\": \"ad6b8019-b356-4099-b70c-a2b503168db3\",\n\t\"customStatus\": null,\n\t\"output\": null,\n\t\"createdTime\": \"2019-01-30T23:09:20Z\",\n\t\"lastUpdatedTime\": \"2019-01-30T23:10:25Z\"\n}\n```\n\nNotice that there is a `runtimeStatus` field there which reports the status of the orchestration. If we could look at the running orchestrations then perhaps we could see if there is already one running. Playing with the status URL I found that you could see all the running instances by simply looking at `http://localhost:7071/runtime/webhooks/durabletask/instances`.\n\nEven better than that the DurableOrchestrationClient used to start an orchestration has on it a function called GetStatusAsync which returns a list of all the running orchestrations currently running on the task hub. This means we can do something like \n\n```csharp\nvar statuses = await starter.GetStatusAsync();\nif (statuses.Any(x => x.RuntimeStatus == OrchestrationRuntimeStatus.Running && x.Name == \"OrchestrationName\"))\n{\n    log.LogWarning(\"An a running instance of the orchestration was detected. Terminating run.\");\n    return new HttpResponseMessage(System.Net.HttpStatusCode.Conflict);\n}\n```\n\nThis will prevent multiple instances of the orchestrator running. Note that we need to filter on orchestration name because it is possible the same task hub has multiple different orchestrations. \n\nIs this perfect? Oh heck no. There are lots of windows in which multiple orchestrations could be started. For my purposes, however, this works just fine. If your orchestrations are run more tightly than this then you might want to look at a distributed lock. My buddy Matt has a solution on the [Stackify blog](https://stackify.com/distributed-method-mutexing/) for this exact problem.","categories":[],"tags":[]},{"title":"Azure Functions and Azure B2C Authentication","authorId":"simon_timms","slug":"Functions-aad-authentication","date":"2019-01-30 18:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Functions-aad-authentication/","link":"","permalink":"https://westerndevs.com/_/Functions-aad-authentication/","excerpt":"I had a pretty good struggle setting up Azure Functions and Azure B2C to work together. There are a few guides out there but I wanted to put my own together because I had a terrible time finding these posts initially. The scenario here is that we want a single page application written in React to talk to an API hosted entirely in Azure Functions such that the functions are authenticated.","raw":"layout: post\ntitle: Azure Functions and Azure B2C Authentication\nauthorId: simon_timms\ndate: 2019-01-30 13:00\noriginalurl: https://blog.simontimms.com/2019/01/30/2019-01-30-Functions-aad-authentication/\n---\n\n\nI had a pretty good struggle setting up Azure Functions and Azure B2C to work together. There are a few guides out there but I wanted to put my own together because I had a terrible time finding these posts initially. The scenario here is that we want a single page application written in React to talk to an API hosted entirely in Azure Functions such that the functions are authenticated. \n\n<!--more-->\n\n## Azure\n\nFirst up you'll need to create a new tenant for Azure B2C. This is a weird two step process which I'm given to understand is going to be improved at some point in the near future. For now in the Azure Portal select `Create a resource` then `Azure B2C`. You'll be presented with these two options.\n\n![Creating a tenant](https://blog.simontimms.com/images/functions-aad/create_tenant.png)\n\nTo start you'll need to create a tenant, so pick the first one. You'll be presented with a few options for organization name and initial domain. These are specific to your organization so I'll leave them up to you but think on the names a little because they cannot be changed.\n\nWith the B2C tenant created you'll now need the second option to link an existing Azure AD B2C tenant to the Azure subscription. This will create a reference to your tenant in your main Azure subscription.\n\n![Linking a tenant](https://blog.simontimms.com/images/functions-aad/linking_tenant.png)\n\nManaging tenants is very confusing because you need to actually switch your Azure portal over to the new tenant. This is the least intuitive part of the process in my mind and shouldn't have been implemented this way. So click on the account selector in the top right of the portal to switch directories. Once you're in the new directory you'll see all the various resources you're use to. Don't both touching those, head straight over to Azure AD B2C in All resources. \n\nIn here you'll want to start in the Applications section. We'll be creating two applications and defining some claims between them. One application will be for the functions and the other for the SPA. You can expand this to more applications as your application grows. \n\nThe SPA should be the first one you create. I, creatively, called mine `SPA`. This one does not need an App ID URI but you should allow implicit flows and include web app/web API.\n\n![Properties for the SPA](https://blog.simontimms.com/images/functions-aad/spa_properties.png)\n\nThe reply URLs should contain a list of all the page from which a user can authenticate. Unfortunately, wildcards are not permitted here so you need to be explicit.\n\nWith that created we can now move onto the API app. This one is similar except that you'll want to put in place an App ID. Mine is simply called `API`\n\n![Properties for the API](https://blog.simontimms.com/images/functions-aad/api_properties.png)\n\nIn that screenshot you'll notice a reply URL. You likely don't have that yet so leave it blank. We'll come back to it. Now the API app will need to publish a scope. This is what ties your two apps (API and SPA) together. I created a single scope called `access` because I don't have anything too complex in my API. You can publish multiple scopes if you wish. \n\n![Scopes](https://blog.simontimms.com/images/functions-aad/scopes.png)\n\nPublishing scopes is half the equation, now back to the API application and select `API access`. In there select `Add` and select `API` in the first dropdown and then select both scopes in the second. Users who have authenticated against the SPA will now be able to access the API. \n\nWe're almost done in the B2C for now but we also want to set up a `User Flow`. This is a hosted login experience. You can customize the experience but that's outside of this tutorial. We want a basic Signup and Sign in policy. I named mine `SignupSignin` but you can pick whatever you like. Be sure to check the Email Signup identity provider. I also drilled into the `User attributes and claims` to select Email Address as a collected attribute and Email Addresses as a returned claim. This means that during the signup process we'll ask for the Email address and when authenticating in the SPA the JWT passed back will contain a collection of Email addresses. You can select more if you like.\n\n![Policy](https://blog.simontimms.com/images/functions-aad/policy.png)\n\nOnce the policy is created you'll want to select it and click `Run user flow` in there you'll find a metadata URL which will be used in your SPA application and functions. Make a note of it. While we're here also make a not of the ID of the API application. \n\nNow all this is created we're now ready to jump over to the function app you want to authenticate. Phew, that took a while.\n\nSwitch back to your primary directory and head over to your function app. If you don't have one created already just create a blank C# one. In the function app click through to the platform features and select Authentication.\n\n![Authentication](https://blog.simontimms.com/images/functions-aad/platform_features.png)\n\nIn authentication turn on `App Service Authentication` and select `Azure Active Directory`. Switch over to advanced and enter the API application Id in the Client ID field and the metadata URL in the Issuer Url field. These are the two things you made notes about before leaving the B2C. \n\n![Active Directory Auth](https://blog.simontimms.com/images/functions-aad/aad_settings.png)\n\nFinally change the `Action to take when request is not authenticated` over to `Log in with Azure Active Directory`. Save the authentication page. Now you'll need to make one final change over in the B2C. You'll need to find the URL for you function app. This is easily found by clicking on a function and running it. Take the URL and back in the B2C head to the API application. \n\nIn the API application you'll need to set the `Reply URL`. The URL should be of the format `https://<functionapp name>/.auth/login/aad/callback` so if you function app is called `bob` the result would be `https://bob/.auth/login/aad/callback`. With that in place we're done the Azure setup.\n\n## In React\n\nOpen up a terminal and install the handy package `react-aad-msal`. (Note: If you run into problems authenticating you might want to try the `react-aad-msal-jfm` package instead. It works around a new domain that Microsoft is using for B2C which isn't accepted by the `react-aad-msal` package.). This package provides some React components you can put on your page to do the actual authentication.\n\nIn a page I put in this control. I'm using Create ReactApp so the values for authority, clientID and scope are taken from a config file. The Authority is the Url for the policy so something like `https://yourtenant.b2clogin.com/tfp/yourtenant.onmicrosoft.com/B2C_1_SiUpIn`. The client Id is the ID of the SPA app from the B2C. Finally the scope is the fully qualified scope `https://yourtenant.onmicrosoft.com/API/access`\n\n```html\nimport { AzureAD, LoginType, MsalAuthProviderFactory } from 'react-aad-msal-jfm';\n...\n <AzureAD\n    provider={new MsalAuthProviderFactory({\n        authority: process.env.REACT_APP_AUTHORITY,\n        clientID: process.env.REACT_APP_CLIENT_ID,\n        scopes: [process.env.REACT_APP_SCOPE],\n        type: LoginType.Popup,\n        persistLoginPastSession: true,\n        validateAuthority: false\n\n    })}\n    unauthenticatedFunction={this.unauthenticatedFunction}\n    authenticatedFunction={this.authenticatedFunction}\n    userInfoCallback={this.userJustLoggedIn} />\n```\n\nWith this in place a login button is rendered. Click on the button and a popup will be shown with your login page. I saved off the bearer token from the `userInfoCallback` which is passed an object containing the `jwtAccessToken`. The emails we permitted as a claim are passed back as `idToken.emails`. My reducer looks a bit like \n\n```javascript\ncase getType(login.loginSuccess):\n    return {\n        ...state,\n        loggedIn: true,\n        bearerToken: action.payload.body.jwtAccessToken,\n        displayName: action.payload.body.user.name,\n        userId: action.payload.body.user.userIdentifier,\n        email: action.payload.body.user.idToken.emails[0]\n    };\n```\n\nThis jwtAccess token should be used in the headers of any fetches against the function app. \n\n```javascript\nlet result = await fetch(`${process.env.REACT_APP_API_URL}/Practices`, {\n            method: 'GET',\n            headers: {\n                'Authorization': `Bearer ${this.props.login.bearerToken}`\n            }\n        });\n```\n\n","categories":[],"tags":[]},{"title":"Paging Large Result Sets with Dapper and SQL Server","authorId":"dave_paquette","slug":"paging-large-result-sets-with-dapper-and-sql-server","date":"2019-01-29 01:15:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/paging-large-result-sets-with-dapper-and-sql-server/","link":"","permalink":"https://westerndevs.com/Dapper/paging-large-result-sets-with-dapper-and-sql-server/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we look at a way to page through large results sets.","raw":"---\nlayout: post\ntitle: Paging Large Result Sets with Dapper and SQL Server\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2019/01/28/paging-large-result-sets-with-dapper-and-sql-server.aspx'\ndate: 2019-01-28 20:15:42\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we look at a way to page through large results sets.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n  \nIn today's post, we explore paging through large result sets. Paging is a common technique that is used when dealing with large results sets. Typically, it is not useful for an application to request millions of records at a time because there is no efficient way to deal with all those records in memory all at once. This is especially true when rendering data on a grid in a user interface. The screen can only display a limited number of records at a time so it is generally a bad use of system resources to hold everything in memory when only a small subset of those records can be displayed at any given time.\n\n![Paged Table](https://www.davepaquette.com/images/dapper/paged_table_example.png)\n_Source: [AppStack Bootstrap Template](https://themes.getbootstrap.com/product/appstack-responsive-admin-template/)_\n\nModern versions of SQL Server support the [OFFSET / FETCH clause](https://docs.microsoft.com//sql/t-sql/queries/select-order-by-clause-transact-sql#using-offset-and-fetch-to-limit-the-rows-returned) to implement query paging.\n\nIn continuing with our airline theme, consider a `Flight` entity. A `Flight` represents a particular occurrence of a `ScheduledFlight` on a particular day. That is, it has a reference to the `ScheduledFlight` along with some properties indicating the scheduled arrival and departure times. \n\n\n{% codeblock lang:csharp %}\npublic class Flight \n{\n    public int Id {get; set;}\n    public int ScheduledFlightId {get; set;}\n    public ScheduledFlight ScheduledFlight { get; set;}\n    public DateTime Day {get; set;}\n    public DateTime ScheduledDeparture {get; set;}\n    public DateTime ScheduledArrival {get; set;}\n}\n{% endcodeblock %}\n\n{% codeblock lang:csharp %}\npublic class ScheduledFlight \n{\n    public int Id {get; set;}\n    public string FlightNumber {get; set;}\n\n    public int DepartureAirportId {get; set;}\n    public Airport DepartureAirport {get; set;}\n    public int DepartureHour {get; set;}\n    public int DepartureMinute {get; set;}\n\n    public int ArrivalAirportId {get; set;}\n    public Airport ArrivalAirport {get; set;}        \n    public int ArrivalHour {get; set;}\n    public int ArrivalMinute {get; set;}\n\n    public bool IsSundayFlight {get; set;}\n    public bool IsMondayFlight {get; set;}\n    // Some other properties\n}\n{% endcodeblock %}\n\n## Writing the query\nAs we learned in a [previous post](https://www.davepaquette.com/archive/2018/02/07/loading-related-entities-many-to-one.aspx), we can load the `Flight` entity along with it's related `ScheduledFlight` entity using a technique called multi-mapping. \n\nIn this case, loading all the flights to or from a particular airport, we would use the following query.\n\n{% codeblock lang:sql %}\nSELECT f.*, sf.*\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\n{% endcodeblock %}\n\nBut this query could yield more results than we want to deal with at any given time. Using OFFSET/FETCH, we can ask for only a block of results at a time.\n\n{% codeblock lang:sql %}\nSELECT f.*, sf.*\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\nORDER BY f.Day, sf.FlightNumber\nOFFSET @Offset ROWS\nFETCH NEXT @PageSize ROWS ONLY\n{% endcodeblock %}\n\nNote that an ORDER BY clause is required when using OFFSET/FETCH. \n\n## Executing the Query\n\n{% codeblock lang:csharp %}\n//GET api/flights\n[HttpGet]\npublic async Task<IEnumerable<Flight>> Get(string airportCode, int page=1, int pageSize=10)\n{\n    IEnumerable<Flight> results;\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var query = @\"\nSELECT f.*, sf.*\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\nORDER BY f.Day, sf.FlightNumber\nOFFSET @Offset ROWS\nFETCH NEXT @PageSize ROWS ONLY;\n\";\n\n        results = await connection.QueryAsync<Flight, ScheduledFlight, Flight>(query,\n          (f, sf) =>\n              {\n                  f.ScheduledFlight = sf;\n                  return f;\n              },\n            new { AirportCode = airportCode,\n                              Offset = (page - 1) * pageSize,\n                              PageSize = pageSize }\n            );        \n    }\n\n    return results;\n}\n{% endcodeblock %}\n\nHere we calculate the offset by based on the `page` and `pageSize` arguments that were passed in. This allows the caller of the API to request a particular number of rows and the starting point.\n\n## One step further\nWhen dealing with paged result sets, it can be useful for the caller of the API to also know the total number of records. Without the total number of records, it would be difficult to know how many records are remaining which in turn makes it difficult to render a paging control, a progress bar or a scroll bar (depending on the use case).\n\nA technique I like to use here is to have my API return a `PagedResults<T>` class that contains the list of items for the current page along with the total count.\n\n{% codeblock lang:csharp %}\npublic class PagedResults<T>\n{\n    public IEnumerable<T> Items { get; set; }\n    public int TotalCount { get; set; }\n}\n{% endcodeblock %}\n\nTo populate this using Dapper, we can add a second result set to the query. That second result set will simply be a count of all the records. Note that the same WHERE clause is used in both queries.\n\n{% codeblock lang:sql %}\nSELECT f.*, sf.*\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\nORDER BY f.Day, sf.FlightNumber\nOFFSET @Offset ROWS\nFETCH NEXT @PageSize ROWS ONLY;\n\nSELECT COUNT(*)\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\n{% endcodeblock %}\n\nNow in our code that executes the query, we will the `QueryMultipleAsync` method to execute both SQL statements in a single round trip. \n\n{% codeblock lang:csharp %}\n//GET api/flights\n[HttpGet]\npublic async Task<PagedResults<Flight>> Get(string airportCode, int page=1, int pageSize=10)\n{\n    var results = new PagedResults<Flight>();\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n        var query = @\"\nSELECT f.*, sf.*\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\nORDER BY f.Day, sf.FlightNumber\nOFFSET @Offset ROWS\nFETCH NEXT @PageSize ROWS ONLY;\n\nSELECT COUNT(*)\nFROM Flight f\nINNER JOIN ScheduledFlight sf ON f.ScheduledFlightId = sf.Id\nINNER JOIN Airport a ON sf.ArrivalAirportId = a.Id\nINNER JOIN Airport d ON sf.DepartureAirportId = d.Id\nWHERE a.Code = @AirportCode OR d.Code = @AirportCode\n\";\n\n        using (var multi = await connection.QueryMultipleAsync(query,\n                    new { AirportCode = airportCode,\n                          Offset = (page - 1) * pageSize,\n                          PageSize = pageSize }))\n        {\n            results.Items = multi.Read<Flight, ScheduledFlight, Flight>((f, sf) =>\n                {\n                    f.ScheduledFlight = sf;\n                    return f;\n                }).ToList();\n\n            results.TotalCount = multi.ReadFirst<int>();\n        }\n    }\n\n    return results;\n}\n    \n{% endcodeblock %}\n\n## Wrapping it up\nPaged result sets is an important technique when dealing with large amounts of data. When using a full ORM like Entity Framework, this is implemented easily using LINQ's  `Skip` and `Take` methods. It's so easy in fact that it can look a little like magic. In reality, it is actually very simple to write your own queries to support paged result sets and execute those queries using Dapper. ","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Using Vue as a drop-in replacement for Knockout in an ASP.NET MVC project","authorId":"dave_paquette","slug":"using-vue-as-a-drop-in-replacement-for-knockout-in-an-MVC-project","date":"2019-01-21 11:57:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET/Vue-js/using-vue-as-a-drop-in-replacement-for-knockout-in-an-MVC-project/","link":"","permalink":"https://westerndevs.com/ASP-NET/Vue-js/using-vue-as-a-drop-in-replacement-for-knockout-in-an-MVC-project/","excerpt":"When maintaining existing ASP.NET applications, we often need to add some client side behaviour. I am a little surprised to see people reaching for Knockout in these scenarios but I think vuejs is a great alternative that is very much worth exploring.","raw":"---\nlayout: post\ntitle: Using Vue as a drop-in replacement for Knockout in an ASP.NET MVC project\ntags:\n  - ASP.NET\n  - ASP.NET Core\n  - Vue.js\n  - Javascript\n  - MVC\n  - Knockout.js\ncategories:\n  - ASP.NET\n  - Vue.js\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2019/01/21/using-vue-as-a-drop-in-replacement-for-knockout-in-an-MVC-project.aspx'\ndate: 2019-01-21 06:57:12\nexcerpt: When maintaining existing ASP.NET applications, we often need to add some client side behaviour. I am a little surprised to see people reaching for Knockout in these scenarios but I think vuejs is a great alternative that is very much worth exploring.\n---\nSo you're working an existing (brown-field) ASP.NET MVC application. The application's views are rendered server-side using Razor. Everything is working great and life is good. Suddenly, someone asks for a bit of additional functionality that will require some client side logic. Okay, no big deal. We do this all the time. The question though, is what framework/JavaScript library you will use to implement that client side functionality. \n\nThe default MVC project templates already include jQuery, so you might use that. You'll probably end up writing a lot of code if you go down that path. Chances are, you will want to use a JavaScript framework that offers two-way data binding between the elements in the DOM to a your model data. \n\nIt seems that for many people, [Knockout.js](https://knockoutjs.com/) is the default library to use in these scenarios. I won't get into the specifics but I think that Knockout is a little dated and that there are better options these days. If you want to dig into some of the issues with Knockout, you can read [Simon Timms' rant on the subject](https://westerndevs.com/a-discussion-on-knockout/).\n\n## Vue.js\nMy fellow [ASP.NET Monster](https://aspnetmonsters.com) [James Chambers](http://jameschambers.com) recently strongly recommended I take a look at [Vue.js](https://vuejs.org). I had been meaning to give Vue a try for some time now and I finally had a chance to use it on a recent project. Let me tell you...I love it. \n\nI love it for a whole bunch of reasons. The [documentation](https://vuejs.org/v2/guide/) is great! It is super easy to get drop in to your existing project and it doesn't get in the way. For what I needed to do, it just did the job and allowed me to get on with my day. It is also designed to be \"incrementally adoptable\", which means you can start out with just using the core view layer, then start pulling in other things like routing and state management if/when you need them. \n\n## A simple example\nI won't go into great detail about how to use Vue. If you want a full tutorial, head on over to the [Vue docs](https://vuejs.org/v2/guide/). What I want to show here is just how simple it is to drop Vue into an existing ASP.NET MVC project and add a bit of client side functionality. \n\nThe simplest example I can think of is a set of cascading dropdowns. Let's consider a form where a user is asked to enter their Country / Province. When the Country is selected, we would expect the Province dropdown to only display the valid Provinces/States for the selected Country. That probably involves a call to an HTTP endpoint that will return the list of valid values.\n\n{% codeblock lang:csharp %}\npublic class ProvinceLookupController : Controller\n{\n    public ActionResult Index(string countryCode)\n    {\n        var provinces = ProvinceLookupService.GetProvinces(countryCode);\n        return Json(provinces, JsonRequestBehavior.AllowGet);\n    }\n}\n{% endcodeblock %}\n\n### Including Vue in your Razor (cshtml) view\nThe easiest way to include Vue on a particular Razor view is to link to the `vue.js` file from a CDN. You can add the following Script tag to your `scripts` section.\n\n{% codeblock lang:html %}\n@section scripts  {\n    <script src=\"https://cdn.jsdelivr.net/npm/vue@2.5.22/dist/vue.js\"></script>\n}\n{% endcodeblock %}\n\nBe sure to [check the docs](https://vuejs.org/v2/guide/installation.html) to make sure you are referencing the latest version of Vue.\n\n### Binding data to the our View\nNow that you have included the core Vue library, you can start using Vue to bind DOM elements to model data. \n\nStart by defining a `Vue` object in JavaScript. You can add this in a new `<script>` tag in your `scripts` section.\n\n{% codeblock lang:javascript %}\n@section scripts  {\n    <script src=\"https://cdn.jsdelivr.net/npm/vue@2.5.22/dist/vue.js\"></script>\n\n    <script type=\"text/javascript\">\n        var app = new Vue({\n            el: '#vueApp',\n            data: {\n                selectedCountryCode: null,\n                countries: [\n                    { code: 'ca', name: 'Canada' },\n                    { code: 'us', name: 'United States' }\n                ]            \n            }\n        });\n    </script>\n}\n{% endcodeblock %}\n\nThis `Vue` object targets the DOM element with id `vueApp` and contains some simple data. The currently selected country code and the list of countries. \n\nNow, back in the HTML part of your csthml, wrap the `form` in a div that has an `id=\"vueApp\"`.\n\n{% codeblock lang:html %}\n<div id=\"vueApp\">\n    <!-- your form -->\n</div>\n{% endcodeblock %}\n\nNext, bind the `<select>` element to the data in your `Vue` object. In Vue, data binding is done using a combination of custom attributes that start with `v-` and the double curly bracket (aka. Mustache) syntax for text. \n\n{% codeblock lang:html %}\n<div class=\"form-group\">\n    @Html.LabelFor(model => model.CountryCode, new { @class = \"control-label col-md-2\" })\n    <div class=\"col-md-10\">\n        <select id=\"CountryCode\" name=\"CountryCode\" class=\"form-control\" \n                v-model=\"selectedCountryCode\">\n            <option v-for=\"country in countries\" v-bind:value=\"country.code\">\n                {{country.name}}\n            </option>\n        </select>\n    </div>\n</div>\n{% endcodeblock %}\n\nNow, when you run the app, you should see a dropdown containing Canada and United States.\n\n![Country Dropdown](https://www.davepaquette.com/images/vue/simple-country-dropdown.png)\n\n### Adding functionality\nNext, you will want to add some client side logic to get the list of valid provinces from the server whenever the selected country changes.\n\nFirst, add an empty `provinces` array and a `selectedProvinceCode` property to the `Vue` object's data.\n\nNext, add a method called `countryChanged` to the `Vue` object. This method will call the `ProvinceLookup` action method on the server, passing in the `selectedCountryCode` as a parameter. Assign the response data to the `provinces` array.\n\n{% codeblock lang:javascript %}\nvar app = new Vue({\n    el: '#vueApp',\n    data: {\n        selectedCountryCode: null,\n        countries: [\n            { code: 'ca', name: 'Canada' },\n            { code: 'us', name: 'United States' }\n        ],\n        selectedProvinceCode: null,\n        provinces: []\n    },\n    methods: {\n        countryChanged: function () {\n            $.getJSON('@Url.Action(\"Index\", \"ProvinceLookup\")?countryCode=' + this.selectedCountryCode, function (data) {\n                this.provinces = data;\n            }.bind(this));\n        }\n    }\n});\n{% endcodeblock %}\n\nHere I used jQuery to make the call to the server. In the Vue community, [Axios](https://github.com/axios/axios) is a popular library for making HTTP requests.\n\nBack in the HTML, bind the `change` event from the country select element to the `countryChanged` method using the `v-on:change` attribute.\n\n{% codeblock lang:html %}\n<select id=\"CountryCode\" name=\"CountryCode\" class=\"form-control\" \n        v-model=\"selectedCountryCode\" v-on:change=\"countryChanged\">\n    <option v-for=\"country in countries\" v-bind:value=\"country.code\">\n        {{country.name}}\n    </option>\n</select>\n{% endcodeblock %}\n\nNow you can add a select element for the provinces.\n{% codeblock lang:html %}\n<div class=\"form-group\">\n    @Html.LabelFor(model => model.ProvinceCode, new { @class = \"control-label col-md-2\" })\n    <div class=\"col-md-10\">\n        <select id=\"ProvinceCode\" name=\"ProvinceCode\" class=\"form-control\"\n                v-model=\"selectedProvinceCode\">\n            <option v-for=\"province in provinces\" v-bind:value=\"province.Code\">\n                {{province.Name}}\n            </option>\n        </select>\n    </div>\n</div>\n{% endcodeblock %}\n\nVoila! You now have a working set of cascading dropdowns.\n\n![Country Dropdown](https://www.davepaquette.com/images/vue/country-province-dropdown.png)\n\n### One last thing\nYou might want to disable the provinces dropdown whenever a request is being made to get the list of provinces for the selected country. You can do this by adding an `isProvincesLoading` property to the `Vue` object's data, then setting that property in the `countryChanged` method.\n\n{% codeblock lang:javascript %}\nvar app = new Vue({\n    el: '#vueApp',\n    data: {\n        selectedCountryCode: null,\n        countries: [\n            { code: 'ca', name: 'Canada' },\n            { code: 'us', name: 'United States' }\n        ],\n        selectedProvinceCode: null,\n        provinces: [],\n        isProvincesLoading: false\n    },\n    methods: {\n        countryChanged: function () {\n            this.isProvincesLoading = true;\n            $.getJSON('@Url.Action(\"Index\", \"ProvinceLookup\")?countryCode=' + this.selectedCountryCode, function (data) {\n                this.provinces = data;\n                this.isProvincesLoading = false;\n            }.bind(this));\n        }\n    }\n});\n{% endcodeblock %}\n\nIn your HTML, bind the `disabled` attribute to the `isProvincesLoading` property.\n{% codeblock lang:html %}\n<div class=\"form-group\">\n<select id=\"ProvinceCode\" name=\"ProvinceCode\" class=\"form-control\"\n        v-model=\"selectedProvinceCode\"\n        v-bind:disabled=\"isProvincesLoading\">\n    <option v-for=\"province in provinces\" v-bind:value=\"province.Code\">\n        {{province.Name}}\n    </option>\n</select>\n{% endcodeblock %}\n\n### Putting it all together\n\nHere is the entire cshtml file.\n\n{% codeblock lang:html%}\n@{\n    ViewBag.Title = \"Location Settings\";\n}\n@model Mvc5VueJsExample.Models.LocationSettingsModel\n\n<h2>@ViewBag.Title.</h2>\n<h3>@ViewBag.Message</h3>\n\n<div id=\"vueApp\">\n    @using (Html.BeginForm(\"LocationSettings\", \"Home\", FormMethod.Post, new { @class = \"form\" }))\n    {\n        <div class=\"form-group\">\n            @Html.LabelFor(model => model.CountryCode, new { @class = \"control-label col-md-2\" })\n            <div class=\"col-md-10\">\n                <select id=\"CountryCode\" name=\"CountryCode\" class=\"form-control\" \n                        v-model=\"selectedCountryCode\" v-on:change=\"countryChanged\">\n                    <option v-for=\"country in countries\" v-bind:value=\"country.code\">\n                        {{ country.name}}\n                    </option>\n                </select>\n            </div>\n        </div>\n\n        <div class=\"form-group\">\n            @Html.LabelFor(model => model.ProvinceCode, new { @class = \"control-label col-md-2\" })\n            <div class=\"col-md-10\">\n                <select id=\"ProvinceCode\" name=\"ProvinceCode\" class=\"form-control\"\n                        v-model=\"selectedProvinceCode\"\n                        v-bind:disabled=\"isProvincesLoading\">\n                    <option v-for=\"province in provinces\" v-bind:value=\"province.Code\">\n                        {{province.Name}}\n                    </option>\n                </select>\n            </div>\n        </div>\n        <button class=\"btn btn-primary\" type=\"submit\">Save</button>\n    }\n\n</div>\n@section scripts  {\n    <script src=\"https://cdn.jsdelivr.net/npm/vue@2.5.22/dist/vue.js\"></script>\n\n    <script type=\"text/javascript\">\n        var app = new Vue({\n            el: '#vueApp',\n            data: {\n                selectedCountryCode: null,\n                countries: [\n                    { code: 'ca', name: 'Canada' },\n                    { code: 'us', name: 'United States' }\n                ],\n                selectedProvinceCode: null,\n                provinces: [],\n                isProvincesLoading: false\n            },\n            methods: {\n                countryChanged: function () {\n                    this.isProvincesLoading = true;\n                    $.getJSON('@Url.Action(\"Index\", \"ProvinceLookup\")?countryCode=' + this.selectedCountryCode, function (data) {\n                        this.provinces = data;\n                        this.isProvincesLoading = false;\n                    }.bind(this));\n                }\n            }\n        });\n    </script>\n}\n{% endcodeblock %}\n\n## Wrapping it up\nI hope this gives you a taste for how easy it is to work with Vue. My current thinking is that Vue should be the default choice for client side frameworks in existing ASP.NET MVC apps.\n\nNOTE: This example uses ASP.NET MVC 5 to illustrate that Vue can be used with brownfield applications. It would be just as easy, if not easier, to use Vue in an ASP.NET Core project.\n","categories":[{"name":"ASP.NET","slug":"ASP-NET","permalink":"https://westerndevs.com/categories/ASP-NET/"},{"name":"Vue.js","slug":"ASP-NET/Vue-js","permalink":"https://westerndevs.com/categories/ASP-NET/Vue-js/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"ASP.NET","slug":"ASP-NET","permalink":"https://westerndevs.com/tags/ASP-NET/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"},{"name":"Vue.js","slug":"Vue-js","permalink":"https://westerndevs.com/tags/Vue-js/"},{"name":"Javascript","slug":"Javascript","permalink":"https://westerndevs.com/tags/Javascript/"},{"name":"Knockout.js","slug":"Knockout-js","permalink":"https://westerndevs.com/tags/Knockout-js/"}]},{"title":"JavaScript and NPM","slug":"javascript-and-npm","date":"2018-12-27 21:35:38+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/javascript-and-npm/","link":"","permalink":"https://westerndevs.com/podcasts/javascript-and-npm/","excerpt":"A discussion on the implications of blindly using NPM","raw":"---\nlayout: podcast\ntitle: JavaScript and NPM\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-javascript-and-npm.mp3\n  length: '33:29'\n  filesize: 37307587\n  libsynId: 8065127\n  anchorFmId: JavaScript-and-NPM-evqdho\nparticipants:\n  - kyle_baley\n  - simon_timms\n  - david_wesst\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"EventStream NPM package targets a bitcoin wallet|https://hub.packtpub.com/malicious-code-in-npm-event-stream-package-targets-a-bitcoin-wallet-and-causes-8-million-downloads-in-two-months/\"\n  - \"EventStream on NPM|https://www.npmjs.com/package/event-stream\"\n  - \"Tweet: Has James Newton-King ever been approached to add questionable code into JSON.NET?|https://twitter.com/stimms/status/1067254124905869312\"\n  - \"GitHub vulnerability scanner|https://blog.github.com/2017-11-16-introducing-security-alerts-on-github/\"\n  - \"SonarQube for vulnerability scanning|https://www.sonarqube.org/\"\n  - \"The LeftPad Debacle|https://www.theregister.co.uk/2016/03/23/npm_left_pad_chaos/\"\n  - \"The Problem of Package Manager Trust|https://haacked.com/archive/2018/11/28/package-manager-trust/\"\ndate: 2018-12-27 16:35:38\nrecorded: 2018-11-27 12:00:00\nexcerpt: A discussion on the implications of blindly using NPM\n---\n\n### Synopsis\n\n* The nature of NPM\n* The problem with EventStream\n* How do we guard against malicious packages?\n* Is this a problem with Nuget too?\n* Does open source help?\n* What is a project owner's responsibility when handing off a package?\n* Why JavaScript makes this a hard problem\n* Plugging analyzers into the build pipeline\n* Being proactive vs. being reactive\n* Architecting to minimize exposure\n* The StackOverflow effect\n* When implicit trust is based into the learning path\n* The organization's responsibilities\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Validating Client Is Sending Cert For Auth In Azure Functions","authorId":"justin_self","slug":"Testing-Client-Cert-Auth","date":"2018-12-13 21:35:38+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"certificates-azurefunctions/Testing-Client-Cert-Auth/","link":"","permalink":"https://westerndevs.com/certificates-azurefunctions/Testing-Client-Cert-Auth/","excerpt":"Do you need to validate a client certificate is being passed to a server correctly but don't want to muck with local TLS and webserver configs? I got you.","raw":"---\nlayout: post\ntitle:  Validating Client Is Sending Cert For Auth In Azure Functions\ndate: 2018-12-13 16:35:38\ncategories: certificates azurefunctions\ncomments: true\nauthorId: justin_self\n---\n\nDo you need to validate a client certificate is being passed to a server correctly but don't want to muck with local TLS and webserver configs? I got you.\n\n<!-- more -->\n\n[1]: https://imgur.com/ehuVZqx.png\n[2]: https://imgur.com/knx5FJd.png\n\nUsing an Azure function, this can be done two ways.\n\n1 - Check for the X-ARR-ClientCert request header and, if present, base64 decode the value and load it into a X509Certificate2. From there, you can check the thumprint to validate the client is correctly sending the certificate with the request.\n\n2 - Get the request context and check to see if the ClientCertificate is null. If it's not then check the thumprint.\n\nI chose the second way for one single reason - I did not know about the first way. So, if you choose the second way you'll need to make a setting change to allow the certificate to be passed in with the request (instead of as part of the request header).\n\nGo to the SSL settings of the function app.\n\n![1]\n\nEnable the `Incoming client certificates` flag.\n\n![2]\n\nHere's some code:\n\n        [FunctionName(\"Function1\")]\n        public static async Task<HttpResponseMessage> Run([HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)]HttpRequestMessage req, TraceWriter log)\n        {\n            var clientCert = req.GetRequestContext().ClientCertificate;\n            if (clientCert == null)\n            {\n                return req.CreateResponse(HttpStatusCode.BadRequest, \"There's no client certificate\");\n            }\n\n            log.Info($\"Client Thumbprint: {req.GetRequestContext().ClientCertificate?.Thumbprint ?? \"No cert found.\"}\");\n            return req.CreateResponse(HttpStatusCode.OK, $\"Thumbprint: {clientCert.Thumbprint}\", new JsonMediaTypeFormatter());\n        }\n\n\nBoom. Done. All in all, this took about 8 minutes to do (including creating the function app) and it saved me from mucking around with my machine, generating a cert, configuring the web server etc., and now others on my team can use it.\n\nUsing the second way gives an added benefit of forcing all requests to include a client cert. So, if your app immediately gets rejected, you know the cert isn't even being loaded.\n","categories":[{"name":"certificates azurefunctions","slug":"certificates-azurefunctions","permalink":"https://westerndevs.com/categories/certificates-azurefunctions/"}],"tags":[]},{"title":"Creating Storage Queues in Azure DevOps","authorId":"simon_timms","slug":"2018-12-06-creating-queues-in-devops","date":"2018-12-06 18:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/2018-12-06-creating-queues-in-devops/","link":"","permalink":"https://westerndevs.com/_/2018-12-06-creating-queues-in-devops/","excerpt":"Storage Queues are one of the original pieces of Azure dating back about a decade now. They are great for deferring work to later or spreading it out over a bunch of consumers. If you're following best practices for DevOps you'll know that the creation of your queues should be done in code. In some cases you can create the queues on application startup but in serverless scenarios there often is no startup code so the responsibility of creating queues falls to your deployment process. Let's look at how to do that on Azure DevOps","raw":"layout: post\ntitle: Creating Storage Queues in Azure DevOps\nauthorId: simon_timms\ndate: 2018-12-06 13:00\noriginalurl: https://blog.simontimms.com/2018/12/06/2018-12-06_creating_queues_in_devops/\n\n---\n\nStorage Queues are one of the original pieces of Azure dating back about a decade now. They are great for deferring work to later or spreading it out over a bunch of consumers. If you're following best practices for DevOps you'll know that the creation of your queues should be done in code. In some cases you can create the queues on application startup but in serverless scenarios there often is no startup code so the responsibility of creating queues falls to your deployment process. Let's look at how to do that on Azure DevOps\n\n<!--more-->\n\nYour first instinct might be to use your ARM templates to build queues. This makes great sense - the storage account is defined in the ARM template so why not also define the queues? Because you can't! \n\nThere is a [request open to add that functionality](https://feedback.azure.com/forums/281804-azure-resource-manager/suggestions/9306108-let-me-define-preconfigured-blob-containers-table) but after 3 years it remains unanswered. Talking to some Azure engineers at Microsoft it seems like the approach they would like people to take is to build the queue in the application. Problem with that is that now your application needs to have rights to create queues instead of just right to write to or read from the queue. You might also not have an appropriate place to put startup code - for instance Azure Functions don't have a good way to run code on startup. Same deal with logic apps.\n\nThe fact that creating storage queues is not officially supported in ARM templates is baloney and the argument that this is an application level create queue to do is proof that parts of Microsoft still doesn't get DevOps. \n\nThis means we have to plug something into the pipeline to create the queues. I quite like the [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest) which is a great little command line tool for interacting with Azure. You can add in the Azure CLI task in Azure DevOps, hook up the subscription and the give it an inline script like so:\n\n```\ncall az storage queue create -n \"awesome-queue-1\" --connection-string \"$(storageAccountConnectionString)\"\n```\n\nIf you're using a Windows build agent then you need to include the `call` to ensure that multiple lines are executed. If you're on a Linux agent then `call` can be omitted.\n\nThat connection string can be exported from your ARM template as an output parameter and then sucked into the DevOps variables using [ARM Outputs](https://github.com/keesschollaart81/vsts-arm-outputs).","categories":[],"tags":[]},{"title":"Installing an Azure Web App Site Extension with PowerShell","authorId":"dave_paquette","slug":"installing-an-azure-web-app-site-extension-with-powershell","date":"2018-11-14 00:43:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Azure/installing-an-azure-web-app-site-extension-with-powershell/","link":"","permalink":"https://westerndevs.com/Azure/installing-an-azure-web-app-site-extension-with-powershell/","excerpt":"I recently ran into a scenario where I needed to script the installation of a site extension into an existing Azure Web App. The solution was not easy to find but I eventually got to a solution.","raw":"---\nlayout: post\ntitle: Installing an Azure Web App Site Extension with PowerShell\ntags:\n  - Azure\n  - App Service\n  - Web App\n  - Powershell\n  - Application Insights\ncategories:\n  - Azure\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2018/11/13/installing-an-azure-web-app-site-extension-with-powershell.aspx'\ndate: 2018-11-13 19:43:50\nexcerpt: I recently ran into a scenario where I needed to script the installation of a site extension into an existing Azure Web App. The solution was not easy to find but I eventually got to a solution.\n---\nI recently ran into a scenario where I needed to script the installation of a site extension into an existing Azure Web App. Typically, I would use an [Azure ARM deployment to accomplish this](https://github.com/tomasr/webapp-appinsights) but in this particular situation that wasn't going to work.\n\nI wanted to install the site extension that enables [Application Insights Monitoring of a live website](https://docs.microsoft.com/en-us/azure/application-insights/app-insights-monitor-performance-live-website-now). After digging into existing arm templates, I found the name of that extension is `Microsoft.ApplicationInsights.AzureWebSites`. \n\nAfter searching for way too long, I eventually found PowerShell command I needed on a forum somewhere. I can't find it again so I'm posting this here in hopes that it will be easier for others to find in the future.\n\n## Installing a site extension to an existing App Service Web App\n\n{% codeblock %}\nNew-AzureRmResource -ResourceType \"Microsoft.Web/sites/siteextensions\" -ResourceGroupName MyResourceGroup -Name \"MyWebApp/SiteExtensionName\" -ApiVersion \"2018-02-01\" -Force\n{% endcodeblock %}\n\nFor example, given a resource group named `Test`, a web app named `testsite` and a site extension named  `Microsoft.ApplicationInsights.AzureWebSites`.\n\n{% codeblock %}\nNew-AzureRmResource -ResourceType \"Microsoft.Web/sites/siteextensions\" -ResourceGroupName \"Test\" -Name \"testsite/Microsoft.ApplicationInsights.AzureWebSites\" -ApiVersion \"2018-02-01\" -Force\n{% endcodeblock %}\n\n## Installing a site extension to a Web App Deployment Slot\n\nThe scenario I ran into was actually attempting to add this site extension to a deployment slot. When you create a deployment slot, it doesn't copy over any existing site extensions, which is a problem because when you swap your new slot to production, your new production slot ends up losing the site extensions that were in the old production slot.\n\n{% codeblock %}\nNew-AzureRmResource -ResourceType \"Microsoft.Web/sites/slots/siteextensions\" -ResourceGroupName MyResourceGroup -Name \"MyWebApp/SlotName/SiteExtensionName\" -ApiVersion \"2018-02-01\" -Force\n{% endcodeblock %}\n\nUsing the same example as above and  a slot named `Staging`:\n\n{% codeblock %}\nNew-AzureRmResource -ResourceType \"Microsoft.Web/sites/slots/siteextensions\" -ResourceGroupName \"Test\" -Name \"testsite/Staging/Microsoft.ApplicationInsights.AzureWebSites\" -ApiVersion \"2018-02-01\" -Force\n{% endcodeblock %}","categories":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/categories/Azure/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"App Service","slug":"App-Service","permalink":"https://westerndevs.com/tags/App-Service/"},{"name":"Web App","slug":"Web-App","permalink":"https://westerndevs.com/tags/Web-App/"},{"name":"Powershell","slug":"Powershell","permalink":"https://westerndevs.com/tags/Powershell/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://westerndevs.com/tags/Application-Insights/"}]},{"title":"Azure Data Factory - a rapid introduction","authorId":"simon_timms","slug":"2018-11-05-datafactory","date":"2018-11-05 18:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/2018-11-05-datafactory/","link":"","permalink":"https://westerndevs.com/_/2018-11-05-datafactory/","excerpt":"Azure is huge. There are probably a dozen ways to host a website, a similar number of different data storage technologies, tools for identity, scaling, DDoS protection - you name it Azure has it. With that many services it isn't unusual for me to find some service I didn't even know existed. Today that service is Data Factory. Data factory is a batch based Extract, Transform and Load(ETL) service which means that it moves data between locations. I mention that it is batch to distinguish it from services which are online and process events as they come in. Data Factory might be used to move data between a production database and the test system or between two data sources.","raw":"layout: post\ntitle: Azure Data Factory - a rapid introduction\nauthorId: simon_timms\ndate: 2018-11-05 13:00\noriginalurl: https://blog.simontimms.com/2018/11/05/2018-11-01_datafactory/\n\n---\n\nAzure is huge. There are probably a dozen ways to host a website, a similar number of different data storage technologies, tools for identity, scaling, DDoS protection - you name it Azure has it. With that many services it isn't unusual for me to find some service I didn't even know existed. Today that service is [Data Factory](https://azure.microsoft.com/en-ca/services/data-factory/). Data factory is a batch based Extract, Transform and Load(ETL) service which means that it moves data between locations. I mention that it is batch to distinguish it from services which are online and process events as they come in. Data Factory might be used to move data between a production database and the test system or between two data sources. \n\n<!--more-->\n\nIn most cases I'd recommend solutions which were more tightly integrated into your business processes than copying data between databases. It is easier to test, closer to real time and easier to update. However, moving to event based systems can be long and difficult so there is certainly a niche for Data Factory. A good application might be migrating data from a service you don't own to your internal database - the external system is unlikely to have data change notifications you could use to drive data population. Azure Data Factory plays in the same space that SQL Server Integration Services played in the past - in fact you can build your Azure Data Factory pipeline in SSIS and simply upload it.\n\nLet's take a look at loading some data from an Azure SQL database, and putting it into Cosmos DB. I've chosen these two systems but there are literally dozens of different data sources and destinations you can use at the click of a mouse. They are not limited to Microsoft offerings, either. There are connectors for Cassandra, Couchbase, MongoDB, Google BigQuery even Oracle.\n\n![A small sample of the various datasources which exist within data factory](https://blog.simontimms.com/images/datafactory/datasources.png)\n\nThe first step is to create a new data factory in Azure. This is as simple as searching for data factory in the Azure portal and clicking create. A few settings are required such as the name and the version. For this I went with v2 because it is a larger number than v1 and ergo way better. I'm pretty sure that's how numbers work. With the factory created the next step is to click on `Author and Monitor`.\n\n![Select author and monitor in the portal](https://blog.simontimms.com/images/datafactory/author.png)\n\nThis opens up a whole editor experience in a new tab. It is still roughly styled like the portal so it isn't as jarring as using the man styling jumble that is AWS' console. In the left gutter click on the little `+` symbol to create a new data set.\n\n![New SQL data source](https://blog.simontimms.com/images/datafactory/newdatasource.png)\n\nI found myself an old backup database I had kicking around still on my Azure account to be the source of data for this experiment. It is a database of data related to the construction of some oil extraction facility somewhere. To protect the innocent I've anonymized the data a little. We'll start by adding this as a source for the data factory. Select Azure SQL as the source and then give it a name in the general pane. Under connection set up a new linked service. This is what holds our data connection information so multiple different data sets can use the same linked service if you wanted to pull from multiple tables. In the linked service set up you can select an existing database from the drop downs and enter the login information. \n\nWith the linked service set up you can select the table you'll be using for the schema information and even preview the data. \n\n![A preview of the data in the SQL database](https://blog.simontimms.com/images/datafactory/preview.png)\n\nNext follow a similar procedure for setting up the cosmos data source. My cosmos data source was brand new so it didn't have any document from which data factory could figure out the schema. This meant that I had to go in and define one in the data source.\n\n![Defining the Cosmos database schema](https://blog.simontimms.com/images/datafactory/schema.png)\n\nWith the two data sources in place all that is needed now is to copy the data from one to another. Data factory is obviously a lot more than being able to copy data between data bases but to do any manipulation of the data you really need to pull in other services. For instance you can manipulate the data with data bricks or HD Insights and, of course, you can analyze the data with Azure ML. What is missing, in my mind, is a really simple way of manipulating fields, concatenating them together, splitting them up that sort of thing. Because data factory is designed to scale it relies on other services which can also scale instead of internalizing too much. On one hand this is good because is formalizes your scaling and makes you think about what you do if you have huge quantities of data. On the other hand is raises the knowledge bar for entry quite high.\n\nOriginally this article was going to cover manipulating data but the difficulty meant that that content had to be pushed off to another post.\n\nReturning to the problem at hand the copy task is added by adding a new pipeline. Within that pipeline we add a copy data task by dragging it to the canvas. In the task we configure the source as being the SQL database and, at the same time, select a query. My query filters for tags which are complete (you don't really need to know what that means).\n\n![Entering a query](https://blog.simontimms.com/images/datafactory/selecttags.png)\n\nNext set up a destination sink as the cosmos db. Finally set up the mapping. Mappings determine which fields go where: from the source into the destination. Because we've gone to the trouble of ensuring field names are the same over our two data sets simply clicking `Import Schemas` is enough to set up the mappings for us. You may need to manually map fields if you're renaming as part of the copy. \n\nPipelines are built by coupling together various tasks to copy, filter, sort and otherwise manipulate data. Each task has a success, completion and failure output which can be wired to the next task allowing you to build pretty complex logic. Of course as with all complex logic it is nice to have automated tests around it. This is a failing of data factory - it is difficult to test the workflow logic. \n\nThe set up of the pipeline is now complete. To start using it you first need to publish it which is done by clicking on the `Publish All` button. Publishing takes a moment but once it is done testing the integration is as simple as clicking on trigger and going down to `Trigger Now`. Within a few seconds I was able to jump to my cosmos and find it filled with all the records from SQL. It was quick and easy to set up. What's really nice too is that the pipeline can easily be scheduled. \n\nData factory is not the right solution for every project. I'd actually argue that it isn't the right solution for most projects but it is a good stop gap until you can move to a more online version of data integration using something like change events and functions. Of course that assumes you have infinite resources to improve your projects...","categories":[],"tags":[]},{"title":"Checking in packages","authorId":"simon_timms","slug":"Checking-In-packages","date":"2018-10-21 15:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development-fundamentals/Checking-In-packages/","link":"","permalink":"https://westerndevs.com/development-fundamentals/Checking-In-packages/","excerpt":"If there is one thing that we developers are good at it is holy wars. Vi vs. Emacs, tabs vs. spaces, Python vs. R, the list goes on. I'm usually smart enough to not get involved in such low brow exchanges... haha, who am I kidding? (vi, spaces and R, BTW) Recently I've been tilting at the windmill that is checking in package files. I don't mean the files that tell what version of files to check in but the actual library files.","raw":"---\nlayout: post\ntitle: Checking in packages\ntags:\n  - nuget\n  - npm\ncategories:\n  - development fundamentals   \nauthorId: simon_timms\ndate: 2018-10-21 11:36:36\noriginalurl: https://blog.simontimms.com/2018/10/21/checking_in_packages/\n---\n\nIf there is one thing that we developers are good at it is holy wars. Vi vs. Emacs, tabs vs. spaces, Python vs. R, the list goes on. I'm usually smart enough to not get involved in such low brow exchanges... haha, who am I kidding? (vi, spaces and R, BTW) Recently I've been tilting at the windmill that is checking in package files. I don't mean the files that tell what version of files to check in but the actual library files. \n\n<!--more-->\n\nPackage managers aren't anything new, we've had them for years, decades even if you consider CPAN which has been online for 23 years. The idea behind them is that they provide an easy mechanism to install dependencies into your project. At the same time you can avoid checking in a bunch of library files and instead run a package restore as one of the build steps. \n\nI've heard a couple of arguments against relying on package managers instead of checking in your libraries. \n\n1. What if the package manager goes away? How could you still reliably build the software years in the future?\n2. What if the specific package being used goes away? It might be unpublished like what happened with [left-pad](https://www.theregister.co.uk/2016/03/23/npm_left_pad_chaos/)\n3. What if a transitive dependency, that is one that is included because it is a dependency of some other package, is revved and the package author of the included dependency left the dependency requirement open?\n4. It takes a long time to restore packages using a package manager, we can speed up builds by not running the package restore during the build.\n5. Everything you need to build your solution should be checked into source control. \n\nLet's break each one of these down and see why they are wrong headed.\n\n1. Package managers have been around for years and aren't going away anytime soon. Perl is hardly a well used language these days but CPAN survives still. Package managers will outlast the applications built using the language. \n2. This is a totally legitimate concern. Fortunately policies have changed at [npm](https://docs.npmjs.com/cli/unpublish) and [NuGet](https://docs.microsoft.com/en-us/nuget/policies/deleting-packages) to no longer allow removing packages after a brief window. \n3. I've seen this happen with some frequency. Again package managers have changed to ensure that we no longer have this problem. Npm has introduced a package-lock file with version 5 and finally fixed how it works with version 6. By checking in this file we can be assured that we get exactly the right version of packages restored in a build. NuGet is, well, [trying to catch up](https://github.com/NuGet/Home/wiki/Enable-repeatable-package-restore-using-lock-file) on package lock files. Paket, which uses NuGet files under the covers does have [proper support for lock files](https://fsprojects.github.io/Paket/lock-file.html). \n4. This is a legitimate concern as well. I'd say that greater than 50% of my build times are spend restoring packages. Some of that time is downloading packages and some of it is solving the dependency graph. [Dylan](https://westerndevs.com/bios/dylan_smith/) has recently been [experimenting with caching packages](https://github.com/Microsoft/hash-and-cache) based on a checksum of the lock file. This, as it turns out, is quite a bit faster than simply downloading individual files from the package repository. This approach has been used for a while by Circle CI.\n5. The goal is to make sure that you can always build every piece of code you own. One solution is to check everything in, but where do you stop? Do you check in the compiler? The system libraries? The operating system? Instead of checking everything in and hoping that they continue to build why not just run builds every night? \n\nWe've countered every one of the big points for checking in packages. Now let me tell you why you shouldn't check packages in. \n\n1. It bloats the size of your repository. It does take time to download a repository and filling it up with multiple copies of no longer used packages is not helpful. Git works in a way that even if you delete a file it still exists in the image which is distributed to everybody who pulls the repository.\n![Heavy weight repositories](https://blog.simontimms.com/images/checking_in_packages/weight.jpeg)\n2. It is really easy to upgrade a package and forget to check it in or otherwise get the package file out of sync with what's on disk. \n3. The whole reason we have package managers is to help us handle installing and reinstalling packages - we should try to trust in them. \n4. If there is concern that packages may stop being available or that package servers will be unavailable then we can stand up an internal package server. This is also beneficial for developer builds. \n\nFor NuGet we have the added problem that the package directory is no longer local to the source control directory so you have to really go out of your way to check in the packages folder. \n\nStop checking in packages, it is a ridiculous outdated practice which is introducing bugs and slowing down builds.","categories":[{"name":"development fundamentals","slug":"development-fundamentals","permalink":"https://westerndevs.com/categories/development-fundamentals/"}],"tags":[{"name":"npm","slug":"npm","permalink":"https://westerndevs.com/tags/npm/"},{"name":"nuget","slug":"nuget","permalink":"https://westerndevs.com/tags/nuget/"}]},{"title":"DevOps and Microservices - Symbiotes","authorId":"simon_timms","slug":"microservices-devops","date":"2018-10-20 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microservices-devops/","link":"","permalink":"https://westerndevs.com/_/microservices-devops/","excerpt":"Two of the major ideas de jour in development circles these past few years have been DevOps and Microservices. That they rose to the forefront at the same time was not a coincidence. They are inexorably linked ideas.","raw":"layout: post\ntitle: DevOps and Microservices - Symbiotes\nauthorId: simon_timms\ndate: 2018-10-20\noriginalurl: https://blog.simontimms.com/2018/10/19/microservices_and_devops/\n---\n\nTwo of the major ideas de jour in development circles these past few years have been DevOps and Microservices. That they rose to the forefront at the same time was not a coincidence. They are inexorably linked ideas. \n\n<!-- more -->\n\nMicroservices Architecture is a top level software design which favours creating small, loosely connected services which maintain data autonomy. Part of this design requires that each service you deploy has its own data storage (this could be as complex as its own SQL Server instance or as simple as an in-memory cache) and that the service be independently deployable. If you've ever built out a deployment pipeline for a monolithic application and thought \"geez, this takes a lot of work\" then imagine scaling that over 20 services or 100 services. Equally deploying a lot of databases can be painful to say nothing of how complex it is to deploy copies of your microservices collection over several environments (dev, test, prod,...).\n\nThe added work of deploying this large number of services necessitates changes to the old models where releases took weeks and infrastructure was manually provisioned. Perhaps the greatest motivator for people is avoiding boring, repetitive work. Microservices accentuate the pain points which traditional, monolithic design has hidden. It is slow to provision severs, difficult to set up build and annoying to log into servers to get logs. Changes had to be made to unblock microservices. Out of the pain was born a desire to make builds and infrastructure provisioning faster, more repeatable and easier to set up.\n\nDevOps is obviously more that just speeding up builds and unblocking infrastructure. It is a cultural mindset together the previously disjoint operations and development groups to better serve the business by providing reliable changes rapidly. DevOps permeates the entire development life-cycle. \n![Image from https://medium.com/@neonrocket/devops-is-a-culture-not-a-role-be1bed149b0](https://blog.simontimms.com/images/devops_microservices/infinity.png)\n\nTo achieve success in DevOps there do need to be some changes to how the software is written. Large applications are obviously slower to build than smaller ones, so that applies pressure to create more smaller applications. As teams become larger to maintain a rate of change which limits the scope of changes pushed to production (which you want to limit investigation when something goes wrong) that also pushes towards smaller services. \n\nLogging and instrumentation is a necessity when there is no simple, single process path for a request. Opening a half dozen log files and trying to hunt through them all is obviously far less efficient than entering a query into a log aggregator. \n\nIf the inputs and outputs from a service are well known then that unlocks the ability to blackbox services during testing. This practice allows for much better testing: another feature of a strong DevOps culture. \n\nIndependent services are needed in order to permit different parts of the business to move at different speeds without blocking each other. A monolithic application must wait for all parties to agree before promoting a build. \n\nWithout DevOps microservices would be so much more difficult to manage that it would no longer be worth it. At the same time a lot of the advantages you get from a DevOps culture push the shape of applications built under it to be more like microservices. The two are strongly related. ","categories":[],"tags":[]},{"title":"Terraform for a statically hosted AWS site","authorId":"simon_timms","slug":"2018-10-12-cloudfront-terraform","date":"2018-10-12 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/2018-10-12-cloudfront-terraform/","link":"","permalink":"https://westerndevs.com/_/2018-10-12-cloudfront-terraform/","excerpt":"Just the other day somebody was mentioning to me that they were having trouble setting up a statically hosted site on AWS. That was the kick in the nose I needed to get this article written as it's been on my back-burner for a while. Terraform makes the whole process easy.","raw":"layout: post\ntitle: Terraform for a statically hosted AWS site\nauthorId: simon_timms\ndate: 2018-10-12\noriginalurl: 'https://blog.simontimms.com/2018/10/12/cloudfront_terraform/#more'\n---\n\nJust the other day somebody was mentioning to me that they were having trouble setting up a statically hosted site on AWS. That was the kick in the nose I needed to get this article written as it's been on my back-burner for a while. Terraform makes the whole process easy.\n\n<!-- more -->\n\nAWS, just like the other cloud platforms, offers a myriad of ways to host a website. Frequently, though, what you need is the simplest approach and that is using S3 to do your hosting. S3 doesn't have any smarts behind it so it is really only useful for static sites. However you can do an awful lot with static sites. This blog is statically generated and hosted but you can also do static hosting for most rich JavasScript applications  be they written in Angular, React, Vue, whatever. The one place where S3 falls down is that it cannot do SSL for custom domains. There is [no excuse](https://www.troyhunt.com/heres-why-your-static-website-needs-https/) for not doing SSL these days so we need to make sure our static site has it. \n\nIn front of the S3 bucket we need to put something to do SSL termination. Of course, there is a service for that in the shape of CloudFront. CloudFront is a content delivery network which means that in addition to doing SSL termination it can improve performance through geo-distribution, caching and load balancing. It can even put in restrictions to disallow certain geographic regions from reaching the site. We're just interested in SSL termination right now.\n\nProbably the hardest part of getting going on static hosting is setting up all the pieces. You can click through the, frankly, terrible AWS UI or you can cut out the mouse and use [Terraform](https://www.terraform.io/). Terraform is a tool for writing infrastructure descriptions and it has adapters for all the major cloud providers and [a bunch which aren't major](https://www.terraform.io/docs/providers/index.html). The descriptions are called templates. \n\nLet's take a look at a template for setting up a statically hosted site. We'll include a few services in this template\n\n* S3 - holds our files\n* CloudFront - does SSL termination for S3\n* Route53 - DNS to get our domain to point at CloudFront\n\n\n## Preamble\n\nThis section sets up the provider (the plugin for terraform which tells it how to talk with a cloud provider) and a storage location for the state. Terraform maintains a state of the deployment so it knows how to update resources instead of just destroying them and recreating them. \n\n```\nprovider \"aws\" {\n  region = \"${var.region}\"\n}\n\nterraform {\n  backend \"s3\" {\n    bucket = \"my-awesome-terraform\"\n    key    = \"environment\"\n    region = \"us-east-1\"\n  }\n}\n```\n\n## S3\n\nNext we set up the S3 bucket to hold our files. By now you might have noticed that we're using some variables denoted by the `${variable.name}` syntax. These variables can be defined in a file passed into the terraform tool. It is nice to make these things configurable so they can be reused. \n\nThe S3 is a little complicated because we need to allow public reading and add some CORS rules. The rules here are in place to allow the site to call out to an API. I've left the rules really open here and you probably shouldn't do that in real life. \n\n```\n# s3 for sites\nresource \"aws_s3_bucket\" \"website\" {\n  bucket = \"${var.domain-name}\"\n  acl    = \"public-read\"\n\n  policy = <<EOF\n{\n    \"Version\":\"2008-10-17\",\n    \"Statement\":[{\n    \"Sid\":\"AllowPublicRead\",\n    \"Effect\":\"Allow\",\n    \"Principal\": {\"AWS\": \"*\"},\n    \"Action\":[\"s3:GetObject\"],\n    \"Resource\":[\"arn:aws:s3:::${var.domain-name}/*\"]\n    }]\n}\nEOF\n\n  website {\n    index_document = \"index.html\"\n    error_document = \"error.html\"\n  }\n\n  cors_rule {\n    allowed_headers = [\"*\"]\n    allowed_methods = [\"PUT\", \"POST\"]\n    allowed_origins = [\"*\"]\n    expose_headers  = [\"ETag\"]\n    max_age_seconds = 3000\n  }\n}\n```\n\n## CloudFront\n\nNext we'll set up the CloudFront. This is a very bare-bones set up of CloudFront and doesn't make use of any of the cool advanced features. However, Terraform can do all that configuration for you. You may notice here that we're using that same variable syntax to reference other resources defined in the template. Specifically check out `domain_name = \"${aws_s3_bucket.website.bucket_domain_name}\"` which references back to the S3 container we set up just above.\n\nAnother thing to note here is that I've skipped creating the SSL certificate as part of this tutorial. I assume you've either got your own or you've set up one in AWS already; we just reference it by ARN.\n\n```\n# cloudfront\n\nresource \"aws_cloudfront_origin_access_identity\" \"access_identity\" {\n  comment = \"Access for cloudfront\"\n}\n\nresource \"aws_cloudfront_distribution\" \"distribution\" {\n  origin {\n    domain_name = \"${aws_s3_bucket.website.bucket_domain_name}\"\n    origin_id   = \"origin\"\n\n    s3_origin_config {\n      origin_access_identity = \"${aws_cloudfront_origin_access_identity.access_identity.cloudfront_access_identity_path}\"\n    }\n  }\n\n  aliases             = [\"${var.domain-name}\", \"www.${var.domain-name}\"]\n  enabled             = \"true\"\n  default_root_object = \"index.html\"\n\n  default_cache_behavior {\n    allowed_methods  = [\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"origin\"\n\n    forwarded_values {\n      query_string = false\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n  }\n\n  viewer_certificate {\n    acm_certificate_arn = \"${var.ssl-arn}\"\n    ssl_support_method  = \"sni-only\"\n  }\n}\n```\n\n## Route53\n\nWe're almost there. This part sets up the domain routing in route 53. We have both bare and www prefixed domains here\n\n```\n\n# route 53\ndata \"aws_route53_zone\" \"route53zone\" {\n  name = \"${var.domain-name}.\"\n}\n\n# bare domain\nresource \"aws_route53_record\" \"domain\" {\n  zone_id = \"${data.aws_route53_zone.route53zone.zone_id}\"\n  name    = \"${var.domain-name}\"\n  type    = \"A\"\n\n  alias {\n    name                   = \"${aws_cloudfront_distribution.distribution.domain_name}\"\n    zone_id                = \"${aws_cloudfront_distribution.distribution.hosted_zone_id}\"\n    evaluate_target_health = false\n  }\n}\n\n#www domain\nresource \"aws_route53_record\" \"www-domain\" {\n  zone_id = \"${data.aws_route53_zone.route53zone.zone_id}\"\n  name    = \"www.${var.domain-name}\"\n  type    = \"A\"\n\n  alias {\n    name                   = \"${aws_cloudfront_distribution.distribution.domain_name}\"\n    zone_id                = \"${aws_cloudfront_distribution.distribution.hosted_zone_id}\"\n    evaluate_target_health = false\n  }\n}\n```\n\nFinally we set up an output from the template which will be printed to the console when we deploy the template. Here we're printing out the name servers from Route53, these can be added to our domain.\n\n```\noutput \"name servers\" {\n  value = \"${data.aws_route53_zone.route53zone.name_servers}\"\n}\n\n```\n\n## Variables\n\nOur variables file looks like.\n\n```\nregion = \"us-east-1\"\ndomain-name = \"simontimms.com\"\nssl-arn=\"arn:aws:acm:us-east-1:194820576566:certificate/aabcd5b3-4f32-4cbf-abd4-3b7e5385018a\"\n```\n\nTo run the terraform we need just do something like\n\n```bash\nterraform init #only needed on the first run\nterraform apply -var-file dev.variables.tfvars\n```\n\nTerraform will spin up a CloudFormation stack containing all the resources you've defined. If you need to make changes you can change the template and run the apply command again.\n\n\nThe whole thing together looks like:\n\n```\nprovider \"aws\" {\n  region = \"${var.region}\"\n}\n\nterraform {\n  backend \"s3\" {\n    bucket = \"my-awesome-terraform\"\n    key    = \"environment\"\n    region = \"us-east-1\"\n  }\n}\n\n\n\n# s3 for sites\nresource \"aws_s3_bucket\" \"website\" {\n  bucket = \"${var.domain-name}\"\n  acl    = \"public-read\"\n\n  policy = <<EOF\n{\n    \"Version\":\"2008-10-17\",\n    \"Statement\":[{\n    \"Sid\":\"AllowPublicRead\",\n    \"Effect\":\"Allow\",\n    \"Principal\": {\"AWS\": \"*\"},\n    \"Action\":[\"s3:GetObject\"],\n    \"Resource\":[\"arn:aws:s3:::${var.domain-name}/*\"]\n    }]\n}\nEOF\n\n  website {\n    index_document = \"index.html\"\n    error_document = \"error.html\"\n  }\n\n  cors_rule {\n    allowed_headers = [\"*\"]\n    allowed_methods = [\"PUT\", \"POST\"]\n    allowed_origins = [\"*\"]\n    expose_headers  = [\"ETag\"]\n    max_age_seconds = 3000\n  }\n}\n\n# cloudfront\n\nresource \"aws_cloudfront_origin_access_identity\" \"access_identity\" {\n  comment = \"Access for cloudfront\"\n}\n\nresource \"aws_cloudfront_distribution\" \"distribution\" {\n  origin {\n    domain_name = \"${aws_s3_bucket.website.bucket_domain_name}\"\n    origin_id   = \"origin\"\n\n    s3_origin_config {\n      origin_access_identity = \"${aws_cloudfront_origin_access_identity.access_identity.cloudfront_access_identity_path}\"\n    }\n  }\n\n  aliases             = [\"${var.domain-name}\", \"www.${var.domain-name}\"]\n  enabled             = \"true\"\n  default_root_object = \"index.html\"\n\n  default_cache_behavior {\n    allowed_methods  = [\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"origin\"\n\n    forwarded_values {\n      query_string = false\n\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n  }\n\n  viewer_certificate {\n    acm_certificate_arn = \"${var.ssl-arn}\"\n    ssl_support_method  = \"sni-only\"\n  }\n}\n\n# route 53\ndata \"aws_route53_zone\" \"route53zone\" {\n  name = \"${var.domain-name}.\"\n}\n\n# bare domain\nresource \"aws_route53_record\" \"domain\" {\n  zone_id = \"${data.aws_route53_zone.route53zone.zone_id}\"\n  name    = \"${var.domain-name}\"\n  type    = \"A\"\n\n  alias {\n    name                   = \"${aws_cloudfront_distribution.distribution.domain_name}\"\n    zone_id                = \"${aws_cloudfront_distribution.distribution.hosted_zone_id}\"\n    evaluate_target_health = false\n  }\n}\n\n#www domain\nresource \"aws_route53_record\" \"www-domain\" {\n  zone_id = \"${data.aws_route53_zone.route53zone.zone_id}\"\n  name    = \"www.${var.domain-name}\"\n  type    = \"A\"\n\n  alias {\n    name                   = \"${aws_cloudfront_distribution.distribution.domain_name}\"\n    zone_id                = \"${aws_cloudfront_distribution.distribution.hosted_zone_id}\"\n    evaluate_target_health = false\n  }\n}\n\noutput \"name servers\" {\n  value = \"${data.aws_route53_zone.route53zone.name_servers}\"\n}\n```\n","categories":[],"tags":[]},{"title":"Streaming Code and Play on Mixer every Thursday and Sunday","authorId":"david_wesst","slug":"streaming-code-and-games-on-mixer","date":"2018-09-27 17:55:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/streaming-code-and-games-on-mixer/","link":"","permalink":"https://westerndevs.com/_/streaming-code-and-games-on-mixer/","excerpt":"This month I started livestreaming game code and gameplay every Thursday and Sunday night at 9:30pm CT","raw":"---\nlayout: post\ntitle: \"Streaming Code and Play on Mixer every Thursday and Sunday\"\nauthorId: david_wesst\ndate: 2018-09-27T08:55:42-05:00\nexcerpt: \"This month I started livestreaming game code and gameplay every Thursday and Sunday night at 9:30pm CT\"\n---\n\n<iframe allowfullscreen=\"true\" src=\"https://mixer.com/embed/player/davidwesst\" width=\"620\" height=\"349\"> </iframe>\n\nI recently started streaming twice a week on [Mixer][1]. The streams happen at 9:30pm CT (7:30pm PT or 10:30pm ET) on Thursdays and Sundays. Thursdays are _Code Nights_ where I fumble through the creation of a video game. Sundays are _Play Nights_ I play and analyze a game that has something interesting in it. No specific topic, just play and talk about game with the intent of finding interesting design in games.\n\nNow, I'll take a few questions.\n\n## Are you a game developer now?\n\nNo, but I'd like to be. More specifically, I'd like _my own_ games.\n\nThat's not to say I wouldn't work somewhere that makes games, as I'm sure I'd learn a lot, but as it stands right now with my real life, I'm not willing take on the cost of doing the whole \"lift and shift\" to my whole career. Maybe another day, but not today.\n\n## What about all that web development stuff?\n\nThat's still something I like and will continue to work on, but making a video game has always been something of a dream. I've dabbled in combining the two, but there is a lot I need to learn about game development and design that has nothing to do with code.\n\n## What is with the streaming?\n\nIf I'm being honest, it's more for me than it is for you. No offense.\n\nThe streams act as milestones to keep it relevent in my real life. The streams are scheduled and planned for, as they are hard dates in a calendar that I can plan my life around. Playing a game keeps me thinking about what I want to see in my game, and code keeps me building the game.\n\n## Is that why you've been in hiding for so long?\n\nNot intentionally, but yeah. \n\nThere is a whole story behind it, but I'll save that for another post. The short of it is that during my self imposed sabatical, I realized that I need to work on something I'm truly passionate about. That passion is video games, and this is me making sure I don't regret trying to get myself involved.\n\n## I have an idea for a game...\n\nAwesome. Me too! Feel free to share it with me on [Twitter][2], [Instagram][3], or [Mixer Chat][1] and we can chat about it.\n\n## Does that mean you're leaving the Western Devs?\n\nHeh, no.\n\nFirst, these people would be lost without me. Second, I'm still planning on talking about code, but likely a little less often. I'll be reviving [davidwesst.com][4] in the near future where you can see all my game related content. For the time being though, only devblogs will show up over here.\n\n## When can I buy your game?\n\nI have no idea. Right now, I'm toying with a prototype I call _Vagabond_ and whether that turns into a game is a whole other story.\n\n## Is your game code open source?\n\nNo, but I'm not against it.\n\nIgnoring the whole selling an open source game discussion, my code is amateur at best. I'm an application web developer trying to wear the shoes of a console/PC game developer. It's going to take some time before I feel like I know what I'm doing to actually put something out there for people to assess.\n\n## I'm interested. How do I support?\n\nLike, share, watch, and participate whenever and however you can.\n\nI'm really just getting started in this, and it's going to have some rough edges. Just stay tuned and we'll see where this takes me.\n\n---\nThanks for playing. ~ DW\n\n[1]: https://mixer.com/davidwesst\n[2]: https://twitter.com/davidwesst\n[3]: https://www.instagram.com/davidwesst/\n[4]: https://davidwesst.com","categories":[],"tags":[]},{"title":"Weird JavaScript - Destructuring","authorId":"simon_timms","slug":"JavaScript-Destructuring","date":"2018-07-02 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/JavaScript-Destructuring/","link":"","permalink":"https://westerndevs.com/_/JavaScript-Destructuring/","excerpt":"I've been at this programming game for a long time and I've written two books on JavaScript. Still today I ran into some code that had me scratching my head. It looked like 1function AppliedRoute (&#123; component: C, props: cProps, ...rest &#125;) &#123; I was converting some JavaScript to TypeScript and this line threw an linting error because of implicit any. That means that the type being passed in has no associated type information and has been assumed to be of type any. This is something we'd like to avoid. Problem was I had no idea what this thing was. It looked like an object but it was being built in the parameters?","raw":"---\nlayout: post\ntitle: Weird JavaScript - Destructuring \nauthorId: simon_timms\ndate: 2018-07-02\n---\n\nI've been at this programming game for a long time and I've written two books on JavaScript. Still today I ran into some code that had me scratching my head. It looked like \n\n```javascript\nfunction AppliedRoute ({ component: C, props: cProps, ...rest }) {\n```\n\nI was converting some JavaScript to TypeScript and this line threw an linting error because of `implicit any`. That means that the type being passed in has no associated type information and has been assumed to be of type `any`. This is something we'd like to avoid. Problem was I had no idea what this thing was. It looked like an object but it was being built in the parameters?  \n\n<!-- more -->\n\nWell turns out that after some digging this was a form of JavaScript destructuring. Destructuring means taking an object or an array and extracting values from it into variables. In this weird case we were passing some object into this function and stripping off variables called C, cProps and rest. C was being bound to the field `component`, cProps to the field `props` and then all the rest of the values in the object were being assigned to an object called `rest`. Destructuring is a powerful tool but if you haven't seen the syntax before (this guy right here) then it is confusing as heck. The ever interesting Dr. Axel Rauschmayer has a fantastic article on it over at [http://2ality.com/2015/01/es6-destructuring.html](http://2ality.com/2015/01/es6-destructuring.html) which is well worth a read if you want to avoid my confusion. \n\nAs for the conversion to TypeScript the solution could be as simple as \n\n```javascript\nfunction AppliedRoute ({ component: C, props: cProps, ...rest }: any) {\n```\n\nOr it could involve creating a strong type to annotate what's being passed into the function. I opted for the former because in this case what was being passed in could take many shapes. ","categories":[],"tags":[]},{"title":"Application Insights Alerts","authorId":"simon_timms","slug":"app_insights_alerts","date":"2018-07-01 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/app_insights_alerts/","link":"","permalink":"https://westerndevs.com/_/app_insights_alerts/","excerpt":"Application Insights is another entry in the vast array of log aggregators that have been springing up in the last few years. I think log aggregators are very important for any deployed production system. They give you insight into what is happening on the site and should be your first stop whenever something has gone wrong. Being able to search logs and correlate multiple log streams give you just that much more power. One feature I don't see people using as much as they should is basing alerting off of log information. Let's mash on that.","raw":"---\nlayout: post\ntitle: Application Insights Alerts\nauthorId: simon_timms\ndate: 2018-07-01\noriginalurl: 'https://blog.simontimms.com/2018/07/01/app_insights_alerts/'\n---\n\nApplication Insights is another entry in the vast array of log aggregators that have been springing up in the last few years. I think log aggregators are very important for any deployed production system. They give you insight into what is happening on the site and should be your first stop whenever something has gone wrong. Being able to search logs and correlate multiple log streams give you just that much more power. One feature I don't see people using as much as they should is basing alerting off of log information. Let's mash on that.\n\n<!-- more -->\n\nLet's define my problem first: I have a number of services which kick off every hour and do some processing. If the services don't run for an hour it isn't a huge deal but a couple of hours of not running and data starts to look stale and that's no good. So I actually want to have an alert when messages don't show up in my logs. \n\nThe first step is to get all your applications reporting correctly to app insights. If you're running in an environment with multiple applications reporting (even if it is just front end and back end) then checkout how to set the [cloud role name](https://blog.simontimms.com/2018/07/01/app_insights_appname/).\n\nNext we set up a monitoring job to use that query. Click into monitor\n\n![Selecting cloud_RoleName](https://blog.simontimms.com/images/app_insights_alerts/monitor.png)\n\nThen you can create a new rule. This process starts by setting up some conditions under which the rule will fire. First select your application insights instance from the target selector. Then under criteria add a `custom log search` and put a query into the `Search Query` field. My query is `customEvents | where cloud_RoleName == \"cloud role name\"` which selects everything that had the `cloud role name` cloud_RoleName. I want to make sure this shows up in logs so I set a threshold of less than 1 meaning that if 0 messages are recorded the alert will fire. \n\n![Selecting conditions](https://blog.simontimms.com/images/app_insights_alerts/conditions.png)\n\nFinally, select what should happen when the alert condition is met; this is called an action group. For me it is simply an e-mail notification but you can do a wide variety of things including firing a custom function or logic app to fix the problem.\n\n![Selecting actions](https://blog.simontimms.com/images/app_insights_alerts/full.png)\n\nAnd just like that we've turned passive monitoring into active monitoring with alerts sent to interested parties. A word of caution: don't abuse this new found power. People rapidly become desensitized to alerts which fire too frequently so make sure you're not clobbering people with too much information.","categories":[],"tags":[]},{"title":"Application Insights Cloud Role Name","authorId":"simon_timms","slug":"app_insights_appname","date":"2018-07-01 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/app_insights_appname/","link":"","permalink":"https://westerndevs.com/_/app_insights_appname/","excerpt":"Logging is super important in any microservices environment or really any production environment. Being able to trace where your log messages are coming from is very helpful. Fortunately Application Insights have a field defined for just that.","raw":"---\nlayout: post\ntitle: Application Insights Cloud Role Name\nauthorId: simon_timms\ndate: 2018-07-01\noriginalurl: 'https://blog.simontimms.com/2018/07/01/app_insights_appname/'\n---\n\nLogging is super important in any microservices environment or really any production environment. Being able to trace where your log messages are coming from is very helpful. Fortunately Application Insights have a field defined for just that.\n\n<!--more-->\n\nThe field we should be looking at is called `cloud_RoleName` in the Analytics page. You'll need to set this in your logging client. Depending on the language you're using the way to do this differs. \n\nLet's say that we're going to call our application EMailSender the this is how to set it. (All of these assume you're Applications Insights Client is called `appInsightsClient`)\n\n## C {% raw %}# {% endraw %}\n\n```csharp\nappInsightsClient.Context.Cloud.RoleName = \"EMailSender\"\n```\n\n## F {% raw %}# {% endraw %}\n\n```fsharp\nappInsightsClient.Context.Cloud.RoleName <- \"EMailSender\"\n```\n\n## VB.NET\n\n```csharp\nappInsightsClient.Context.Cloud.RoleName = \"EMailSender\"\n```\n\n## Server-side JavaScript\n\n```javascript\nappInsightsClient.context.tags[appInsightsClient.context.keys.cloudRole] = 'EMailSender';\n```\n\n## Client-side JavaScript\n\nThis one is a little different \n\n```javascript\nappInsightsClient.queue.push(() => {\n    appInsightsClient.context.addTelemetryInitializer((envelope: Microsoft.ApplicationInsights.IEnvelope) => {\n                    envelope.tags['ai.cloud.role'] = \"EMailSender\";\n                });\n});\n```\n\n## Python\n\n```python\nappInsightsClient.context.device.role_name = 'EMailSender'\n```\n\nIn the portal we can now add a new column to our search  results called cloud_RoleName which should be populated with `EMailSender`. We can use this field in any query as needed.\n\n![Selecting cloud_RoleName](https://blog.simontimms.com/images/app_insights_appname/select.png)\n\nI like to drag that column to the left-hand side so I can see it right away.","categories":[],"tags":[]},{"title":"Developer Accountability","slug":"Developer-Accountability","date":"2018-06-07 18:26:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Developer-Accountability/","link":"","permalink":"https://westerndevs.com/podcasts/Developer-Accountability/","excerpt":"Should developers be held accountable for security breaches?","raw":"---\nlayout: podcast\ntitle: Developer Accountability\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-developer-accountability.mp3\n  length: '28:47'\n  filesize: 21112694\n  libsynId: 6680526\n  anchorFmId: Developer-Accountability-evqdhg\nparticipants:\n  - kyle_baley\n  - simon_timms\n  - dylan_smith\n  - justin_self\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Halifax data breach|https://globalnews.ca/news/4191414/halifax-police-data-breach/\"\n  - \"OWASP top ten security risks|https://www.owasp.org/images/7/72/OWASP_Top_10-2017_%28en%29.pdf.pdf\"\ndate: 2018-06-07 14:26:00\nexcerpt: Should developers be held accountable for security breaches?\n---\n\n### Synopsis\n\n* Recap of Nova Scotia data breach\n* Was it hacking?\n* Should developers be liable for ignoring common security breaches?\n* Expectations for junior vs. senior developers\n* Consulting company liability\n* Single developer vs. team\n* Security/code reviews\n* On recouping losses\n* Minimum level of reasonable care?\n* Will GDPR help?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"How To Be a Pretentious Douche Canoe","authorId":"justin_self","slug":"How_To_Be_A_Pretentious_Douche_Canoe","date":"2018-05-21 10:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"mistakes/How_To_Be_A_Pretentious_Douche_Canoe/","link":"","permalink":"https://westerndevs.com/mistakes/How_To_Be_A_Pretentious_Douche_Canoe/","excerpt":"I don't know everything. But that's too much of an obvious statement to mean anything. I don't know everything and I'm ok with that.","raw":"---\nlayout: post\ntitle:  How To Be a Pretentious Douche Canoe\ndate: 2018-05-21T00:00:00-06:00\ncategories: mistakes\ncomments: true\nauthorId: justin_self\n---\n\nI don't know everything. \n\nBut that's too much of an obvious statement to mean anything.\n\nI don't know everything and I'm ok with that.\n\n<!-- more -->\n\nOver the past decade of software development, I've created opinions of my own rather than regurgitating the opinions of my mentors, blogs I've read or books I've skimmed. I don't mean to use \"regurgitating opinions\" in any negative connotation. When I first started in this career, I didn't know anything and I looked to my mentors for guidance and advice. I looked to them for my opinions. When they were disgusted by SOAP interfaces, so was I. When they said guids should never be used as primary keys, I believed them and never tried to use them. But eventually, someone would challenge my \"beliefs\" and I would either:\n\n1. Discredit them or assume they didn't know what they were talking about\n2. Take to heart their point of view and try to reconcile it with what I knew\n\nThe first part of my career was centered a lot around number 1, but I've worked really hard the past several years to move my ego out of the way in order to grow.\n\nIt took me a while to be honest with my self and realize that I took people's challenges very personally at times. Digging deep, I realized I felt it was almost an attack on my competency, as if they were denouncing my experience or skills. Couple this with the fact that I'm a two time college drop out and my ego suddenly became very brittle.\n\nThere was one time, in particular, when I was first leading a team. A junior dev would ask me random trivia questions about .NET or C# to see if I knew them. Some of them I didn't know and he would proudly tell me the answer. To me, that was challenging some pseudo authority I had granted myself. I felt as if he was asserting his dominance over my informal education or trying to show that I wasn't qualified to be where I was. This, in turn, made me dismissive of his input or propel me to be extremely critical of his ideas and approaches.\n\nDamn, even as I type this out, I'm still ashamed of how I felt back then. It was pathetic and my heart hurts when I think about my son and daughter learning how fragile daddy's ego can be.\n\nIt took me a long time, and a lot of introspection, to realize that he merely wanted to impress me. Yeah, I felt like a big douche canoe afterwards. Instead of challenging me, he saw me as an authority and wanted to try to prove herself against a higher bar she set. A bar she set with me in mind.\n\nI've got many stories of how my ego precluded me from adding value or cultivating deeper relationships with those around me. Thankfully, I know myself better now and I'm better equipped to handle it. Every now and then, however, I can feel myself slip into that deep abyss of self-doubt and sulk at the sunken pillar constructed of my fears and failures. This can cause me to lash out to those around me and manifest in ways that people can't really see. \n\nWhat's interesting, though, is since I'm a very sensitive person, I'm often acutely aware how I come across to others. Once I realize I allowed a vein of fragility to infect my confidence and composure, I'll go back to the person afterwards and apologize. However, they are typically completely oblivious to the internal Goliath I was facing at the time and had no reason to believe waters beneath didn't match the calm, glass like surface.\n\nI've gotten a lot better at this over the past 7 years; it's been a real focus of mine. When I feel myself hardening within a shell of pretentiousness and entitlement, it's a sign to me that I need to humble myself and remember that I'm not playing some zero-sum game where if someone else wins then I lose.  I haven't opened up to many people about this struggle. I'm writing this post as a cathartic means of freeing myself from those chains in hopes others can tell me their experiences and tell me if they struggle with their own ego at times as I do.\n\nHowever, even if no one agrees and all I get is an inbox full of \"don't be a pretentious douche canoe\", I'll still be content. This career I've chosen is starting to become far less about software and far more about value. Don't get me wrong, I feel very competent as a software developer. But I'll admit when I don't know something and I'll be the first person to ask you what the acronym means that you said in a side remark discussing your project's problem.\n","categories":[{"name":"mistakes","slug":"mistakes","permalink":"https://westerndevs.com/categories/mistakes/"}],"tags":[]},{"title":"SignalR as a Service","authorId":"simon_timms","slug":"SignalR-Service","date":"2018-05-17 17:10:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/SignalR-Service/","link":"","permalink":"https://westerndevs.com/Development/SignalR-Service/","excerpt":"The SignalR service which recently entered public preview in Azure closes the loop on building rich serverless applications.","raw":"---\nlayout: post\ntitle: \"SignalR as a Service\"\ntags:\n  - SignalR\n  - Azure\n  - Serverless\ncategories:\n  - Development\nauthorId: simon_timms\ndate: 2018-05-17 13:10:00\nexcerpt: The SignalR service which recently entered public preview in Azure closes the loop on building rich serverless applications.\n---\n\nI've been pretty impressed with the Azure Functions over the last year or so. The team started off pretty far behind AWS Lambda but they're accelerating quickly and are, in many ways, leading. A while back I was playing with the idea of building an entire web application in a serverless fashion. Most of the problems have been solved. \n\nThe application would be a single page application with the content served out from blob storage. The blob storage is [fronted by Azure CDN](https://docs.microsoft.com/en-us/azure/cdn/cdn-create-a-storage-account-with-cdn) to enable custom domains and HTTPS. The API which the application talks to is all handled by Azure functions. Logging into the application is handled by Azure Active Directory and an [OAuth 2.0 implicit authorization workflow](https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-authentication-scenarios#single-page-application-spa) for which there is even [a library](https://github.com/AzureAD/azure-activedirectory-library-for-js). The token from the authorization allows calling the function directly from the browser. \n\nFor most applications this would be enough but there is a certain class of application which needs a little bit more: real time updates. There are tons of applications which would benefit from this sort of interaction. Yes there are chat applications and mapping applications which benefit from this but I actually think the scope of applications which benefit from real time updates is larger than that. \n\nCQRS+ES or microservice based applications struggle with how to build user interfaces which are resilient to the possibility that command will be executed in a delayed fashion. Over the years I've played with a few models of how to build the user interface without finding a really satisfactory solution. If events from the command handlers could be published out to browsers directly then I think opens up some really nice interaction models. Anything which lowers the bar to adopting applications which are resilient to the delays inherent in out of process microservices is a great step towards building better applications in general.  \n\nFunctions don't lend themselves to the kinds of long running processes necessary to hold open a web socket or doing long poling. It would be expensive and likely wouldn't scale all that well. \n\nThat's why I'm so excited about the new [SignalR service](): it is the missing puzzle piece in building out distributed applications which can scale both in an upwards and downwards fashion and are highly reliable. The case for building server side applications in something like ASP.NET or Rails is getting narrower and narrower. \n\nDeath to managing servers. Long live serverless.","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"SignalR","slug":"SignalR","permalink":"https://westerndevs.com/tags/SignalR/"},{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/tags/Azure/"},{"name":"Serverless","slug":"Serverless","permalink":"https://westerndevs.com/tags/Serverless/"}]},{"title":"Reporting Success in JavaScript Lambdas when Using AppInsights","authorId":"simon_timms","slug":"javascript-lambdas","date":"2018-05-15 17:10:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/javascript-lambdas/","link":"","permalink":"https://westerndevs.com/Development/javascript-lambdas/","excerpt":"AWS Lambda provides a solid platform for doing serverless coding but when used in conjunction with Application Insights there are some tricks to get the function to return properly.","raw":"---\nlayout: post\ntitle: \"Reporting Success in JavaScript Lambdas when Using AppInsights\"\ntags:\n  - JavaScript\n  - AWS\n  - Lambda\n  - AppInsights\ncategories:\n  - Development\nauthorId: simon_timms\ndate: 2018-05-15 13:10:00\nexcerpt: AWS Lambda provides a solid platform for doing serverless coding but when used in conjunction with Application Insights there are some tricks to get the function to return properly.\n---\n\nIn the last week I've run into two separate AWS Lambdas related to reporting success. See when you have a lambda which is trigged by a cloud watch event (a scheduled lambda) then the runtime will attempt to execute it 3 times in the event of an error. In order to detect if a function has succeeded the runtime needs you to fire the callback with `null` as the first parameter. This can be a little tricky to get right in an asynchronous world. \n\nThe signature of a JavaScript lambda is \n\n```\nlambda(event: any, context: Context, callback: Function)\n```\n\nThat callback is actually critical to reporting success back to lambda. The reason is that lambda keeps the nodejs runtime active to speed up subsequent calls to the lambda. You certainly don't want to call `process.exit()`, which is something I've done incorrectly a few times now. If you don't report success the function will run until they have used up their time allocation, which is 6 seconds by default, and are killed by the runtime. \n\nNow with Application Insights you need to call `flush()` before you terminate the function. This ensure that all the logs gathered actually make it up to the Application Insights endpoint. The thing with the flush is that it is actually an asynchronous call. This means you have to be a little careful about when you call the callback. \n\nThe best method I've found is to do\n\n```\ntc.flush({\n    callback: () => {\n        context.succeed('Messages sent');\n        callback(null, {});\n    }\n});\n```\n\nor, in a catch block which wraps the lambada body \n\n```\ntc.flush({\n    callback: () => {\n        context.fail(e);\n        callback(e);\n    }\n});\n```\n\nUnfortunately this best practice isn't quite enough for the nodejs Application Insights client. It maintains some active tasks in the event loop which means that the lambda runtime believes that the lambda is still running and doesn't terminate. You can get over this by setting\n\n```\ncontext.callbackWaitsForEmptyEventLoop = false;\n```\n\nas the first thing you function does. With all of this in place I've found the writing lambdas in typescript is somewhat palatable.  \n\n# Bonus\n\n*Why the heck are you using Application Insights on AWS? They have Cloud Watch, you know*\n\nOh because Application Insights is madly, wildly better than anything on AWS. It provides searching, notification, graphing and aggregating at a level that is on par or better than anything I've seen on the market from the likes of Sumo Logic, Log Entries or Tableau. ","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"AWS","slug":"AWS","permalink":"https://westerndevs.com/tags/AWS/"},{"name":"Lambda","slug":"Lambda","permalink":"https://westerndevs.com/tags/Lambda/"},{"name":"AppInsights","slug":"AppInsights","permalink":"https://westerndevs.com/tags/AppInsights/"}]},{"title":"Starting meetings with six-page memos","slug":"Six-page-memos","date":"2018-05-01 20:09:08+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Six-page-memos/","link":"","permalink":"https://westerndevs.com/podcasts/Six-page-memos/","excerpt":"Wherein the Western Devs expect you not only to show up to meetings, but to read as well","raw":"---\nlayout: podcast\ntitle: Starting meetings with six-page memos\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-six-page-memo-meetings.mp3\n  length: '36:51'\n  filesize: 27362220\n  libsynId: 6545481\n  anchorFmId: Starting-meetings-with-six-page-memos-evqdhe\nparticipants:\n  - simon_timms\n  - david_wesst\n  - dylan_smith\n  - kyle_baley\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"What Jeff Bezos learned from six-page memos|https://www.cnbc.com/2018/04/23/what-jeff-bezos-learned-from-requiring-6-page-memos-at-amazon.html\"\n  - \"Amazon's letter to shareholders|https://www.sec.gov/Archives/edgar/data/1018724/000119312518121161/d456916dex991.htm\"\n  - \"Julia Evans comics|https://drawings.jvns.ca/\"\n  - \"Julia Evans on Twitter|https://twitter.com/b0rk\"\n  - \"Elon Musk: It's okay to walk out|https://www.inc.com/kevin-j-ryan/elon-musk-email-to-tesla-employees-productivity-tips.html\"\ndate: 2018-05-01 16:09:08\nrecord: 2018-04-26\nexcerpt: Wherein the Western Devs expect you not only to show up to meetings, but to read as well\n---\n\n### Synopsis\n\n* Amazon's six-page memo meetings\n* Written vs. spoken word\n* First impressions: This is stupid\n* Does this discourage participants from preparing?\n* The art of preparation\n* What kind of meetings is this for?\n* Allowing time for reflection\n* What if not everyone has the same native language?\n* How to introduce it into an organization\n* The entertainment factor\n* Would it work at a conference?\n* Couching the practice as continuous improvement\n* Meeting etiquette\n* What makes a good memo\n* Methods of padding the memo\n* If it's good enough for Jeff Bezos...\n* Remote meetings\n* For next time: How to walk out of a meeting","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Moving to SSL","slug":"Moving-to-SSL","date":"2018-04-23 16:57:03+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Moving-to-SSL/","link":"","permalink":"https://westerndevs.com/podcasts/Moving-to-SSL/","excerpt":"Sleep easy knowing all your interactions with the Western Devs are secure and free from prying eyes","raw":"---\nlayout: podcast\ntitle: Moving to SSL\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-moving-to-ssl.mp3\n  length: '33:12'\n  filesize: 79666647\n  libsynId: 6514831\n  anchorFmId: Moving-to-SSL-evqdjq\nparticipants:\n  - kyle_baley\n  - david_wesst\n  - donald_belcham\n  - dylan_smith\n  - simon_timms\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Does my site need HTTPS?|https://doesmysiteneedhttps.com/\"\n  - \"westerndevs.com SSL report|https://www.ssllabs.com/ssltest/analyze.html?d=westerndevs.com&latest\"\n  - \"Let's Encrypt|https://letsencrypt.org/\"\n  - \"Travis CI|https://travis-ci.org/\"\n  - \"Prairie Dev Con|http://prairiedevcon.com\"\ndate: 2018-04-23 12:57:03\nrecorded: 2018-04-19\nexcerpt: Sleep easy knowing all your interactions with the Western Devs are secure and free from prying eyes\n---\n\n### Synopsis\n\n* Why move to SSL for a static site?\n* Why AWS?\n* When physical security is at stake\n* Just how easy is it (hint: very easy but watch out for CDN)\n* Where we came from and where we are now\n* C'mon Microsoft, make this easier/cheaper on Azure\n* The AWS vs. Azure user experience\n* DNS issues between GoDaddy and AWS Route 53\n* Cloudfront and the origin\n* Travis CI rocks!\n* Invalidating the cache\n* What's next: CDN, Git LFS and images\n* IBM has a cloud?","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Loading Related Entities with Dapper Many-to-One - Part 2","authorId":"dave_paquette","slug":"loading-related-entities-many-to-one-part-2","date":"2018-04-10 22:04:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/loading-related-entities-many-to-one-part-2/","link":"","permalink":"https://westerndevs.com/Dapper/loading-related-entities-many-to-one-part-2/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we look at a second option for loading Many-to-One related entities.","raw":"---\nlayout: post\ntitle: Loading Related Entities with Dapper Many-to-One - Part 2\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2018/04/10/loading-related-entities-many-to-one-part-2.aspx'\ndate: 2018-04-10 18:04:42\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we look at a second option for loading Many-to-One related entities.\n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n\n*Update: April 16, 2018* Something really cool happened in the comments. The amazing [Phil Bolduc](https://twitter.com/PathTooLong) very kindly pointed out that the query I wrote was not optimal and as a result, my benchmarks were not showing the best results. He didn't stop there, he also [submitted a pull request](https://github.com/AspNetMonsters/DapperSeries/pull/2) to the sample repo so I could rerun my benchmarks. Great job Phil and thanks a ton for being constructive in the comments section! I have updated the post to include Phil's superior query.   \n\nIn today's post, we look at another option for how to load Many-to-One relationships. In the last post, we used a technique called Multi-Mapping to load related Many-to-One entities. In that post, I had a theory that maybe this approach was not the most efficient method for loading related entities because it duplicated a lot of data.\n\n![Many-to-One](https://www.davepaquette.com/images/dapper/flight_to_airport_many_to_one.png)\n\nTo recap, we would like to load a list of `ScheduledFlight` entities. A `ScheduleFlight` has a departure `Airport` and an arrival `Airport`.\n\n{% codeblock lang:csharp %}\npublic class ScheduledFlight \n{\n    public int Id {get; set;}\n    public string FlightNumber {get; set;}\n\n    public Airport DepartureAirport {get; set;}\n    public int DepartureHour {get; set;}\n    public int DepartureMinute {get; set;}\n\n    public Airport ArrivalAirport {get; set;}        \n    public int ArrivalHour {get; set;}\n    public int ArrivalMinute {get; set;}\n\n   //Other properties omitted for brevity \n}\n\npublic class Airport \n{\n    public int Id {get; set;}\n    public string Code {get; set;}\n    public string City {get; set;}\n    public string ProvinceState {get; set;}\n    public string Country {get; set;}\n}\n{% endcodeblock %} \n\n## Using Multiple Result Sets\nIn the previous post, we loaded the `ScheduledFlight` entities and all related `Airport` entities in a single query. In this example we will use 2 separate queries: One for the `ScheduledFlight` entities, one for the related arrival and departure `Airport` entities. These 2 queries will all be executed as a single sql command that returns multiple result sets. \n\n{% codeblock lang:sql %}\nSELECT s.Id, s.FlightNumber, s.DepartureHour, s.DepartureMinute, s.ArrivalHour, s.ArrivalMinute, s.IsSundayFlight, s.IsMondayFlight, s.IsTuesdayFlight, s.IsWednesdayFlight, s.IsThursdayFlight, s.IsFridayFlight, s.IsSaturdayFlight,\ns.DepartureAirportId, s.ArrivalAirportId\nFROM ScheduledFlight s\n\tINNER JOIN Airport a1\n\t\tON s.DepartureAirportId = a1.Id\n    WHERE a1.Code = @FromCode\n    \nSELECT Airport.Id, Airport.Code, Airport.City, Airport.ProvinceState, Airport.Country\nFROM Airport\n  WHERE Airport.Id = @DepartureAirportId\n    OR Airport.Id IN (SELECT s.ArrivalAirportId\n    FROM ScheduledFlight s\n\t\tWHERE s.DepartureAirportId = @DepartureAirportId)\n{% endcodeblock %}  \n\nUsing Dapper's `QueryMultipleAsync` method, we pass in 2 arguments: the query and the parameters for the query. \n\n{% codeblock lang:csharp %}\npublic async Task<IEnumerable<ScheduledFlight>> GetAlt(string from)\n{\n  IEnumerable<ScheduledFlight> scheduledFlights;\n  using (var connection = new SqlConnection(_connectionString))\n  {\n  await connection.OpenAsync();\n  var query = @\"\nSELECT s.Id, s.FlightNumber, s.DepartureHour, s.DepartureMinute, s.ArrivalHour, s.ArrivalMinute, s.IsSundayFlight, s.IsMondayFlight, s.IsTuesdayFlight, s.IsWednesdayFlight, s.IsThursdayFlight, s.IsFridayFlight, s.IsSaturdayFlight,\ns.DepartureAirportId, s.ArrivalAirportId\nFROM ScheduledFlight s\n\tINNER JOIN Airport a1\n\t\tON s.DepartureAirportId = a1.Id\n    WHERE a1.Code = @FromCode\n    \nSELECT Airport.Id, Airport.Code, Airport.City, Airport.ProvinceState, Airport.Country\nFROM Airport\n\tWHERE Airport.Id = @DepartureAirportId\n\t  OR Airport.Id IN (SELECT s.ArrivalAirportId\n       FROM ScheduledFlight s\n\t\t   WHERE s.DepartureAirportId = @DepartureAirportId)\";\n\n    using (var multi = await connection.QueryMultipleAsync(query, new{FromCode = from} ))\n    {\n        scheduledFlights = multi.Read<ScheduledFlight>();\n        var airports = multi.Read<Airport>().ToDictionary(a => a.Id);\n        foreach(var flight in scheduledFlights)\n        {\n            flight.ArrivalAirport = airports[flight.ArrivalAirportId];\n            flight.DepartureAirport = airports[flight.DepartureAirportId];\n        }\n\n    }\n  }\n  return scheduledFlights;\n}\n{% endcodeblock %}  \n\nThe `QueryMultipleAsync` method returns a `GridReader`. The `GridReader` makes it very easy to map mutliple result sets to different objects using the `Read<T>` method. When you call the `Read<T>` method, it will read all the results from the next result set that was returned by the query. In our case, we call `Read<ScheduledFlight>` to read the first result set and map the results into a collection of `ScheduledFlight` entities. Next, we call `Read<Airport>` to read the second result set. We then call `ToDictionary(a => a.Id)` to populate those `Airport` entities into a dictionary. This is to make it easier to read the results when setting the `ArrivalAirport` and `DepartureAirport` properties for each `ScheduledFlight`. \n\nFinally, we iterate through the scheduled flights and set the `ArrivalAirport` and `DepartureAirport` properties to the correct `Airport` entity.  \n\nThe big difference between this approach and the previous approach is that we no longer have duplicate instances for `Airport` entities. For example, if the query returned 100 scheduled flights departing from Calgary (YYC), there would be a single instance of the `Airport` entity representing YYC, whereas the previous approach would have resulted in 100 separate instances of the `Airport` entity.\n\nThere is also less raw data returned by the query itself since the columns from the `Airport` table are not repeated in each row from the `ScheduleFlight` table.\n\n## Comparing Performance\nI had a theory that the multi-mapping approach outlined in the previous blog post would be less efficient than the multiple result set approach outlined in this blog post, at least from a memory usage perspective. However, a theory is just theory until it is tested. I was curious and also wanted to make sure I wasn't misleading anyone so I decided to test things out using [Benchmark.NET](http://benchmarkdotnet.org/). Using Benchmark.NET, I compared both methods using different sizes of data sets.\n\nI won't get into the details of Benchmark.NET. If you want to dig into it in more detail, visit the [official site](http://benchmarkdotnet.org/) and read through the docs. For the purposes of this blog post, the following legend should suffice:\n\n```\n  Mean      : Arithmetic mean of all measurements\n  Error     : Half of 99.9% confidence interval\n  StdDev    : Standard deviation of all measurements\n  Gen 0     : GC Generation 0 collects per 1k Operations\n  Gen 1     : GC Generation 1 collects per 1k Operations\n  Gen 2     : GC Generation 2 collects per 1k Operations\n  Allocated : Allocated memory per single operation (managed only, inclusive, 1KB = 1024B)\n```\n\n### 10 ScheduledFlight records\n\n|             Method |     Mean |     Error |    StdDev |  Gen 0 | Allocated |\n|------------------- |---------:|----------:|----------:|-------:|----------:|\n|       MultiMapping | 397.5 us |  3.918 us |  4.192 us | 5.8594 |   6.77 KB |\n| MultipleResultSets | 414.2 us |  6.856 us |  6.077 us | 4.8828 |   6.69 KB |\nAs I suspected, the difference is minimal when dealing with small result sets. The results here are in microseconds so in both cases, executing the query and mapping the results takes less 1/2 a millisecond. The mutliple result sets approach takes a little longer, which I kind of expected because of the overhead of creating a dictionary and doing lookups into that dictionary when setting the `ArrivalAirport` and `DepartureAirport` properties. The difference is minimal and in a most real world scenarios, this won't be noticable. What is interesting is that even with this small amount of data, we can see that there is ~1 more Gen 0 garbage collection happening per 1,000 operations. I suspect we will see this creep up as the amount of data increases.\n\n\n### 100 ScheduledFlight records\n\n|             Method |     Mean |     Error |    StdDev |   Gen 0 |  Gen 1 | Allocated |\n|------------------- |---------:|----------:|----------:|--------:|-------:|----------:|\n|       MultiMapping | 1.013 ms | 0.0200 ms | 0.0287 ms | 25.3906 | 5.8594 |   6.77 KB |\n| MultipleResultSets | 1.114 ms | 0.0220 ms | 0.0225 ms | 15.6250 |      - |   6.69 KB |\n\n|             Method |       Mean |     Error |    StdDev |   Gen 0 | Allocated |\n|------------------- |-----------:|----------:|----------:|--------:|----------:|\n|       MultiMapping |   926.5 us | 21.481 us | 32.804 us | 25.3906 |   6.77 KB |\n| MultipleResultSets |   705.9 us |  7.543 us |  7.056 us | 15.6250 |   6.69 KB |\n\n When mapping 100 results, the multiple result sets query is already almost 25% faster. Keep in mind though that both cases are still completing in less than 1ms so this is very much still a micro optimization (pun intented). Either way, less than a millsecond to map 100 records is crazy fast. \n\n ### 1000 ScheduledFlight records\n|             Method |     Mean |     Error |    StdDev |    Gen 0 |   Gen 1 | Allocated |\n|------------------- |---------:|----------:|----------:|---------:|--------:|----------:|\n|       MultiMapping | 5.098 ms | 0.1135 ms | 0.2720 ms | 148.4375 | 70.3125 |   6.77 KB |\n| MultipleResultSets | 2.809 ms | 0.0549 ms | 0.0674 ms | 109.3750 | 31.2500 |   6.69 KB |\n\nHere we go! Now the multiple result sets approach finally wins out, and you can see why. There are way more Gen 0 and Gen 1 garbage collections happening per 1,000 operations when using the multi-mapping approach. As a result, the multiple result sets approach is nearly twice as fast as the multi mapping approach. \n\n ### 10,000 ScheduledFlight records\n|             Method |     Mean |     Error |    StdDev |     Gen 0 |    Gen 1 |    Gen 2 | Allocated |\n|------------------- |---------:|----------:|----------:|----------:|---------:|---------:|----------:|\n|       MultiMapping | 56.08 ms | 1.5822 ms | 1.4026 ms | 1687.5000 | 687.5000 | 187.5000 |   6.78 KB |\n| MultipleResultSets | 24.93 ms | 0.1937 ms | 0.1812 ms |  843.7500 | 312.5000 | 125.0000 |   6.69 KB \n\nOne last test with 10,000 records shows a more substantial difference. The multiple result sets approach is a full 22ms faster!\n\n## Wrapping it up\nI think that in most realistic scenarios, there is no discernable difference between the 2 approaches to loading many-to-one related entities. If you loading larger amounts of records into memory in a single query, then the multiple result sets approach will likely give you better performance. If you are dealing with < 100 records per query, then you likely won't notice a difference. Keep in mind also that your results will vary depending on the specific data you are loading. ","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Environment Agnostic Packaging - Just Do It","authorId":"simon_timms","slug":"Environment-independent-packaging","date":"2018-03-28 23:38:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/Environment-independent-packaging/","link":"","permalink":"https://westerndevs.com/Development/Environment-independent-packaging/","excerpt":"I've been noticing a bit of a trend lately around how some tools suggest you package your builds: they build differently for each environment. This is super-inconvenient if you're trying to progress a package through multiple environments. Just don't package configuration in with your build packages.","raw":"---\nlayout: post\ntitle: Environment Agnostic Packaging - Just Do It\ntags:\n  - devops\n  - builds\ncategories:\n  - Development \nauthorId: simon_timms\ndate: 2018-03-28 19:38:30 \n---\n\nI've been noticing a bit of a trend lately around how some tools suggest you package your builds: they build differently for each environment. This is super-inconvenient if you're trying to progress a package through multiple environments. Just don't package configuration in with your build packages. \n\n<!-- more -->\n\nOkay, let's break this down: when we build software as part of a release process on a CI server we end up with a package at the end. In modern devops, or what I would have called 'release engineering' back in the day, we then take this package and run it through the rest of the pipeline. The rest of the pipeline may consist of deploying to a test server, running integration tests, running UI tests, and promoting to higher environments. You can think the pipeline extending all the way out to the end user. The rule of thumb is that it is cheaper to catch errors and problems earlier in the build pipeline than later. That makes a lot of sense: a unit test catching a problem has an impact only on that developer while a user discovering a problem involves all sorts of layers of technical support and issue tracking, not to mention the cost of redeploying. \n<style>\n@keyframes slidein {\n  from {\n    left: 0%;\n  }\n\n  to {\n    left: 620px;\n  }\n}\n.block{\nwidth: 50px; \nheight: 50px; \n  background-color: #39d; \n  animation-duration: 12s;\n  animation-name: slidein;\n  animation-iteration-count: infinite;\n  animation-timing-function: linear;\n  display: inline-block;\n  position: absolute;\n}\n</style>\n<div style=\"width: 100%; border-bottom: 3px black solid; height: 50px\">\n<div style=\"animation-delay: 0s;\" class=\"block\"></div>\n<div style=\"animation-delay: -8s;\" class=\"block\"></div>\n<div style=\"animation-delay: -4s;\" class=\"block\"></div>\n</div>\n\nOne of the key features of the build pipeline is that the package which is released to production is the same package which has gone through the pipeline. If the package is different then what is the point of all the testing done in the pipeline? \n\nThat questions was rhetorical but let me answer it anyway: There is no point, it is a waste and it wrecks the quality of your software. This is why I'm so surprised to see a number of really popular software packages which suggest compiling environment information into the package. This means that it no longer possible to deploy the package to testing environments or to environments not envisioned at build time. \n\nThe two packages I've encountered recently which commit this sin are Angular 2/4/5 and the Serverless framework. Angular's build packages everything up using the [environment](https://blog.angulartraining.com/how-to-manage-different-environments-with-angular-cli-883c26e99d15) selected at build time and Serverless uses [stages](https://serverless.com/framework/docs/providers/aws/guide/variables/) which are basically the same thing. \n\nNow some people will claim that just rebuilding the same source code for each environment isn't a big deal: it will result it the same executable. Except that there are tons of times when it won't. `#ifdefs` can exercise different code paths, optimization levels on the compiler can produce different output, any one of a hundred other things could be different. The only way to be sure the package progressing through environments is correct is to ensure that it is, indeed, the same package. \n\nIf you're a developer choosing frameworks I implore you to consider the full lifecycle of your package as it travels down the pipeline. Just pick a framework which supports sane devops practices. Just do it.  ","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"},{"name":"builds","slug":"builds","permalink":"https://westerndevs.com/tags/builds/"}]},{"title":"F12 Chooser is a Dev Tool Thing","authorId":"david_wesst","slug":"f12-chooser-is-dev-tool-thing","date":"2018-03-28 17:10:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/f12-chooser-is-dev-tool-thing/","link":"","permalink":"https://westerndevs.com/Development/f12-chooser-is-dev-tool-thing/","excerpt":"The F12 Chooser is a development tool thing that I like and you should know about if you want your web application or web extension to support Microsoft Edge.","raw":"---\nlayout: post\ntitle: \"F12 Chooser is a Dev Tool Thing\"\ntags:\n  - JavaScript\n  - TypeScript\n  - Web Extensions\n  - Office Add-In\ncategories:\n  - Development\nauthorId: david_wesst\ndate: 2018-03-28 13:10:00\n---\n\nThe F12 Chooser is a development tool thing that I like and you should know about if you want your web application or web extension to support Microsoft Edge.\n\n<!-- more -->\n\n[1]: https://i.imgur.com/wt4L09z.png\n[2]: https://i.imgur.com/ch1TjEr.gif\n[3]: https://docs.microsoft.com/en-us/office/dev/add-ins/testing/debug-add-ins-using-f12-developer-tools-on-windows-10\n[4]: https://docs.microsoft.com/en-us/microsoft-edge/devtools-guide\n\n![1]\n\nThe [F12 Developer Tools][4] are pretty great. They are the original in-browser developer tools (included in Internet Explorer 7), and have evolved into something more modern for all us \"modern\" developers.\n\nWith all in-browser developer tools, I've found that every once and I come across an application I'm trying to debug that is so unstable that F12 can't seem to attache properly. Whether that's because of the application locking up the browser or whatever, without being able to attach a debugger I can't really get into the code and start sorting out the issue.\n\nThat's where the F12 Chooser comes into play.\n\n## What is F12 Chooser?\nF12 Chooser is a utility built into Windows that allows you choose the target application for the F12 Developer Tools without having to open Microsoft Edge itself.\n\n## How do I run it?\nOn Windows 10, you run `\\\\Windows\\System32\\F12\\F12Chooser.exe`. The window that comes up will display a list of targets for which you can attach the F12 tools. You can find the 64-bit version in `C:\\Windows\\SysWOW64\\F12\\F12Chooser.exe`.\n\n![2]\n\n## Why does this matter?\nBecause it gives you another option when it seems like the F12 tools are failing. If your browser locks up when you try and debug your application code, you should try the F12 Chooser once the application has loaded in the browser.\n\nIt also allows you to target applications that aren't necessarily web applications that you view in a browser. For example, maybe you're looking to debug an [Office Add-In][3].\n\n## Conclusion\nIn conclusion, you have the F12 Chooser as another way to load up and attach the F12 developer tools in Windows 10 to help you with debugging web applications, web extensions, and even other things like Office Add-Ins.\n\nAnd now you know it exists. You're welcome.","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://westerndevs.com/tags/TypeScript/"},{"name":"Web Extensions","slug":"Web-Extensions","permalink":"https://westerndevs.com/tags/Web-Extensions/"},{"name":"Office Add-In","slug":"Office-Add-In","permalink":"https://westerndevs.com/tags/Office-Add-In/"}]},{"title":"Picking between TypeScript and JavaScript","authorId":"david_wesst","slug":"typescript-or-javascript","date":"2018-03-15 15:40:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/typescript-or-javascript/","link":"","permalink":"https://westerndevs.com/Development/typescript-or-javascript/","excerpt":"Which one should you pick: TypeScript or JavaScript? They are both great languages, people always ask me when you should use one or the other. This post puts that to rest.","raw":"---\nlayout: post\ntitle: \"Picking between TypeScript and JavaScript\"\ntags:\n  - JavaScript\n  - TypeScript\ncategories:\n  - Development\nauthorId: david_wesst\ndate: 2018-03-15 11:40:00\n---\n\nWhich one should you pick: TypeScript or JavaScript? They are both great languages, people always ask me when you should use one or the other. This post puts that to rest.\n\n<!-- more -->\n\n[1]: https://i.imgur.com/Wxx5fMI.png\n[2]: https://i.imgur.com/IAPRGfv.png\n[3]: https://i.imgur.com/yCn8NUQ.png\n[4]: https://code.visualstudio.com\n[5]: https://github.com/tc39/ecma262\n[6]: https://code.visualstudio.com/docs/languages/javascript#_type-checking-and-quick-fixes-for-javascript-files\n\nWhether it's a work project or a personal one, the question \"TypeScript or JavaScript\" always seems to come up in my mind. Utlimately, they provide a very similiar function considering that TypeScript is a superset of JavaScript, and compiles down to JavaScript itself. \n\nI'm not the only person that has this question either. Over the past year, I've asked a number of JavaScript/TypeScript developers about how they pick between the two and I wanted to sum up my thoughts here after being influenced by my private panel of experts.\n\n## It Depends on the Project\nOf course it does. \n\nThere is never one answer for everything, and this is no different. That being said, there are a few criteria or \"flags\" that help me select when I want to use one over the other.\n\n### JavaScript Knowledge is Assumed\nBefore we get into it, let me clarify that I'm assuming that the developer(s) working on the project already know JavaScript. They don't need to be experts, but they are familiar with writing vanilla JavaScript for applications, whether that be client or server side code.\n\nThat being said, I'm also assuming that a TypeScript-focued developer can get their way around JavaScript code.\n\n## When to TypeScript\n\n![][2]\n\nI fall to TypeScript when I am writing more than one or two code files or if I'm writing code that I expect someone else to have to run. Although I use TypeScript, the it's not necessarily the language itself that I want, but the TypeScript compiler as it helps the other developers running my code, and removes the ambiguity of types between functions or classes that need to work togther.\n\nIt does a lot of stuff for me:\n\n  + Catches errors, especially typing ones, at compile time rather than run time\n  + Sets standards around what JS-like conventions I want to use\n  + Better legacy browser support\n  + Supports multiple module practices\n\nUltimately, that compiler is powerful and I put a lot of trust into it considering I expect that the compiled code to be optimal.\n\n### But doesn't the compiler work on JavaScript too?\nYes. Yes it does.\n\nThe catch is that the compiler is not as thurough as it is with TypeScript. When you add the `//@ts-check` [reference][6] at the top of your JavaScript file in Visual Studio Code, that really helps with the development story of any JavaScript code, but it's still not as deep as using TypeScript itself.\n\nUsing TypeScript with the TypeScript compiler gives you that little bit of extra help in development, and that is really where the value comes in for me.\n\n## When to JavaScript\n\n![][3]\n\nI tend to use JavaScript when I'm only writing a little bit of code and don't want to deal with the overhead of setting up the compiler for the project. For examples, when I'm writing a little Node script, or experimenting with REST API and want a simple GUI, I'll quickly put together some vanilla JS code and get something working quickly.\n\nThat being said, I write the majority of my JavaScript in [Visual Studio Code][4] which provides a lot of JavaScript tooling support using the TypeScript compiler underneath the hood.\n\n## Conclusion\nIn conclusion, when I have to pick between JavaScript or TypeScript I lean towards TypeScript. It provides the better development story between the two, and that's a really important factor when I'm writing and sharing code.\n\nWhen I'm lazy and don't want to setup a TypeScript project, I fall back on JavaScript but still rely on the built-in TypeScript tooling in Visual Studio Code.\n\nTypeScript provides the tooling and needed bit of abstraction from the implementation with its compiler. Even though JavaScript has come a long way with [ECMA-262][5] getting plenty of updates, there is still the challenge of browser vendors supporting the spec and so on. In the end, the overhead of setting up your project to using the TypeScript compiler outweighs the complexity that large JavaScript projects bring to the table.\n\nEven when JavaScript is \"feature complete\", I'm guessing that TypeScript will still provide a stronger developer story for larger software projects, while JavaScript will continue to provide the foundation for the web platform, and TypeScript itself.","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://westerndevs.com/tags/TypeScript/"}]},{"title":"Pi Day 2018","authorId":"donald_belcham","slug":"PiDay","date":"2018-03-14 13:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/PiDay/","link":"","permalink":"https://westerndevs.com/Development/PiDay/","excerpt":"I have a bunch of RaspberryPi devices kicking around the house and office. There are some Pi 2s, Pi 3s, and a Pi Zero. I use them for a lot of different things. I just setup a Pi 3 to act as a scanning server for the Fujitsu iX500 ScanSnap that we use to keep a paperless home. There's a Pi 2 running NUT, some cron scripts, and some other admin stuff for my home network. We got a Pi Zero from an Ada Box that we setup to run RetroPie. While interesting, it's not overly complicated stuff. Places like Los Alamos National Labratory use Raspberry Pis for prototyping their large systems.","raw":"---\nlayout: post\ntitle: Pi Day 2018\ntags:\n  - RPi\ncategories:\n  - Development\nauthorId: donald_belcham\ndate: 2018-03-14 09:00:00\n---\n![](https://www.igloocoder.com/images/RPi-Logo.png)\n\nI have a bunch of [RaspberryPi devices](https://www.raspberrypi.org) kicking around the house and office. There are some Pi 2s, Pi 3s, and a Pi Zero. I use them for a lot of different things. I just setup a Pi 3 to act as a scanning server for the [Fujitsu iX500 ScanSnap](http://www.fujitsu.com/uk/products/computing/peripheral/scanners/scansnap/ix500/) that we use to keep a paperless home. There's a Pi 2 running [NUT](http://networkupstools.org/), some cron scripts, and some other admin stuff for my home network. We got a Pi Zero from an [Ada Box](https://www.adafruit.com/adabox/) that we setup to run [RetroPie](https://retropie.org.uk/). While interesting, it's not overly complicated stuff. Places like [Los Alamos National Labratory use Raspberry Pis for prototyping their large systems](https://www.youtube.com/watch?v=78H-4KqVvrg).\n<!-- more -->\nThe versatility of the Raspberry Pi is undeniable. When my colleagues at Particular Software started working on a .NET Core version of NServiceBus I had to know; would NServiceBus run on the Pi? An hour of .NET Core research plus the [Learning Transport sample](https://docs.particular.net/samples/learning-transport/) and I had this\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Step 2 complete: <a href=\"https://twitter.com/hashtag/NServiceBus?src=hash&amp;ref_src=twsrc%5Etfw\">#NServiceBus</a> running the Learning Transport in Docker on a RaspberryPi3. <a href=\"https://t.co/OixMIjWbCB\">pic.twitter.com/OixMIjWbCB</a></p>&mdash; Donald Belcham (@dbelcham) <a href=\"https://twitter.com/dbelcham/status/903418955989217280?ref_src=twsrc%5Etfw\">September 1, 2017</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThe only trick with it was that I had to `dotnet publish` using the `-r linux-arm` parameter. \n\nThis doesn't seem like much, but it can open up a lot of options for a team developing distributed apps. One of the biggest challenges for those teams is having a non-production environment that mimics the topology of the ultimate solution.. In my experience teams struggle with getting the infrastructure required to properly test. Servers are allocated by a different team. Once setup, the topology is fixed which makes it difficult for the team to experiment and/or adjust their architecture.\n\nNow, what could you do with 8 Raspberry Pis if you were building a distributed system? Well you could have one running a database server (SQL Server, MySQL, and Postgres will all run on ARM hardware), one Pi could run your messaging system (RabbitMQ for example), and the rest could represent nodes for your distributed code. Great, but that's no different than using 8 VMs you say?\n\nUnplug one of the distributed nodes. Just unplug it. Don't shut it down gracefully. With a Pi that's easy, you just reach for the plug and pull. With a VM it's a fair bit harder, and if you can the infrastructure folks will blow a gasket that you just did that to their precious, stable systems. This is really important in a distributed system though. How does your system handle uncontrolled shutdowns? How do you plan your recovery in that scenario? What if you just pull the network cable but leave the power on? How does recovery differ? What do your reporting tools tell you when this happens, and what do they tell you right after the problem is fixed?\n\nThere are a lot more moving parts in a distributed system which lead to a lot more disaster prevention and recovery needs. A collection of Raspberry Pis running as a testing or development environment gives you a great platform to simulate many of these situations. They don't allow you to do all your testing though. I'd never pretend that a setup of Pis would be adequate, or appropriate, for load testing a system.\n\nRaspberry Pi is a great platform for prototyping (and more). Don't relegate it to just prototyping things you'd like to play with at home. They can be a huge tool for distributed system development too. Heck, if Los Alamos is using them, why can't you?\n\nHappy Pi Day!","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"RPi","slug":"RPi","permalink":"https://westerndevs.com/tags/RPi/"}]},{"title":"Crowdsourcing Documentation is Cool","authorId":"david_wesst","slug":"crowdsourcing-docs-is-cool","date":"2018-03-13 13:40:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/crowdsourcing-docs-is-cool/","link":"","permalink":"https://westerndevs.com/Development/crowdsourcing-docs-is-cool/","excerpt":"The idea of these large enterprises crowdsourcing their technical documentation is pretty cool. More cool than I had originally realized, and I want to take a moment to explain why I like it and why you should get involved yourself.","raw":"---\nlayout: post\ntitle: Crowdsourcing Documentation is Cool\ntags:\n  - JavaScript\n  - Documentation\ncategories:\n  - Development\nauthorId: david_wesst\ndate: 2018-03-13 09:40:00\n---\n\nThe idea of these large enterprises crowdsourcing their technical documentation is pretty cool. More cool than I had originally realized, and I want to take a moment to explain why I like it and why you should get involved yourself.\n\n<!-- more -->\n\n[1]: https://i.imgur.com/5ptKftE.png\n[2]: https://docs.microsoft.com/en-us/microsoft-edge/extensions/extensions-for-enterprise\n[3]: https://github.com/awsdocs\n[4]: https://github.com/MicrosoftDocs\n[5]: https://github.com/mdn\n\nI updated the Microsoft Edge documentation on web extensions. As of this writing, you can see it [here][2], but just in case you can't I've included an image.\n\n![][1]\n\nThe idea of these large enterprises crowdsourcing their technical documentation is pretty cool. More cool than I had originally realized and, for that reason, I want to take a moment to explain why I like it and why you should get involved yourself.\n\n## What do you mean by \"Crowdsourcing\"?\nWhen it comes to web platforms, many of the platform owners ([Amazon][3], [Microsoft][4], [Mozilla][5]) have started crowdsourcing their technical documentation. When I say _crowdsourcing_ I mean that the organization opens up the conversation about what the documentation should say to the community at large.\n\nThe community, being the consumers of the product (and the documentation) can have input into adding, editing, or removing sections of official product or platform documentation. Assuming the vendor agrees with the changes being suggested, then the change is accepted and the official documentation is updated.\n\nThis whole process if facilitated generally by GitHub, where documentation is published as source code and pull requests act as the avenue submitting changes. This way, the conversation about the changes is tracked, shared, and kept in the open for people to review and understand.\n\nPlus, using things like contributors guides and automated build tools can be integrated with GitHub to validate the change, to make sure that the change to the documentation doesn't break anything and follows any rules the vendor has in place.\n\n## Where is the coolness?\nThere are a couple of cool points I'd like to highlight.\n\n### Consumers are More Qualified than Vendors\nThe developers of the platform itself are somewhat qualified considering they know the inner workings of the product, but they aren't the ones using it. The people using the product don't need to know how the guts work, they need to how to use it.\n\nThere is nobody more qualified to update product documentation than the consumers of the documentation and technology. The people that are neck deep and actually _using_ in tech to make things happen. Those are the people that are best suited to critique and ultimately improve the documentation.\n\n### Transparent Conversations\nWhen you crowdsource your documentation, you need to make it open and accessible, which tends to make conversation around the documentation transparent. In our case, GitHub provides the facilty to make this happen with public repositories filled with documentation and through the issue and pull request interface.\n\nPeople can submit pull requests and issues and have a conversation with the vendor about their documentation and ultimately their product.\n\nSomething that starts out like a minor update, could result in an entire section. In my case, I was confident that I'd be adding new pages of content, but once I got into the thick of it, I realized all the parts were already present in the docs. I just needed to add some context and minor updates to what was already there.\n\n## Conclusion\nIn conclusion, anyone reviewing the documentation for a tool or technology should check to see if:\n\n1. It's open to improvement through crowdsourcing\n2. They can think they can make it better.\n\nIt's a great way to get involved in your technology community, all while improving the developer experience for the next person that comes along.","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"Documentation","slug":"Documentation","permalink":"https://westerndevs.com/tags/Documentation/"}]},{"title":"Angular Testing Patterns - Leverage Observable","authorId":"simon_timms","slug":"angular-testing-2","date":"2018-02-21 19:38:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/angular-testing-2/","link":"","permalink":"https://westerndevs.com/Development/angular-testing-2/","excerpt":"One of the most challenging parts of testing is finding seams to reduce the scope of tests. Doing so is important because it make your tests smaller and cleaner which makes them more resilient to changes in the rest of your code base. Testing isn't helping you if every minor change breaks dozens of interconnected tests. Angular's heavy use of Observable provides us a great seam.","raw":"---\nlayout: post\ntitle: Angular Testing Patterns - Leverage Observable\ntags:\n  - Angular\n  - Testing\nfeatureImage: src/angular.png\ncategories:\n  - Development \nauthorId: simon_timms\ndate: 2018-02-21 14:38:30 \n---\n\nOne of the most challenging parts of testing is finding seams to reduce the scope of tests. Doing so is important because it make your tests smaller and cleaner which makes them more resilient to changes in the rest of your code base. Testing isn't helping you if every minor change breaks dozens of interconnected tests. Angular's heavy use of Observable provides us a great seam. \n\n<!-- more -->\nThis post is part two of a series on angular testing. You can check out the other posts here:\n\n1. [Keep the number of test bed tests to a minimum](/Development/angular-testing-1/). \n2. Leverage `Observable` as a testing seam (this post)\n3. Leverage monkey patching as a testing seam\n4. No matter what anybody says e2e tests still need sleeps\n\nReactive programming is a really nice paradigm which extend the Promise or Task patterns that have become quite popular over the last decade. Instead of returning a single value upon completion as Promises do an observable allows subscriptions which are handlers executed every time a value is returned from the observable. \n\nI have seen a lot of people who ignore the complexity of observables by simply converting to a promise using `toPromise` but this ignores some of the really cool things you can do with promises. For instances if I have a component that requires talking to multiple HTTP endpoints I'll [zip](http://reactivex.io/documentation/operators/zip.html) the responses together so that the rendering doesn't happen until all the requests are complete. There are a ton of other cool patters you can use if you stick with observables. \n\nAnyway, I'm not currently here to sell you on RxJS (it _is_ awesome) but tell you how you can use observables to act as a good seam to limit the scope of your tests. \n\nLet's look at a snippet of a component that makes use of observables and see how to shim in tests. \n\n```typescript\nconstructor(private vendorService: VendorService, \n            private deviceViewService: DeviceViewService, \n            private originsService: OriginsService){\n\n}\n\nngOnInit() {\n    this.vendorService.get().subscribe((v: Vendor[]) => this.vendors = v);\n\n    this.deviceViewService.getDevices().subscribe(devices => {\n      this.devices = devices;\n      activateDevices();\n    });\n\n    let originsObservable = this.originsService.getOrigins();\n    let destinationsObservable = this.originsService.getDestinations();\n    Observable.zip(originsObservable, destinationsObservable, (origins: Origin[], destinations: Destination[])=>[origins,destinations])\n        .subscribe((result)=>{\n            this.setupOriginsAndDestinations(result);\n        });\n  }\n\n```\n\nHere we have 4 observables which complete in various ways. The first, `vendorService.get()`, simply assigns vendors to an existing variable. The devices observable does the same but also calls a function and, finally, the last two observables are synchronized via a zip operator. It looks like a lot is going on here but we can isolate things to test easily. \n\nFirst up we want to test to make sure that whatever is returned by the vendor service is properly assigned to the vendors collection. We can us a combination of mocks and observables to focus just on the vendor service like so \n(I'm using [ts-mockito](https://github.com/NagRock/ts-mockito)'s mocking syntax here):\n\n```javascript\ndescribe('Demo Component', () =>\n{\n    it('should set vendors', () => {\n        //arrange\n        let mockVendorService = mock(VendorService);\n        let vendors = [{'id': 1, name: 'Vendor ABC'}];\n        when(mockVendorService.get()).thenReturn(Observable.from([vendors]));\n        let mockDeviceViewService = mock(DeviceViewService);\n        when(mockDeviceViewService.getDevices()).thenReturn(Observable.from([]));\n        let mockOriginsService = mock(OriginsService);\n        when(mockOriginsService.getOrigins()).thenReturn(Observable.from([]));\n        when(mockOriginsService.getDestinations()).thenReturn(Observable.from([]));\n        let sut = new DemoComponent(instance(mockVendorService), instance(mockDeviceService), instance(mockOriginsService));\n\n        //act\n        sut.ngOnInit();\n\n        //assert\n        expect(sut.vendors).to.equal(vendors);\n    });\n});\n\n```\n\nAs you can see we set up the mocks to return either an `Observable` with a single result to test the code or with an empty result to never trigger the subscriptions to that observable. So even though the ngOnInit is quite complex the testing doesn't have to be. \n\nLet's look at one more example for the zip case\n\n```javascript\nit('origins and destinations being complete should trigger setup', () => {\n        //arrange\n        let mockVendorService = mock(VendorService);\n        when(mockVendorService.get()).thenReturn(Observable.from([]));\n        let mockDeviceViewService = mock(DeviceViewService);\n        when(mockDeviceViewService.getDevices()).thenReturn(Observable.from([]));\n        let mockOriginsService = mock(OriginsService);\n        let origins = [{'id': 1, name: 'Origin ABC'}];\n        when(mockOriginsService.getOrigins()).thenReturn(Observable.from([origins]));\n        let destinations = [{'id': 1, name: 'Destination ABC'}];\n        when(mockOriginsService.getDestinations()).thenReturn(Observable.from([destinations]));\n        let sut = new DemoComponent(instance(mockVendorService), instance(mockDeviceService), instance(mockOriginsService));\n\n        let didWork = false;\n        sut.setupOriginsAndDestinations = (passedOrigins, passedDestinations) => {\n            didWork = passedOrigins === origins && passedDestinations === destinations;\n        }\n\n        //act\n        sut.ngOnInit();\n\n        //assert\n        expect(didWork).to.be.true;\n    });\n```\n\nYou might also have equivalent tests to ensure that just completing one or the other of `getOrigins` and `getDestinations` doesn't cause the setup to be fired. \n\nThe crux of this post is that observables provide for a nice place to hook into tests because you can use them to isolate large chunks of subscription code or exercise that code with arbitrary values. The more seams you have the easier testing becomes.\n\nI already gave away a bit of the third post in this series when I overrode the setup method in the last example: this is called monkey patching and it is slick beans for isolating code to test. ","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"Testing","slug":"Testing","permalink":"https://westerndevs.com/tags/Testing/"},{"name":"Angular","slug":"Angular","permalink":"https://westerndevs.com/tags/Angular/"}]},{"title":"Loading Related Entities: Many-to-One","authorId":"dave_paquette","slug":"loading-related-entities-many-to-one","date":"2018-02-07 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/loading-related-entities-many-to-one/","link":"","permalink":"https://westerndevs.com/Dapper/loading-related-entities-many-to-one/","excerpt":"This is a part of a series of blog posts on data access with Dapper. In today's post, we will start our journey into more complex query scenarios by exploring how to load related entities. There are a few different scenarios to cover here. In this post we will be covering the Many-to-One scenario.","raw":"---\nlayout: post\ntitle: 'Loading Related Entities: Many-to-One'\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2018/02/07/loading-related-entities-many-to-one.aspx'\ndate: 2018-02-07\nexcerpt: This is a part of a series of blog posts on data access with Dapper. In today's post, we will start our journey into more complex query scenarios by exploring how to load related entities. There are a few different scenarios to cover here. In this post we will be covering the Many-to-One scenario.\n---\n\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n \nIn today's post, we will start our journey into more complex query scenarios by exploring how to load related entities. There are a few different scenarios to cover here. In this post we will be covering the Many-to-One scenario.\n\n![Many-to-One](https://www.davepaquette.com/images/dapper/flight_to_airport_many_to_one.png)\n\nContinuing with our sample domain for the ever expanding _Air Paquette_ airline, we will now look at loading a list of `ScheduledFlight` entities. A `ScheduleFlight` has a departure `Airport` and an arrival `Airport`.\n\n{% codeblock lang:csharp %}\npublic class ScheduledFlight \n{\n    public int Id {get; set;}\n    public string FlightNumber {get; set;}\n\n    public Airport DepartureAirport {get; set;}\n    public int DepartureHour {get; set;}\n    public int DepartureMinute {get; set;}\n\n    public Airport ArrivalAirport {get; set;}        \n    public int ArrivalHour {get; set;}\n    public int ArrivalMinute {get; set;}\n\n   //Other properties omitted for brevity \n}\n\npublic class Airport \n{\n    public int Id {get; set;}\n    public string Code {get; set;}\n    public string City {get; set;}\n    public string ProvinceState {get; set;}\n    public string Country {get; set;}\n}\n{% endcodeblock %} \n\n_Side Note:_ Let's ignore my poor representation of the arrival and departure times of the scheduled flights. In a future most we might look using Noda Time to properly represent these values.\n\n## Loading everything in a single query\nUsing Dapper, we can easily load a list of `ScheduledFlight` using a single query. First, we need to craft a query that returns all the columns for a `ScheduledFlight`, the departure `Airport` and the arrival `Airport` in a single row.\n\n{% codeblock lang:sql %}\nSELECT s.Id, s.FlightNumber, s.DepartureHour, s.DepartureMinute, s.ArrivalHour, s.ArrivalMinute, s.IsSundayFlight, s.IsMondayFlight, s.IsTuesdayFlight, s.IsWednesdayFlight, s.IsThursdayFlight, s.IsFridayFlight, s.IsSaturdayFlight,\n       a1.Id, a1.Code, a1.City, a1.ProvinceState, a1.Country,\n\t   a2.Id, a2.Code, a2.City, a2.ProvinceState, a2.Country\nFROM ScheduledFlight s\n\tINNER JOIN Airport a1\n\t\tON s.DepartureAirportId = a1.Id\n\tINNER JOIN Airport a2\n\t\tON s.ArrivalAirportId = a2.Id\n{% endcodeblock %}\n\nWe use the `QueryAsync` method to load a list of `ScheduledFlight` entities along with their related `DepartureAirport` and `ArrivalAirport` entities. The parameters we pass in are a little different from what we saw in our previous posts. \n\n{% codeblock lang:csharp %}\n[HttpGet]\npublic async Task<IEnumerable<ScheduledFlight>> Get(string from)\n{\n    IEnumerable<ScheduledFlight> scheduledFlights;\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n\n            var query = @\"\nSELECT s.Id, s.FlightNumber, s.DepartureHour, s.DepartureMinute, s.ArrivalHour, s.ArrivalMinute, s.IsSundayFlight, s.IsMondayFlight, s.IsTuesdayFlight, s.IsWednesdayFlight, s.IsThursdayFlight, s.IsFridayFlight, s.IsSaturdayFlight,\n     a1.Id, a1.Code, a1.City, a1.ProvinceState, a1.Country,\n     a2.Id, a2.Code, a2.City, a2.ProvinceState, a2.Country\nFROM ScheduledFlight s\n     INNER JOIN Airport a1\n          ON s.DepartureAirportId = a1.Id\n    INNER JOIN Airport a2\n          ON s.ArrivalAirportId = a2.Id\nWHERE a1.Code = @FromCode\";\n\n        scheduledFlights = \n            await connection.QueryAsync<ScheduledFlight, Airport, Airport, ScheduledFlight>(query,\n                    (flight, departure, arrival ) => {\n                        flight.DepartureAirport = departure;\n                        flight.ArrivalAirport = arrival;\n                        return flight;\n                    },\n                    new{FromCode = from} );\n    }\n    return scheduledFlights;\n}\n\n{% endcodeblock %}\n\nFirst, instead of a single type parameter `<ScheduledFlight>`, we need to provide a series of type parameters: `<ScheduledFlight, Airport, Airport, ScheduledFlight>`. The first 3 parameters specify the types that are contained in each row that the query returns. In this example, each row contains columns that will be mapped to `ScheduledFlight` and 2 `Airports`. The order matters here, and Dapper assumes that when it seems a column named `Id` then it is looking at columns for the next entity type. In the example below, the columns from `Id` to `IsSaturdayFlight` are mapped to a `ScheduledFlight` entity. The next 5 columns `Id, Code, City, ProvinceState, Country` are mapped to an `Airport` entity, and the last 5 columns are mapped to a second `Airport` entity. If you aren't using `Id`, you can use the optional `splitOn` argument to specify the column names that Dapper should use to identity the start of each entity type.\n\nWhat's that last type parameter? Why do we need to specify `ScheduledFlight` again? Well, I'm glad you asked. The thing about Dapper is that it doesn't actually know much about the structure of our entities so we need to tell it how to wire up the 3 entities that it just mapped from a row. That last `ScheduledFlight` type parameter is telling Dapper that `ScheduledFlight` is ultimately the entity we want to return from this query. It is important for the second argument that is passed to the `QueryAsync` method. \n\nThat second argument is a function that takes in the 3 entities that were mapped back from that row and returns and entity of the type that was specified as the last type parameter. In this case, we assign the first `Airport` to the flight's `DepartureAirport` property and assign the second `Airport` to the flight's `ArrivalAiport` parameter, then we return the flight that was passed in.\n\n{% codeblock lang:csharp %}\n  (flight, departure, arrival ) => {\n      flight.DepartureAirport = departure;\n      flight.ArrivalAirport = arrival;\n      return flight;\n  }\n{% endcodeblock %}\n\nThe first argument argument passed to the `QueryAsync` method is the SQL query, and the third argument is an anonymous object containing any parameters for that query. Those arguments are really no different than the simple examples we saw in [previous blog posts](https://www.davepaquette.com/archive/2018/01/22/loading-an-object-graph-with-dapper.aspx).\n\n## Wrapping it up\nDapper refers to this technique as [Multi Mapping](https://github.com/StackExchange/Dapper#multi-mapping). I think it's called that because we are mapping multiple entities from each row that the query returns. In a fully featured ORM like Entity Framework, we call this feature Eager Loading. It is an optimization technique that avoids the need for multiple queries in order to load an entity and it's associated entities. \n\nThis approach is simple enough to use and it does reduce the number of round trips needed to load a set of entities. It does, however, come at a cost. Specifically, the results of the query end up causing some duplication of data. As you can see below, the data for the Calgary and Vancouver airports is repeated in each row. \n\n![Data Duplication](https://www.davepaquette.com/images/dapper/multi_mapping_data_duplication.png)\n\nThis isn't a huge problem if the result set only contains 3 rows but it can become problematic when dealing with large result sets. In addition to creating somewhat bloated result sets, Dapper will also create new instances of those related entities for each row in the result set. In the example above, we would end up with 3 instances of the `Airport` class representing YYC - Calgary and 3 instances of the `Airport` class representing YVR - Vancouver. Again, this isn't necessarily a big problem when we have 3 rows in the result set but with larger result sets it could cause your application to use a lot more memory than necessary. \n\nIt is worth considering the cost associated with this approach. Given the added memory cost, this approach might be better suited to One-to-One associations rather than the Many-to-One example we talked about in this post. In the next post, we will explore an alternate approach that is more memory efficient but probably a little more costly on the CPU for the mapping. \n\n","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Using Stored Procedures to Load Data with Dapper","authorId":"dave_paquette","slug":"using-stored-procedures-to-load-data-with-dapper","date":"2018-01-29 01:00:01+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/using-stored-procedures-to-load-data-with-dapper/","link":"","permalink":"https://westerndevs.com/Dapper/using-stored-procedures-to-load-data-with-dapper/","excerpt":"Let's just get this one out of the way early. Stored procedures are not my favorite way to get data from SQL Server but there was a time when they were extremely popular. They are still heavily used today and so this series would not be complete without covering how to use stored procedures with Dapper.","raw":"---\nlayout: post\ntitle: Using Stored Procedures to Load Data with Dapper\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2018/01/28/using-stored-procedures-to-load-data-with-dapper.aspx'\ndate: 2018-01-28 20:00:01\nexcerpt: Let's just get this one out of the way early. Stored procedures are not my favorite way to get data from SQL Server but there was a time when they were extremely popular. They are still heavily used today and so this series would not be complete without covering how to use stored procedures with Dapper. \n---\nThis is a part of a series of blog posts on data access with Dapper. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n \nLet's just get this one out of the way early. Stored procedures are not my favorite way to get data from SQL Server but there was a time when they were extremely popular. They are still heavily used today and so this series would not be complete without covering how to use stored procedures with Dapper. \n\n## A Simple Example\nLet's imagine a simple stored procedure that allows us to query for `Aircraft` by model.\n\n{% codeblock lang:sql %}\nCREATE PROCEDURE GetAircraftByModel @Model NVARCHAR(255) AS\nBEGIN\n    SELECT \n       Id\n      ,Manufacturer\n      ,Model\n      ,RegistrationNumber\n      ,FirstClassCapacity\n      ,RegularClassCapacity\n      ,CrewCapacity\n      ,ManufactureDate\n      ,NumberOfEngines\n      ,EmptyWeight\n      ,MaxTakeoffWeight\n    FROM Aircraft a\n    WHERE a.Model = @Model\nEND\n{% endcodeblock %}\n\nTo execute this stored procedure and map the results to a collection of `Aircraft` objects, use the `QueryAsync` method almost exactly like we did in the [last post](https://www.davepaquette.com/archive/2018/01/22/loading-an-object-graph-with-dapper.aspx). \n\n{% codeblock lang:csharp %}\n//GET api/aircraft\n[HttpGet]\npublic async Task<IEnumerable<Aircraft>> Get(string model)\n{\n    IEnumerable<Aircraft> aircraft;\n\n    using (var connection = new SqlConnection(_connectionString))\n    {\n        await connection.OpenAsync();\n\n        aircraft = await connection.QueryAsync<Aircraft>(\"GetAircraftByModel\",\n                        new {Model = model}, \n                        commandType: CommandType.StoredProcedure);\n    }\n    return aircraft;\n}\n{% endcodeblock %}\n\nInstead of passing in the raw SQL statement, we simply pass in the name of the stored procedure. We also pass in an object that has properties for each of the stored procedures arguments, in this case `new {Model = model}` maps the `model` variable to the stored procedure's `@Model` argument. Finally, we specify the `commandType` as `CommandType.StoredProcedure`. \n\n## Wrapping it up\n\nThat's all there is to using stored procedures with Dapper. As much as I dislike using stored procedures in my applications, I often do have to call stored procedures to fetch data from legacy databases. When that situation comes up, Dapper is my tool of choice. \n\nStay tuned for the next installment in this Dapper series. Comment below if there is a specific topic you would like covered.\n","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Loading an Object From SQL Server Using Dapper","authorId":"dave_paquette","slug":"loading-an-object-graph-with-dapper","date":"2018-01-23 02:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Dapper/loading-an-object-graph-with-dapper/","link":"","permalink":"https://westerndevs.com/Dapper/loading-an-object-graph-with-dapper/","excerpt":"I was recently asked to create a read-only web API to expose some parts of a system's data model to third party developers. While Entity Framework is often my go-to tool for data access, I thought this was a good scenario to use Dapper instead. This series of blog posts explores dapper and how you might use it in your application. Today, we will start with the basics of loading and mapping a database table to a C# class.","raw":"---\nlayout: post\ntitle: Loading an Object From SQL Server Using Dapper\ntags:\n  - Dapper\n  - .NET \n  - .NET Core\n  - Micro ORM\ncategories:\n  - Dapper\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2018/01/22/loading-an-object-graph-with-dapper.aspx'\ndate: 2018-01-22 21:30:00\nexcerpt: I was recently asked to create a read-only web API to expose some parts of a system's data model to third party developers. While Entity Framework is often my go-to tool for data access, I thought this was a good scenario to use Dapper instead. This series of blog posts explores dapper and how you might use it in your application. Today, we will start with the basics of loading and mapping a database table to a C# class. \n---\nI was recently asked to create a read-only web API to expose some parts of a system's data model to third party developers. While [Entity Framework](https://docs.microsoft.com/en-us/ef/) is often my go-to tool for data access, I thought this was a good scenario to use Dapper instead. This series of blog posts explores dapper and how you might use it in your application. To see the full list of posts, visit the [Dapper Series Index Page](https://www.davepaquette.com/archive/2018/01/21/exploring-dapper-series.aspx).\n \n Today, we will start with the basics of loading a mapping and database table to a C# class. \n\n# What is Dapper?\n[Dapper](https://github.com/StackExchange/Dapper) calls itself a simple object mapper for .NET and is usually lumped into the category of micro ORM (Object Relational Mapper). When compared to a fully featured ORM like Entity Framework, Dapper lacks certain features like change-tracking, lazy loading and the ability to translate complex LINQ expressions to SQL queries. The fact that Dapper is missing these features is probably the single best thing about Dapper. While it might seem like you're giving up a lot, you are also gaining a lot by dropping those types of features. Dapper is fast since it doesn't do a lot of the magic that Entity Framework does under the covers. Since there is less magic, Dapper is also a lot easier to understand which can lead to lower maintenance costs and maybe even fewer bugs. \n\n# How does it work?\nThroughout this series we will build on an example domain for an airline. All airlines need to manage a fleet of aircraft, so let's start there. Imagine a database with a table named `Aircraft` and a C# class with property names that match the column names of the `Aircraft` table.\n\n{% codeblock lang:sql %}\nCREATE TABLE Aircraft\n    (\n        Id int not null IDENTITY(1,1) CONSTRAINT pk_Aircraft_Id PRIMARY KEY,\n        Manufacturer nvarchar(255),\n        Model nvarchar(255),\n        RegistrationNumber nvarchar(50),\n        FirstClassCapacity int,\n        RegularClassCapacity int,\n        CrewCapacity int,\n        ManufactureDate date,\n        NumberOfEngines int,\n        EmptyWeight int,\n        MaxTakeoffWeight int\n    )\n{% endcodeblock %}\n\n\n{% codeblock lang:csharp %}\npublic class Aircraft \n{\n    public int Id { get; set; }\n    public string Manufacturer {get; set;}\n    public string Model {get; set;}\n    public string RegistrationNumber {get; set;}\n    public int FirstClassCapacity {get; set;}\n    public int RegularClassCapacity {get; set;}\n    public int CrewCapacity {get; set;}\n    public DateTime ManufactureDate {get; set; }\n    public int NumberOfEngines {get; set;}\n    public int EmptyWeight {get; set;}\n    public int MaxTakeoffWeight {get; set;}\n}\n{% endcodeblock %}\n\n## Installing Dapper\nDapper is available as a [Nuget package](https://www.nuget.org/packages/Dapper/). To use Dapper, all you need to do is add the `Dapper` package to your project.\n\n **.NET Core CLI**: `dotnet add package Dapper`\n\n**Package Manager Console**: `Install-Package Dapper`\n\n## Querying a single object\nDapper provides a set of extension methods for .NET's `IDbConnection` interface. For our first task, we want to execute a query to return the data for a single row from the `Aircraft` table and place the results in an instance of the `Aircraft` class. This is easily accomplished using Dapper's `QuerySingleAsync` method.\n\n{% codeblock lang:csharp %}\n[HttpGet(\"{id}\")]\npublic async Task<Aircraft> Get(int id)\n{\n  Aircraft aircraft;\n  using (var connection = new SqlConnection(_connectionString))\n  {\n    await connection.OpenAsync();\n    var query = @\"\nSELECT \n       Id\n      ,Manufacturer\n      ,Model\n      ,RegistrationNumber\n      ,FirstClassCapacity\n      ,RegularClassCapacity\n      ,CrewCapacity\n      ,ManufactureDate\n      ,NumberOfEngines\n      ,EmptyWeight\n      ,MaxTakeoffWeight\n  FROM Aircraft WHERE Id = @Id\";\n\n    aircraft = await connection.QuerySingleAsync<Aircraft>(query, new {Id = id});\n  }\n  return aircraft;\n}\n{% endcodeblock %} \n\nBefore we can call Dapper's `QuerySingleASync` method, we need an instance of an open `SqlConnection`. If you are an Entity Framework user, you might not be used to working directly with the `SqlConnection` class because Entity Framework generally manages connections for you. All we need to do is create a new `SqlConnection`, passing in the connection string, then call `OpenAsync` to open that connection. We wrap the connection in a `using` statement to ensure that `connection.Dispose()` is called when we are done with the connection. This is important because it ensures the connection is returned to the connection pool that is managed by .NET. If you forget to do this, you will quickly run into problems where your application is not able to connect to the database because the connection pool is starved. Check out the [.NET Docs](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql-server-connection-pooling) for  more information on connection pooling.\n\nWe will use the following pattern throughout this series of blogs posts:\n\n{% codeblock lang:csharp %}\nusing(var connection = new SqlConnection(_connectionString))\n{\n  await connection.OpenAsync();\n  //Do Dapper Things\n}\n{% endcodeblock %}\n\nAs @Disman pointed out in the comments, it is not necessary to call `connection.OpenAsync()`. If the connection is not already opened, Dapper will call `OpenAsync` for you. Call me old fashioned but I think that whoever created the connection should be the one responsible for opening it, that's why I like to open the connection before calling Dapper.\n\nLet's get back to our example. To query for a single `Aircraft`, we call the `QuerySingleAsync` method, specifying the `Aircraft` type parameter. The type parameter tells Dapper what class type to return. Dapper will take the results of the query that gets executed and map the column values to properties of the specified type. We also pass in two arguments. The first is the query that will return a single row based on a specified `@Id` parameter.\n\n{% codeblock lang:csharp %}\nSELECT \n       Id\n      ,Manufacturer\n      ,Model\n      ,RegistrationNumber\n      ,FirstClassCapacity\n      ,RegularClassCapacity\n      ,CrewCapacity\n      ,ManufactureDate\n      ,NumberOfEngines\n      ,EmptyWeight\n      ,MaxTakeoffWeight\n  FROM Aircraft WHERE Id = @Id\n{% endcodeblock %} \n\nThe next parameter is an anonymous class containing properties that will map to the parameters of the query. \n\n{% codeblock lang:csharp %}\n new {Id = id}\n{% endcodeblock %}\n\nPassing the parameters in this way ensures that our queries are not susceptible to SQL injection attacks.\n\nThat's really all there is to it. As long as the column names and data types match the property of your class, Dapper takes care of executing the query, creating an instance of the `Aircraft` class and setting all the properties.\n\nIf the query doesn't contain return any results, Dapper will throw an `InvalidOperationException`.\n\n> InvalidOperationException: Sequence contains no elements\n\nIf you prefer that Dapper returns null when there are no results, use the `QuerySingleOrDefaultAsnyc` method instead.\n\n## Querying a list of objects\n\nQuerying for a list of objects is just as easy as querying for a single object. Simply call the `QueryAsync` method as follows.\n\n{% codeblock lang:csharp %}\n[HttpGet]\npublic async Task<IEnumerable<Aircraft>> Get()\n{\n  IEnumerable<Aircraft> aircraft;\n\n  using (var connection = new SqlConnection(_connectionString))\n  {\n    await connection.OpenAsync();\n    var query = @\"\nSELECT \n       Id\n      ,Manufacturer\n      ,Model\n      ,RegistrationNumber\n      ,FirstClassCapacity\n      ,RegularClassCapacity\n      ,CrewCapacity\n      ,ManufactureDate\n      ,NumberOfEngines\n      ,EmptyWeight\n      ,MaxTakeoffWeight\n  FROM Aircraft\";\n    aircraft = await connection.QueryAsync<Aircraft>(query);\n    }\n  return aircraft;\n}\n{% endcodeblock %}\n\nIn this case, the query did not contain any parameters. If it did, we would pass those parameters in as an argument to the `QueryAsync` method just like we did for the `QuerySingleAsync` method.\n\n## What's next?\nThis is just the beginning of what I expect will be a long series of blog posts. You can follow along on this blog and you can track the [sample code on GitHub](https://github.com/AspNetMonsters/DapperSeries). \n\nLeave a comment below if there is a topic you would like me to cover. ","categories":[{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/categories/Dapper/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Dapper","slug":"Dapper","permalink":"https://westerndevs.com/tags/Dapper/"},{"name":".NET","slug":"NET","permalink":"https://westerndevs.com/tags/NET/"},{"name":"Micro ORM","slug":"Micro-ORM","permalink":"https://westerndevs.com/tags/Micro-ORM/"}]},{"title":"Home Networking - Racking","authorId":"donald_belcham","slug":"Home-Networking-3","date":"2018-01-10 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Networking/Home-Networking-3/","link":"","permalink":"https://westerndevs.com/Networking/Home-Networking-3/","excerpt":"I didn't just terminate cables in a mechanical room and mount a few pieces of hardware directly to a wall. Instead I got a wall mount rack to organize everything in.","raw":"---\nlayout: post\ntitle: Home Networking - Racking\ndate: 2018-01-09 23:00:00\ncategories:\n  - Networking\ntags:\n  - networking\nexcerpt: I didn't just terminate cables in a mechanical room and mount a few pieces of hardware directly to a wall. Instead I got a wall mount rack to organize everything in.\nauthorId: donald_belcham\n---\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/raw-wiring.jpg\"/>\n\nAs you can see in the picture, I didn't just terminate cables in a mechanical room and mount a few pieces of hardware directly to a wall. Instead I got a wall mount rack to organize everything in. The rack I chose had to meet a few base criteria:\n * standard `U` sizing so mounting equipment would be easy\n * significant number of `U`s to fit current and future equipment\n * easy open access for setup and maintenance\n * enough depth to accept the networking hardware I had chosen\n\n In the end I got a [Tripp Lite rack](https://www.amazon.ca/Tripp-Lite-SRW08U22-2-Post-Cabinet/dp/B0041W55YE/ref=sr_1_6?ie=UTF8&qid=1515542698&sr=8-6&keywords=wall+mount+rack) that I could lag into some wall studs and fill with up to 150lbs of equipment. The one thing that is weak on this kind of solution is the cable management options. There are no cable raceways down the sides that you can use and third-party options are limited if they exist at all. If you look closely at the picture you can see that I ended up having to velcro strap the cable bundles to the rack posts and supports to be able to maintain some semblance of cabling hygiene.\n\n Getting the rack setup was pretty easy, but I did run into one issue that slowed me down. The rack has threaded holes for mounting, not holes for cage nuts. The threaded holes are ever so slightly smaller than those in cage nuts which made it impossible to use the bolts that came with most of the hardware I was mounting. If I were to do this again, I'd make the effort (and pay the necessary price) to get a wall rack that used cage nuts.\n\n In addition to mounting the rack on the wall I had the electricians (who pulled all my Cat6 cabling) put two plugs on the all behind the rack. The plugs are on separate circuits for future expansion and/or high draw equipment. As of now, I only need one but I did not want to have to watch someone pounding nails or running screws behind my rack in the future. I'll cover the whole power system in another post, but safe to say it's not just a bunch of wall-warts plugged into power bars.\n\n The last pieces added to the rack were accessories; horizontal cable management, shelves, and blank face plates. Of those, probably the only truly necessary items were the shelves. I have one mounted at the top of the rack that holds all of my ISP's hardware. I wanted to keep that crap isolated. The other shelf is mounted near the bottom of the rack and holds a collection of Raspberry Pi computers that I use for various things.\n\n<img src=\"https://www.igloocoder.com/images/rack-layout.jpg\"/>\n\n I spent a lot of time planning how to fill the rack. I didn't want network wires running all over the place, and I didn't want a mess of power cables either. In fact, I decided early on that I didn't want the networking and power cables to intermingle at all. The best tool that I found for doing this was Google Sheets (or Excel if you must). The diagram above shows the plan that I came up with to meet those goals.\n\n When I started the process of building this infrastructure, I figured getting the rack mounted was going to be one of the easier tasks. It turned out not to be hard, but to be quite time consuming as I searched for the best setup.","categories":[{"name":"Networking","slug":"Networking","permalink":"https://westerndevs.com/categories/Networking/"}],"tags":[{"name":"networking","slug":"networking","permalink":"https://westerndevs.com/tags/networking/"}]},{"title":"Angular Testing Patterns - TestBed","authorId":"simon_timms","slug":"angular-testing-1","date":"2017-12-31 19:38:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/angular-testing-1/","link":"","permalink":"https://westerndevs.com/Development/angular-testing-1/","excerpt":"Spec files are automatically generated by Angular 5's CLI but most projects leave them empty. Why not actually write some tests? This post covers some useful patterns to make the whole process as painless as possible.","raw":"---\nlayout: post\ntitle: Angular Testing Patterns - TestBed\ntags:\n  - Angular\n  - Testing\nfeatureImage: src/angular.png\ncategories:\n  - Development \nauthorId: simon_timms\ndate: 2017-12-31 14:38:30 \n---\n\nSpec files are automatically generated by Angular 5's CLI but most projects leave them empty. Why not actually write some tests? This post covers some useful patterns to make the whole process as painless as possible.\n\n<!-- more -->\n\nI've recently been working on a team which has some downright amazing leadership on the testing side. As a result I've had to raise my testing game to a level I've not been at before. During the process the team developed some testing patters which might be useful to the general populace. Here they are:\n\n1. Keep the number of test bed tests to a minimum. \n2. Leverage `Observable` as a testing seam\n3. Leverage monkey patching as a testing seam\n4. No matter what anybody says e2e tests still need sleeps\n\nI'm going to split this post into parts to keep the posts relatively short. \n\n# Test Bed\n\nAngular 2 introduced the idea of the `TestBed` which is basically a way of testing out a component with a \"real\" DOM behind it. There is support for injecting services either real or mock into your component as well as binding your component's model to the template. TestBed tests are the default type of test generated by the angular-cli when you create a new component. They are great and can be used to test a component much more thoroughly than testing with isolated tests alone. \n\nThe issue with them is that they tend to be quite slow to run. The interaction with the DOM and the setup of an entire dependency injection instance per test adds several hundred milliseconds for every TestBed test run. Just watching the test counter tick up in my command-line test reporter I can easily see when TestBed tests are encountered as the counter slows right now. The added time may not be huge in isolation but if we add 500ms (pretty conservative in my experience) per test on a collection of 1500 tests (pretty small project) then we're talking twelve and a half minutes. Angular testing is already glacial so adding this coupled with the Karma runner's inability to selectively run tests and you're really in trouble. \n\nTesting should be lightening fast because you want the feedback loop to be as tight as possible. That's why I'm such a big fan of [Live Unit Testing](https://blogs.msdn.microsoft.com/visualstudio/2017/03/09/live-unit-testing-in-visual-studio-2017-enterprise/). My mantra is that you should be able to hold your breath during a test run without feeling uncomfortable (this makes former pearl divers well adapted to being Angular developers). Most of the functionality that we test on a component doesn't need to be tested using a full featured TestBed. Any functions which mutate the state or call out to other services can be written without the need for the TestBed. Many of my components contain just two TestBed tests: one to check the component can be created and one to check it can be initted. These two test generally catch any typos in the template which is a big source of errors as the TypeScript compiler doesn't catch things in there. In the init test you can also check that appropriate bindings are in place. It is faster to have a test which tests a bunch of properties at once than one test per property. \n\nThis being said there are still plenty of time when TestBed tests do come in useful, typically any time you're building a complex user interface and want to validate that it works cross browsers. I'm certainly not saying don't use TestBed at all but rather that its use should be limited and isolation tests should be favoured. \n\nLet's take a look at an example test which we can migrate away from the TestBed. \n\nThis component does some simple addition. Left side + right side = answer, unless the answer is less than 0 then 'Value too small':\n\n```javascript\nimport { Component, OnInit } from '@angular/core';\n\n@Component({\n  selector: 'app-math-component',\n  templateUrl: './math-component.component.html',\n  styleUrls: ['./math-component.component.css']\n})\n\nexport class MathComponent implements OnInit {\n\n  model: MathComponentModel = {\n    left: 0,\n    right: 0,\n    answer: 0\n  };\n\n  constructor() { }\n\n  ngOnInit() {\n  }\n\n  update() {\n    this.model.answer = this.model.left + this.model.right;\n    if (this.model.answer < 0) {\n      this.model.answer = 'Value too small';\n    }\n  }\n}\n\nexport class MathComponentModel {\n  left: number;\n  right: number;\n  answer: number | string;\n}\n\n```\n\nThe template for it is equally simple\n\n```html\n<p>\n  <input [(ngModel)]=\"model.left\" (change)=\"update()\" type=\"number\"/>\n  <input [(ngModel)]=\"model.right\" (change)=\"update()\" type=\"number\"/>=\n  <input [(ngModel)]=\"model.answer\" disabled>\n</p>\n```\n\nA fully testbed test for this might look like\n\n```javascript\ndescribe('MathComponent', () => {\n  let component: MathComponent;\n  let fixture: ComponentFixture<MathComponent>;\n\n  beforeEach(async(() => {\n    TestBed.configureTestingModule({\n      declarations: [MathComponent],\n      imports: [FormsModule]\n    })\n      .compileComponents();\n  }));\n\n  beforeEach(() => {\n    fixture = TestBed.createComponent(MathComponent);\n    component = fixture.componentInstance;\n    fixture.detectChanges();\n  });\n\n  it('should create', () => {\n    expect(component).toBeTruthy();\n  });\n\n  it('should add up two numbers', fakeAsync(() => {\n    const compiled = fixture.debugElement.nativeElement;\n    compiled.querySelector('[data-autom=left]').value = '2';\n    compiled.querySelector('[data-autom=left]').dispatchEvent(new Event('input'));\n\n    compiled.querySelector('[data-autom=right]').value = '3';\n    compiled.querySelector('[data-autom=right]').dispatchEvent(new Event('input'));\n\n    component.update();\n    fixture.detectChanges();\n    tick();\n    expect(compiled.querySelector('[data-autom=answer]').value).toBe('5');\n  }));\n\n  it('should set answer to \"Value too small\" if answer < 0', fakeAsync(() => {\n    const compiled = fixture.debugElement.nativeElement;\n    compiled.querySelector('[data-autom=left]').value = '2';\n    compiled.querySelector('[data-autom=left]').dispatchEvent(new Event('input'));\n\n    compiled.querySelector('[data-autom=right]').value = '-3';\n    compiled.querySelector('[data-autom=right]').dispatchEvent(new Event('input'));\n\n    component.update();\n    fixture.detectChanges();\n    tick();\n\n    expect(compiled.querySelector('[data-autom=answer]').value).toBe('Value too small');\n  }));\n});\n```\n\nA couple of things to point out here: the first is that there is quite a bit of magic to interact with input boxes on the page. The second thing is that compiled component tests seem to be [quite slow](https://github.com/angular/angular/issues/12409), doubly so if you haven't made your modules highly granular. Much of the testing here could be handled by testing the model rather than the rendering. A testbed test is still needed to check the rendering once but after that we're good with simpler tests.\n\n```javascript\ndescribe('MathComponent bindings', () => {\n  let component: MathComponent;\n  let fixture: ComponentFixture<MathComponent>;\n\n  beforeEach(async(() => {\n    TestBed.configureTestingModule({\n      declarations: [MathComponent],\n      imports: [FormsModule]\n    })\n      .compileComponents();\n  }));\n\n  beforeEach(() => {\n    fixture = TestBed.createComponent(MathComponent);\n    component = fixture.componentInstance;\n    fixture.detectChanges();\n  });\n\n  it('should init', () => {\n    expect(component).toBeTruthy();\n  });\n\n  it('should create', fakeAsync(() => {\n    component.model.answer = 5;\n    component.model.answer = 5;\n    component.model.left = 3;\n    component.model.right = 2;\n\n    fixture.detectChanges();\n    tick();\n    const compiled = fixture.debugElement.nativeElement;\n    expect(compiled.querySelector('[data-autom=answer]').value).toBe(component.model.answer.toString());\n    expect(compiled.querySelector('[data-autom=left]').value).toBe(component.model.left.toString());\n    expect(compiled.querySelector('[data-autom=right]').value).toBe(component.model.right.toString());\n  }));\n});\ndescribe('MathComponent', () => {\n  it('should add up two numbers', () => {\n    const component = new MathComponent();\n    component.model.left = 1;\n    component.model.right = 2;\n    component.update();\n    expect(component.model.answer).toBe(3);\n  });\n\n  it('should set answer to Value too small if answer < 0', () => {\n    const component = new MathComponent();\n    component.model.left = 1;\n    component.model.right = -2;\n    component.update();\n    expect(component.model.answer).toBe('Value too small');\n  });\n});\n```\n\nThe advantage here is that the tests are simpler and run faster. We also don't have to worry about fiddling with fake async or ticks.\n\nIn the next article we'll visit how we can use Observables, which are pretty popular in angular, as a seam to help write tests. ","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"Testing","slug":"Testing","permalink":"https://westerndevs.com/tags/Testing/"},{"name":"Angular","slug":"Angular","permalink":"https://westerndevs.com/tags/Angular/"}]},{"title":"Authorize Resource Tag Helper for ASP.NET Core","authorId":"dave_paquette","slug":"authorize-resource-tag-helper","date":"2017-11-29 01:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/Tag-Helpers/authorize-resource-tag-helper/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/Tag-Helpers/authorize-resource-tag-helper/","excerpt":"ASP.NET Core has a powerful mechanism for implementing resource-based authorization using the IAuthorizationService and resource-based AuthorizationHandlers. In this blog post, we build a tag helper that makes it simple to use resource-based auhtorization to Razor views without writing any C# code in the view.","raw":"---\nlayout: post\ntitle: Authorize Resource Tag Helper for ASP.NET Core\ntags:\n  - ASP.NET Core\n  - MVC\n  - Tag Helpers\n  - Authorization\ncategories:\n  - ASP.NET Core\n  - Tag Helpers\nauthorId: dave_paquette\noriginalurl: 'https://www.davepaquette.com/archive/2017/11/28/authorize-resource-tag-helper.aspx'\ndate: 2017-11-28 20:30\nexcerpt: ASP.NET Core has a powerful mechanism for implementing resource-based authorization using the IAuthorizationService and resource-based AuthorizationHandlers. In this blog post, we build a tag helper that makes it simple to use resource-based auhtorization to Razor views without writing any C# code in the view.\n---\nIn my previous blog post, I wrote an [Authorize tag helper](https://www.davepaquette.com/archive/2017/11/05/authorize-tag-helper.aspx) that made it simple to use role and policy based authorization in Razor Views. In this blog post, we will take this one step further and build a tag helper for resource-based authorization.\n\n# Resource-Based Authorization\nUsing the `IAuthorizationService` in ASP.NET Core, it is easy to implement an authorization strategy that depends not only on properties of the User but also depends on the resource being accessed. To learn how resource-based authorization works, take a look at the well written [offical documentation](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/resourcebased?tabs=aspnetcore2x).\n\nOnce you have defined your authorization handlers and setup any policies in `Startup.ConfigureServices`, applying resource-based authorization is a matter of calling one of two overloads of the `AuthorizeAsync` method on the `IAuthorizationService`.\n\n{% codeblock lang:csharp %}\nTask<AuthorizationResult> AuthorizeAsync(ClaimsPrincipal user,\n                          object resource,\n                          string policyName);\n\nTask<AuthorizationResult> AuthorizeAsync(ClaimsPrincipal user,\n                          object resource,\n                          IAuthorizationRequirement requirements);\n{% endcodeblock %}                          \n\nOne method takes in a policy name while the other takes in an `IAuthorizationRequirement`. The resulting `AuthorizationResult` has a `Succeeded` boolean that indicates whether or not the user meets the requirements for the specified policy. Using the `IAuthorizationService` in a controller is easy enough. Simply inject the service into the controller, call the method you want to call and then check the result.\n\n{% codeblock lang:chsarp %} \npublic async Task<IActionResult> Edit(int id)\n{\n    var document = _documentContext.Find(documentId);\n\n    var authorizationResult = await _authorizationService.AuthorizeAsync(User, Document, \"EditDocument\");\n\n    if (authorizationResult.Succeeded)\n    {\n        return View(document);\n    }    \n    else\n    {\n        return new ChallengeResult();\n    }\n}\n{% endcodeblock %}\n\nUsing this approach, we can easily restrict which users can edit specific documents as defined by our EditDocument policy. For example, we might limit editing to only users who originally created the document. \n\nWhere things start to get a little ugly is if we want to render a UI element based on resource-based authorization. For example, we might only want to render the edit button for a document if the current user is actually authorized to edit that document. Out of the box, this would require us to inject the `IAuthorizationService` in the Razor view and use it like we did in the controller action. The approach works, but the Razor code will get ugly really fast.\n\n# Authorize Resource Tag Helper\n\nSimilar to the Authorize Tag Helper from the last blog post, this Authorize Resource Tag Helper will make it easy to show or hide blocks of HTML by evaluating authorization rules.\n\n## Resource-Based Policy Authorization\nLet's assume we have a named \"EditDocument\" that requires a user to be the original author of a `Document` in order to edit the document. With the authorize resource tag helper, specify the resource instance using the `asp-authorize-resource` attribute and the policy name using the `asp-policy` attribute. Here is an example where `Model` is an instance of a `Document`\n\n{% codeblock lang:html %}\n<a href=\"#\" asp-authorize-resource=\"Model\" \n    asp-policy=\"EditDocument\" class=\"glyphicon glyphicon-pencil\"></a>\n{% endcodeblock %}\n\nIf the user meets the requirments for the \"EditDocument\" policy and the specified resource, then the block of HTML will be sent to the browser. If the requirements are not met, the tag helper will suppress the output of that block of HTML. The tag helper can be applied to any HTML element.\n\n## Resource-Based Requirement Authorization\nInstead of specifying a policy name, authorization can be evaluated by specifying an instance of an `IAuthorizationRequirement`. When using requirements directly instead of policies, specify the requirement using the `asp-requirement` attribute.\n\n{% codeblock lang:html %}\n<a href=\"#\" asp-authorize-resource=\"document\"\n            asp-requirement=\"Operations.Delete\" \n            class=\"glyphicon glyphicon-trash text-danger\">                            \n</a>\n{% endcodeblock %}\n\nIf the user meets `Operations.Delete` requirement for the specified resource, then the block of HTML will be sent to the browser. If the requirement is not met, the tag helper will suppress the output of that block of HTML. The tag helper can be applied to any HTML element.\n\n\n## Implementation Details\nThe authorize resource tag helper itself is fairly simple. The implementation will likely evolve after this blog post so you can check out the latest version [here](https://github.com/dpaquette/TagHelperSamples/blob/master/TagHelperSamples/src/TagHelperSamples.Authorization/AuthorizeResourceTagHelper.cs).\n\nThe tag helper needs an instance of the `IHttpContextAccessor` to get access to the current user and an instance of the `IAuthorizationService`. These are injected into the constructor. In the `ProcessAsync` method, either the specified `Policy` or the specified `Requirement` are passed in to the `IAuthorizationService` along with the resource.\n\n{% codeblock lang:csharp %}\n[HtmlTargetElement(Attributes = \"asp-authorize-resource,asp-policy\")]\n[HtmlTargetElement(Attributes = \"asp-authorize-resource,asp-requirement\")]\npublic class AuthorizeResourceTagHelper : TagHelper\n{\n    private readonly IAuthorizationService _authorizationService;\n    private readonly IHttpContextAccessor _httpContextAccessor;\n\n    public AuthorizeResourceTagHelper(IHttpContextAccessor httpContextAccessor, IAuthorizationService authorizationService)\n    {\n        _httpContextAccessor = httpContextAccessor;\n        _authorizationService = authorizationService;\n    }\n\n    /// <summary>\n    /// Gets or sets the policy name that determines access to the HTML block.\n    /// </summary>\n    [HtmlAttributeName(\"asp-policy\")]\n    public string Policy { get; set; }\n\n    /// <summary>\n    /// Gets or sets a comma delimited list of roles that are allowed to access the HTML  block.\n    /// </summary>\n    [HtmlAttributeName(\"asp-requirement\")]\n    public IAuthorizationRequirement Requirement { get; set; }\n\n\n    /// <summary>\n    /// Gets or sets the resource to be authorized against a particular policy\n    /// </summary>\n    [HtmlAttributeName(\"asp-authorize-resource\")]\n    public object Resource { get; set; }\n\n    public override async Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n    {\n        if (Resource == null)\n        {\n            throw new ArgumentException(\"Resource cannot be null\");                \n        }\n        if (string.IsNullOrWhiteSpace(Policy) && Requirement == null)\n        {\n            throw new ArgumentException(\"Either Policy or Requirement must be specified\");\n                \n        }\n        if (!string.IsNullOrWhiteSpace(Policy) && Requirement != null)\n        {\n            throw new ArgumentException(\"Policy and Requirement cannot be specified at the same time\");\n        }\n\n        AuthorizationResult authorizeResult;\n\n        if (!string.IsNullOrWhiteSpace(Policy))\n        {\n            authorizeResult = await _authorizationService.AuthorizeAsync(_httpContextAccessor.HttpContext.User, Resource, Policy);\n        }\n        else if (Requirement != null)\n        {\n            authorizeResult =\n                await _authorizationService.AuthorizeAsync(_httpContextAccessor.HttpContext.User, Resource,\n                    Requirement);\n        }\n        else\n        {\n            throw new ArgumentException(\"Either Policy or Requirement must be specified\");\n        }\n\n        if (!authorizeResult.Succeeded)\n        {\n            output.SuppressOutput();\n        }\n    }\n{% endcodeblock %} \n\nNote that either a policy or a requirement must be specified along with a resource, but you can't specify both a policy AND a requirement. Most of the code in the `ProcessAsync` method is checking the argument values to make sure a valid combination was used.\n\n# Try it out\nYou can see the authorize resource tag helper in action on my tag helper samples site [here](http://taghelpersamples.azurewebsites.net/Samples/Authorize). The sample site contains the examples listed in this blog post and also provides a way to log in as different users to test different scenarios.\n\nThe authorize resource tag helper is also available on [NuGet](https://www.nuget.org/packages/TagHelperSamples.Authorization/) so you can use it in your own ASP.NET Core application.\n\n```\ndotnet add package TagHelperSamples.Authorization\n```\n\nLet me know what you think. Would you like to see this tag helper including in the next release of ASP.NET Core?\n\n*NOTE:* If you choose to use the authorize resource tag helper in your application, you should remember that hiding a section of HTML is not enough to fully secure your application. You also need to make sure that resource-based authorization is applied to any related controllers and action methods.\n\n\n# What's Next?\n\nThere is one more authorization scenario related to supporting different authorization schemes that I hope to cover. Watch out for that in a future blog post. Also, this tag helper project is all open source so feel free to jump in on [GitHub](https://github.com/dpaquette/TagHelperSamples).","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"},{"name":"Tag Helpers","slug":"ASP-NET-Core/Tag-Helpers","permalink":"https://westerndevs.com/categories/ASP-NET-Core/Tag-Helpers/"}],"tags":[{"name":"Tag Helpers","slug":"Tag-Helpers","permalink":"https://westerndevs.com/tags/Tag-Helpers/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"},{"name":"Authorization","slug":"Authorization","permalink":"https://westerndevs.com/tags/Authorization/"}]},{"title":"Authorize Tag Helper for ASP.NET Core","authorId":"dave_paquette","slug":"authorize-tag-helper","date":"2017-11-05 19:38:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/Tag-Helpers/authorize-tag-helper/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/Tag-Helpers/authorize-tag-helper/","excerpt":"In ASP.NET Core, it's easy to control access to Controllers and Action Methods using the Authorize attribute. This attribute provides a simple way to ensure only authorized users are able to access certain parts of your application. While the Authorize attribute makes it easy to control authorization for an entire page, the mechanism for controlling access to a section of a page is a little clumsy. In this blog post, we build a Tag Helper that makes it incredibly easy to control access to any block HTML in a Razor view.","raw":"---\nlayout: post\ntitle: Authorize Tag Helper for ASP.NET Core\ntags:\n  - ASP.NET Core\n  - MVC\n  - Tag Helpers\n  - Authorization\ncategories:\n  - ASP.NET Core\n  - Tag Helpers\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2017/11/05/authorize-tag-helper.aspx'\ndate: 2017-11-05 14:38:30\nexcerpt: In ASP.NET Core, it's easy to control access to Controllers and Action Methods using the Authorize attribute. This attribute provides a simple way to ensure only authorized users are able to access certain parts of your application. While the Authorize attribute makes it easy to control authorization for an entire page, the mechanism for controlling access to a section of a page is a little clumsy. In this blog post, we build a Tag Helper that makes it incredibly easy to control access to any block HTML in a Razor view.\n---\nIn ASP.NET Core, it's easy to control access to Controllers and Action Methods using the `[Authorize]` attribute. This attribute provides a simple way to ensure only authorized users are able to access certain parts of your application. While the `[Authorize]` attribute makes it easy to control authorization for an entire page, the mechanism for controlling access to a section of a page is [a little clumsy](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/views?tabs=aspnetcore2x), involving the use of a the `IAuthorizationService` and writing C# based `if` blocks in your Razor code.\n\nIn this blog post, we build an Authorize tag helper that makes it easy to control access to any block HTML in a Razor view.\n\n# Authorize Tag Helper\nThe basic idea of this tag helper is to provide similar functionality to the `[Authorize]` attribute and it's associated action filter in ASP.NET Core MVC. The authorize tag helper will provide the same options as the `[Authorize]` attribute and the implementation will be based on the authorize filter. In the MVC framework, the `[Authorize]` attribute provides data such as the names of roles and policies while the authorize filter contains the logic to check for roles and policies as part of the request pipeline. Let's walk through the most common scenarios.\n\n## Simple Authorization\nIn it's [simplest form](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/simple), adding the `[Authorize]` attribute to a controller or action method will limit access to that controller or action method to users who are authenticated. That is, only users who are logged in will be able to access those controllers or action methods.\n\nWith the Authorize tag helper, we will implement a similar behaviour. Adding the `asp-authorize` attribute to any HTML element will ensure that only authenticated users can see that that block of HTML. \n\n{% codeblock lang:html %}\n<div asp-authorize class=\"panel panel-default\">\n    <div class=\"panel-heading\">Welcome !!</div>\n    <div class=\"panel-body\">\n        If you're logged in, you can see this section\n    </div>\n</div>\n{% endcodeblock %}\n\nIf a user is not authenticated, the tag helper will suppress the output of that entire block of HTML. That section of HTML will not be sent to the browser.\n\n## Role Based Authorization\nThe `[Authorize]` attribute provides an option to specify the role that a user must belong to in order to access a controller or action method. For example, if a user must belong to the _Admin_ role, we would add the `[Authorize]` attribute and specify the `Roles` property as follows:\n\n{% codeblock lang:csharp %}\n[Authorize(Roles = \"Admin\")]\npublic class AdminController : Controller\n{\n  //Action methods here\n}\n{% endcodeblock %}\n\nThe equivalent using the Authorize tag helper would be to add the `asp-authorize` attribute to an HTML element and then also add the `asp-roles` attribute specifying the require role.\n\n{% codeblock lang:html %}\n<div asp-authorize asp-roles=\"Admin\" class=\"panel panel-default\">\n    <div class=\"panel-heading\">Admin Section</div>\n    <div class=\"panel-body\">\n        Only admin users can see this section. Top secret admin things go here.\n    </div>\n</div>\n{% endcodeblock %}\n\nYou can also specify a comma separated list of roles, in which case the HTML would be rendered if the user was a member of any of the roles specified.\n\n## Policy Based Authorization\nThe `[Authorize]` attribe also provides an option to authorize users based on the requirements specified in a Policy. You can learn more about the specifics of this approach by reading the offical docs on [Claims-Based Authorization](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/claims) and [Custom-Policy Based Authorization](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies). Policy based authorization is applied by specifying `Policy` property for the `[Authorize]` attribute as follows:\n\n{% codeblock lang:csharp %}\n[Authorize(Policy = \"Seniors\")]\npublic class AdminController : Controller\n{\n  //action methods here\n}\n{% endcodeblock %}\n\nThis assumes a policy named _Seniors_ was defined at startup. For example:\n\n{% codeblock lang:csharp %}\nservices.AddAuthorization(o =>\n    {\n        o.AddPolicy(\"Seniors\", p =>\n        {\n            p.RequireAssertion(context =>\n            {\n                return context.User.Claims\n                      .Any(c => c.Type == \"Age\" && Int32.Parse(c.Value) >= 65);\n            });\n        });\n\n    }\n);\n{% endcodeblock %}\n\nThe equivalent using the Authorize tag helper would be to add the `asp-authorize` attribute to an HTML element and then also add the `asp-policy` attribute specifying the policy name.\n\n{% codeblock lang:html %}\n<div asp-authorize asp-policy=\"Seniors\" class=\"panel panel-default\">\n    <div class=\"panel-heading\">Seniors Only</div>\n    <div class=\"panel-body\">\n        Only users age 65 or older can see this section. Early bird dinner coupons go here. \n    </div>\n</div>\n{% endcodeblock %}\n\n## Combining Role and Policy Based Authorization\nYou can combine the role based and policy based approaches by specifying both the `asp-roles` and `asp-policy` attributes. This has the effect of requiring that the user meets the requiremnts for both the role and the policy. For example, the following would require that the usere were both a member of the Admin role and meets the requirements defined in the Seniors policy.\n\n{% codeblock lang:html %}\n<div asp-authorize asp-roles=\"Admin\" asp-policy=\"Seniors\" class=\"panel panel-default\">\n    <div class=\"panel-heading\">Admin Seniors Only</div>\n    <div class=\"panel-body\">\n        Only users who have both the Admin role AND are age 65 or older can see this section.\n    </div>\n</div>\n{% endcodeblock %}\n\n## Implementation Details\nThe Authorize tag helper itself is fairly simple. The implementation will likely evolve after this blog post so you can check out the latest version [here](https://github.com/dpaquette/TagHelperSamples/blob/master/TagHelperSamples/src/TagHelperSamples.Authorization/AuthorizeTagHelper.cs).\n\nThe tag helper implements the `IAuthorizeData` interface. This is the interface implemented by the [Authorize](https://github.com/aspnet/Security/blob/dev/src/Microsoft.AspNetCore.Authorization/AuthorizeAttribute.cs) attribute in ASP.NET Core. In the `ProcessAsync` method, the properties of `IAuthorizeData` are used to create an effective policy that is then evaluated against the current `HttpContext`. If the policy does not succeed, then the output of the tag helper is supressed. Remember that supressing the output of a tag helper means that the HTML for that element, including it's children, will be NOT sent to the client.\n\n{% codeblock lang:csharp %}\n[HtmlTargetElement(Attributes = \"asp-authorize\")]\n[HtmlTargetElement(Attributes = \"asp-authorize,asp-policy\")]\n[HtmlTargetElement(Attributes = \"asp-authorize,asp-roles\")]\n[HtmlTargetElement(Attributes = \"asp-authorize,asp-authentication-schemes\")]\npublic class AuthorizationPolicyTagHelper : TagHelper, IAuthorizeData\n{\n    private readonly IAuthorizationPolicyProvider _policyProvider;\n    private readonly IPolicyEvaluator _policyEvaluator;\n    private readonly IHttpContextAccessor _httpContextAccessor;\n\n    public AuthorizationPolicyTagHelper(IHttpContextAccessor httpContextAccessor, IAuthorizationPolicyProvider policyProvider, IPolicyEvaluator policyEvaluator)\n    {\n        _httpContextAccessor = httpContextAccessor;\n        _policyProvider = policyProvider;\n        _policyEvaluator = policyEvaluator;\n    }\n\n    /// <summary>\n    /// Gets or sets the policy name that determines access to the HTML block.\n    /// </summary>\n    [HtmlAttributeName(\"asp-policy\")]\n    public string Policy { get; set; }\n\n    /// <summary>\n    /// Gets or sets a comma delimited list of roles that are allowed to access the HTML  block.\n    /// </summary>\n    [HtmlAttributeName(\"asp-roles\")]\n    public string Roles { get; set; }\n\n    /// <summary>\n    /// Gets or sets a comma delimited list of schemes from which user information is constructed.\n    /// </summary>\n    [HtmlAttributeName(\"asp-authentication-schemes\")]\n    public string AuthenticationSchemes { get; set; }\n    \n    public override async Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n    {\n        var policy = await AuthorizationPolicy.CombineAsync(_policyProvider, new[] { this });\n\n        var authenticateResult = await _policyEvaluator.AuthenticateAsync(policy, _httpContextAccessor.HttpContext);\n\n        var authorizeResult = await _policyEvaluator.AuthorizeAsync(policy, authenticateResult, _httpContextAccessor.HttpContext, null);\n\n        if (!authorizeResult.Succeeded)\n        {\n            output.SuppressOutput();\n        }\n    }\n}\n{% endcodeblock %} \n\nThe code in the `ProcessAsync` method is based on the [AuthorizeFilter](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.Core/Authorization/AuthorizeFilter.cs) from ASP.NET Core MVC.\n\n# Try it out\nYou can see the Authorize tag helper in action on my tag helper samples site [here](http://taghelpersamples.azurewebsites.net/Samples/Authorize). The sample site contains the examples listed in this blog post and also provides a way to log in as different users to test different scenarios.\n\nThe Authorize tag helper is also available on [NuGet](https://www.nuget.org/packages/TagHelperSamples.Authorization/) so you can use it in your own ASP.NET Core application.\n\n```\ndotnet add package TagHelperSamples.Authorization\n```\n\nLet me know what you think. Would you like to see this tag helper included in the next release of ASP.NET Core?\n\n# What's Next?\nIf you choose to use the Authorize tag helper in your application, you should remember that hiding a section of HTML is not enough to fully secure your application. You also need to make sure that authorization is applied to any related controllers and action methods. The Authorize tag helper is meant to be used in conjugtion with the `[Authorize]` attribute, not as a replacement for it.\n\nThere are a couple more scenarios I would like to go through and I will address those in a future post. One of those is supporting different Authorization Schemes and the other resource based authorization. Of course, this project is all open source so feel free to jump in on [GitHub](https://github.com/dpaquette/TagHelperSamples).","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"},{"name":"Tag Helpers","slug":"ASP-NET-Core/Tag-Helpers","permalink":"https://westerndevs.com/categories/ASP-NET-Core/Tag-Helpers/"}],"tags":[{"name":"Tag Helpers","slug":"Tag-Helpers","permalink":"https://westerndevs.com/tags/Tag-Helpers/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"},{"name":"Authorization","slug":"Authorization","permalink":"https://westerndevs.com/tags/Authorization/"}]},{"title":"Home Networking - Cabling","authorId":"donald_belcham","slug":"Home-Networking-2","date":"2017-10-03 03:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Networking/Home-Networking-2/","link":"","permalink":"https://westerndevs.com/Networking/Home-Networking-2/","excerpt":"One of the things that I knew I wanted to do with this home network was run cables to as many places as possible","raw":"---\nlayout: post\ntitle: Home Networking - Cabling\ndate: 2017-10-02 23:00:00\ncategories:\n  - Networking\ntags:\n  - networking\nexcerpt: One of the things that I knew I wanted to do with this home network was run cables to as many places as possible\nauthorId: donald_belcham\n---\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/raw-wiring.jpg\"/>\n\nOne of the things that I knew I wanted to do with this home network was run cables to as many places as possible. Some were non-negotiable: \n * the office needed multiple drops (8 in the end)  \n * the TV areas needed multiple drops  \n * each of the wireless access points needed a drop\n\nI really didn't want to deal with pulling the cable into some of those areas. Hanging out in the attic wasn't way up on my list of fun things. And, to be honest, I had no clue how I was going to get cables from the main house into the attached in-law suite. So I called the local electrician. I needed a couple of circuits to power the equipment in the server rack so it seemed like something that would make sense to bundle together.\n\nThis was the best option. The electrician and his helper pulled about 1700 feet of Cat6 cable for me. Some were my required cables and a few were extras that they pulled to the attics so that I had extra capacity when I need it. They spent a bunch of time in the heat of the attic. They also pulled cables into wall cavities so that it looks like the house has always had Cat6.\n\nAll of the cable drops were pulled to the area that I was going to put the server racks. All of this is pretty basic stuff, but I did do one thing different than I did in my last house; I didn't cut the cables short on the rack end. Instead I terminated them into Cat6 female keystone jacks as close to the ends as possible.\n\nThis left a lot of extra cable by the rack. And that's fine. Heck, in the end I had to re-terminate a handful of the cables because I screwed them up or did them poorly.\n\n> NOTE: It's worth buying a proper punch down tool if you're going to go do your own ends. Between my patch panel and the wall plates around the house I terminated about 50 keystones.\n\nSo what do you do with all that excess cable? You dress it of course. If you've never seen well dressed network cables, you need to spend a bit of time over at [https://cableporn.reddit.com](/r/cableporn). I hate messy cables. It could be my desk, behind the TV, or my networking stuff, so the people of /r/cableporn have a dear place in my heart.\n\nI dressed all the excess cables into what's known as a \"service loop\". The loop provides you with an excess of cable really close to the rack. If you have to fix terminations you have extra cable that you can use. This was something that I didn't have at the old house and it almost bit me in the ass. So now I have a service loop attached right to the rack which gives me about 4-5 ft of extra cable. \n\n> I had so much excess cable that I ended up with a second service loop that has about 8ft of cable in it too...so I should never have short cable problems.\n\nHaving a service loop is great, but having a messy one is almost as bad as not having one at all. I bundled up all of the cables in the service loop using [velcro straps](https://www.amazon.ca/gp/product/B001E1Y5O6/ref=oh_aui_detailpage_o00_s00?ie=UTF8&psc=1). Not zip ties, velcro straps. You can easily undo them and move cables as needed. With zip ties I'd have to cut them and add new ones every time. It's also possible to over tighten a zip tie and end up cutting into the cabling...not good. Velcro straps were the right choice based on the amount of times that I had to loosen them to move cables.\n\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/rack-loop.jpg\"/>\n\nI probably could have bundled the cables better if I'd used a [cable comb](https://www.amazon.ca/ACOM-PIECE-CONTRACTOR-INSTALLATION-YELLOW/dp/B01BTUI1TQ/ref=sr_1_1?ie=UTF8&qid=1501120387&sr=8-1&keywords=cable+comb), but for the amount of cables that I had I didn't think buying one was worth it.\n\nThe only other Cat6 cable work that I did was to make a bunch of short cables to go between the patch panel and the switch. Yah, I probably could have bought them and saved myself a bunch of time. I needed something to do in my evenings, and getting them the exact length that I wanted would make the front of the panel look a little better too. \n\nSo all that Cat6 cable management is great, but there are other cables in my rack area that needed to be managed too. The switches, routers, UPS, power distribution, etc all had power cables. Instead of running them all over the place to get them plugged in, I bundled them up too. I didn't bundle them with the Cat6 though. Power cables can cause interference in unshielded network cabling. So with the Cat6 service loop attached to one side of the rack, I bundled the power cables and ran them along the other side of the rack.\n\nSo cabling was important, and maybe I went a bit overboard on it for a house. I should never have issues with the cables I have, and if I do I have extra cable in the service loops to fix it. I have an organized rack cabling solution that makes it really easy to trace and replace/fix cables as required. It's also really easy to move things around, change ports being used, or completely remove hardware as needed.\n\nPut some time into planning your cable needs, some effort into installing them, and some patience into organizing them. The end result will pay off.","categories":[{"name":"Networking","slug":"Networking","permalink":"https://westerndevs.com/categories/Networking/"}],"tags":[{"name":"networking","slug":"networking","permalink":"https://westerndevs.com/tags/networking/"}]},{"title":"Creating a Custom Matcher for TS-Mokito","authorId":"simon_timms","slug":"TsMokito-custom-matcher","date":"2017-09-29 23:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/TsMokito-custom-matcher/","link":"","permalink":"https://westerndevs.com/development/TsMokito-custom-matcher/","excerpt":"Mocking libraries can be useful, even in JavaScript testing. One of my favorites is ts-mokito a TypeScript mocking library. One minor problem with it is that it is lacking a good array matcher. In this article we'll see how to fix that.","raw":"---\nlayout: post\ntitle: Creating a Custom Matcher for TS-Mokito\ntags:\n  - javascript\n  - testing\ncategories:\n  - development\nauthorId: simon_timms\ndate: 2017-09-29 19:36:36\nexcerpt: Mocking libraries can be useful, even in JavaScript testing. One of my favorites is ts-mokito a TypeScript mocking library. One minor problem with it is that it is lacking a good array matcher. In this article we'll see how to fix that. \n---\n\nFor better or worse I've been working with a lot of Angular as of late. One of the better tools I've found for helping out with testing is a great little mocking library called [ts-mokito](https://github.com/NagRock/ts-mockito). I don't want to get into why I still like to have a mocking library even with a language like JavaScript - that's another post. \n\nTesting today I ran into an annoying problem in that I was testing an angular route like so \n\n```js\nverify(mockRouter.navigate(['/some/route'])).called();\n```\n\nThis test was always failing. This is because the argument to navigate is an array and in JavaScript \n\n```js\n['/some/route'] !== ['/some/route']\n```\n\nHow unfortunate. Fortunately there isn't much to a custom matcher in ts-mokito. You can already do \n\n```typescript\nverify(mockRouter.navigate(anything())).called();\n```\n\nTo simply check that something was passed in but we'd like our tests to be a bit more robust than that. Enter the ArraryMatcher\n\n```typescript\nimport * as _ from 'lodash';\nimport { Matcher } from 'ts-mockito/lib/matcher/type/Matcher';\n\nexport class ArrayMatcher extends Matcher {\n    private value: any;\n    constructor(private expected) {\n        super();\n    }\n\n    match(value: any): boolean {\n        return _.isEqual(this.expected, value);\n    }\n\n    toString(): string {\n        return `Did not match ${this.expected}`;\n    }\n}\n```\n\nAs you can see it has an implementation of the `match` function which uses the `isEqual` from lodash. This class can be substituted into our verify \n\n```typescript\nverify(mockRouter.navigate(<any>new ArrayMatcher(['/some/route'])).called();\n```\n\nThat's a bit ugly with casting in there. This can be fixed by exporting a function from the ArrayMatcher\n\n```typescript\nexport function arrayMatches(expected): any { \n    return new ArrayMatcher(expected); \n}\n```\n\nWhich allows for the much more pleasant syntax\n\n```typescript\nverify(mockRouter.navigate(arrayMatches([`/some/route`]))).called();\n```\n\nEasily done and it should be simple to implement any other sort of matcher you like.","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"testing","slug":"testing","permalink":"https://westerndevs.com/tags/testing/"}]},{"title":"Windows Subsystem for Linux is Cool. No really, it is.","authorId":"david_wesst","slug":"websummit-wsl","date":"2017-09-20 22:35:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"web/websummit-wsl/","link":"","permalink":"https://westerndevs.com/web/websummit-wsl/","excerpt":"With the Fall Creators update of Windows 10, you can go to the Windows Store and install Linux. Yeah, that's a thing now and it's pretty cool.","raw":"---\ntitle: Windows Subsystem for Linux is Cool. No really, it is.\ndate: 2017-09-20 12:35:00 -600\ncategories: \n    - web\ntags:\n    - microsoft-edge\n    - f12\n    - linux\n    - windows subsystem for linux\n    - wsl\nexcerpt: \"With the Fall Creators update of Windows 10, you can go to the Windows Store and install Linux. Yeah, that's a thing now and it's pretty cool.\"\nauthorId: david_wesst\nlayout: post\n---\n\n[1]: https://summit.microsoftedge.com/\n[2]: https://davidwesst.blob.core.windows.net/blog/websummit-wsl/ubuntu-install.gif\n[3]: https://www.cygwin.com/\n[4]: https://msdn.microsoft.com/en-us/commandline/wsl/install_guide\n[5]: https://insider.windows.com/en-us/\n[6]: https://blogs.msdn.microsoft.com/commandline/\n\n_This is part of a series of posts capturing the highlights from my experience at the [Microsoft Edge Web Summit 2017][1]_\n\n![Installing Ubuntu on Windows 10 from the Windows Store][2]\n\nWith the Fall Creators Update for Windows 10, you can go to the Windows Store and install Linux.\n\nYeah, that's a thing now and it's pretty cool.\n\n## What is the Windows Subsystem for Linux (WSL)?\nIt's a new Windows feature that allows Linux distributions like Ubuntu and OpenSUSE to run inside of Windows. Essentially, this let's you run Linux-based command line applications against your files stored in Windows.\n\n## Why is that cool?\nIt's cool because a lot of the web runs on Linux, but many of us develop on Windows because the business runs on the Windows platform. This means that the dev tools you have installed to run your build and test your application are the Windows versions.\n\nIt might not seem like a big deal, but it's definitely a discrepancy. How are you expected to catch Linux issues before you deploy if you're running Windows? The WSL, that's how.\n\n## Isn't this just CygWin?\nNo, it's different.\n\nAccording to the [CygWin homepage][3]\n\n> Cygwin is:\n> * a large collection of GNU and Open Source tools which provide functionality similar to a Linux distribution on Windows.\n> * a DLL (cygwin1.dll) which provides substantial POSIX API functionality.\n\nWSL, is a layer inside of Windows that allows actual Linux distributions to run against. Microsoft is providing the foundation for Linux distributions to build upon, and keeping their hands out of the tooling itself.\n\nWhen you're using CygWin, you're not using a Linux distribution. You're using CygWin. With WSL, you're not using any Linux distributions unless you install them on top of the WSL. Once you do, you're using the tooling provided and build _for that Linux distribution_.\n\n## Isn't this just Bash for Windows 10?\nNo, it's also different.\n\nAlthough I won't get into the weeds with it, Bash for Windows 10 was something of a precursor to the Ubuntu distribution that is in the Windows Store. Think of it as an Ubuntu for Windows preview.\n\nNow, we're not limited to just Bash on Ubuntu. We can install OpenSUSE and run bash on that, and eventually Fedora, and probably other flavours of Linux as time goes on. So you can run two different or three different versions of Linux against the Windows filesystem at the same time, without needing a bunch of VMs running. \n\n## Why don't I just run Linux?\nYou totally can, this doesn't change that.\n\nThis is a dev tool, first and foremost. It's meant to (IMHO) provide developers a easier way to run Linux tools on Windows, with the resource boundaries and extra resource consumption of a virutal machine.\n\nFor example, if you're running Apache on your Linux-based web server, you no longer have to run Windows-based Apache. Rather, you can install the Linux version of Apache on Ubunut or whatever, adn directly against your Windows filesystem. No VM to prep, or system boundaries to cross. Just install and run it.\n\n## How do I start?\nYou can start by [installing the WSL on your Windows 10 machine][4]. \n\nThe Windows Store animation I showed at the beginning of this post is using an [Insiders Build][5] of Fall Creators update for Windows 10. You can join the insiders program yourself learn about that [here][5].\n\nUntil the Fall Creators update, you can still start tinkering with Bash on Ubuntu for Windows 10 by following the [install instructions][4] provided earlier.\n\n## Anything else?\nYeah. \n\nThey are finally fixing the console window in Windows, so that's a thing too. Read about it on [the team's blog][6].\n\n---\n\n## Resources\n\n* [Windows Command Line Tools Blog][6]\n* [Installing WSL Instructions][4]","categories":[{"name":"web","slug":"web","permalink":"https://westerndevs.com/categories/web/"}],"tags":[{"name":"microsoft-edge","slug":"microsoft-edge","permalink":"https://westerndevs.com/tags/microsoft-edge/"},{"name":"f12","slug":"f12","permalink":"https://westerndevs.com/tags/f12/"},{"name":"linux","slug":"linux","permalink":"https://westerndevs.com/tags/linux/"},{"name":"windows subsystem for linux","slug":"windows-subsystem-for-linux","permalink":"https://westerndevs.com/tags/windows-subsystem-for-linux/"},{"name":"wsl","slug":"wsl","permalink":"https://westerndevs.com/tags/wsl/"}]},{"title":"Sonar, the Linter I Never Knew I Wanted","authorId":"david_wesst","slug":"websummit-sonar","date":"2017-09-19 22:35:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"web/websummit-sonar/","link":"","permalink":"https://westerndevs.com/web/websummit-sonar/","excerpt":"Meet Nellie the Narwhal, the official mascot for Sonar, the linting tool for the web. Nellie represents a tool that is long-overdue. It provides any web application the ability to verify they are meeting a high level of quality when it comes to their web applications, and not miss any common mistakes.","raw":"---\ntitle: Sonar, the Linter I Never Knew I Wanted\ndate: 2017-09-19 12:35:00 -600\ncategories: \n    - web\ntags:\n    - microsoft-edge\n    - sonar\n    - accessibility\n    - devops\nexcerpt: \"Meet Nellie the Narwhal, the official mascot for Sonar, the linting tool for the web. Nellie represents a tool that is long-overdue. It provides any web application the ability to verify they are meeting a high level of quality when it comes to their web applications, and not miss any common mistakes.\"\nlayout: post\nauthorId: david_wesst\n---\n\n[1]: https://summit.microsoftedge.com/\n[2]: https://davidwesst.blob.core.windows.net/blog/websummit-sonar/sonar-bttf.png\n[3]: https://sonarwhal.com/\n[4]: http://www.westerndevs.com/web/websummit-pwa/\n[5]: https://davidwesst.blob.core.windows.net/blog/websummit-sonar/sonar-in-action.gif\n\n_This is part of a series of posts capturing the highlights from my experience at the [Microsoft Edge Web Summit 2017][1]_\n\n![Nellie the Narwhal in the Back to the Future DeLorean][2]\n\nMeet Nellie the Narwhal. Nellie is the official mascot for [Sonar][3], the linting tool for the web. Nellie represents a tool that is long-overdue. It provides any web application the ability to verify they are meeting a high level of quality when it comes to their web applications, and catch any improvements to implement or common pitfalls to fix.\n\n## What is Sonar?\nSonar is a linting tool for the web. Plain and simple.\n\nIt provides command line linting tool, along with some rules, that prevent developers from making some common mistakes or pitfalls with their web applications. It covers things like security, accessbility, and progressive web applications, to name a few.\n\nYou can also create your own rules for the linter, allowing you to extend the tool to check your solutions for business specific requirements.\n\nOh, and it's been [donated to the JS Foundation](https://js.foundation/announcements/2017/06/22/sonar-js-foundation-welcomes-newest-project), where Microsoft continues to contribute to it.\n\n## Why do you want this?\nBecause of devops, that's why.\n\nThis is a command line tool that can break a build if your site doesn't meet specific requirements. Personally, I'm happy about the security and accessiblity rules they provide, but being that [I'm pretty focused on PWAs][4], I'm sure that Sonar is going to help me write better PWAs than I would have done learning on my own.\n\nPersonally, I'm looking forward to breaking a build because I forgot to implement some accessibility rules. It will help me and my team learn some of these optimizations to make our web projects work best for everyone on any device.\n\n## How can I start using it?\n![Sonar running against WesternDevs.com showing lots of issues][5]\n\nIt's pretty easy: just install, initialize, and run it.\n\nAs you can see, there is work for us to do if we want to bring our blog up to the recommended specification provided by Sonar. \n\nIf you're looking to set a bar of quality, whether it be the recommended standard or just your own set of standards, Sonar is the tool that can make that happen.\n\n---\n\n## Resources\n* [Sonar](https://sonarwhal.com/)\n* [JS Foundation](https://js.foundation/)\n\n## Image Credit\n* [Nellie's Photo Album | Back to the Future](https://github.com/sonarwhal/nellie)","categories":[{"name":"web","slug":"web","permalink":"https://westerndevs.com/categories/web/"}],"tags":[{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"},{"name":"microsoft-edge","slug":"microsoft-edge","permalink":"https://westerndevs.com/tags/microsoft-edge/"},{"name":"sonar","slug":"sonar","permalink":"https://westerndevs.com/tags/sonar/"},{"name":"accessibility","slug":"accessibility","permalink":"https://westerndevs.com/tags/accessibility/"}]},{"title":"Service Workers and PWAs are Super Cool","authorId":"david_wesst","slug":"websummit-pwa","date":"2017-09-18 23:35:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"web/websummit-pwa/","link":"","permalink":"https://westerndevs.com/web/websummit-pwa/","excerpt":"One of the core items highlighted by the Microsoft Edge team, along with many others who were just web professionals, was the importance of Progressive Web Apps (PWAs). I started out thinking they were something that could be interesting one day, but left the conference convinced that this will change the way we think of the web.","raw":"---\ntitle: Service Workers and PWAs are Super Cool\ndate: 2017-09-18 13:35:00 -600\ncategories: \n    - web\ntags:\n    - microsoft-edge\n    - service-workers\n    - progressive-web-apps\n    - javascript\nexcerpt: \"One of the core items highlighted by the Microsoft Edge team, along with many others who were just web professionals, was the importance of Progressive Web Apps (PWAs). I started out thinking they were something that could be interesting one day, but left the conference convinced that this will change the way we think of the web.\"\nlayout: post\nauthorId: david_wesst\n---\n\n[1]: https://summit.microsoftedge.com/\n[2]: https://developers.google.com/web/progressive-web-apps/\n[3]: https://davidwesst.blob.core.windows.net/blog/websummit-pwa/pwa-logo.svg\n[4]: https://diekus.net/logo-pwinter/\n[5]: https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API\n[6]: https://developer.mozilla.org/en-US/docs/Web/API/Push_API\n[7]: https://developer.mozilla.org/en-US/docs/Web/API/Payment_Request_API\n[8]: https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API\n\n_This is part of a series of posts capturing the highlights from my experience at the [Microsoft Edge Web Summit 2017][1]_\n\nOne of the core items highlighted by the Microsoft Edge team, along with many others who were just web professionals, was the importance of Progressive Web Apps (PWAs). I started out thinking they were something that _could_ be interesting one day, but left the conference convinced that this will change the way we think of the web.\n\n## What is a PWA?\n![PWA Logo from diekus.net/logo-pwinter][3]\n\n[Google has been talking about PWAs][2] for a while now, but in my words they are web applications that use a progressive enhancement design strategy to add device native features, when available.\n\nSo, it's just a web app with some fancy bells and whistles, like offline loading once the site has been downloaded once and native device API access, like camera access or push notifications straight to the device.\n\nIf it sounds like regular installed application rather than a web application, then you're understanding this correctly.\n\nPWAs _could_ be the next wave of \"apps\" for our devices, but they won't need a store front. Rather, the user can just navigate to the site and \"install\" the site, which can be cached for offline usage amongst other things.\n\n## What makes them super cool?\nOutside of the coolness of extending the reach of the web into offline world, it's also built on a set of open web standards.\n\nStandards give developers APIs to use across platforms, but they also give the platform holders something common to build against. Both Windows and Android have big plans for PWAs, giving web developers a whole new opportunity to use our existing skills to deliver great software. \n\nJust to be clear, software that can be offline and installed natively, using web development tools and skills.\n\nThat _is_ super cool.\n\n## So, where do I start?\nWith the [Service Worker API][5].\n\nThis is the first step in building a great PWA, as it allows you to \"install\" the web application on the device. Once you have it installed, then you can worry about the rest of the functionality and how it should work.\n\nOn top of that, start looking at some of the new APIs that have been coming out of the W3C like the [Push API][6], the [Payment Request API][7], and [IndexedDB][8].\n\nThis should give you a good idea of what can do with a PWA rather than a regular old web application. \n\n\n","categories":[{"name":"web","slug":"web","permalink":"https://westerndevs.com/categories/web/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"microsoft-edge","slug":"microsoft-edge","permalink":"https://westerndevs.com/tags/microsoft-edge/"},{"name":"service-workers","slug":"service-workers","permalink":"https://westerndevs.com/tags/service-workers/"},{"name":"progressive-web-apps","slug":"progressive-web-apps","permalink":"https://westerndevs.com/tags/progressive-web-apps/"}]},{"title":"Is there still a place for the server-side web?","slug":"Server-side-web","date":"2017-09-10 15:34:08+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Server-side-web/","link":"","permalink":"https://westerndevs.com/podcasts/Server-side-web/","excerpt":"In a leaner half-hour podcast, we discuss whether we should still be rendering web pages on the server","raw":"---\nlayout: podcast\ntitle: Is there still a place for the server-side web?\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-server-side-rendering.mp3\n  length: '31:35'\n  filesize: 29604629\n  libsynId: 5725116\n  anchorFmId: Is-there-still-a-place-for-the-server-side-web-evqdj0\nparticipants:\n  - kyle_baley\n  - simon_timms\n  - dave_paquette\n  - dylan_smith\n  - damian_brady\n  - david_wesst\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - Razor Pages on MS Docs|https://docs.microsoft.com/en-us/aspnet/core/mvc/razor-pages/?tabs=visual-studio\n  - Client-side vs. server-side rendering|https://medium.com/@adamzerner/client-side-rendering-vs-server-side-rendering-a32d2cf3bfcc\n  - \"Hacker news: Is it ok to use traditional server-side rendering these days?|https://news.ycombinator.com/item?id=13212465\"\n  - \"Why it's not always black and white|https://medium.freecodecamp.org/what-exactly-is-client-side-rendering-and-hows-it-different-from-server-side-rendering-bd5c786b340d\"\ndate: 2017-09-10 11:34:08\nrecorded: 2017-09-08\nexcerpt: In a leaner half-hour podcast, we discuss whether we should still be rendering web pages on the server\n---\n\n### Synopsis\n\n* Microsoft docs recommends using Razor pages by default for server-side rendering\n* Razor pages != Razor views in MVC (though they're close)\n* Making the case for ditching server-side rendering altogether\n* Pre-rendering pages with a lot of data\n* Targeting an appropriate level of interactivity\n* If it's simple, there's often a SaaS for it\n* Deciding between client-side and server-side\n* Content management systems/Wordpress\n* Internationalization\n* Bootstrapping\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Awareness, Acknowledgement, and Adjustment","authorId":"damian_brady","slug":"Awareness-Acknowledgement-Adjustment","date":"2017-08-23 13:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"devops/Awareness-Acknowledgement-Adjustment/","link":"","permalink":"https://westerndevs.com/devops/Awareness-Acknowledgement-Adjustment/","excerpt":"There's a common pattern in IT that frequently leads to new buzzwords: Awareness, Acknowledgement, and Adjustment. Make it work for you!","raw":"---\ntitle: Awareness, Acknowledgement, and Adjustment\ndate: 2017-08-23 09:00:00\ncategory:\n    - devops\ntags:\n  - devops\n  - alm\nexcerpt: \"There's a common pattern in IT that frequently leads to new buzzwords: Awareness, Acknowledgement, and Adjustment. Make it work for you!\"\nlayout: post\nauthorId: damian_brady\noriginalurl: https://damianbrady.com.au/2017/08/23/awareness-acknowledgement-and-adjustment/\n---\n\nThere's a common pattern in IT. In many cases, it ultimately leads to a new buzzword. It's not complicated. In fact it's fairly obvious. But *understanding what you can do with it* can help immensely when it comes to pushing change in your organisation.\n\nAgile, DevOps, Microservices, Containerization, whatever the next buzzword is - all of these \"revolutions\" are really a result of these three steps.\n\n> 1. Awareness\n\n> 2. Acknowledgement\n\n> 3. Adjustment\n\n![You need awareness and acknowledgement before things will change](https://damianbrady.com.au/content/images/2017/08/binoculars.jpg)\n\n## Examples:\n\nLet's look at a couple of examples to solidify what I'm talking about. I'm simplifying for the purposes of brevity, but consider Agile and Microservices:\n\n### Agile:\n1. *Awareness* of slipping timelines, incorrect estimates, project failures.\n2. *Acknowledgement* that big, upfront plans never seem to go smoothly, no matter how long you spend on them or how many people are involved. Some things you just can't know until you start.\n3. *Adjustment* of how projects are run - you can't plan for the long term, so let's just plan a short distance ahead and iterate.\n\n### Microservices:\n1. *Awareness* that even small changes can be expensive in a monolithic architecture, and side-effects can be hard to predict.\n2. *Acknowledgement* that isolated and decoupled services can make change easier and more predictable.\n3. *Adjustment* of how software is put together - small, self-contained components that can be iterated on more quickly.\n\n**The same pattern can apply for DevOps:**\n\n### DevOps:\n1. *Awareness* that software in prodduction is the aim, and that there are some obvious bottlenecks that slow things down.\n2. *Acknowledgement* that software should be able to help with this - most of what we do is able to be automated!\n3. *Adjustment* by removing the bottlenecks using software and scripts, adding surety through automated testing, and reducing the cycle time from idea to production.\n\n![](https://damianbrady.com.au/content/images/2017/08/lightbulbs-hanging.jpg)\n\n## So what can I do with this?\n\nI've given a few talks lately on the challenges of implementing DevOps in an organisation where you have little to no control over how the business operates.\n\nIn particular, my \"[Doing DevOps as a Politically Powerless Developer](https://vimeo.com/223984407)\" talk has resonated with a lot of people lately.\n\nAs a \"politically powerless developer\" in an organisation, it can be difficult to instigate change, even when you know it's the right thing to do. In short, you probably don't have the power to implement the *Adjustment* part of this pattern.\n\n**But you can definitely affect *Awareness*. It's easy - start measuring.**\n\n![Make people aware there's a problem](https://damianbrady.com.au/content/images/2017/08/ruler.jpg)\n\nMeasure how long it takes for a change you commit to make it to production. What happens in that time? Is there a lot of waiting?\n\nMeasure how long it takes for a user-reported issue to get to your backlog. Add some production monitoring, and measure how much time is saved by learning about issues first-hand. How much time did you save?\n\nMeasure the time it takes to write tests, and compare that to the time you take fixing bugs for code that doesn't have tests around it. Can you prove that writing tests is beneficial overall?\n\nOnce you have information, you can give it to your boss or pass it up the chain. You've already acknowledged there's a better way, so you'll be ready with solutions once management acknowledges there's a problem!\n\n![Drive change by measuring and passing that information up the chain](https://damianbrady.com.au/content/images/2017/08/report.jpg)\n\n## Summary\n\nTo really instigate change and *adjust* how your organisation delivers software, there needs to be an *acknowledgement* that there's a problem. That won't happen unless there's *awareness*.\n\n**If you're having trouble pushing change, focus on *awareness* first.**","categories":[{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/categories/devops/"}],"tags":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"},{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"}]},{"title":"Posh-GVM, the Groovy Version Manager for Powershell","authorId":"david_wesst","slug":"posh-gvm","date":"2017-08-17 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"java/posh-gvm/","link":"","permalink":"https://westerndevs.com/java/posh-gvm/","excerpt":"Here's another dev thing I use: Posh-GVM, a Groovy version manager that works for Windows.","raw":"---\ntitle: Posh-GVM, the Groovy Version Manager for Powershell\ndate: 2017-08-17 10:00:00\ncategory:\n    - java\ntags:\n    - grails\n    - groovy\n    - gradle\n    - powershell\n    - version manager\nexcerpt: \"Here's another dev thing I use: Posh-GVM, a Groovy version manager that works for Windows.\"\nlayout: post\nauthorId: david_wesst\n---\n\n[1]: https://davidwesst.blob.core.windows.net/blog/posh-gvm/poshgvm-example.gif \"Posh-GVM in action in a Powershell terminal\"\n[2]: http://www.westerndevs.com/java/jabba/ \"My post on Jabba, the Java version manager for everyone\"\n\nHere's another dev thing I use: [Posh-GVM](https://github.com/flofreud/posh-gvm), a Groovy version manager that works for Windows.\n\nYou remember Groovy right? The language that was all the rage at some point with Groovy on Grails?\n\nAll kidding aside, in my adventures as an enterprise JDK developer, I've come across a number of Groovy on Grails applications. These projects tend to span multiple versions of Grails, ranging from 2.3.x which works on older versions of Tomcat, upto 3.2.x for of of our newer solutions. Instead of manually configuring my system for each project, I just use Posh-GVM as recommended by the folks who brought you [SDKMan for Unix systems](http://sdkman.io/).\n\n![Posh-GVM in action][1]\n\n## What does it do?\nIt handles switching between versions of Groovy, Grails, and a bunch more technologies without having to fuss with configuring your system. It switch between versions of Grails, Groovy, Gradle, Koitlin, and more with `gvm use <candidate> <version>` where candidate is the technology and version is...well, the version.\n\n## Using Posh-GVM\nPosh-GVM is a Windows port to [SDKMan](http://sdkman.io/) formerly known as the GVM, or Groovy eVironment Manager. Instead of requiring a Unix system to run, it requires Powershell.\n\nTo install it, I followed the README instructions to install it via [a short script](https://github.com/flofreud/posh-gvm#via-short-script). I tried using the PsGet method described, but didn't have any luck finding the module. More on that later.\n\nOnce installed (and added to your profile) you can run `gvm help` in the Powershell terminal and you should see a lovely help menu with all the goodies you can install and switch your fingertips.\n\n### Won't these conflict with the versions I already have installed?\nNo. It installs the tools in a different directory, so you should be good.\n\nThat being said, you probably don't need to have a local version of Grails or whatever tool installed anymore because Posh-GVM will handle that for you. \n\n## What makes it cool?\nIt's cool because it works on Windows, without the need for Bash or Cygwin.\n\nThe fact that it covers a number of tools, including Grails, Groovy, and Gradle (and many more) is a pretty nifty too. \n\n## What are the drawbacks?\nThere are two that stand out to me, but nothing that has made me abandon the tool for something else.\n\n### Java not included\nThe first being that it doesn't support Java like it's Unix couterpart. My guess is that Java is something special when it comes to Unix VS Windows and was eliminated for that reason. We have [Jabba][2] for that on Windows, but it would be nice to have all the pieces in to the puzzle in a single tool.\n\n### Lack of Project Activity\nThe second is the lack of updates.\n\nAs of this writing, it hasn't been updated since [December 2015](https://github.com/flofreud/posh-gvm/commit/2145f8a65c5bf317e96664ebb03bf84c569ba770) while SDKMan has continued to be actively developed.\n\nThis isn't necessarily a bad thing, as there haven't been any pull requests in quite some time either. It's just something I note as a risk when I adopt an open source tool.\n\n---\n\nUltimately, I think this tool is a great solution for people that need to use any of these tools, but don't want to couple themselves to Cygwin or Bash for Windows. It has solved my Grails, Groovy, and Gradle versioning issues on Windows, and that is more than enough to make it a win in my books.","categories":[{"name":"java","slug":"java","permalink":"https://westerndevs.com/categories/java/"}],"tags":[{"name":"powershell","slug":"powershell","permalink":"https://westerndevs.com/tags/powershell/"},{"name":"version manager","slug":"version-manager","permalink":"https://westerndevs.com/tags/version-manager/"},{"name":"grails","slug":"grails","permalink":"https://westerndevs.com/tags/grails/"},{"name":"groovy","slug":"groovy","permalink":"https://westerndevs.com/tags/groovy/"},{"name":"gradle","slug":"gradle","permalink":"https://westerndevs.com/tags/gradle/"}]},{"title":"Jabba, the Java Version Manager for Everyone","authorId":"david_wesst","slug":"jabba","date":"2017-08-16 13:35:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"java/jabba/","link":"","permalink":"https://westerndevs.com/java/jabba/","excerpt":"Here's another dev thing I use: Jabba, a cross-platform Java version manager that works for Windows.","raw":"---\ntitle: Jabba, the Java Version Manager for Everyone\ndate: 2017-08-16 09:35:00\ncategory:\n    - java\ntags:\n    - java\n    - powershell\n    - version manager\n    - jabba\nexcerpt: \"Here's another dev thing I use: Jabba, a cross-platform Java version manager that works for Windows.\"\nlayout: post\nauthorId: david_wesst\n---\n\n[1]: https://davidwesst.blob.core.windows.net/blog/jabba/jabba-example.gif \"Jabba in Action in a Powershell terminal\"\n\nHere's another dev thing I use: [Jabba](https://github.com/shyiko/jabba), a cross-platform Java version manager that works for Windows.\n\nOver the the past few years, I've been doing JDK-based development in an enterprise Windows environment. In that time, I've continually struggled with being able to easily switch between versions of Java on my machine, depending on the project. We have legacy application that run old Java, and modern applications that run newer versions of Java. Being able to switch versions, without having to manually change my environment variables or handle mula\n\nNow, that is no longer a problem thanks to my good friend, Jabba.\n\n![Jabba in Action][1]\n\n## What does it do?\nExactly what you think: it changes the version of Java you're running on the fly. No need to install anything or worry about conflicting versions, or searching out and installing the specific Java version you need for your project.\n\n## Using Jabba\nFirst thing is installing Jabba, which is a breeze thanks to the following the [instructions provided](https://github.com/shyiko/jabba#windows-10) in the repository README. After that, I included it in my Powershell profile so it initializes it when I start Powershell.\n\n```\n# Jabba\nif (Test-Path \"H:\\Users\\dw\\.jabba\\jabba.ps1\") \n{ \n   . \"${HOME}\\.jabba\\jabba.ps1\" \n}\n```\n\nTo test it out, I run a `refreshenv` command in the Powershell window, and run `jabba -h` to see if I get the help file.\nThe commands are pretty straightforward. You can list all the available versions, using `jabba ls-remote`, install the one(s) you need need with `jabba install my-version` and you're good to go to run that version of Java.\n\n```\nH:\\src\n> jabba\nJava Version Manager (https://github.com/shyiko/jabba).\n\nUsage:\n  jabba [flags]\n  jabba [command]\n\nAvailable Commands:\n  install     Download and install JDK\n  uninstall   Uninstall JDK\n  link        Resolve or update a link\n  unlink      Delete a link\n  use         Modify PATH & JAVA_HOME to use specific JDK\n  current     Display currently 'use'ed version\n  ls          List installed versions\n  ls-remote   List remote versions available for install\n  deactivate  Undo effects of `jabba` on current shell\n  alias       Resolve or update an alias\n  unalias     Delete an alias\n  which       Display path to installed JDK\n\nFlags:\n      --version   version of jabba\n\nUse \"jabba [command] --help\" for more information about a command.\n```\n\nAnd now you're good to go with whatever version of Java your current command line needs.\n\n### Won't this conflict with my installed version of Java?\nNope. The [FAQ](https://github.com/shyiko/jabba#faq) section of the README covers that.\n\nIn my case, I uninstalled all the different JDK's I had installed to ensure there were no conflicts, and I like to remove tools I'm no longer needing on my machine.\n\n### But what about my really old legacy JDK on my machine?\nIf you check the [usage](https://github.com/shyiko/jabba#usage) section of the README, you can use Jabba to install JDKs that are hosted in a custom spot.\n\nIn the case of the depracated versions of Java that are difficult to come by, I use the Zulu or OpenJDK versions that are available through Jabba. You can see them when you run `jabba ls-remote`. It's not an exact replica of the Oracle JDK, but I haven't hit any issues in my legacy enterprise applications.\n\n## What makes it cool?\nMy prefernce for any dev tool is to have it available through the command line.\n\nWhen it comes to Java and Windows, the command line tools out there for Java are a bit limited. The standard answer seems to be to use Powershell to update your environment variables, but that doesn't solve the need to find and install the version I need.\n\nJabba solves that for me.\n\nPlus, since it's written in Go, it works on OSX and Linux, so anyone can use the tool.\n\nAnd just to put some more icing on the cake, the solo developer building Jabba was kind enough to implement [a feature I supported](https://github.com/shyiko/jabba/issues/67#issuecomment-300869749) on over a weekend which made the tool work even better for me at work and at home. So, thank you [Stanley Shyiko](https://github.com/shyiko).\n\n## What are the drawbacks?\nI haven't hit any so far, which is pretty impressive considering I use this tools almost every day.\n\n---\n\nIf you're an enterprise Java developer that needs to support legacy applications, I would strongly suggest taking a look at Jabba. With [Windows 7 extended support ending](https://support.microsoft.com/en-ca/help/13853/windows-lifecycle-fact-sheet) in the next few years, your enterprise will be looking to move you to a new OS, Windows 10 or otherwise. \n\nWith Jabba, you'll at least have a tool that works regardless of how your development machine changes.\n\n","categories":[{"name":"java","slug":"java","permalink":"https://westerndevs.com/categories/java/"}],"tags":[{"name":"powershell","slug":"powershell","permalink":"https://westerndevs.com/tags/powershell/"},{"name":"version manager","slug":"version-manager","permalink":"https://westerndevs.com/tags/version-manager/"},{"name":"java","slug":"java","permalink":"https://westerndevs.com/tags/java/"},{"name":"jabba","slug":"jabba","permalink":"https://westerndevs.com/tags/jabba/"}]},{"title":"Developer Health","slug":"Developer-Health","date":"2017-08-15 21:02:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Developer-Health/","link":"","permalink":"https://westerndevs.com/podcasts/Developer-Health/","excerpt":"The Western Devs aren't getting any younger. We take time out of our exercise routines to talk about keeping healthy","raw":"---\nlayout: podcast\ntitle: Developer Health\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-developer-health.mp3\n  length: '48:40'\n  filesize: 45838670\n  libsynId: 5646339\n  anchorFmId: Developer-Health-evqdj8\nparticipants:\n  - kyle_baley\n  - simon_timms\n  - dave_white\n  - darcy_lussier\n  - david_wesst\n  - justin_self\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Fitbit|https://www.fitbit.com/\"\n  - \"Weight Watchers|https://www.weightwatchers.com\"\n  - \"Strong lifts plan|https://stronglifts.com/\"\n  - \"Microsoft Band|\"\n  - \"Pokemon Go|https://www.microsoft.com/microsoft-band/en-us\"\n  - \"Map My Run|http://www.mapmyrun.com/\"\n  - \"Withings scale|https://www.withings.com\"\n  - \"Paleo diet|https://en.wikipedia.org/wiki/Paleolithic_diet\"\n  - \"Ketogenic diet|https://en.wikipedia.org/wiki/Ketogenic_diet\"\ndate: 2017-08-15 17:02:36\nrecorded: 2016-09-22\nexcerpt: The Western Devs aren't getting any younger. We take time out of our exercise routines to talk about keeping healthy\n---\n\n### Synopsis\n\n* Dealing with \"you have to do this for the rest of your  life\"\n* Evaluating consumption\n* Cross-fit is ~~fun~~ a thing\n* Maintaining motivation\n* Creating measurable and appropriate goals\n* Combining exercise with work\n* Weight vs. body fat\n* Tweaking the metrics\n* Importance of having a plan\n* The motivation of being yelled at\n* Find a community\n* Outsourcing your health plan\n* Keeping it simple and creating habits\n* Using technology\n* Measuring regularly\n* The challenge of eating properly when you work from home\n* Getting your kids involved\n* The psychology of hunger\n* Activities performed by Western Devs in the making of this episode: running, cross-fit, hockey, boxing, weights, tennis, devopsing, microservicing, and competitive breathing.\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"NVS, the Node Version Manger for Everyone","authorId":"david_wesst","slug":"nvs","date":"2017-08-15 16:35:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"javascript/nvs/","link":"","permalink":"https://westerndevs.com/javascript/nvs/","excerpt":"Here's another dev thing I use: NVS, or the Node Version Switcher. It works on Windows and it's great.","raw":"---\ntitle: NVS, the Node Version Manger for Everyone\ndate: 2017-08-15 12:35:00\ncategory:\n    - javascript\ntags:\n    - javascript\n    - node\n    - powershell\n    - nvs\n    - version manager\nexcerpt: \"Here's another dev thing I use: NVS, or the Node Version Switcher. It works on Windows and it's great.\"\nauthorId: david_wesst\nlayout: post\n---\n\n[1]: https://davidwesst.blob.core.windows.net/blog/nvs/nvs-menu.gif \"NVS Consle Menu in Action\"\n\nHere's another dev thing I use: [NVS](https://github.com/jasongin/nvs), a cross-platform Node version manager that works for Windows.\n\nIt's no secret that I like JavaScript, which includes [Node](nodejs.org/). The history of Node releases has been fast, furious, and [somewhat turbulent](https://stackoverflow.com/questions/27309412/what-is-the-difference-between-node-js-and-io-js) which led to a lot of different versions of Node being released. Manually managing the version of Node in you development enviornment is painful, just like it is for Java. For the Linux and Unix people, there was [nvm](https://github.com/creationix/nvm) and [n](https://github.com/tj/n), but nothing really comparable for Windows.\n\nUntil NVS that is.\n\n## What does it do?\nThe Node Version Switcher switches versions of Node in environment. So, if you need to jump from 4.8.4 to 6.11.1, no problem. Just a quick `nvs add 6.11.1` and `nvs use 6.11.1` and you're ready to go.\n\nNo downloading binaries. No changing environment variables.\n\n## Using NVS\nAlthough supported on [OSX and Linux](https://github.com/jasongin/nvs#mac-linux), I'm going to focus on Windows as that is the environment where I use it the most.\n\nYou have two different installtion options on Windows, the first being a traditional installer file that you can download from [the release page](https://github.com/jasongin/nvs/releases) for the project.\n\nThe second is using [Powershell](https://github.com/jasongin/nvs/blob/master/doc/SETUP.md#manual-setup---powershell) or the good old fashioned [command line](https://github.com/jasongin/nvs/blob/master/doc/SETUP.md#manual-setup---command-prompt), both of which are described on the [setup page](https://github.com/jasongin/nvs/blob/master/doc/SETUP.md) for the project.\n\nNVS even supported Bash for Windows, which is pretty great for those Linux-y Windows people, although it requires a few manual configuration steps.\n\nOnce you get things installed, you can run `nvs` and go through the interactive menu goodness to get your favourite flavour of Node installed.\n\n![NVS Interactive Console Menu][1]\n\nI really dig this interactive command line menu, which was created by the NVS author for NVS, and eventually turned into it's own library called [console-menu](https://github.com/jasongin/console-menu). But that is post for another time.\n\n### Won't this conflict with my installed version of Node?\nNot from my experience.\n\nWhen I started with NVS, I had a version of Node installed, but I ended up uninstalling just to simplify my development environment. I kept forgetting that I had a base installation of Node installed. This confusion would result in me running `node --version` only to get a conflicting version number than the one I would see in my Windows Application list, and get me triaging an issue that didn't exist.\n\nPlus, NVS provides a feature to set a default version of using the `nvs link` command.\n\n### What about the global packages I've installed?\nIn the case where you install a package globally using a certain version of Node, when you switch to a different version, you could face some problems. More specifically, around code features available in Node based on what version of Node you've installed, or around the nested dependencies that get installed as part of a package your project needs.\n\nNot an issue, as NVS provides the `migrate` command to move packages, global or otherwise from one version of Node to another.\n\n### But what about my weird, custom Node versions?\nNo issue, because you can point to whatever directory you want as source for Node binaries using the [aliasing capabilities](https://github.com/jasongin/nvs/blob/master/doc/ALIAS.md#aliasing-directories).\n\n```\nC:\\Users\\dw\\src\\_scratch\n> node --version\nv8.1.4\n\nC:\\Users\\dw\\src\\_scratch\n> nvs use chakracore/8.1.2\nPATH -= $env:LOCALAPPDATA\\nvs\\chakracore\\8.1.4\\x64\nPATH += $env:LOCALAPPDATA\\nvs\\chakracore\\8.1.2\\x64\n\nC:\\Users\\dw\\src\\_scratch\n> node --version\nv8.1.2\n```\n\n## What makes it cool?\nIt's cross-platform, so that's pretty awesome. But, there's more stuff that I haven't touched on in this post.\n\nFor example, there is bundled integration with [Visual Studio Code](https://github.com/jasongin/nvs#vs-code-support) which is a huge cross-platform bonus for me. VS Code is my editor of choice, and considering that it too is cross-platform, this is pretty great.\n\nOther coolness to note would be things like [aliasing](https://github.com/jasongin/nvs#aliases) and [automatic directory swtiching]()(https://github.com/jasongin/nvs#automatic-switching-per-directory). Not to mention that it supports [ChrakraCore](https://github.com/nodejs/node-chakracore), making it easy to turn it on for all your [Time Travel Debugging](https://github.com/nodejs/node-chakracore#time-travel-debugging) needs.\n\nAll of that is icing on a delicious dev tool cake.\n\n## What are the drawbacks?\nHonestly, I'm not sure. I haven't found any so far, so that counts for something.\n\n---\n\nAt the end of the day NVS does the job, and it does the job well. Plus, it comes with a bunch of cool extras that can make your Node development experience even more smooth.\n","categories":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"node","slug":"node","permalink":"https://westerndevs.com/tags/node/"},{"name":"powershell","slug":"powershell","permalink":"https://westerndevs.com/tags/powershell/"},{"name":"nvs","slug":"nvs","permalink":"https://westerndevs.com/tags/nvs/"},{"name":"version manager","slug":"version-manager","permalink":"https://westerndevs.com/tags/version-manager/"}]},{"title":"IstanbulJS Code Coverage Reports in VSTS","authorId":"david_wesst","slug":"istanbuljs-in-vsts","date":"2017-08-03 16:10:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/istanbuljs-in-vsts/","link":"","permalink":"https://westerndevs.com/development/istanbuljs-in-vsts/","excerpt":"Here's another dev thing I use: IstanbulJS in Visual Studio Team Services (VSTS) builds and display the test reports as part of the build reports.","raw":"---\ntitle: IstanbulJS Code Coverage Reports in VSTS\ncategory:\n    - development\ntags:\n    - javascript\n    - visual studio team services\n    - istanbuljs\n    - nyc\n    - testing\nexcerpt: \"Here's another dev thing I use: IstanbulJS in Visual Studio Team Services (VSTS) builds and display the test reports as part of the build reports.\"\nlayout: post\ndate: 2017-08-03 12:10:00\nauthorId: david_wesst\n---\n\n[1]: https://istanbul.js.org/\n[2]: https://www.visualstudio.com/team-services/continuous-integration/\n[3]: https://davidwesst.blob.core.windows.net/blog/istanbuljs-in-vsts/vsts-code-coverage-report.gif \"VSTS Build Report with an IstanbulJS code coverage report\"\n[4]: https://github.com/istanbuljs/nyc\n[5]: https://mochajs.org/\n[6]: http://www.westerndevs.com/development/mocha-in-vsts/\n[7]: https://docs.npmjs.com/misc/scripts\n[8]: https://github.com/istanbuljs/nyc#configuring-nyc\n[9]: http://anthonychu.ca/post/css-styles-vsts-code-coverage/\n[10]: https://www.npmjs.com/package/inline-css\n\nHere's another dev thing I use: [IstanbulJS][1] in [Visual Studio Team Services][2] (VSTS) builds and display the test reports as part of the build reports. When a build completes, I get a report like this one.\n\n![VSTS Build Report with an IstanbulJS code coverage report][3]\n\nI can browse the report right in the build report, and drill into the results for each file.\n\nThis is how I did it.\n\n## Step 0: Assumptions\nI'm not going to go into the details on how to setup IstanbulJS or a test suite, but you'll need a project with tests and uses the IstanbulJS command line tool, [NYC][4], to run them. My suggestion is to use [Mocha][5] as [the test report can be integreated into VSTS as well][6].\n\nYou'll also need a VSTS account, which is free and worth the effort.\n\n## Step 1: Script Your Command\nThe goal here is to be able to run a script command that will execute the appropriate code coverage command, complete with parameters, easily. I use [npm scripts][7] for this tutorial, but you can use whatever scripting tool you'd like.\n\nIn my case, I like to run the code coverage report everytime I run my Mocha tests. So, I've updated my `npm test` command in _package.json_ to use NYC. It looks like:\n\n```\n  \"scripts\": {\n    \"test\": \"./node_modules/.bin/nyc ./node_modules/.bin/mocha --recursive --reporter=mocha-multi-reporters \"\n  }\n```\n\nNote, that I don't use globally installed packages. I only use the local ones installed in my _node\\_modules_ folder.\n\n## Step 2: Configuring NYC\nI've configured it in the _package.json_ file with an `\"nyc\"` configuration object. Mine looks like this:\n\n```\n  \"nyc\": {\n    \"check-coverage\": true,\n    \"per-file\": true,\n    \"lines\": 99,\n    \"statements\": 99,\n    \"functions\": 99,\n    \"branches\": 99,\n    \"include\": [\n      \"src/**/*.js\"\n    ],\n    \"reporter\": [\n      \"text\",\n      \"cobertura\",\n      \"html\"\n    ],\n    \"report-dir\": \"./.test_output/coverage\"\n  }\n  ```\n\nThe part of the configuration we care about for this tutorial are the `\"reporter\"` and `\"report-dir\"` properties. The rest of the configuration is out of scope for this post, but you can learn more in the [nyc README configuration section][8].\n\nFor `\"reporters\"`, you can see that we are using three different reporters. The _text_ reporter is the one that displays in the terminal, the _cobertura_ reporter generates an XML file with all of the results which we'll need, and the _html_ reporter generates the HTML files you saw me browsing at the beginning of this post.\n\nAt this point, when we run `npm test` we get run our tests and generate the code coverage assets we want.\n\n## Step 3: Post-Processing the HTML Report\nThis one isn't obvious, but I'm going to save you the trouble of discovering it for yourself.\n\nThat being said, if you don't mind the plain text reports sans-CSS, you can skip this step altogether.\n\nOur HTML report is going to get displayed in VSTS. Remember, the HTML report isn't just a single file, it's a bunch of HTML files complete with CSS for styling. VSTS doesn't natively load up the extra CSS files, which means we'll need to embed the CSS right into the files themselves to create a copy of the report that'll look good in VSTS.\n\nThanks to [this post from Anthony Chu][9], I had a headstart on figuring out how to solve this issue. The plan is to run a post-processing script on the _posttest_ script command in npm. I called my script file _process-coverage-report.js_ and updated the scripts section of my _package.json_ to look like this:\n\n```\n\"scripts\": {\n  \"test\": \"./node_modules/.bin/nyc ./node_modules/.bin/mocha --recursive --reporter=mocha-multi-reporters \",\n  \"posttest\": \"node ./tools/process-coverage-report.js\"\n},\n```\n\nThe _posttest_ script will everytime we run `npm test`. You can also call it directly by running `npm run posttest` but be sure to have test results for it to process.\n\nI'll cut to the case, and just show you my post-processing code.\n\n```\nlet fs = require(\"fs\"),\n    path = require(\"path\"),\n    inline = require(\"inline-css\");\n\nconst TEST_RESULTS_DIRECTORY = \"./.test_output\";\nconst CODE_COVERAGE_DIRECTORY = \"./.test_output/coverage\";\n\nfs.readdir(CODE_COVERAGE_DIRECTORY, (err, files)=> {\n    if(err) { throw new Error(err); }\n\n    let reports = files.filter((report)=> {\n        return report.endsWith(\".html\");\n    });\n\n    reports.forEach((report)=> {\n        let filePath = path.join(CODE_COVERAGE_DIRECTORY, report);\n        let options = { \n            url: \"file://\" + path.resolve(filePath),\n            extraCss: \".pad1 { padding: 0; }\"\n        };\n\n        fs.readFile(path.resolve(filePath), (err, data)=> {\n            inline(data, options)\n                .then((html)=> {\n                    let outputFile = path.join(TEST_RESULTS_DIRECTORY, report);\n                    fs.writeFile(outputFile, html, (err)=> {\n                        if(err) { throw err; }\n                    });\n                })\n                .catch((err)=> {\n                    console.log(err);\n                });\n        });\n    });\n});\n```\n\nMy build doesn't use a task runner like Gulp I settled on [inline-css][10] because I liked the API and it returned promises. If you're using Gulp or Grunt, there are some good options ([as suggested by Anthony][10]) for you to create a task to do this for you.\n\nNow, when you run `npm test` you'll end up with an extra copy of the HTML report, where you have nothing but HTML files with CSS embedded in the files themselves.\n\n## Step 4: Adding this to VSTS\nAll you need to do here, is configure your build to use the new code coverage setup. We do that by adding the _Publish Code Coverage Results_ task as a build step and configuring properly. Here's what my configuration looks like:\n\n* Version:  1.*\n* Display Name: Publish Code Coverage Results \\| NYC\n* Code Coverage Tool: Cobertura\n* Summary File: $(System.DefaultWorkingDirectory)/.test_output/coverage/cobertura-coverage.xml\n* Report Directory: $(System.DefaultWorkingDirectory)/.test_output     |\n\nYour properties may vary, depending on how to configured NYC.\n\n## Step 5: Done\nAnd now we code coverage reporting showing up in VSTS. Huzzah!\n\nHappy code covering!","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"testing","slug":"testing","permalink":"https://westerndevs.com/tags/testing/"},{"name":"istanbuljs","slug":"istanbuljs","permalink":"https://westerndevs.com/tags/istanbuljs/"},{"name":"nyc","slug":"nyc","permalink":"https://westerndevs.com/tags/nyc/"}]},{"title":"Mocha Test Reports in VSTS","authorId":"david_wesst","slug":"mocha-in-vsts","date":"2017-08-01 15:19:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/mocha-in-vsts/","link":"","permalink":"https://westerndevs.com/development/mocha-in-vsts/","excerpt":"Here's another dev thing I do: Display my MochaJS test report in the Visual Studio Team Services (VSTS) build report.","raw":"---\ntitle: Mocha Test Reports in VSTS\ncategory:\n    - development\ntags:\n    - javascript\n    - visual studio team services\n    - mocha\n    - testing\nexcerpt: \"Here's another dev thing I do: Display my MochaJS test report in the Visual Studio Team Services (VSTS) build report.\"\nlayout: post\ndate: 2017-08-01 11:19:00\nauthorId: david_wesst\n---\n\n[1]: https://mochajs.org/\n[2]: https://www.visualstudio.com/team-services/continuous-integration/\n[3]: https://davidwesst.blob.core.windows.net/blog/mocha-in-vsts/vsts-test-results-in-action.gif \"Screenshot of a MochaJS test report in VSTS\"\n[4]: https://www.visualstudio.com/team-services/\n[5]: https://docs.npmjs.com/misc/scripts\n[6]: https://github.com/michaelleeallen/mocha-junit-reporter\n[7]: https://github.com/stanleyhlng/mocha-multi-reporters\n[8]: https://github.com/glenjamin/mocha-multi\n[9]: https://davidwesst.blob.core.windows.net/blog/mocha-in-vsts/vsts-npm-test-task.png \"Screenshot of VSTS Build Task that runs the tests\"\n[10]: https://davidwesst.blob.core.windows.net/blog/mocha-in-vsts/vsts-publish-test-results.png \"Screenshot of VSTS Build Task that consumes the test report\"\n[11]: https://github.com/stanleyhlng/mocha-multi-reporters/issues/35 \"My bug report for the XUnit issue in mocha-multi-reporters\"\n\nHere's another dev thing I use: [MochaJS][1] in [Visual Studio Team Services (VSTS) builds][2] and display the test reports as part of the build reports. See? Like this.\n\n![Mocha Test Report in VSTS][3]\n\nThis wasn't me trying to fit a square peg into a round hole. VSTS is exceptionally flexible and it comes bundled with all the pieces you need to do this out of the box. The key is making sure that we setup our test runner to produce the output VSTS needs.\n\n## Step 0: Assumptions\nI'm not going to explain how to do this, but I'm going to assume you have a project with tests that use [MochaJS][1]. So, you can run `mocha` from the terminal and your tests run.\n\nI'm also not going to explain that to use VSTS, you need a VSTS account. They are [free to start][4] and you'll need one to make this work. It's worth the effort.\n\n## Step 1: Script Your Test Command\nPersonally, I use [npm scripts][5] for this. I just figure out what my test command is, and then have the `npm test` script run that. In my project, I run the locally installed version of MochaJS and use the `recursive` flag.\n\n```\n./node_modules/.bin/mocha --recursive\n```\n\nIn my _package.json_ file, I have:\n\n```\n\"scripts\": {\n    \"test\": \"./node_modules/.bin/mocha --recursive\"\n}\n```\n\nYou can just as easily use a Bash or Powershell script for this too if you're not a fan of npm scripts. But you should be.\n\n## Step 2: Use the mocha-junit-reporter\nWoah, wait a minute? This is JavaScript not _Java_.\n\nI know, but JUnit reports are a standard report format that is supported by VSTS. The key is making sure that our mocha test reports are being output into a format that VSTS can understand. VSTS does not care about your test report to standard out.\n\nMocha doesn't come bundled with a JUnit reporter, so I used [mocha-junit-reporter][6] which outputs a _test-results.xml_ file to the root project directory by default. I don't like the default, so I have it output to a directory of my choosing.\n\nSo, first we run: `npm install --save-dev mocha-junit-reporter`\n\nThen, we update our `npm test` command in _package.json_ to\n\n```\n\"scripts\": {\n    \"test\": \"./node_modules/.bin/mocha --recursive --reporter mocha-unit-reporter --reporter-options mochaFile=./test-output/test-results.xml\"\n}\n```\n\nDon't forget to add test output directory to your _.gitignore_ file.\n\n### (OPTIONAL) Step 2a: Using Mutliple Reporters\nBut now I can't see my tests in the terminal output!\n\nI didn't like that either, so I fixed it by using another Mocha extension called [mocha-multi-reporters][7]. It lets us define mutlple reporters for MochaJS and specify reporter parameters in a _config.json_ file that we save in the project root.\n\nFirst, install the tool: `npm install --save-dev mocha-multi-reporters`.\n\nThen, we update our `npm test` command to\n\n```\n\"scripts\": {\n    \"test\": \"./node_modules/.bin/mocha --recursive --reporter=mocha-multi-reporters\"\n}\n```\n\nAnd finally, we add a _config.json_ file to the project root. I'm using the spec and mocha-junit-reporter, which result in this _config.json_:\n\n```\n{\n    \"reporterEnabled\": \"spec,mocha-junit-reporter\",\n    \"mochaJunitReporterReporterOptions\": {\n        \"mochaFile\": \"./.test_output/test-results.xml\"\n    }\n}\n```\n\nIt's not perfect, but it works well enough for my purposes.\n\n#### Gotcha! You should use Mocha-Multi\nI tried using [mocha-multi][8] and I couldn't get it to work with the parameter for [mocha-junit-reporter][6].\n\n#### Gotcha! This produces an xunit.xml file in the root directory\nIt's a [bug with mocha-multi-reporters][11] that I've reported. I'm hoping to get a pull request in for it soon, as it's a pretty easy fix.\n\nThe workaround is to add _xunit.xml_ to your _.gitignore_ file and ignore it yourself.\n\n## Step 3: Run Test Script in your VSTS Build\nTo run your test script, you need to add a build task  in VSTS. In our case, we're adding the NPM buid task, and configuring it to run our `npm test` command. The build task properties I use are:\n\n- Version: 1.*\n- Display Name: npm test\n- Command: custom\n- Command and arguments: test\n\nHere's what it looks like:\n\n![Screenshot of npm build step][9]\n\n## Step 4: Publish the Test Results in VSTS\nFor our last step, we need to publish the test results report to VSTS and tell it how to read it.\n\nWe do this with the Publish Test Results build step in VSTS and configure it with the following properties.\n\n- Version: 2.*\n- Display name: Publish Test Results \\| Mocha\n- Test result format: JUnit\n- Search folder: $(System.DefaultWorkingDirectory)\n\nWhich looks something like this:\n\n![VSTS Publish Test Results Build Step][10]\n\n## Step 5: Done\nAnd with that, you're good to go to capture and explore your MochaJS test results from within your VSTS build report.\n\nHappy test reporting!","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"mocha","slug":"mocha","permalink":"https://westerndevs.com/tags/mocha/"},{"name":"testing","slug":"testing","permalink":"https://westerndevs.com/tags/testing/"}]},{"title":"Scrum with Kanban Class of Service","authorId":"dave_white","slug":"Kanban_Helps_Scrum_with_Emergent_Work","date":"2017-07-14 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Kanban/Kanban_Helps_Scrum_with_Emergent_Work/","link":"","permalink":"https://westerndevs.com/Kanban/Kanban_Helps_Scrum_with_Emergent_Work/","excerpt":"Kanban's concept of Class of Service helps Scrum teams deal with emergent work","raw":"---\nlayout: post\ntitle: Scrum with Kanban Class of Service\ncategories:\n  - Kanban\ntags:\n  - kanban\n  - agile\n  - scrum\n  - myths\n  - class of service\ndate: 2017-07-14 12:00:00\nexcerpt: Kanban's concept of Class of Service helps Scrum teams deal with emergent work\nauthorId: dave_white\n---\nInspired by [Steve Porter's][1] efforts to bring process practitioners closer together and educate Scrum practitioners, I'm writing a shadow series of posts that will follow the [Kanban and Scrum - Stronger Together][2] series and continue [my own efforts][5] to clear up misconceptions between practitioners of these methods.\n\n[In my last post][3], I discussed how a Scrum team could add the concepts of WIP limits to their process and derive measurable delivery performance benefits. In this post, I'm hoping to show how we can use Kanban's concept of Class of Service to help Scrum teams deal with emergent work.\n\n## Emergent/Unplanned Work\n\nOne of the things that is very natural in Kanban but less so in Scrum implementations is the ability to deal with emergent or unplanned work during a Sprint. In this case, I am not talking about the anticipated, natural growth of a User Story or PBI as more information is discovered about it. I am talking about un-anticipated work such as emergency issues, newly discovered high-value opportunities, or newly understood schedule risks. \n\nI want to be clear that Scrum does not mandate that work cannot be introduced into the Sprint. The Scrum Guide allows for a negotiation to occur that would allow work to be introduce into the Sprint if there is capacity available, either by removing an existing PBI or by understanding that there is more room in the sprint than anticipated. In my experience, this does not come naturally to Scrum teams. Years of defending the Sprint backlog make this thinking hard to change.\n\nBut emergent and unplanned work are a reality for many knowledge work teams, especially in our DevOps world where increments of work are discovered, triaged, implemented, and deployed daily, if not many times per day. And as we will learn, this work can be easily tracked, analyzed, and we may be able to anticipate it.\n\n## A brief Introduction to Class of Service\n\nBefore I get too deep, I wanted to present a description of Class of Service. \n\n> ### Description \n> A Class of Service is simply a policy that a team will explicitly create in order to guide team behaviour in scheduling and delivering an increment of work. They may take the form of rules for prioritization, swarming, or delaying the uptake of work by the team. There are 4 classic policy examples (Standard, Expedite, Due Date, Intangible) but you can create more or less. It is important to remember though that they are simply a policy which can be discussed, altered, or abandoned.\n\n## Our First Class of Service\n\nAs a Scrum team, it is very easy to add your first class of service policy without any significant change to your Scrum practices. We will focus on a very common class of service policy, the **Expedite** class of service. This policy indicates that we have discovered work where the risk of delaying it is higher than we are comfortable with and we need to start a negotiation with the Product Owner about altering the Sprint Plan. We may also have to swarm on the work as a team because the impact of delay is severe. Instances of this type of emergent work happen often in DevOps scenarios where the development team is also responsible for problems that occur in the system in production. \n\nOur goal with trying out this minimum viable change to our Scrum process is to make expedite work very visible, make it very explicit, and try to understand how this risk mitigation policy is impacting our Sprints.\n\n> **Note** - By creating an expedite Class of Service policy, we have actually created 2 Class of Service policies. Standard work (everything that is not expedited) and Expedite. Standard work is just _normal_ so we won't really discuss it further.\n\n## The Expedite Lane\n\nKeeping in mind our goal with this experiment, the first thing we want to do is simply make the Expedite policy and work visible, and we can easily do this with a simple change to our board. I will continue using our example board from the [previous blog post][3].\n\nThere are many ways that we can indicate that a work needs to be expedited. We could annotate a card, change its color, or put it in a special place on our board. To keep this simple and in the spirit of using boards, I'm going to suggest that we create a location on our board to indicate that we have an expedite policy and what work is currently affected by that policy.\n\n![Scrum team's kanban board with Expedite policy depicted as a lane][6]\n\nWhat you see in the image is a visualization of our policy that indicates:\n\n- we have an explicit Expedite policy\n- our policy is that we only work on one expedite item at a time\n  - the swimlane has a WIP limit of 1 PBI\n\nWhat is not clear on my board, but could be made clear with a Definition of Done (DoD) or similar annotations on the board, is \n\n  - expedited work is top priority\n  - teams may be required to swarm to get that work done, which pauses all normally PBIs\n  - work in the Expedite lane does not obey column WIP limits\n\nWe do not need to alter our cards in any way. Their presence in the lane indicates that the policy is being applied.\n\nOne additional change I would ask a team to make is that once a card enters the expedite lane and is now managed by the policy, mark the card/PBI with some sort of indicator that it was expedited. \n\nAnd that is it! We haven't changed how items get into that lane. We will still perform the negotiation with the Product Owner. We probably haven't changed our behaviour around emergent work, but we have made it clear that it is a reality for us and when something is in that lane, we are probably swarming on it if necessary to get it fixed before everything else.\n\n### So Why Do this?\n\nI would suggest using this technique to a team as a possible solution to a few problems.\n\n1) There is not clear or commonly understood policy\n2) There is a problem understanding/communicating when there is emergency work\n    - in the team or external to the team\n3) There is a problem understanding how much emergency work there is\n4) There is a problem understanding how emergency work impacts our Sprint\n5) There is a problem with the team not being designed to handle this kind of work\n\nBy understanding the problem that prompted the team to adopt this practice, we will understand if the practice is working and/or adding value and make adjustments as necessary.\n\n## Early Benefits\n\nNow that we have an explicit policy and we are tracking our expedited work, we can reflect on that work. \n\nWe can look at how that work flows in conjunction with our normal PBIs and try to determine how one affects the other. Emergency work tends to be disruptive: we put down everything else, re-plan, negotiate, etc. All of these things add to the amount of time it takes for a team to deliver.  \n\nWith that information, we could re-design our work flow to minimize the impact of expedites on the PBIs in the original Sprint Plan. We could have a conversation with a sponsor to offload that type of work onto another team. Many opportunities, to design a system that satisfy all the types of work we need to do, can be explored with the information we know have because we are explicitly tracking emergent work.\n\nAnd we may discover that while this work is not planned, it can be anticipated. And in anticipating it, we can more effectively deliver all of our work.\n\n## What's Next\n\nThose are just some of the opportunities that you have! I'm not suggesting you can't find those opportunities elsewhere, but you shouldn't be afraid to approach kanban! It will happily support the way you want to work today and help you continue your growth into the future!\n\nBy this point in the series, I hope that I've encouraged you to learn more about Kanban. And the best place to learn more is a certified training class from a LeanKanban.com Accredited Kanban Trainer. [You can find more about the recommended first class, Kanban System Design, here.][4]\n\nIn our next post, we will discuss how a Scrum team could enhance their understanding of how long it takes them to deliver a PBI by calculating a Lead Time!\n\n\n[1]: https://www.scrum.org/user/119\n[2]: https://www.scrum.org/resources/blog/scrum-and-kanban-stronger-together\n[3]: http://www.westerndevs.com/Kanban/Scrum_with_WIP_Limits/\n[4]: http://leankanban.com/kmp-i/\n[5]: https://agileramblings.com/2013/03/10/the-difference-between-the-kanban-method-and-scrum/\n[6]: https://i.imgur.com/WUKSbXn.png \"Basic Scrum board with Expedite Lane\"\n","categories":[{"name":"Kanban","slug":"Kanban","permalink":"https://westerndevs.com/categories/Kanban/"}],"tags":[{"name":"kanban","slug":"kanban","permalink":"https://westerndevs.com/tags/kanban/"},{"name":"agile","slug":"agile","permalink":"https://westerndevs.com/tags/agile/"},{"name":"scrum","slug":"scrum","permalink":"https://westerndevs.com/tags/scrum/"},{"name":"myths","slug":"myths","permalink":"https://westerndevs.com/tags/myths/"},{"name":"class of service","slug":"class-of-service","permalink":"https://westerndevs.com/tags/class-of-service/"}]},{"title":"A review of Scrum for Kanban Teams","authorId":"dave_white","slug":"Scrum_for_Kanban_Teams","date":"2017-07-14 16:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Kanban/Scrum_for_Kanban_Teams/","link":"","permalink":"https://westerndevs.com/Kanban/Scrum_for_Kanban_Teams/","excerpt":"A review of a Scrum Primer for Kanban teams","raw":"---\nlayout: post\ntitle: A review of Scrum for Kanban Teams\ncategories:\n  - Kanban\ntags:\n  - kanban\n  - agile\n  - scrum\n  - myths\ndate: 2017-07-14 12:00:00\nexcerpt: A review of a Scrum Primer for Kanban teams \nauthorId: dave_white\n---\nInspired by [Steve Porter's][1] efforts to bring process practitioners closer together and educate Scrum practitioners, I'm writing a shadow series of posts that will follow the [Kanban and Scrum - Stronger Together][2] series and continue [my own efforts][5] to clear up misconceptions between practitioners of these methods.\n\n[In the most recent post in Steve Porter's series][3], [Yuval Yuret][6] presents Scrum in a manner that is intended to educate Kanban teams.\n\n**Disclaimer**\n\nFirst off, I have to say that I'm not 100% certain how I feel about [Yuval's blog post][3]. It feels like a forced comparison of apples and oranges. But in critically reading it, it forces me to think about why I feel that way. And as always, these are all just my opinions. No harm in sharing them, right? \n\n## TL;DR\n\nIn case you haven't read Yuval's post, basically it presents a map of values and practices in Scrum to Kanban language, and encourages Kanban teams to approach Scrum from a practices point of view. It also encourages everyone to review/adopt the values (in Scrum language) that can help software development teams succeed in building software. [You should go read it now.][3] :D\n\n## Values\n\nAs [I wrote in 2013][5], Scrum and Kanban both share a use of values to encourage users of the methods to behave a certain way. The explicit inclusion of the Scrum Values is a relatively recent ([2016 Scrum Guide][7]) addition, but the [Agile Manifesto][8] is definitely a value system and Scrum fully supports those values.\n\nThe Kanban Method also has principles that have been included since its formation. The presentation of these principles have been refined and one addition was made for clarity. And recently, a significant amount of work has been made to further evolve our understanding of the principles and turn them into a description of more concrete values. [Andy Carmchael][10] and [David J Anderson][11] have created a **free** [Essential Kanban Condensed][9] eBook that lays out the value system for the Kanban Method on Page 3, and [Mike Burrows][12] has written a fantastic book [Kanban from the Inside][13] that discusses the values of Kanban in great detail.\n\nI think it is great that Yuval is including the values mappings in his primer, but I think that some of the mappings he has created reinforce my apples and oranges feelings. He doesn't compare the current state of the art in Kanban values and maps some kanban practices to Scrum values.\n\nI would encourage you to quickly read the values section (3 minutes) of the [Essential Kanban Condensed][9] eBook starting on Page 3 and judge for yourself how the values comparison feels to you.\n\nAnd, as hard as this is to say because I think the values are important, the Scrum guide seems to specify an intent as opposed to a way to think. Values shouldn't be expressed as goals like they are in the Scrum Guide. And maybe that isn't how Scrum is taught. I haven't been to a modern PSM class.  \n\n## Roles\n\nI'm glad that Yuval presents the Scrum roles. The Kanban Method neither advocates nor condemns any of these roles. It really has no opinion. The Kanban community has discovered that there are specialists who are good at fulfilling useful services when optimizing virtual kanban systems and participating in Kanban implementations. A Service Request Manager is focused on the needs and expectations of the customer. This is comparable to a Product Owner. A Service Delivery Manager is focused on kanban system performance. A Kanban Coach is focused on organizational adoption. \n\nTo be clear, the Kanban Method has no opinion about roles. It guides people to respect everything until you've gained the emotional maturity as an organization to change. The Kanban community has discovered, in practice, that there are roles and responsibilities that should be encouraged to appear and supported as organizations progress down the Kanban path.\n\n## Events\n\nThis is probably the set of things that, regardless of the name, Scrum and Kanban teams will have the most in common. Kanban teams are fully capable of doing everything that Scrum teams do, described as some sort of feedback meetings that happen on a cadence. People on software development teams, regardless of Scrum or Kanban, will goal set, seek feedback, deliver increments of product, and reflect on how they work. If you are a team that is not performing this practices in some form, you should look at either Scrum or Kanban.\n\nOne of the things that is spiritually very different about the Scrum events (as described in the Scrum Guide) and the Kanban feedback meetings is usually that the Scrum events are focusing on what people do during the meeting. Kanban feedback guidance focuses on the work that needs to be done. \n\nAn example of this is the description of the daily stand-up. In the Scrum guide, this is the following description of the daily scrum:\n\n>During the meeting, the Development Team members explain:\n>\n>   - What did I do yesterday that helped the Development Team meet the Sprint Goal?\n>   - What will I do today to help the Development Team meet the Sprint Goal?\n>   - Do I see any impediment that prevents me or the Development Team from meeting the Sprint Goal?\n\nIn the Kanban community, we have The Kanban Meeting, a daily 'stand-up' style meeting whose focus is work items. You can find a brief description of this meeting on page 25 of [Essential Kanban Condensed][9]. And Yuval also makes this point in his mapping.\n\n> Kanban teams typically focus on the flow of work instead of the people doing the work. They work the board right to left focusing on flow problems.\n\nI will state that being prescriptive of what people should do is not necessarily a bad thing. That is one of the things that Kanban has suffered with in that people want prescriptive guidance and in the past, the Kanban community didn't an authoritative source of concrete practices. That has changed dramatically in the last couple years with many initiatives within the Kanban community providing kanban practitioners with examples of concrete practices that could be used prescriptively. [Essential Kanban Condensed][9] provides specific examples. [Enterprise Services Planning][13] or ESP as it is commonly referred as in the Kanban community, is full of specific activities that large organizations adopting Kanban at scale will benefit from implementing and understanding.\n\n## Artifacts\n\nGenerally speaking, Yuval hits this point right on the head. Software development teams, striving to be agile, using Scrum or Kanban, will generally need/produce the same things. They need PBIs/User Stories/Work Items to describe demand. They will produce Features/Functions/Components that are cohesive. They will ship increments of software on a cadence or on demand. \n\nYuval suggested that Kanban teams limit the size of a product backlog, which may be true in some cases, but this guidance is not from The Kanban Method. Now a days, Kanban teams may have an Upstream Kanban System that is full of work items that are being refined, analyzed, discarded, or finally passed on to the development team as a User Story/PBI that needs to be delivered. Smaller teams can do this within their own kanban system.\n\n[Slide 13 in Patrick Steyaert's LKCE2016 presentation][15] shows a good example of the guidance practicing Kanban teams are sharing in the community.\n\n## Conclusions \n\nI think Yuval's options in his conclusion are all viable experiments to try! Kanban teams should never be afraid to take practices from anywhere that they see them. Arguably, kanban actively promotes the constant experimentation of practices to see if they improve the delivery capability of an organization or team.\n\n## My Final Thoughts\n\nWhile I may disagree with some of the details as outlined in this post, I agree with the spirit of what Yuval is suggesting in his article. Kanban teams need to be open minded when looking for practices that may enhance the way that they work. They should not be afraid to look at Scrum as a source of those activities. And I sincerely hope that everyone came away more educated about Scrum AND Kanban having read these articles.\n\n>By this point in the series, I hope that I've encouraged you to learn more about Kanban. And the best place to learn more is a certified training class from a LeanKanban.com Accredited Kanban Trainer. [You can find more about the recommended first class, Kanban System Design, here.][4]\n\n[1]: https://www.scrum.org/user/119\n[2]: https://www.scrum.org/resources/blog/scrum-and-kanban-stronger-together\n[3]: https://www.scrum.org/resources/blog/scrum-primer-kanban-teams\n[4]: http://leankanban.com/kmp-i/\n[5]: https://agileramblings.com/2013/03/10/the-difference-between-the-kanban-method-and-scrum/\n[6]: https://www.linkedin.com/in/yuvalyeret/\n[7]: http://www.scrumguides.org/revisions.html\n[8]: http://agilemanifesto.org/\n[9]: http://leankanban.com/guide/\n[10]: https://www.linkedin.com/in/andycarmichael/\n[11]: https://www.linkedin.com/in/agilemanagement/\n[12]: https://www.linkedin.com/in/asplake/\n[13]: https://www.amazon.ca/Kanban-Inside-Understand-connect-introduce/dp/0985305193/\n[14]: http://leankanban.com/esp/ \n[15]: https://www.slideshare.net/lkce/lkce16-upstream-customer-kanban-by-patrick-steyaert\n\n","categories":[{"name":"Kanban","slug":"Kanban","permalink":"https://westerndevs.com/categories/Kanban/"}],"tags":[{"name":"kanban","slug":"kanban","permalink":"https://westerndevs.com/tags/kanban/"},{"name":"agile","slug":"agile","permalink":"https://westerndevs.com/tags/agile/"},{"name":"scrum","slug":"scrum","permalink":"https://westerndevs.com/tags/scrum/"},{"name":"myths","slug":"myths","permalink":"https://westerndevs.com/tags/myths/"}]},{"title":"Estimations and Mistake Driven Development","authorId":"justin_self","slug":"Estimations_And_Mistake_Driven_Development","date":"2017-07-12 10:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"mistakes/Estimations_And_Mistake_Driven_Development/","link":"","permalink":"https://westerndevs.com/mistakes/Estimations_And_Mistake_Driven_Development/","excerpt":"Mistake Driven Development, or MDD (because we need another TLA in our lives), is my thought process on how I grow as a human both personally and professionally.","raw":"---\nlayout: post\ntitle:  Estimations and Mistake Driven Development\ndate: 2017-07-12T00:00:00-06:00\ncategories: mistakes\ncomments: true\nauthorId: justin_self\noriginalurl: http://www.justinself.com/mistake-driven-development-estimations/\n---\nMistake Driven Development, or MDD (because we need another TLA in our lives), is my thought process on how I grow as a human both personally and professionally.\n\n<!--more-->\n\n The basic idea is that I, as I'm sure others do, learn best after making mistakes. I can be told the right way to do something and I can follow the approach, but it never *really* sinks in until I see what happens when I don't do it. The reason I care about things like dependency management or domain models is because I've felt the pain of not using them. It's the pain that drives me to do better. It's the pain that gives me an opportunity to improve. Were it not for the pain, I'd never want to change. For me, I have to make mistakes before I can grow.\n\nMDD doesn't stop with my technical skill sets. When I married my wife, we were both pretty young (21). I hadn't had a chance to really get myself together and now I had to be a husband. I made several mistakes (we both did but I'd never tell her that) and I felt pain. Sometimes I felt that pain several times because I can be slow learner. But eventually, I find that I want to stop feeling the pain enough to change my ways and that's always when I grow. We're still young and have a lot of years to continue to grow, but I can at least look back as we clear our first decade and see improvements.\n\n## News Flash: I suck at estimates\n\nI've been guilty of trying to anticipate what my client wants to hear and craft a pleasing response that orbits the truth instead of landing on it. This most always manifests itself in over promising. This is an area of particular interest for me. I've felt the pain of over promising (often in the way of low balling estimates) time and time again. I've walked that emotional path so many times that my feet would take me there without any conscious effort on my part. In other words, I was so good at doing it that I sometimes didn't realize I was doing it until it's too late.\n\nOne mistake started out the same way. A client was pressuring me for an estimate. I gave one and she didn't like it. So we \"negotiated\" until I walked out of the room with a familiar sense of foreboding. As a human, I suck at estimates in general. Put me under the pressure of some forced negotiation (whether real or by my own imagination) and by the end of it I've probably blacked out halfway through and temporarily made my client happy at the expense of several future sleepless nights.\n\n---\nBefore I go any further, in no way am I attempting to blame a client for my lack of directness. A client's job is to maximize value for her company which includes motivating those who work for/with her to deliver quality content quickly. As an executive for the company, she is expected to drive success hard.\n\n---\n\nI developed a theory for why I've done this. I think I subconsciously simulate the client interactions with a legitimate, variable estimation. For example, maybe the simulation starts with an estimation I feel comfortable with. If I think she'll blow up, I'll re-estimate the work with the goal of reducing the length of time required. The problem is that this distorts my vision to the point that I began to overestimate my capacity or ability. I also see this as a self fulfilling prophecy of sorts. If I'm too afraid of what the client will do when I say 6 weeks, I'm going to justify a way to myself to say something less.\n\nThis will lead to a new estimate of 5 weeks. She still won't be happy but I'm sure if we work *really* hard we can *possibly* get it done in 4 weeks. So I can just tell her 4-5 weeks. Cut to me delivering the estimate and the only thing the client hears is \"4 weeks\". Now she doesn't know that I've already tried to cut through the \"what if things go perfect\" scenario and thinks she can make me work harder to get it done faster by imposing a deadline. Before I know it, I'm walking out of the meeting and the client has used her knife to carve a giant X on a delivery date three weeks from now... Mother Francis.\n\nSo what happens next? I get myself and the team pumped up all the while hoping my face doesn't betray my confident facade. The familiar anxious feeling sits in the bottom of my stomach as if I had extra servings of stone soup. For the next two weeks, I ignore the signs while confidently thinking that we are going to deliver a miracle (with a few swishes of pepto-bismol added for good measure). The last week rolls around and it looks like we are really going to do it. But then something happens, like it does every time. Something happens in the story that we didn't foresee, production blows up and pulls half of the team away, or the client introduces a \"simple\" change.\n\nTwo or three days before we reach the giant X on the calendar, I realize it's hopeless. I began to think that if I work the next 72 hours, ignoring distractions like sleep, food, or time with my beautiful wife and kids, we *might* have a 30% chance of making.\n\nReality quickly sets in and I began customizing my most dreaded email template. You know what it is: the \"we need to delay our release\" one.\n\n## Enough\n\nI eventually got tired of this. I was ready to start making changes that would help prevent or control situations like this in the future. I had felt the pain enough now that I was ready to do something about it. This was a great opportunity to let my mistake drive my own development. \n\nNote: my point isn't to make perfect estimate, I don't even think that's possible. My point is to work  to create more realistic estimations all the while being honest with myself and my client.\n\nSo here are some things I started doing:\n\n### Embrace the suck\nWhen I was a kiddo, if I ever lied to my mother, I would be punished twice as hard (doing the bad thing I lied about + lying). If I have tough news for a client, I embrace the suck and set the expectations from the beginning. Over time, I learned techniques of delivering bad news in ways that weren't so terrible including presenting options for remediation and giving the client a chance to make a business choice regarding the matter.\n\n### Stop giving estimates to the client the first time I'm presented with the scope of work\nThis seems like a very obvious thing, but the problem is that I would forget about my previous mistakes and over estimate my own ability. Even if the scope is extremely well defined and I know the code base like I know my refrigerator, taking a moment of pause will allow me to clear my mind and provide time for historical reflection. Afterwards, I'll approach the client with an estimation completed free from pressure.\n\n### Stop giving perfect world estimates\nIf my physics classes taught me anything, it's that there is a perfect world that exists where there is no air resistance and all cows are spherical. Also, in this perfect world, nothing unplanned ever happens. My team suffers no illness, family emergencies, or destroyed laptops. We make no incorrect assumptions and introduce no bugs and every solution comes to us immediately without needing to ponder it for days. Maybe this world does exist... however, it's just not the world we live in.\n\nThe only constant is change. Life is unpredictable and I need to remember that when I'm thinking about how much time we'll need to complete something. While my client might be sympathetic to one of my teammates needing to take a week and half off because of a death in the family, she doesn't want to hear that as an excuse for why we are late. My estimates need to take into account the unknown. Some people call this padding, I'm calling it being realistic.\n\n### Take my time\nSometimes, I'll think that I know the problem and solution set so well that I don't need to dig deeper. Making estimates within a really short period of time is like trying to quickly eat a loaf of bread with a rock hidden inside. When you find that damn rock, it's going to hurt like hell.\n\n### Involve other people\nI need to stop thinking I'm always the right person to make the call on how long something will take. I might be the lead on the team, but I'm not the best. It's not my job to know everything. It's my job to utilize my teammates' strengths to create a cohesive tandem of individuals. We should be estimating as a team, not just me. Again, this seems very obvious, but I'm admitting to making this mistake.\n\n### Stop thinking everyone has my strengths and weaknesses\nMaybe I really can do something in 3 days. But does that mean everyone on my team will take the same amount of time? I can crank out some front end code pretty dang quickly. But you need me to write a complex SQL query? Hello Google (ok, really it's StackOverflow). Another person on my team might happen to be the person who has to do some CSS work and she might not be very good at it. 79.1% of all developers are intimidated by CSS... yes, I just made that up.\n\n### Break it down then break it down more\nWe suck at estimates, but we suck gloriously worse the larger the workload is. Breaking the work down into smaller items, taking the aggregate then applying overall ranges has been much more effective for me.\n\nThis isn't an exhaustive list by which I use to do better; it's just a start.\n\n### But Agile... Scrum?! What about using velocity? Why estimates?\nI love Scrum and Kanban (each in different scenarios) but when I'm working on an estimate for a client who wants to know how much something is going to cost ***before*** they sign the statement of work, sometimes you've just gotta estimate.\n\nMaybe some of this resonates with you... maybe not. In the end, this is just me pulling back the curtains and showing how I took some mistakes I made and turned them into growth opportunities.\n","categories":[{"name":"mistakes","slug":"mistakes","permalink":"https://westerndevs.com/categories/mistakes/"}],"tags":[]},{"title":"Scrum with Kanban WIP Limits","authorId":"dave_white","slug":"Scrum_with_WIP_Limits","date":"2017-07-07 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Kanban/Scrum_with_WIP_Limits/","link":"","permalink":"https://westerndevs.com/Kanban/Scrum_with_WIP_Limits/","excerpt":"A natural, easy first step for enhancing Scrum with a Kanban practice is a WIP limit","raw":"---\nlayout: post\ntitle: Scrum with Kanban WIP Limits\ncategories:\n  - Kanban\ntags:\n  - kanban\n  - agile\n  - scrum\n  - myths\ndate: 2017-07-06 20:00:00\nexcerpt: A natural, easy first step for enhancing Scrum with a Kanban practice is a WIP limit\nauthorId: dave_white\n---\nInspired by [Steve Porter's][1] efforts to bring process practitioners closer together and educate Scrum practitioners, I'm writing a shadow series of posts that will follow the [Kanban and Scrum - Stronger Together][2] series and continue [my own efforts][5] to clear up misconceptions between practitioners of these methods.\n\nIn my last post, I discussed how a Scrum team could change nothing about their process and organically start describing how they work in Kanban terms. In this post, I'm hoping to show a minimally viable change to their process that could lead to enhanced team delivery performance.\n\n## Scrum's WIP Limit Policy\n\nAs we described last post, many Scrum teams limit work in progress (WIP) at the beginning of the Sprint by filling the Sprint Plan with work. The teams are given the ownership of deciding how much work to _pull_ into the sprint. In kanban terms, we would probably call this a [CONWIP (CONstant Work In Progress)][4] WIP control policy. We have constant WIP in the sprint, and we don't control WIP at individual stages of the workflow. One thing that Scrum teams will do that doesn't exactly fit with a CONWIP policy is that CONWIP policies normally just counts cards. In Scrum, cards are assigned a _relative size_ value (Story point) that describes the size of the card. So instead of having a CONWIP limit of 5 cards, Scrum teams will a CONWIP policy of 25 story points. That may be 3 cards, it may be 20 cards, depending on the nature of the work.\n\nWe're not going to change that at all. Limiting work in any manner is a **GREAT** start!\n\nWhat we can do though is introduce count-based WIP limits at stages in the virtual kanban system. And that is it! Remember we are doing a minimum viable change to minimize risk, build experience and comfort, and see if this even works.\n\n## Scrum is naturally enhanced by Kanban\n\nKanban practitioners usually strive to enhance the effect of the WIP limit policies on their system, constantly tuning them to get optimal performance. In order to do this, Kanban tends to promote more fine-grained WIP policies at the workflow stage level. This isn't the only place or way that we can use WIP policies, but it is a really great next step for a Scrum team to take. \n\nSo very simply, the next step for Scrum teams to take is to put a WIP limit policy indicator at the top of a their kanban board!\n\nLet's walk thorough an example and see how simple that would be!\n\n### Scrum CONWIP policy controlled board\n\n![Scrum Board with CONWIP policy][7]\n\nWe can see here that we are only controlling WIP for the entire system by limiting how much work can be pulled in per sprint. This is a great start to limiting WIP, but we might be able to improve the overall flow of work within the team's workflow. I have seen Scrum teams that start **everything** at the beginning of the sprint instead of starting only as much as they can handle and trying to finish that before starting a new story. Starting everything at once isn't good behaviour or encouraged behaviour in a Scrum team, but it happens without any other policies to guide team members to better behaviour.\n\n### Scrum Board with Workflow Stage WIP control policies added\n\n![Scrum Board with WIP Limits per stage policy][8]\n\nWe can see here that we have simply added some indicators of the WIP limit policy on the Kanban board. I just put a few sample numbers in place, but it is now clear what the policy is for the team with regard to pulling the work **through** the sprint and not just pulling work _into_ the sprint.\n\nSo what these numbers mean is that we believe that there should only be _n_ # of PBIs in a stage at once. Anything more is going to lead to emotional distress due to overburdening and probably slow delivery due to multi-tasking. Using these policies as an example, we believe we should only have 2 PBIs in analysis at a time and if someone is not busy in Dev and Test, they can help out with work on Analysis of a PBI to help flow work through the system.\n\nIt is also very important to understand that in the same way that the sprint capacity is determined during the Sprint Planning meeting and adjusted per sprint, these intra-sprint WIP limit policies should be adjusted when there is new information available about the capabilities of the team.\n\nWe also gain and share information about how we believe the team _should_ behave to help deliver PBIs more effectively. We can discuss work and policies instead of discussing people and why they are working a certain way.\n\nAnd that is it! That is as easy as it is to add the idea of intra-workflow WIP limit policies to a Scrum team's kanban board. We didn't have to change anything about the way that we worked. We just enhanced and communicated our team's understanding of how we want to work.\n\n## Did it Work?!?!\n\nBefore we call this experiment a success, we need to know, did this even work to improve our delivery capability?\n\nIf you're been practicing Scrum for a bit, you will have some historical information and hopefully trend data about your team's ability to deliver Story Points/PBIs to the customer. It is this information that we use in the Sprint Planning meeting to determine what our CONWIP number should be.\n\nAfter you've implemented your intra-workflow WIP limit policies, doing nothing else, you should be able to detect if these policies helped you, or hindered you by measuring your Story Point/PBI delivery rate per sprint. You may also be able to capture some qualitative information in your Sprint Retrospective about the positive or negative impact of these policies on the team members.\n\nIf the experiment produced a measurable improvement in your team's ability to delivery, congratulations!! That is great news and I'd love to hear about it!\n\nIf the experiment produced _no_ measurable improvement in your team's delivery rate, congratulations!! You've discovered that it didn't work! And you have more information and I'd love to hear about it!  But if we had hoped to improve and discovered we didn't, we have a decision to make. Revert the changes as easily as removing the WIP limits from your board and continue life as it was before, or dig into why the policies didn't have the desired effect. Did you constantly exceed the WIP limits? Did you not measure all work? We know from experience that reducing WIP should improve delivery rates.\n\nBut again, if you don't want to figure that out, just remove the WIP limits from your board. Easy!\n\n## Is there more\n\nThere is more to the Kanban Method and virtual kanban systems, but we were just planning to demonstrate a minimum viable change that gives Scrum teams a chance to try out a little kanban practice. Once we've mastered the understanding of WIP limits and their implementation, we can pick our next technique to start to learn about like demand shaping, kanban system designs that are fit for purpose, capacity allocations, managing emergent work, or any of the other practices that our community uses as we gain experience with Kanban.\n\n## What's Next\n\nThose are just some of the opportunities that you have! I'm not suggesting you can't find those opportunities elsewhere, but you shouldn't be afraid to approach kanban! It will happily support the way you want to work today and help you continue your growth into the future! \n\nIn our next post, we will discuss how a Scrum team could enhance their practices to handle emergent (or emergency) work!\n\n\n[1]: https://www.scrum.org/user/119\n[2]: https://www.scrum.org/resources/blog/scrum-and-kanban-stronger-together\n[4]: https://en.wikipedia.org/wiki/CONWIP\n[5]: https://agileramblings.com/2013/03/10/the-difference-between-the-kanban-method-and-scrum/\n[6]: https://agileramblings.com/2013/04/07/kanban-change-catalyst-with-no-changes-planned/\n[7]: https://i.imgur.com/sWbffZY.png \"Basic Scrum board with CONWIP policy\"\n[8]: https://i.imgur.com/WVnJClc.png \"Basic Scrum board with WIP limits per phase\" ","categories":[{"name":"Kanban","slug":"Kanban","permalink":"https://westerndevs.com/categories/Kanban/"}],"tags":[{"name":"kanban","slug":"kanban","permalink":"https://westerndevs.com/tags/kanban/"},{"name":"agile","slug":"agile","permalink":"https://westerndevs.com/tags/agile/"},{"name":"scrum","slug":"scrum","permalink":"https://westerndevs.com/tags/scrum/"},{"name":"myths","slug":"myths","permalink":"https://westerndevs.com/tags/myths/"}]},{"title":"Home Networking - What and Why","authorId":"donald_belcham","slug":"Home-Networking-1","date":"2017-07-03 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Networking/Home-Networking-1/","link":"","permalink":"https://westerndevs.com/Networking/Home-Networking-1/","excerpt":"Moving into the new house meant a new internet provider and the standard installation of home networking gear that exists in (hundreds of) thousands of houses throughout North America","raw":"---\nlayout: post\ntitle: Home Networking - What and Why\ncategories:\n  - Networking\ntags:\n  - networking\ndate: 2017-07-02 20:00:00\nexcerpt: Moving into the new house meant a new internet provider and the standard installation of home networking gear that exists in (hundreds of) thousands of houses throughout North America\nauthorId: donald_belcham\n---\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/network-cables.jpg\"/>\n\nLate last year we moved into a new house. Leaving the other house was tough, and not just because I had built out a [fairly good networking solution](https://www.igloocoder.com/2014/03/25/A-solid-foundation/) for it. Moving into the new house meant a new internet provider and the standard installation of home networking gear that exists in (hundreds of) thousands of houses throughout North America. Not only did we have crap hardward, the installer obviously had taken the path of least resistence when running the wire from the street and locating it in the house. The best part of the installation was that the wire from the street entered into the mechanical room. The install or previous owners had also run a small mess of Cat5 to get from the mechanical room to a central location where the ISP's router was expected to be located. From that location, they had run more wire to an \"office\" room in another part of the house.\n\nThis setup wasn't a bad one, but it did have it's limitations. The main router had to sit along a wall in the eating area of the kitchen. That router was also the single wireless access point in the house. ISP provided hardware being what it is (substandard at best), the wifi wasn't stable and, in some locations of the house, there was no identifiable signal. The first 6 months living in the new house have been filled with a host of first world problems. But, being the first world, we had solutions at our disposal.\n\n### Past experience\nOur old home had similar issues and I solved many of them, but I also introduced a few different problems along the way. At it's roots that house used a Rosewill gigabit switch and a handful of DD-WRT powered routers. If nothing else, these devices were stable...after I solved the long-running issue of random router reboots that, ultimately, were being caused by a loose power plug.\n\nProbably the biggest ongoing problem in that house was wifi handoffs. There was one DD-WRT wireless access point on each of the three floors in the house. If I moved from the basement to the main floor, my phone (or other device) would stay connected to the basement access point...even if the signal strength dropped to barely noticeable levels. The result was that you could be standing right next to an access point but you wouldn't be connected to it and you'd likely be suffering from a poor connection to another access point. Definitely not a good thing in a house of highly connected and frequently moving people.\n\nThere were a number of things that I really liked about the physical setup in that house, and I knew that I needed to carry them forward to the new one. One of those was the use of a patch panel where all of the wired connections in the house terminated. Being able to easily interact with the different locations throughout the house was a godsend many times. I added more than just the network cables to the patch panel. Using a Keystone based panel I was able to have all of the Cat5e cables in the panel, the coaxial cables and the phone lines all in that one place. Interestingly, I benefited from the patch panel flexibility more times with the coax and phone lines than I did with the network cables.\n\nOne of the other nice thing was the use of a [PDU in the rack](https://www.amazon.ca/StarTech-com-RKPW081915-19-Inch-Rackmount-Distribution/dp/B0035PS5AE/ref=sr_1_4?ie=UTF8&qid=1493169057&sr=8-4&keywords=rack+pdu). Instead of having multiple powered devices all searching for a wall plug and, ultimately, resorting to some kind of consumer power bar, I was able to nicely organize the power distribution.\n\nThe final \"must have\" that I learned in the old house was the use of a wall mount rack. Originally I had aspirations of using a 48U free standing rack for my networking and entertainment needs. Realistically I didn't need that much space. It wasn't just the vertical space of the free standing rack that was overkill, but also the depth. Most rack mount networking hardware doesn't need more than 20 inches of depth. A good wall mount rack can easily provide the depth required. A rack also saves you from having a bunch of crappy shelves holding one-off hardware and a tangle of cables. For me, a wall-mount rack was a must-have.\n\nI did a lot of good things in the old house. I did a number of bad things too. My only goal was making home-network-v2 better than its predecessor.\n\n### Needs\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/MaslowsHierarchyOfNeeds.jpg\"/>\n\nObviously there were things I thought we needed in our new house network. They weren't \"needs\" in the same sense as [Maslow's Hierarchy](https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs), but they sure felt important to me.\n\nTop of the list, which I was constantly hearing about from the other people in the house, was strong, stable WiFi that covered all the areas we used around the house, the inlaw suite and the pool. The longer I waited to build this network, the more I was being reminded of how important this was to the other people in the house.\n\nI also wanted to setup a good infrastructure base to build from. Strong hardware, good cable management and room for expansion. The expansion part was key. I have lots of plans.\n\n### Wants\nThere are a lot of things that I want to do with a home network. A big one is supporting my desire to build a full sensor network to track things like water spills/leaks, windows, doors, and many other things. One of the biggest issues with adding those features to a house is that the sensor devices require power. The easiest solution to that was a PoE capable switch and a full 48 ports.\n\nI also really wanted to completely eliminate all of the ISP provided hardware. The WiFi on it sucks, it reboots randomly and I have little or no control over the patching of it. The biggest challenge here is that the ISP router also provides our TV service.\n\nOne of the goals with moving to this new house was for me to spend some more time in a workshop making sawdust. I'd love to be 100% disconnected while doing that, but the reality is that I do a lot of research/learning about woodworking online. Some internet connectivity in my shop, which is about 150 feet from the house, was pretty high up on my list.\n\n### Dreams\nWe all have them: plans for our \"ideal\" network. The platinum plated solution. I'm definitely not immune to this.\n\nOne of the things I hate the most about the default ISP provided solution is that all of the TV set-top boxes get their signals via WiFi. Obviously, the farther from the base unit, the more walls, bad weather, invisible temporary Faraday Cage's, days that end in 'Y' and other things will cause instability in the TV signals. I really wanted to get rid of this and go to a wired solution so that I would know that the TVs would all work whenever I needed to binge watch ~~Master Chef Junior~~ The Expanse. This is _not_ a normal way to configure this provider's TV sytems.\n\nAnother dream was to setup the telephone wiring in a patch panel like I had done at the old house. This might seem like a simple thing to do, but you haven't seen the wiring in the new house. It's a 40+ year old place that has had low voltage wiring cobbled together through its life. Getting all of the different phone jacks routed back to a central location was going to be no small task.\n\n### Thinking and Planning and Thinking\nI spent somewhere between 4 and 6 months formulating ideas, prioritizing them, re-thinking possible solutions and scrapping some of them. It was probably one of the best things that I could have done. Instead of rushing into any solution that looked and felt better than what was available when we moved in, I lived with what we had (much to the chagrin of the other WiFi users in the house). I didn't just go out and buy the hardware I thought I needed. I thought through the problem area, researched options, considered if the problem really would exist, looked for alternatives and then re-thought everything.\n\nI discovered a lot of problems that I would have overlooked if I'd moved faster. Things like the importance of having system management solutions when you're using enterprise level hardware. I found a lot of hardware solutions that I didn't know existed. One of those was front and back cable management trays. I also scrapped a few ideas, like running a buried strand of fiber from the house to the garage.\n\nThere were a few adjustments and ideas that did surface that I couldn't ignore though.\n\n### Adjustments\n<img style=\"float: right;padding-left:10px\" src=\"https://www.igloocoder.com/images/deskphone.jpg\"/>\n\nOur new house has the main living quarters and an attached in-law suite. When people are staying in the suite the only way to communicate with them is to walk to their door and knock or to call their cell phones. Since that space isn't going to be occupied full time, installing a separate land-line isn't a good option. I did, however, run across some information on open source VoIP solutions. All I needed was a PoE powered data line and I could put a phone anywhere in the house, inlaw suite or the garage. While not a \"need\" or a \"want\", this idea is certainly on the \"dreams\" list and likely will happen at some point in the future.\n\nAnother problem that I struggled to solve was how I could monitor people who came to the door of the inlaw suite. It is situated such that I can't see the door or the driveway from any part of the main house. I looked at some IoT-ish video doorbells, but none of them gave me the warm and fuzzies. One night I went down a rabbit hole around IP based video monitoring systems. Not only could I see who was knocking at the door, but I could also put some eyes on other parts of the yard like the detatched garage and the pool. IP based video was not just going to get added to the list or requirements, but it was going to apparear at the \"want\" level.\n\nI read a lot of forums as I did research for this project. One night while looking for something completely unrelated I stumbled across a conversation that had an off-hand comment about the UPS that my ISP provides for some of their hardware. It turns out that the way this UPS is connected to the hardware, it only powered the phone functionality when it was running on the battery. That meant that all internet would be cutoff immediately when the power went out...even if there still was signal in the fibre lines. Prior to this little bit of information I had only intended on using a PDU in the rack. A quick trip to Amazon and I was expecting delivery of a 1U UPS that could provide battery backup to the UPS that provides battery backup to the ISP hardware. Because daddy can't be without his internet when the power bumps.\n\n### Filled to overflowing\nOur new house has a very nice wood burning fireplace in it, and I spent the better part of the winter sitting by it reading and researching all things networking. It was fun to spend time looking at concepts and problem areas that I used to work with early in my career. I'm not sure if it is the variety of new (to me) hardware and software, or the prospect of building v2 of something, but either way it is something that I was energized by.\n\nSome of the other Western Devs have asked that I publish a bunch of content around this project, so this is just the first of many.","categories":[{"name":"Networking","slug":"Networking","permalink":"https://westerndevs.com/categories/Networking/"}],"tags":[{"name":"networking","slug":"networking","permalink":"https://westerndevs.com/tags/networking/"}]},{"title":"Nothing in Kanban Prevents Scrum","authorId":"dave_white","slug":"Nothing_in_Kanban_Prevents_Scrum","date":"2017-07-02 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Kanban/Nothing_in_Kanban_Prevents_Scrum/","link":"","permalink":"https://westerndevs.com/Kanban/Nothing_in_Kanban_Prevents_Scrum/","excerpt":"Inspired by a colleague","raw":"---\nlayout: post\ntitle: Nothing in Kanban Prevents Scrum\ncategories:\n  - Kanban\ntags:\n  - kanban\n  - agile\n  - scrum\n  - myths\ndate: 2017-07-01 20:00:00\nexcerpt: Inspired by a colleague \nauthorId: dave_white\n---\nInspired by [Steve Porter's][1] efforts to bring process practitioners closer together and educate Scrum practitioners, I'm writing a shadow series of posts that will follow the [Kanban and Scrum - Stronger Together][2] series and continue [my own efforts][5] to clear up misconceptions between practitioners of these methods.\n\n# Kanban Easily Supports All of Scrum\n\nOne of the things that I see very often is a belief that Scrum and Kanban cannot work together and nothing is farther from the true. If we look at one of the core values of [The Kanban Method][6] you'll see that the first principle is:\n\n> Start with what you do now\n\nThis should instantly put many change-fearing professionals at ease with regard to The Kanban Method, if not the practitioner helping you with it. But in this conversation, this should put Scrum team members at ease. There is nothing in the method that will disrespect your current practices or experiences.\n\n[Yuval Yeret][4] has done a great job of giving us a [Primer to Kanban from Scrum Teams][3] but I thought I'd step back from that without introducing too much Kanban and try to diminish this fear that you can't do both by showing that it can be done easily. \n\n One thing that a Scrum team would naturally want to do is understand the parallels or mappings between the two processes. Familiarity promotes comfort, so let's build a bit of a map for a _pure_ Scrum team to describe their approach in Kanban terms because generally speaking, Scrum teams are already **doing kanban**.\n\n## Sprint/Iteration\n\nOne of the core tenants of Scrum is the Sprint. This time box is designed to give teams a few things:\n\n1. consistent schedule for important, collaborative activities\n1. control over work selection for the time period\n1. meet daily to discuss current state and plan for the day\n1. reduction in changes induced by external parties\n1. an end date that the delivery team can use as a goal for delivery\n1. an end date that a customer can use as an expectation\n\nScrum achieves these goals by using particular activities around and in the time box. As an example, let us say that a Scrum team has a 2 week Sprint, so at the start of a 2 week period, they replenish their sprint backlog in the Spring Planning meeting. They will select how much work to accept as the goal (Sprint goal/forecast) for the 2 week period. They will ~~not accept~~ _strongly discourage_ changes to the contents of the sprint backlog for the 2 week period. They will meet daily to discuss the current state of things and adjust any daily plans as appropriate. They will strive to complete the Sprint goal (all of the forecasted work) by the end of the Sprint, and they will plan to demonstrate their accomplishments to external parties in a Sprint Review. They will also plan reflect on their own activities and try to identify opportunities to improve their own capabilities in a Sprint Retrospective.\n\nSo what we've discovered is that:\n\n1. They have a meeting at the start of the 2 week period to fill the Sprint backlog\n   - the Scrum name: Sprint Planning meeting\n   - The team chooses how much work goes into that backlog\n   - The team will generally be allowed to finish that work before starting new work\n1. They will meet daily to create effective daily plans\n   - the Scrum name: Daily Scrum\n1. They will have a meeting at the end of the 2 week period to review what they have produces\n   - The Scrum name: Sprint Review\n1. They will have a meeting at the end of the 2 week period to review their own process\n   - the Scrum name: Sprint Retrospective\n\nHow do we model that in kanban? \n\n#### Cadences\n\n... are the kanban term used to describe the things that happen on schedule. Kanban easily supports the Sprint Planning activity by creating an analogous cadence for a Replenishment activity, where the team interacts with an upstream partner (read: customer) to determine what to work on until the next time we get to meet with the customer. A Scrum team can easily describe their scheduled meeting as happening on predictable cadences.\n\n1. Once every two weeks, we will collaborate to fill our backlog\n   - a kanban name: Replenishment meeting\n   - the team understand how much work will occur in 2 weeks\n1. They will meet daily to discuss current state and plan for the day\n   - a kanban name: Kanban meeting\n1. Once every two weeks, we will meet to discuss/demonstrate what we've accomplished \n   - a kanban name: Product demo\n1. Once every two weeks, we will discuss our own processes with an eye on improvement \n   - a kanban name: Service Delivery Review\n\n#### WIP Limits\n\n... are the kanban equivalent to picking (pulling) the work that we feel we can accomplish and being allowed to focus on finishing that work before starting or being interrupted by new work\n\nA Scrum team picks how much work to pull into the Sprint. The kanban name for this is a WIP limit. A Kanban team can pull 10 PBIs into their process every 2 weeks. \n\nIn Kanban, teams are not required to put WIP limits on columns. This is a natural growth path for many teams, but it certainly isn't required. A limit on the # of items accepted into the sprint is an acceptable form of limiting WIP. These limits are guides to optimal workflow behaviour, but they are not laws. Kanban teams, just like Scrum teams, can adapt their plan to accomodate newly discovered information.\n\n#### Visualize\n\n... is the kanban approach to using visualizations of intangible work (code, features, etc) to help us understand and manager our own process. We typically classify the things as _work items_ but they often have more descriptive names. \n\nMost Scrum teams already use boards and they use PBIs to describe intangible things like software and code. \n\n\n## Is there more\n\nThere is more to the Kanban Method, but we were just planning to model what a Scrum team does in kanban terms in a comfort-building exercise. Kanban does not require you to remove estimation (planning poker) as a means of filling your sprint. Kanban does not require you to abandon story points as a means of representing _size_ or _effort_. You can still use story points as a means of measuring _how much_ you did.\n\nAnd that's it! Scrum teams visualize work, limit WIP, and have cadences! Any Scrum team can call themselves a Kanban team. They simply need to make the decision.\n\n## What's Next\n\nWell, if you are a new to Kanban team, there is lots of opportunity to grow. You have the opportunity to:\n\n1. refine your understanding of your work\n1. better know who your customers really are\n1. understand what you do and how you do it\n   - how we measure progress\n   - how we maximize flow (team level)\n1. who are your partners in your organization helping you deliver to your customers\n   - how we maximize flow (organization level)\n1. how we scale our approach across the organizations, not just multiple development teams\n   - kanban in the HR dept. Who knew?!?! :D\n\nThose are just some of the opportunities that you have! I'm not suggesting you can't find those opportunities elsewhere, but you shouldn't be afraid to approach kanban! It will happily support the way you want to work today!\n\n\n[1]: https://www.scrum.org/user/119\n[2]: https://www.scrum.org/resources/blog/scrum-and-kanban-stronger-together\n[3]: https://www.scrum.org/resources/blog/kanban-primer-scrum-teams\n[4]: https://www.linkedin.com/in/yuvalyeret/\n[5]: https://agileramblings.com/2013/03/10/the-difference-between-the-kanban-method-and-scrum/\n[6]: https://agileramblings.com/2013/04/07/kanban-change-catalyst-with-no-changes-planned/","categories":[{"name":"Kanban","slug":"Kanban","permalink":"https://westerndevs.com/categories/Kanban/"}],"tags":[{"name":"kanban","slug":"kanban","permalink":"https://westerndevs.com/tags/kanban/"},{"name":"agile","slug":"agile","permalink":"https://westerndevs.com/tags/agile/"},{"name":"scrum","slug":"scrum","permalink":"https://westerndevs.com/tags/scrum/"},{"name":"myths","slug":"myths","permalink":"https://westerndevs.com/tags/myths/"}]},{"title":"Kanban and Scrum Together - Not so fast","authorId":"dave_white","slug":"Kanban_and_Scrum_Together","date":"2017-06-30 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Kanban/Kanban_and_Scrum_Together/","link":"","permalink":"https://westerndevs.com/Kanban/Kanban_and_Scrum_Together/","excerpt":"A colleague of mine who works at Scrum.org now posted a blog about how Kanban and Scrum are stronger together.","raw":"---\nlayout: post\ntitle: Kanban and Scrum Together - Not so fast\ncategories:\n  - Kanban\ntags:\n  - kanban\n  - agile\n  - scrum\n  - myths\ndate: 2017-06-29 20:00:00\nexcerpt: A colleague of mine who works at Scrum.org now posted a blog about how Kanban and Scrum are stronger together. \nauthorId: dave_white\n---\nA colleague of mine who works at Scrum.org now posted a blog about how Kanban and Scrum are stronger together. [Steve Porter][1] had this to say in his blog post...\n\n[Kanban and Scrum - Stronger Together][2]\n\nYou might find my first comment there if someone at Scrum.org approves it in the pursuit of open and shared discussions.\n\n![Missing Comments??][3]\n\n**Update** The comments have finally been approved. \n\nSince it hasn't been approved at the time of me writing this blog post, here it is again...\n\n>Hey Steve,\n>\n>I've been sitting with this tab open for a month, trying to decide what to say. I felt like it was time to close the tab by providing my thoughts. To be clear, I'm speaking about The Kanban Method when I use the word kanban and I believe that is what this article is talking about when it says Kanban and Scrum are stronger together.\n>\n>Let me start out stating that I believe that teams and organizations need to be better at addressing customer demand. That pressure is what drives organizations and teams to adopt a 'process' that they think will help. We would call those processes Scrum and Kanban and that is probably the first point of failure. It is unfortunate that these processes are seen as incompatible and I applaud your decision to try and change that thinking. It isn't the practices that are incompatible, but the religions that sprout up around the organizations that evangelize the processes.\n>\n>Part of me is disappointed with the misconceptions of what kanban is and how it can be applied as demonstrated by the comments. Going through some of the the comments, I see...\n>\n>**Hiren Pandya** ... the requirements needed to transition to kanban.\n>\n>Anyone can begin transitioning to kanban at any time because of the lack of prescription of \"the one\" kanban approach. Kanban has a natural and specific mindset that accommodates the current state and an evolutionary path to maturity, of which we are all at different points in our journey. Honestly, transitioning to kanban is as simple as changing the label we use describe our mindset and values.\n>\n>**Bruno BaketariÄ‡** Create hybrids: Beware! --and-- along with an absence of time-boxes or any other time-bound constraint (the Kanban cadences are not constraints).\n>\n>The problem with this statement is that any approach that is open to adding/removing practices as their value to the team changes will create a hybrid solution. Kanban by it's very nature creates hybrid solutions as teams take tactical practices from anywhere they choose to enhance they way they deliver value to their customers. The kanban value system openly embraces the concept of taking practices from anywhere that might improve your team/organizations ability to deliver value to customers. It has to accept all practices with respect.\n>\n>Regarding cadences not being constraints, I would posit that a Replenishment meeting with a cadence of 2 weeks is exactly the same kind of time-bound constraint as a Sprint Planning meeting that happens every 2 weeks. Practically, they are equivalent constraints.\n>\n>**Mark Chapman** I don't quite get what the point is here, they have different uses for different teams.\n>\n>This seems to be a reinforcement of the myth that Kanban is for Type A teams and Type B teams shouldn't use it. Which is a myth.\n>\n>**Final Thoughts...**\n>\n>After reading your article, and the comments, it makes me wonder why people think that a team couldn't implement a fully \"Scrum\" set of practices and processes and name it Kanban. It is 100% possible to do that. I don't know that the opposite is true, in that a team would fully implement a kanban system and call it Scrum. Kanban allows for far more variation than the Scrum guide does.\n>\n>I guess in the long run, we are trying to foster collaboration and enhance the strength of our professions by bringing these two communities together, which I think is very noble and the right thing to do. What I would like to see though is a properly educated discussion about where and how they (Kanban and Scrum) are different (or not) so that people can make decisions from how to describe what they are doing.\n>\n>Thank you and [Dan][4] for stepping forward and taking on this challenge.\n\n(I added some Markdown to simulate the Disqus formatting)\n\nSteve kindly responded (without approving the original comment) with some questions..\n\n> if you can truly transition to a Kanban system as easily as you described\n\nand \n\n> if you're not limiting WIP, you're not implementing a Kanban system\n\nI responded with this ~~still considered spam and unapproved~~ comment. **Update** - The comments have finally been approved. \n\n>First of all, I spoke of transitioning to The Kanban Method as being very easy. I didn't discuss the implementation of a virtual kanban system as easy, although in truth, most Scrum teams are already using a virtual kanban system.\n>\n>[Dan][4] and I have had great conversations about WIP limits. Last time might have been in Germany over cocktails, but it was a great conversation.\n>\n>There are many ways to limit WIP. I, and probably Dan too, would actually probably prefer to call it an optimal WIP policy, but if we are speaking only about limiting WIP, there are different kinds of policies along the maturity curve that we can use to limit WIP, all with different pros and cons that are discovered as we go. We choose which of those policies to start with based on organization emotional resistance and education/experience, all the while understanding that we are using this first policy as a starting point and we expect it to change, sometime frequently, as information about team workflow evolves.\n>Some possible policies include...\n> - A Sprint backlog is a WIP limiting policy. \n> - A policy of not starting anything new until your current work is done, is a WIP limit.\n> - Stating everyone can work on 2 things at a time is a WIP limit policy.\n> - A visual token indicating there is capacity to pull is the visualization of a WIP limit policy. \n> - Keeping a flow efficiency number high is a WIP limiting policy.\n> - Canonically, a number at the top of a column on a kanban board is a visualization of an explicit WIP limit policy.\n>\n>The problem with WIP policies (and this certainly applies to many Scrum teams I've seen) is that there is usually no penalty for breaking the policy. It is an indicator of bad behaviour if the team violates a WIP limit, but it isn't a law and we acknowledge that sometimes we have to violate our WIP limits due to unforeseen events.\n>\n>So while I speak about limiting WIP, I don't necessarily write a # on the kanban board until the team discovers the problem with the # not being on the board, then we decide what to do about it. If I recall correctly, Dan always puts a # on the board, but makes it high enough that there should be no emotional resistance to it, and then he starts talking about lowering it. I think both approaches have merit.\n>\n>About 'no WIP limit == no kanban system', generally speaking, I'd state that you have an immature virtual kanban system if you don't have pull-based work scheduling mechanics in play to manage the flow of work. This is subtly different than saying if your not limiting WIP, you're not implementing kanban.\n\nI'm presenting all of this for a couple reasons.\n\n1) I don't like being censored \n2) It is, imho, really important for the community to get access to all of the information\n\nIf someone is going to present two competitive concepts and state that they shouldn't be competitive, that is great. And I've stated that Kanban and Scrum should **not** be seen as competitive and and incompatible. \n\nBut if you are going to explore the merits of each concept and allow for the continued misunderstanding of a either of those concepts in your dialog, you're biased and doing a disservice to the community.\n\nPlease check back soon for my exploration of each comparison that arises as we follow [Steve's][2] series of posts.\n\n\n[1]: https://www.scrum.org/user/119\n[2]: https://www.scrum.org/resources/blog/scrum-and-kanban-stronger-together\n[3]: https://i.imgur.com/DuwmZIf.png \"Missing comments on Scrum.org\"\n[4]: https://www.actionableagile.com/about-us/\n[5]: https://letsencrypt.org\n","categories":[{"name":"Kanban","slug":"Kanban","permalink":"https://westerndevs.com/categories/Kanban/"}],"tags":[{"name":"kanban","slug":"kanban","permalink":"https://westerndevs.com/tags/kanban/"},{"name":"agile","slug":"agile","permalink":"https://westerndevs.com/tags/agile/"},{"name":"scrum","slug":"scrum","permalink":"https://westerndevs.com/tags/scrum/"},{"name":"myths","slug":"myths","permalink":"https://westerndevs.com/tags/myths/"}]},{"title":"Dylan Joins Microsoft","authorId":"dylan_smith","slug":"Dylan-Joins-Microsoft","date":"2017-06-13 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Dylan-Joins-Microsoft/","link":"","permalink":"https://westerndevs.com/_/Dylan-Joins-Microsoft/","excerpt":"After 6.5 rewarding years with Imaginet, I'm joining Microsoft as a DevOps Architect.","raw":"---\nlayout: post\ntitle: Dylan Joins Microsoft\ntags:\n  - devops\n  - microsoft\n  - vsts\n  - azure\ndate: 2017-06-12 20:00:00\nexcerpt: After 6.5 rewarding years with Imaginet, I'm joining Microsoft as a DevOps Architect.\nauthorId: dylan_smith\n---\nI'm excited to announce that **Iâ€™m joining Microsoft!!!**  Iâ€™ve had a GREAT 6.5 years with [Imaginet](http://www.imaginet.com) doing DevOps, ALM, Agile, and architecture consulting.  I got the opportunity to work with many smart people on a TON of interesting projects.  And by the way â€“ they are hiring right now!\n\nMy new role at Microsoft is a DevOps Architect on a brand new team whose goal is to drive VSTS and ultimately Azure adoption, in this case by focusing on helping (large) customers apply effective DevOps practices. The idea is to build relationships with Microsoft's biggest customers, and act in an advisory role to help them be successful in deploying things to azure and/or adopting VSTS. One of our first activities will be providing the [Enterprise DevOps Accelerator](https://www.visualstudio.com/vs/enterprise-devops-offer/) benefit that was announced at the VS 2017 launch.  This is a free 2-week engagement provided by Microsoft or one of our partners to customers that purchase 50+ VS Enterprise licenses.  The goal is to help a customer migrate an application to Azure using DevOps practices and tools (i.e. VSTS). My team will be responsible for those engagements, determining what we can do to add the most value in 2 weeks, delivering some of them ourselves and working with partners to deliver others, and evolving the offering as we learn.\n\nWhat I find most exciting is itâ€™s a brand new team - with the broad goal of helping drive Azure/VSTS adoption via good DevOps â€“ and me (and my team) will have a large amount of autonomy to decide exactly how we go about achieving that goal.  On that note, my first order of business will be building a team â€“ specifically Iâ€™m looking for a world-class DevOps expert to work alongside me to build the team, and help determine our direction and strategy.  If you or somebody you know are a world-class DevOps expert, please get in touch with me and lets chat!  For now the best way to reach me is <dylansmith256@hotmail.com>\n\nPS - It will also be nice to not be a billable consultant anymore, not having to always be selling and convincing customers to give us more business.\n","categories":[],"tags":[{"name":"azure","slug":"azure","permalink":"https://westerndevs.com/tags/azure/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"},{"name":"microsoft","slug":"microsoft","permalink":"https://westerndevs.com/tags/microsoft/"}]},{"title":"Running Kubernetes on Azure Container Services","authorId":"simon_timms","slug":"Kubernetes_on_azure","date":"2017-06-07 23:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/Kubernetes_on_azure/","link":"","permalink":"https://westerndevs.com/development/Kubernetes_on_azure/","excerpt":"Docker is cool and all but shipping containers to production has a lot of added challenges. Scaling, deploying, rolling deployments, storage,... the list of challenges goes on and on. An orchestration engine, like Kubernetes, can solve many of the challenges.","raw":"---\nlayout: post\ntitle: Running Kubernetes on Azure Container Services\ntags:\n  - kuberntes\n  - c#\ncategories:\n  - development\nauthorId: simon_timms\ndate: 2017-06-07 19:36:36\nexcerpt: Docker is cool and all but shipping containers to production has a lot of added challenges. Scaling, deploying, rolling deployments, storage,... the list of challenges goes on and on. An orchestration engine, like Kubernetes, can solve many of the challenges. \n---\n\nThis blog post will walk through how to set up a small Kubernetes cluster on Azure Container Services, manage it with Kubernetes and do a rolling deployment. You may think that sounds like kind of a lot. You're not wrong. Let's dig right in.\n\n*Note:* If you're a visual or auditory learner then check out the channel 9 video version of this blog post.\n\nWe're going to avoid using the point and click portal for this entire workflow, instead we'll lean on the really fantastic Azure command line. This tool can be installed locally or you can use the version built into the portal. \n\n![Azure CLI in Portal](http://i.imgur.com/JjmmFvg.png)\n\nUsing the commandline is great for this sort of thing because there are quite a few moving parts and we can use variables to keep track of them. Let's start with all the variables we'll need, we'll divide them up into two sets.\n\n```shell\nRESOURCE_GROUP=kubernetes\nREGION=australiasoutheast\nDNS_PREFIX=prdc2017\nCLUSTER_NAME=prdc2017\n```\nThe first set of variables here are needed to stand up the resource group and Azure Container Service. The resource group is called `kubernetes` which is great for my experiments but not so good for your production system. You'll likely want a better name, or, if you're still governed by legacy IT practices you'll want a name like `IL004AB1` which encodes some obscure bits of data. Next up is the region in which everything should be created. I chose Australia South-East because it was the first region to have a bug fix I needed rolled out to it. Normally I'd save myself precious hundreds of miliseconds by using a closer region. Finally the DNS_PREFIX and CLUSTER_NAME are used to name items in the ACS deployment.\n\nNext variables are related to the Azure container registry frequently called ACR. \n\n```\nREGISTRY=prdc2017registry\n\nDOCKER_REGISTRY_SERVER=$REGISTY.azurecr.io\nDOCKER_USER_NAME=$REGISTRY\nDOCKER_PASSWORD=yAZxyNVN8yIs5uln9yNQ\nDOCKER_EMAIL=stimms@gmail.com\n```\n\nHere we define the name of the registry, the URL, and some login credentials for it. \n\nWith the variables all defined we can move on to actually doing things. First off let's create a resource group to hold the twenty or so items which are generated by the default ACS ARM template. \n\n```\naz group create --name $RESOURCE_GROUP --location $REGION\n```\n\nThis command takes only a few seconds. Next up we need to create the cluster. To create a Linux based cluster we'd run \n\n```\naz acs create --orchestrator-type=kubernetes --resource-group $RESOURCE_GROUP --name=$CLUSTER_NAME --dns-prefix=$DNS_PREFIX --generate-ssh-keys\n```\n\nWhereas a Windows cluster would vary only slightly and look like:\n\n```\naz acs create --orchestrator-type=kubernetes --resource-group $RESOURCE_GROUP --name=$CLUSTER_NAME --dns-prefix=$DNS_PREFIX --generate-ssh-keys --windows --admin-password $DOCKER_PASSWORD\n```\n\nFor the purposes of this article we'll focus on a Windows cluster. You can mix the two in a cluster but that's a bit of an advanced topic. Running this command takes quite some time, typically on the order of 15-20 minutes. However, the command is doing quite a lot: provisioning servers, IP addresses, storage, installing kubernetes,...\n\nWith the cluster up and running we can move onto building the registry (you could actually do them both at the same time, there is no dependency between them). \n\n```\n#create a registry\naz acr create --name $REGISTRY --resource-group $RESOURCE_GROUP --location $REGION --sku Basic\n#assign a service principal\naz ad sp create-for-rbac --scopes /subscriptions/5c642474-9eb9-43d8-8bfa-89df25418f39/resourcegroups/$RESOURCE_GROUP/providers/Microsoft.ContainerRegistry/registries/$REGISTRY --role Owner --password $DOCKER_PASSWORD\naz acr update -n $REGISTRY --admin-enabled true\n```\n\nThe first line creates the registry and the second sets up some credentials for it. Finally we enable admin logins.\n\nOf course we'd really like our Kubernetes cluster to be able to pull images from the registry so we need to give Kubernetes an idea of how to do that. \n\n```\naz acr credential show -n $REGISTRY\n```\n\nThis command will dump out the credentials for the admin user. Notice that there are two passwords, either of them should work. \n\n```\nkubectl create secret docker-registry $REGISTRY --docker-server=https://$DOCKER_REGISTRY_SERVER --docker-username=$REGISTRY --docker-password=\"u+=+p==/x+E7/b=PG/D=RIVBMo=hQ/AJ\" --docker-email=$DOCKER_EMAIL\n```\n\nThe password is the one taken from the previous step, everything else from our variables at the start. This gives Kubernetes the credentials but we still need to instruct it to make use of them as the default. This can be done by editing one of the configuration ymls in Kubernetes. \n\n```\nkubectl get serviceaccounts default -o yaml > ./sa.yaml\n```\n\nRegreives the YML for service accounts. In there two changes are required: first removing the resource version by deleting `resourceVersion: \"243024\"`. Next the credentials need to be specified by adding \n\n```\nimagePullSecrets:\n- name: prdc2017registry\n```\n\nThis can now be sent back to Kubernetes \n\n```\nkubectl replace serviceaccounts default -f ./sa.yaml\n```\n\nThis interaction can also be done in the Kubernetes UI which can be accessed by running \n\n```\nkubectl proxy\n```\n\nWe've got everything set up on the cluster now and can start using it in earnest. \n\n## Deploying to the Cluster\n\nFirst step is to build a container to use. I'm pretty big into ASP.NET Core so my first stop was to create a new project in Visual Studio and then drop to the command line for packaging. There is probably some way to push containers from Visual Studio using a right click but I'd rather learn it the command line way. \n\n```\ndotnet restore\ndotnet publish -c Release -o out\ndocker build -t dockerproject .\n```\n\nIf all goes well these commands in conjunciton with a simple Dockerfile should build a functional container. I used this docker file\n\n```\nFROM microsoft/dotnet:1.1.2-runtime-nanoserver\nWORKDIR /dockerProject\nCOPY out .\nEXPOSE 5000\nENTRYPOINT [\"dotnet\", \"dockerProject.dll\"]\n```\n\nThis container can now make its way to our registry like so\n\n```\ndocker login $DOCKER_REGISTRY_SERVER -u $REGISTRY -p \"u+=+p==/x+E7/b=PG/D=RIVBMo=hQ/AJ\"\ndocker tag dockerproject prdc2017registry.azurecr.io/dockerproject:v1\ndocker push prdc2017registry.azurecr.io/dockerproject:v1\n```\n\nThis should upload all the layers to the registry. I don't know about you but I'm getting excited to see this thing in action. \n\n```\nkubectl run dockerproject --image prdc2017registry.azurecr.io/dockerproject:v1\n```\n\nA bit anti-climactically this is all that is needed to trigger Kubernetes to run the container. Logging into the UI should show the container deployed to a single pod. If we'd like to scale it all that is needed is to increase the replicas in the UI or run \n\n```\nkubectl scale --replicas=3 rc/dockerproject\n```\n\nThis will bring up two additional replicas so the total is three. Our final step is to expose the service port externally so that we can hit it from a web browser. Exposing a port of Kubernetes works differently depending on what service is being used to host your cluster. On Azure it makes use of the Azure load balancer. \n\n```\nkubectl expose deployments dockerproject --port=5000 --type=LoadBalancer\n```\n\nThis command does take about two minutes to run and you can check on the progress by running \n\n```\nkubectl get svc\n```\n\nWith that we've created a cluster and deployed a container to it. \n\n##Bonus: Rolling Deployment\n \n Not much point to having a cluster if you can't do a zero downtime rolling deployment, right? Let's do it!\n\n You'll need to push a new version of your container up to the registry. Let's call it v2.\n\n```\ndocker tag dockerproject prdc2017registry.azurecr.io/dockerproject:v2\ndocker push prdc2017registry.azurecr.io/dockerproject:v2\n```\n\nNow we can ask Kubernetes to please deploy it\n\n```\nkubectl set image deployments/dockerproject prdc2017registry.azurecr.io/dockerproject:v2\n```\n\nThat's it! Now you can just watch in the UI as new pods are stood up, traffic rerouted and the old pods decomissioned. \n\n## Conclusion \n\nIt is a credit to the work of mnay thousands of people that it is so simple to set up an entire cluster and push an image to it forget that we can do zero downtime deployments. A cluster like this is a bit expensive to run so you have to be serious about getting good use out of it. Deploy early and deploy often. I'm stoked about containers and orchestration - I hope you are too!\n\nOh, and don't forget to tear down you're cluster when you're done playing.\n\n```\naz group delete --name $RESOURCE_GROUP\n```","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"c#","slug":"c","permalink":"https://westerndevs.com/tags/c/"},{"name":"kuberntes","slug":"kuberntes","permalink":"https://westerndevs.com/tags/kuberntes/"}]},{"title":"The Great RS-232 Adventure","authorId":"simon_timms","slug":"AndroidSerialPorts","date":"2017-05-11 23:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/AndroidSerialPorts/","link":"","permalink":"https://westerndevs.com/development/AndroidSerialPorts/","excerpt":"Talking over the RS-232 serial protocol is a bit of a blast from the past but I needed to use in on an Android tablet from within Xamarin. This is the, painfully complete, story of my journey.","raw":"---\nlayout: post\ntitle: The Great RS-232 Adventure\ntags:\n  - xamarin\n  - c#\ncategories:\n  - development\nauthorId: simon_timms\ndate: 2017-05-11 19:36:36\nexcerpt: Talking over the RS-232 serial protocol is a bit of a blast from the past but I needed to use in on an Android tablet from within Xamarin. This is the, painfully complete, story of my journey.\n---\n\nUh oh, where did the post go? Well turns out that my former employer believe that serial ports are in some way proprietary information. They've asked me to take this post down. I don't believe anything in this was secret or proprietary but I also don't like being sued. Sorry, I guess you'll have to figure out RS-232 yourself.  ","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"xamarin","slug":"xamarin","permalink":"https://westerndevs.com/tags/xamarin/"},{"name":"c#","slug":"c","permalink":"https://westerndevs.com/tags/c/"}]},{"title":"Transparency","slug":"Transparency","date":"2017-05-09 23:25:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Transparency/","link":"","permalink":"https://westerndevs.com/podcasts/Transparency/","excerpt":"Wherein the Western Devs determine if, when, and how companies should publish salary and diversity numbers","raw":"---\nlayout: podcast\ntitle: Transparency\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-transparency.mp3\n  length: '42:02'\n  filesize: 38458238\n  libsynId: 5325631\n  anchorFmId: Transparency-evqdj1\nparticipants:\n  - kyle_baley\n  - donald_belcham\n  - darcy_lussier\n  - dave_paquette\n  - james_chambers\n  - lori_lalonde\n  - rob_windsor\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\n  - title: Introduction\n    artist: Acapela Group\n    url: 'http://www.acapela-group.com/'\nlinks:\n  - \"University of Waterloo wage gap|https://www.thestar.com/opinion/editorials/2016/08/08/university-of-waterloo-must-resolve-the-root-causes-of-the-pay-gap-between-male-and-female-faculty-members-editorial.html\"\n  - \"Transparency at Buffer|https://buffer.com/transparency\"\n  - \"PayScale|http://www.payscale.com/\"\n  - \"GlassDoor|https://www.glassdoor.com\"\n  - \"Tech companies join White House diversity pledge|http://thehill.com/policy/technology/284517-white-house-pressures-30-tech-companies-to-sign-diversity-pledge\"\n  - \"Sunshine list|https://en.wikipedia.org/wiki/Sunshine_list\"\n  - \"Laurier raises 152 womenâ€™s salaries after equity report|http://www.therecord.com/news-story/7294215-laurier-raises-152-women-s-salaries-after-equity-report/\"\ndate: 2017-05-09 19:25:00\nrecorded: 2016-08-12\nexcerpt: \"Wherein the Western Devs determine if, when, and how companies should publish salary and diversity numbers\"\n---\n\n### Synopsis\n\n* University of Waterloo salary adjustment\n* Transparency as a means to resolve the gender gap\n* Problems that go away when salaries are publicized\n* How do you account for high performers who demand more?\n* Pros and cons of salary bands\n* The White House diversity pledge\n* Preparing for transparency\n* Why are North Americans so squeamish about discussing salary?\n* Advantages for companies not having to compete on salary\n* The effect of an information-based economy vs. a widget-based economy\n* \"All else being equal\"\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"JSON.net not just for serialization","authorId":"simon_timms","slug":"Json.net-not-just-for-sereialization","date":"2017-05-03 15:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"json/Json.net-not-just-for-sereialization/","link":"","permalink":"https://westerndevs.com/json/Json.net-not-just-for-sereialization/","excerpt":"If you happen to head over to https://www.nuget.org/packages and look at which package has been downloaded the most there is a clear winner: JSON.net. It is in everything, every where. JSON is so wildly ubiquitous that I play a little game with myself when I start a new project: how long can I go before I need to serialize or deserialize JSON and need to pull in JSON.net. I rarely last more than a couple of hours. But it turns out that there is a lot more that JSON.net can do.","raw":"---\nlayout: post\ntitle: JSON.net not just for serialization \ntags:\n  - json.net\n  - c#\ncategories:\n  - json   \nauthorId: simon_timms\ndate: 2017-05-03 11:36:36\n---\n\nIf you happen to head over to [https://www.nuget.org/packages](https://www.nuget.org/packages) and look at which package has been downloaded the most there is a clear winner: JSON.net. It is in everything, every where. JSON is so wildly ubiquitous that I play a little game with myself when I start a new project: how long can I go before I need to serialize or deserialize JSON and need to pull in JSON.net. I rarely last more than a couple of hours.\n\nBut it turns out that there is a lot more that JSON.net can do.\n\n<!-- more -->\n\nMy good buddy [Eric Fleming](https://ericflemingblog.wordpress.com/) found this one and I'm really just stealing it from him(although [James](http://jameschambers.com/) claims he found it). The problem that we were trying to solve was that we wanted to patch together a new JSON object out of a bunch of C# objects. It could have been done by building a new DTO, mapping a number of objects to it and then serializing it to JSON. This was kind of a lot of work. Static languages are nice but chucking together ad hoc objects isn't a strong suit. In this case we used JObject to structure the new object\n\n```csharp\nclass Program\n{\n    static void Main(string[] args)\n    {\n        var sock = new Sock{\n            Colour = \"blue\",\n            Size = \"medium\"\n        };\n        var shoe = new Shoe{\n            Material = \"leather\"\n        };\n        var ensemble = JObject.FromObject(sock);\n        ensemble.Merge(JObject.FromObject(shoe));\n        Console.WriteLine(ensemble.ToString());\n    }\n}\n\nclass Sock{\n    public string Colour {get; set;}\n    public string Size {get; set;}\n}\n\nclass Shoe{\n    public string Material{get; set;}\n}\n\n```\n\nThe output looks like\n\n```json\n{\n  \"Colour\": \"blue\",\n  \"Size\": \"medium\",\n  \"Material\": \"leather\"\n}\n```\n\nThis approach can be useful in a number of scenarios\n\n - Treating an object as a mixin and applying it to a bunch of differently shaped JSON\n - Merging existing JSON with C# objects\n\nThe latter scenario can be achieved like so \n\n```csharp\nvar hatJObject = JObject.Parse(@\"\n                {\n                    'HatSize': 'Large'\n                }\n            \");\nvar ensemble = JObject.FromObject(sock);\nvar shoeJObject = JObject.FromObject(shoe);\nshoeJObject.Merge(JObject.FromObject(shoeLace));\nensemble.Merge(shoeJObject);\nensemble.Merge(hatJObject);\n```\n\nThis outputs\n\n```json\n{\n  \"Colour\": \"blue\",\n  \"Size\": \"medium\",\n  \"Material\": \"leather\",\n  \"LaceLength\": 30,\n  \"HatSize\": \"Large\"\n}\n```\n\n\nThere are also `JObject.Load` and `JObject.Read` for reading from JSON streams.\n\nNewtonsoft.JSON is such a well known and well developed library that it is a shame to just use `JsonConvert` methods when there is such additional richness. ","categories":[{"name":"json","slug":"json","permalink":"https://westerndevs.com/categories/json/"}],"tags":[{"name":"c#","slug":"c","permalink":"https://westerndevs.com/tags/c/"},{"name":"json.net","slug":"json-net","permalink":"https://westerndevs.com/tags/json-net/"}]},{"title":"Conquest April 2017 Devblog","authorId":"david_wesst","slug":"Conquest-April-2017-Devblog","date":"2017-05-02 12:31:41+0000","updated":"2017-05-02 12:31:41+0000","comments":true,"path":"devblog/Conquest-April-2017-Devblog/","link":"","permalink":"https://westerndevs.com/devblog/Conquest-April-2017-Devblog/","excerpt":"This is the April 2017 update for my video game project I call &quot;Conquest&quot;.","raw":"---\nlayout: post\ntitle: Conquest April 2017 Devblog\ntags:\n  - conquest\n  - ink\n  - vsts\n  - itch.io\n  - javascript\ncategories:\n  - devblog\ndate: 2017-05-02 08:31:41\nupdated: 2017-05-02 08:31:41\nauthorId: david_wesst\noriginalurl: \"https://blog.davidwesst.com/2017/05/Conquest-April-2017-Devblog/\"\n---\n\nThis is the April 2017 update for my video game project I call \"Conquest\".\n\n<!-- more -->\n\nThis month, I have continued to make progress on my game project. The unexpected thing that happened was the fact that I decided to take a step back and do some research and development before continuing forward with the game idea I have in mind.\n\n## Status Update\n\nHere's the thing: my game project is too big. At least for now, it's too big, and I don't have enough experience and knowledge to be co\nfortable taking this idea and turning it into a game.\n\nIt's my first original video game, after all, and if you watch or read game design resources like [Extra Credits](https://www.youtube.com/user/ExtraCreditz), you'll often hear that you should start small and move up from there.\n\nSo that's what I did. And I made [Breakout](https://github.com/davidwesst/breakout) by following [this MDN](https://developer.mozilla.org/en-US/docs/Games/Tutorials/2D_Breakout_game_pure_JavaScript) tutorial.\n\n![](http://i.imgur.com/TnoJp0Gm.png)\n\nI know that this isn't going to blow anyone's mind, but it's first game I've made with nothing but vanilla JavaScript and let me get familiar with the basics of JavaScript game development, without blindly relaying on a framework.\n\nI intend on continuing with this breakout game for another month as I prepare for [Prairie Dev Con](http://prairiedevcon.com/) to add some polish, clean up the code, and maybe add a few gameplay elements that I worked on this month.\n\nBut now, onto the update.\n\n### What I've Done\n\nLike last time, I'll keep it short and in bullet points:\n\n+ Development\n    + Did an automated deployment to [Itch.io](https://itch.io/) using Powershell and VSTS\n    + Experimented with the [Ink](http://www.inklestudios.com/ink/) dialogue system and found how to integrate into build\n    + Taken a step back on Conquest, in lieu of more R&D through smaller games\n    + Created [Breakout](https://github.com/davidwesst/breakout) with vanilla ES6 JavaScript, complete with a transpiler and SystemJS modules\n        + Thanks [Chris](https://github.com/chrinkus/) for the suggestion and [Chris Love from Love2Dev](https://love2dev.com/) for providing constructive feedback regarding framework dependent developers.\n    + Setup my Vim development environment to be extra cool. \n+ Design\n    + Met with an _actual video game writer_ to talk about the best way to start including narrative and dialogue into a game\n        + Thanks [R. Morgan Slade](http://www.rmorganslade.ca/) for taking the time and providing some really good feedback and insight\n    + Started migrating some design elements from Conquest into Breakout\n\n### What I've Faced\n\nThis month, the big thing I faced was the realization that my project is too big for me. It's not that I don't think I would finish it eventually. It's that I don't think I have the skills to make the game fun when I'm done with it.\n\nI also realized that I am too dependent on frameworks when it comes to game development. It's not that I don't think frameworks have a place or that I'll eventually use one or more of them in my game. It's that I don't know what the framework brings to the table other than an abstraction in development.\n\nThere are plenty of game design tools with full UI's that remove the need to code everything from the ground up, but since I've opted to go the code-focused route because that's what I know best, I should probably know a bit more about the layers code before I start abstracting them away.\n\n### Where I'm Going\n\nMay is going to be busy with Prarie Dev Con happening in June, but that won't stop me from working on Conquest. I should also document these discoveries I make a little more, so I'll be doing that through the blog.\n\nThat being said, my plan is to focus on polishing up Breakout by using some of the planned features for Conquest in Breakout. They might not work all that well in that game, but the goal of Breakout isn't to make it a hit, but to experiment with these systems I have planned for Conquest.\n\nTo summarize, here's the plan:\n\n+ Refactor Breakout to have a cleaner code base (i.e. modules, objects, etc...)\n+ Share my VSTS game development discoveries via my blog\n+ Add some polish to Breakout to complete it\n+ Prepare my demos for [Prairie Dev Con](http://prairiedevcon.com/) using Breakout as the demo project\n\n## Conclusion\n\nWhat I've concluded this month is that I need to make games that match my skills as software developer. For that reason, I'm going to focus on learning the guts of the JavaScript by improving my vanilla JS Breakout game.\n\nAll in all, this month has been quite the shift in direction. I went from making one big game, to making one small game that has nothing to do with the original. It's been challenging, but in a good way. Now I can move forward with developing these systems in smaller pieces, refine them, and eventually recombine them into my original game design.\n\nSee you next month.\n","categories":[{"name":"devblog","slug":"devblog","permalink":"https://westerndevs.com/categories/devblog/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"conquest","slug":"conquest","permalink":"https://westerndevs.com/tags/conquest/"},{"name":"ink","slug":"ink","permalink":"https://westerndevs.com/tags/ink/"},{"name":"itch.io","slug":"itch-io","permalink":"https://westerndevs.com/tags/itch-io/"}]},{"title":"Working From Home","slug":"working-from-home","date":"2017-04-23 20:43:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/working-from-home/","link":"","permalink":"https://westerndevs.com/podcasts/working-from-home/","excerpt":"Yeah, so Pokemon Go was still a thing when we originally recorded this. Add procrastination to the list of hazards for working from home.","raw":"---\nlayout: podcast\ntitle: Working From Home\ncategories:\n  - podcasts\ncomments: true\nalias: /podcasts/podcast/\ndate: 2017-04-23 16:43:26\nrecorded: 2016-07-15 12:00:00\npodcast:\n  filename: westerndevs-working-from-home.mp3\n  length: '42:36'\n  filesize: 40115726\n  libsynId: 5291799\n  anchorFmId: Working-From-Home-evqdji\nparticipants:\n  - lori_lalonde\n  - donald_belcham\n  - simon_timms\n  - rob_windsor\n  - dave_woods\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Working from home and walking to work|http://jameschambers.com/2015/03/working-from-home-and-walking-to-work-surviving-remote-work/\"\n  - \"How we work remotely at Particular Software|https://www.infoq.com/presentations/remote-work-particular-software\"\n  - \"Making working from home work for you|https://www.pluralsight.com/courses/make-work-from-home-work\"\n  - \"Is Daddy on a call?|https://www.hanselman.com/blog/IsDaddyOnACallABusyLightPresenceIndicatorForLyncForMyHomeOffice.aspx\"\n  - \"Why working from home is both awesome and horrible|http://theoatmeal.com/comics/working_home\"\nexcerpt: Yeah, so Pokemon Go was still a thing when we originally recorded this. Add procrastination to the list of hazards for working from home.\n---\n\n### Synopsis\n\n* Round table: Who works from home?\n* What hours do you keep?\n* Handling meetings\n* Separating work time from home time\n* The importance of a routine\n* Interruptions\n* Ensuring you put in the hours\n* Do you account for water cooler time?\n* Remote friendly vs. remote first organizations\n* Training your family\n* Are you lonely?\n* Getting out of the house\n* Competing globally\n* Communication\n* Do you have to work from _home_?\n* Vacation\n* Co-working spaces\n* Staying focused\n* Do you even eat (healthy), bro?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"The Cloud Is The Internet","authorId":"darcy_lussier","slug":"The-Cloud-Is-The-Internet","date":"2017-04-22 15:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"cloud/azure/The-Cloud-Is-The-Internet/","link":"","permalink":"https://westerndevs.com/cloud/azure/The-Cloud-Is-The-Internet/","excerpt":"Let's reset our definitions and thinking around what cloud computing and cloud service providers are.","raw":"---\nlayout: post\ntitle: The Cloud Is The Internet\ncategories:\n  - cloud\n  - azure\ndate: 2017-04-22 11:30:00\nexcerpt: Let's reset our definitions and thinking around what cloud computing and cloud service providers are.\ncomments: true\nauthorId: darcy_lussier\n---\n\nA number of years ago someone made this sticker and it became popular on the internet:\n\n&nbsp;\n\n![][1]\n\nIt spoke to everyone's frustration with the failure of marketing and sales to properly communicate what \ncloud computing was and how platforms like Azure and AWS fit into this new computing paradigm. In this post I want to give a fresh perspective on what cloud computing is and what we can do with it.\n\n### Defining Cloud\n\nMicrosoft defines the cloud this way:\n\n>**\"Simply put, cloud computing is the delivery of computing servicesâ€”servers, storage, databases, networking, software, analytics, and moreâ€”over the Internet (â€œthe cloudâ€).\"**\n\nCloud computing is leveraging the internet in new ways. We've been evolving cloud computing since the internet existed.\n\nConsider the earliest days - the dawn of the World Wide Web that allowed us to post static pages of information that was discoverable over the internet. Soon\nsearch providers emerged, making information even easier to find; these were the first cloud applications.\n\nWe began adding dynamic aspects to web development. Technologies like Classic ASP, Java, PHP, and JavaScript emerged that opened up the internet to a new era of\ninformation sharing and ecommerce. Messaging and chat applications emerged and while they were still installed locally on the client pc, it was the internet\nthat provided the ability to communicate.\n\nMore complex web frameworks emerged like ASP.NET, Cold Fusion, Java Server Pages, and others. The new scenarios they introduced required new connectivity options,\nand we saw service frameworks like WCF and REST emerge to better communicate over the internet.\n\nMore recently we've seen how client side libraries and asynchornous programming is changing internet computing again. Alongside that, \ncloud platforms like Azure and AWS have emerged offering evolved services from the simple data-transfer ones.\n\nSo all of this is to say: Cloud computing is the evolution of using the internet as a development platform.\n\n### Cloud Service Providers\n\nNow that we've defined what cloud is, how does this change our view of what Azure, AWS, and other cloud service providers are? Microsoft provides us with\na great definition of Azure, which we can apply to the others:\n\n>**Microsoft Azure is a growing collection of integrated cloud services that developers and IT professionals use to build, deploy, and manage applications through Microsoftâ€™s global network of datacenters.**\n\nThis is an important distinction. Azure itself is not a cloud; its cloud enabled services (meaning it relies on the internet) and they happen to use a\nhuge number of large datacenters to operate those services.\n\nWhat differentiates cloud service platforms from previous cloud computing implementations (like web mail or web app hosting) are certain tenets that cloud service platforms share:\n\n* Reduced Costs - You aren't buying the servers or network components or running the datacenters yourself\n\n* Speed â€“ Able to provision resources in minutes, capacity planning becomes administrative instead of managing physical assets\n\n* Global Scale â€“ Scale elastically and provide the right amount of IT resources when its needed from the right geographical location\n\n* Productivity â€“ Removes mundane IT activities, focuses on high value activities\n\n* Performance â€“ Worldwide networks that offer benefits like reduced network latency and greater economies of scale\n\n* Reliability â€“ Data backup, disaster recovery, and business continuity easier and less expensive\n\n\n### The Cloud Enables our Applications\n\nWhen we shift our view of cloud away from the physical datacenters and infrastructure powering cloud service providers and to the original meaning of cloud - the internet - then things make a lot more sense.\nCloud computing has been changing the way we work, play, and connect ever since the internet was invented. We're just now seeing how far the cloud service providers are able to push the envelope.\n\nThere is a cloud, and it enables our applications to do amazing things!\n\n&nbsp;\n\n![][2]\n\n[1]: https://darcyblogimages.blob.core.windows.net/wdimages/ThereIsNoCloud.png\n[2]: https://darcyblogimages.blob.core.windows.net/wdimages/ThereIsACloud.png\n\n\n","categories":[{"name":"cloud","slug":"cloud","permalink":"https://westerndevs.com/categories/cloud/"},{"name":"azure","slug":"cloud/azure","permalink":"https://westerndevs.com/categories/cloud/azure/"}],"tags":[]},{"title":"Using azure-cli in windows bash","authorId":"simon_timms","slug":"az_command_line-on_windows_bash","date":"2017-04-19 05:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"messaging/az_command_line-on_windows_bash/","link":"","permalink":"https://westerndevs.com/messaging/az_command_line-on_windows_bash/","excerpt":"The latest versions of Windows support running linux executables. The technical trickery to get that done boggle my mind. I wanted to get the Azure command line tools working inside of the bash. The tools are written in python so we need to get that installed.","raw":"---\nlayout: post\ntitle: Using azure-cli in windows bash\ntags:\n  - azure\n  - windows bash\ncategories:\n  - messaging   \nauthorId: simon_timms\ndate: 2017-04-19 01:36:36\n\n---\n\nThe latest versions of Windows support running linux executables. The technical trickery to get that done boggle my mind. I wanted to get the Azure command line tools working inside of the bash. The tools are written in python so we need to get that installed.\n\n<!-- more -->\n\n```\n   sudo apt-get install python-pip python-dev libffi-dev libssl-dev libxml2-dev libxslt1-dev zlib1g-dev\n```\nThis also installs some build tools which we'll need to install the actual azure-cli and pip which is kind of nuget for python. Now we just need to install the tools\n\n```\n   pip install --user azure-cli\n```\n\nThis will install the tools to `~/.local/bin`. You might need to add that to your path or at least reload the profile by running `. ~/.profile`. Now you can login with \n\n```\naz login\n```\n\nThis will give you a code to enter in a browser which will complete you login and Bob's your uncle. Because python is portable this could all be done on Windows as well but I'm still more comfortable scripting against bash than powershell.  You can read more about az and all the sub-commands like `az acr` at https://docs.microsoft.com/en-us/cli/azure/overview. I'll probably also post some more content on it soon. ","categories":[{"name":"messaging","slug":"messaging","permalink":"https://westerndevs.com/categories/messaging/"}],"tags":[{"name":"azure","slug":"azure","permalink":"https://westerndevs.com/tags/azure/"},{"name":"windows bash","slug":"windows-bash","permalink":"https://westerndevs.com/tags/windows-bash/"}]},{"title":"Introducing My Game Project - March 2017 Devblog","authorId":"david_wesst","slug":"Conquest-March-2017-Devblog","date":"2017-04-03 10:00:00+0000","updated":"2017-04-03 10:00:00+0000","comments":true,"path":"devblog/Conquest-March-2017-Devblog/","link":"","permalink":"https://westerndevs.com/devblog/Conquest-March-2017-Devblog/","excerpt":"This is the first of monthly status update posts on my video game project I call Conquest.","raw":"---\nlayout: post\ntitle: \"Introducing My Game Project - March 2017 Devblog\"\ntags:\n  - conquest\n  - typescript\n  - vsts\n  - phaser\ncategories:\n  - devblog\ndate: 2017-04-03 06:00:00\nupdated: 2017-04-03 06:00:00\noriginalurl: \"https://blog.davidwesst.com/2017/04/Conquest-March-2017-Devblog/\"\nauthorId: david_wesst\nexcerpt: This is the first of monthly status update posts on my video game project I call Conquest.\n---\n\nI've wanted to make a video game since I was very young. It got me into programming during my university career, and is something of a passion of mine now that I'm older and more of a seasoned developer.\n\nI've toyed and tinkered with different game ideas and technologies through the years, but never really got anything done. At the beginning of this year, I started working on something I decided would be my first \"real\" game. I'm not sure what it is yet, but it's turning into something after three month of development and design in my spare time.\n\nToday, I'm going to share a bit about it with you, the public, for the first time.\n\n## Introducing \"Conquest\"\nThis game is something of a life long conquest for me, but that isn't where the name comes from. This game started out as one idea where conquest made a lot of sense, but has transformed in a few ways over the past month to something completely different. I've been rolling with it and I like where it's headed now, but the name is going to stay the same until I get something a little more locked down.\n\nIn the meantime, you can take a look the [first official screenshots](http://imgur.com/a/x7eGr):\n\n![](http://i.imgur.com/dqEGoFf.png)\n\nIt's not much to look at, but this is just the beginning.\n\nThe goal is to get the gameplay loop and core systems down, based on the bit of design I have in mind. I've been doing pitch sessions with my SO (significant other), who has helped keep me and the multitude of ideas in check. Graphics and polish will come later, but for now it's all about the gameplay.\n\nBut enough of that, let's get status update.\n\n## Status Update\nThe point of these posts is to try and help me reflect on what I've done, what issues I've faced, and where I'm going from here. Think of it as a sort of sprint review. Although this is three months of effort, I have started planning month long iterations where each iteration will end with a devblog post.\n\nFor those wondering, I use Visual Studio Team Services for that planning, but I'll discuss that in future regularily scheduled blog posts.\n\n### What I've Done\nOver the past three months, I've done quite a bit, but I'll keep it brief with bullet points.\n\n+ Development\n    + Selected [Phaser](http://phaser.io/) as the base game framework\n    + Implemented signals for game events, triggered through timers and through player interaction\n    + Implemented in-game time\n    + Implemented map metadata layer\n    + Setup issue and bug tracking in [VSTS](http://phaser.io/)\n    + Setup contiuous integration and deployment to Itch.io using VSTS\n+ Design\n    + Did a \"pitch\" to solidify game idea and core gameplay concepts\n    + Setup a map design workflow using [Tiled](http://www.mapeditor.org/) map editor\n\nThe gist of it is that I've focused on figuring out what sort of game I want to make by focusing on the things I already know: the tech.\n\n### What I've Faced\nPlenty.\n\nI'll be more specific in future posts, but most of everything I've faced over these past three months has been around discovery and learning how to do basic game development. By using TypeScript (with it's definition files) and Phaser as my foundation, I've been moving pretty quickly and learning something new every time I sit down to work on the game. \n\nThe other challenge I've is figuring out where to draw the line between development and design. At this point, I feel like I know where the line is and actually know that there _is_ a line between them. The challenge is making sure that I make sure to keep progress happening in both streams. Development work is familiar to me compared to design and ultimately more of a comfort zone for me to fall back on.\n\nThe problem is that without design, I'm just building game technology without purpose. How do I know what systems to build if I don't know what kind of game I'd like to build?\n\nI've gotten much better at this over the course of February and March, and intend on keeping that going in future iterations by making sure that the number of dev and design issues are balanced each month.\n\n### Where I'm Going\nFor the next iteration, I'm going to try and add two more systems to the game: a dialogue system, and an objective/goal system. There are more I'd like to add, but those two are the most critical. With these two in place I think I would have all the systems I need to the first part of the game playable and in front of players.\n\nFor the design side, I have a vision in mind for the first playable part. To build it, I need to design my first \"real\" map, complete with metadata and a story that is told through interaction with the map. The other thing  will be to make sure that I use the _existing_ systems I've developed to tell the story through the map, rather that defining new systems.\n\nIn summary, the goals for the sprint are:\n\n+ Development\n    + Display scripted dialogue in-game\n    + Include objective for the player to accomplish\n+ Design\n    + Map of first playable section that tells a story through interaction\n    + Leverage the each of the existing systems in the map to aid in story telling\n\n## Conclusion\nThis post is the first of many. They will be monthly, and have more specific content about progress.\n\nFor a first post, this is pretty light on the details. That's mainly because there aren't a lot of details to share just yet. For now, all I can say is that I plan on continuing to blog about both the technical and the design challenges that I face along the way. Hopefully, over the next few posts, I will have something for you to play.\n\nUntil next month.\n","categories":[{"name":"devblog","slug":"devblog","permalink":"https://westerndevs.com/categories/devblog/"}],"tags":[{"name":"typescript","slug":"typescript","permalink":"https://westerndevs.com/tags/typescript/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"phaser","slug":"phaser","permalink":"https://westerndevs.com/tags/phaser/"},{"name":"conquest","slug":"conquest","permalink":"https://westerndevs.com/tags/conquest/"}]},{"title":"Getting Started with RabbitMQ in ASP.NET","authorId":"simon_timms","slug":"MassTransit_and_ASP","date":"2017-03-19 05:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"messaging/MassTransit_and_ASP/","link":"","permalink":"https://westerndevs.com/messaging/MassTransit_and_ASP/","excerpt":"In the last post we looked at how to set up RabbitMQ in a Windows container. It was quite the adventure and I'm sure it was woth the time I invested. Probably. Now we have it set up we can get to writing an application using it. A pretty common use case when building a web application is that we want to do some background processing which takes longer than we'd like to keep a request open for. Doing so would lock up an IIS thread too, which ins't optimal. In this example we'd like to make our user creation a background process.","raw":"---\nlayout: post\ntitle: Getting Started with RabbitMQ in ASP.NET\ntags:\n  - .net\n  - RabbitMQ\ncategories:\n  - messaging   \nauthorId: simon_timms\ndate: 2017-03-19 01:36:36\noriginalurl: https://aspnetmonsters.com/2017/03/2017-03-18-RabbitMQ%20from%20ASP/\n---\n\nIn the last post we looked at how to set up RabbitMQ in a Windows container. It was quite the adventure and I'm sure it was woth the time I invested. Probably. Now we have it set up we can get to writing an application using it. \n\nA pretty common use case when building a web application is that we want to do some background processing which takes longer than we'd like to keep a request open for. Doing so would lock up an IIS thread too, which ins't optimal. In this example we'd like to make our user creation a background process.\n\n<!-- more -->\n\nTo start we need a command which is just a plain old CLR object\n\n```\npublic class AddUser\n{\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n    public string Password { get; set; }\n    public string EmailAddress { get; set; }\n}\n```\n\nThat all looks pretty standard. In our controller, we'll just use the handy UserCreationSender\n\n```\npublic class HomeController : Controller\n{\n    IUserCreationSender _userCreationSender;\n    public HomeController(IUserCreationSender userCreationSender)\n    {\n        _userCreationSender = userCreationSender;\n    }\n\n    public IActionResult Index()\n    {\n        _userCreationSender.Send(\"simon\", \"tibbs\", \"stimms@gmail.com\");\n        return View();\n    }\n}\n```\n\nThere that was easy. In our next post, we'll... what's that? I've missed actually showing any implementation. Fair point, we can do that. \n\n```\npublic void Send(string firstName, string lastName, string emailAddress)\n{\n    var factory = new ConnectionFactory()\n    {\n        HostName = \"172.22.144.236\",\n        Port = 5672,\n        UserName = \"guest\",\n        Password = \"guest\"\n    };\n    using (var connection = factory.CreateConnection())\n    using (var channel = connection.CreateModel())\n    {\n        channel.QueueDeclare(queue: \"niftyqueue\",\n                                durable: false,\n                                exclusive: false,\n                                autoDelete: false,\n                                arguments: null);\n\n        var command = new AddUser\n        {\n            FirstName = firstName,\n            LastName = lastName,\n            EmailAddress = emailAddress,\n            Password = \"examplePassword\"\n        };\n        string message = JsonConvert.SerializeObject(command);\n        var body = Encoding.UTF8.GetBytes(message);\n\n        channel.BasicPublish(exchange: \"\",\n                                routingKey: \"niftyqueue\",\n                                basicProperties: null,\n                                body: body);\n    }\n}\n```\n\nValues here are hard coded which we don't want to do usually, check out https://aspnetmonsters.com/2016/01/Configuration-in-ASP-NET-Core-MVC/ for how to pull in configuration. Ignoring that we start by creating a conneciton factory with connection information for RabbitMQ. We then create a new queue (or ensure that it already exists) called \"niftyqueue\". There are some other parameters in the queue creation we can get into in a future article. \n\nNext we'll create an AddUser command and serialize it to JSON using good old Json.net then get the bytes. Rabbit messages contain a byte array so we have to do a tiny bit of leg work to get our CLR object into a form usable by the transport. JSON is the standard for everything these days so we'll go with the flow. In a real system you might want to investigate Protocol Buffer or something else. \n\nFinally we perform a basic publish, sending our message. The Rabbit management site provides a super cool view of the messages being published on it\n\n![The dashboard](http://i.imgur.com/odiUxPh.png)\n\nHow cool is that? Man I like real time charts. \n\nShoving messages into the bus is half the equation, the other half is getting it out again. We want to have a separate process handle getting the message. That looks quite similar to the message sending.\n\n```\npublic static void Main(string[] args)\n{\n    Console.WriteLine(\"starting consumption\");\n    var factory = new ConnectionFactory()\n    {\n        HostName = \"172.22.144.236\",\n        Port = 5672,\n        UserName = \"guest\",\n        Password = \"guest\"\n    };\n    using (var connection = factory.CreateConnection())\n    using (var channel = connection.CreateModel())\n    {\n        channel.QueueDeclare(queue: \"niftyqueue\",\n                                durable: false,\n                                exclusive: false,\n                                autoDelete: false,\n                                arguments: null);\n\n        var consumer = new EventingBasicConsumer(channel);\n        consumer.Received += (model, ea) =>\n        {\n            var body = ea.Body;\n            var message = Encoding.UTF8.GetString(body);\n            var deserialized = JsonConvert.DeserializeObject<AddUser>(message);\n            Console.WriteLine(\"Creating user {0} {1}\", deserialized.FirstName, deserialized.LastName);\n        };\n        channel.BasicConsume(queue: \"niftyqueue\",\n                                noAck: true,\n                                consumer: consumer);\n\n        Console.WriteLine(\"Done.\");\n        Console.ReadLine();\n    }\n}\n```\n\nAgain we create the factory and the queue (some opportunity there for refactoring, me thinks). Next we start up an EventingBasicConsumer on top of the channel. There are a couple of different ways to consume messages none of which I really love. The eventing model seem the leas objectionable. You simply assign a delegate to the event handler and it will fire when a message is recieved. \n\nIn the next post I'll start taking a look at how we can layer [MassTransit](http://masstransit-project.com/), a .NET message bus, on top of raw RabbitMQ. The result is a much more pleasant experience then simply hammering together raw RabbitMQ. \n\n","categories":[{"name":"messaging","slug":"messaging","permalink":"https://westerndevs.com/categories/messaging/"}],"tags":[{"name":".net","slug":"net","permalink":"https://westerndevs.com/tags/net/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://westerndevs.com/tags/RabbitMQ/"}]},{"title":"Creating a Rabbit MQ Container","authorId":"simon_timms","slug":"RabbitContainer","date":"2017-03-16 11:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"docker/RabbitContainer/","link":"","permalink":"https://westerndevs.com/docker/RabbitContainer/","excerpt":"I bought a new laptop, a Dell XPS 15 and my oh my is it snazzy. The thing I was most excited about was that I'd get to play with Windows containers again. I have 3 other machines in the house but they're either unsuitable for containers (OSX running Windows in parallels) or I've so toally borked them playing with early betas of containers they need to be formatted and reinstalled - possibly also thrown into the sun. So when I found myself presented with the question &quot;how can we get into messaging in our apps for free?&quot; I figured I'd crack open the laptop and build something with MassTransit. I found that MassTransit supports running on RabbitMQ. Why that sounds like a perfect opportunity to deploy RabbitMQ to a container. Only problem was that I didn't really know how to do that.","raw":"---\nlayout: post\ntitle: Creating a Rabbit MQ Container\ntags:\n  - docker\ncategories:\n  - docker   \nauthorId: simon_timms\ndate: 2017-03-16 07:36:36\noriginalurl: https://aspnetmonsters.com/2017/03/2017-03-09-rabbitmq/\n---\n\nI bought a new laptop, a Dell XPS 15 and my oh my is it snazzy. The thing I was most excited about was that I'd get to play with Windows containers again. I have 3 other machines in the house but they're either unsuitable for containers (OSX running Windows in parallels) or I've so toally borked them playing with early betas of containers they need to be formatted and reinstalled - possibly also thrown into the sun.\n\nSo when I found myself presented with the question \"how can we get into messaging in our apps for free?\" I figured I'd crack open the laptop and build something with MassTransit. I found that MassTransit supports running on RabbitMQ. Why that sounds like a perfect opportunity to deploy RabbitMQ to a container. Only problem was that I didn't really know how to do that. \n\n<!-- more -->\n\nIn my heart I felt like running the installer wasn't quite the right way to go. I'd just copy the installation file into their destination. Problem is that RabbitMQ relies on erlang so I'd have to install that too. I build a docker file which looked something like \n\n```\nFROM microsoft/windowsservercore\nENV rabbitSourceDir \"RabbitMQ Server\"\nENV erlngDir \"C:/program files/erl8.2/\"\nENV rabbitDir \"C:/program files/RabbitMQ Server/\"\nADD ${rabbitSourceDir} ${rabbitDir}\nADD erl8.2 ${erlngDir}\nENV ERLANG_HOME \"c:\\program files\\erl8.2\\erts-8.2\"\n```\n\nIn the erlngDir and rabbitDir I dumped the contents of an install of erlang and rabbitmq. Then I built the container with \n\n`docker build -t monsters/rabbitmq .`\n\nDidn't work. There must be something useful the installer actually does as part of installing files. So next I considered putting in the installers and running them when building the container. That seemed like a huge pain so I got to thinking about using chocolatey. At first I was pretty deadset against using choco my reasoning being that containers should be lightweight and have only one purpose. Having one time software like chocolatey on there which wouldn't ever be used seemed like it would make... whoever invented containers mad. \n\nSo attempt number two:\n\n```\nFROM microsoft/windowsservercore\n\n#install chocolatey\nRUN @powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n\n#install rabbitmq\nRUN choco install -y rabbitmq\n\n```\n\nThat was enough to get Rabbit MQ installed. I still needed to expose some ports for RabbitMQ so I added \n\n```\n####### PORTS ########\n#Main rabbitmq port\nEXPOSE 5672\n#port mapper daemon (epmd)\nEXPOSE 4369\n#inet_dist_listen\nEXPOSE 35197\n#rabbitmq management console\nEXPOSE 15672\n```\n\nRabbit also likes to know where Erlang lives so some environmental variables for that aren't going to hurt. \n\n```\n#set the home directory for erlang so rabbit can find it easily\nENV ERLANG_HOME \"c:\\program files\\erl8.2\\erts-8.2\"\nENV ERLANG_SERVICE_MANAGER_PATH \"c:\\program files\\erl8.2\\erts-8.2\"\n```\n\nWe could forward the RabbitMQ ports to our local machine but I like the idea of using the container as if it were a distinct machine so let's also enable the management UI from anywhere on the network. To do that we'll replace the default config file with one that has \n\n```\n{loopback_users, []},\n```\n\nin it. We can copy our new config file over the one in the container from the dockerfile and set up a variable to point Rabbit at it.\n\n```\nENV RABBITMQ_CONFIG_FILE \"C:\\rabbitmq\"\nCOPY [\"rabbitmq.config\",\" C:/\"]\n```\n\nThe config file looks like\n\n```\n[{rabbit, [{loopback_users, []}]}].\n```\n\nFinally we'll start the actual rabbit process as the default action of the container\n\n```\nENV RABBIT_MQ_HOME \"C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.5\"\nCMD \"${RABBIT_MQ_HOME}/sbin/rabbitmq-server.bat\"\n```\n\nNow you can log into the management portal using the guest/guest account.\n\n![Admin login](http://i.imgur.com/KvDVTb9.png)\n\nIt takes quite a while to start up the container and it took me close to 40 years to figure out building the container but it does save me installing rabbitmq on my local machine and makes experimenting with multiple instances pretty jolly easy.\n\nThe complete docker file is here:\n\n```\nFROM microsoft/windowsservercore\n\n####### PORTS ########\n#Main rabbitmq port\nEXPOSE 5672\n#port mapper daemon (epmd)\nEXPOSE 4369\n#inet_dist_listen\nEXPOSE 35197\n#rabbitmq management console\nEXPOSE 15672\n\n#set the home directory for erlang so rabbit can find it easily\nENV ERLANG_HOME \"c:\\program files\\erl8.2\\erts-8.2\"\nENV ERLANG_SERVICE_MANAGER_PATH \"c:\\program files\\erl8.2\\erts-8.2\"\n\n#install chocolatey\nRUN @powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n\n#install rabbitmq\nRUN choco install -y rabbitmq\n\n#set up the path to the config file\nENV RABBITMQ_CONFIG_FILE \"C:\\rabbitmq\"\n\n#copy a config file over\nCOPY [\"rabbitmq.config\",\" C:/\"]\n\n#set the startup command to be rabbit\nCMD [\"C:/Program Files/RabbitMQ Server/rabbitmq_server-3.6.5/sbin/rabbitmq-server.bat\"]\n\n```\n\nIn my next post I'll get around to actually using Rabbit MQ because all the yaks are shaved now... I hope.","categories":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/tags/docker/"}]},{"title":"How to Compile TypeScript into a Single File with SystemJS Modules with Gulp","authorId":"david_wesst","slug":"How-to-Compile-TypeScript-into-a-Single-File-with-SystemJS-Modules-with-Gulp","date":"2017-03-14 13:47:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"javascript/How-to-Compile-TypeScript-into-a-Single-File-with-SystemJS-Modules-with-Gulp/","link":"","permalink":"https://westerndevs.com/javascript/How-to-Compile-TypeScript-into-a-Single-File-with-SystemJS-Modules-with-Gulp/","excerpt":"I decided to move a TypeScript project from AMD modules (i.e. RequireJS) to SystemJS, still using Gulp. In this post, I walk you through the sample project I've created and share the lessons I learned along the way.","raw":"---\nlayout: post\ntitle: How to Compile TypeScript into a Single File with SystemJS Modules with Gulp\ncategories:\n  - javascript\ndate: 2017-03-14 09:47:22\ntags:\n  - javascript\n  - typescript\n  - systemjs\n  - modules\n  - gulpjs\nexcerpt: I decided to move a TypeScript project from AMD modules (i.e. RequireJS) to SystemJS, still using Gulp. In this post, I walk you through the sample project I've created and share the lessons I learned along the way.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2017/03/How-to-Compile-TypeScript-into-a-Single-File-with-SystemJS-Modules-with-Gulp/\n---\n\nI've been messing around with TypeScript again for my [game project](https://blog.davidwesst.com/2017/03/Intital-Thoughts-on-Using-Phaser/) and wanted a module loader to consume the single file produced by the TypeScript compiler. This time around I decided to use SystemJS and figured I'd share the lessons I learned along the way.\n\n##### Sample Project\nIf you're interested in playing with the code, you can checkout [this GitHub project](https://github.com/davidwesst/ts-systemjs) I setup just for that reason.\n\n##### Previous Post\nI also posted about doing the same sort of thing [with AMD and RequireJS](https://blog.davidwesst.com/2016/09/How-to-Compile-Typescript-into-a-Single-File-with-AMD-Modules/) complete with [a GitHub sample project](https://github.com/davidwesst/ts-project-template)\n\n## Project Breakdown\nHere's the gist of it. My project has the following requirements:\n\n1. Source code in TypeScript, organized in to multiple modules\n2. Load external modules into application as dependencies\n3. Transpile down to a single bundle file\n4. Load the bundle in the browser\n\nIt seems pretty straight forward, right? Plus, because I'm using TypeScript I figured this would be easy peezy lemon-squeezy with the [TypeScript compiler](https://www.typescriptlang.org/docs/handbook/compiler-options.html) and rich documentation.\n\nAs it turns out, it wasn't that simple.\n\n### Wait. Where's GulpJS?\nIt's in the sample project handling the transpiling the TypeScript through a task. \n\nIt's actually not required, but rather a convienience for keeping all my build tasks together. I just put it in the title, because it matches the previous post.\n\n## Problem 1: Using an External Module\nI wanted to use [Moment.js](https://momentjs.com/) to help handle date objects with my code.\n\nThere were two parts to this: \n\n* Getting it working in the development environment\n* Getting it bundled up with SystemJS.\n\n### Using it in Development\nI use [Visual Studio Code](https://code.visualstudio.com/), which is a great TypeScript development environment. \n\nNormally, you would use the [`@types`](https://www.npmjs.com/search?q=%40types) collection of defintion files from the NPM which is wired up by default. For Moment, we need to break that.\n\nThe definition file for Moment is found in the library itself. Since I use NPM to handle all my dependencies, you just set this up in your `tsconfig.json` file.\n\n![](http://i.imgur.com/TyAgU0N.png)\n\nThen, in code, we import it.\n\n```javascript\nimport moment from \"moment\";\n```\n\nRemember: if your project is already using `@types` definition files, you'll need to add that folder to the `typeRoots` collection yourself.\n\n### Bundling it Up\nBecause we're using SystemJS, we need to do is configure it as a path to understand where to find the library when it gets referenced.\n\nIn the [sample project](https://github.com/davidwesst/ts-systemjs), we do it in `script` tag on the HTML page, but you can do this in wherever you end up doing your SystemJS configuration.\n\n```javascript\nSystemJS.config({\n    \"paths\": {\n        \"moment\": \"https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.17.1/moment.min.js\"\n    }\n});\n```\n\n## Problem 2: Loading the Bundle\nMaking a bundle is easy. Consuming the bundle is something different.\n\n### Making a Bundle\nIf you're interested in bundling your code into a single file with the compiler, you're limited to AMD or SystemJS modules. This is configured in the `tsconfig.json` file included in [the sample project](https://github.com/davidwesst/ts-systemjs) with the module property. You can read more about it [here in the TypeScript Handbook](https://www.typescriptlang.org/docs/handbook/modules.html).\n\n![](http://i.imgur.com/gUGeHfI.png)\n\n### Consuming the Bundle\nThis is where I got stuck.\n\nNow I have this fancy bundle, but I need to figure out how to consume it in my HTML page. The solution is pretty simple, but it took some research and some tinkering, but I got there.\n\nTake a look at the `<body>` take of the HTML file:\n\n```html\n<body>\n    <div id=\"display\">\n        <!-- script will display content here -->\n    </div>\n\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/systemjs/0.20.9/system.js\"></script>\n    <script src=\"bundle.js\"></script>\n    <script>\n        SystemJS.config({\n            \"paths\": {\n                \"moment\": \"https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.17.1/moment.min.js\"\n            }\n        });\n\n        SystemJS.import(\"game\")\n            .then((module)=> {\n                let g = new module.Game(\"display\");\n                g.start();\n            })\n            .catch((error)=> {\n                console.error(error);\n            });\n    </script>\n</body>\n```\n\nI blame myself for getting stuck considering this sort all documented well in the [SystemJS documentation on GitHub](https://github.com/systemjs/systemjs). Either way, I had issues finding solid resources about using bundles. Hopefull this can help someone else in the future.\n\n## Conclusion\nMy problems can be traced back to my lack of experience with JavaScript module loaders. And yes, I know that [ES6 Modules are coming](http://caniuse.com/#feat=es6-module), but the browsers are a ways away from having a full implementation (except for Safari). \n\nUntil then, we'll be using TypeScript and [Babel](http://babeljs.io/) to help us get our modular JavaScript working in the browser.\n\n","categories":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"typescript","slug":"typescript","permalink":"https://westerndevs.com/tags/typescript/"},{"name":"systemjs","slug":"systemjs","permalink":"https://westerndevs.com/tags/systemjs/"},{"name":"modules","slug":"modules","permalink":"https://westerndevs.com/tags/modules/"},{"name":"gulpjs","slug":"gulpjs","permalink":"https://westerndevs.com/tags/gulpjs/"}]},{"title":"Issues are not free","authorId":"donald_belcham","slug":"Issues-are-not-cheap","date":"2017-03-14 02:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Issues-are-not-cheap/","link":"","permalink":"https://westerndevs.com/_/Issues-are-not-cheap/","excerpt":"Thoughts about the hidden costs of actions","raw":"---\nlayout: post\ntitle: Issues are not free\ntags:\n  - process\ndate: 2017-03-13 22:00:00\nexcerpt: Thoughts about the hidden costs of actions\nauthorId: donald_belcham\n---\n\nAt [Particular Software](https://particular.net/) we manage all of our work flow using [Github](https://github.com/) repositories and the issues in them. These issues are what drive our work day-to-day, week-to-week and all the way into longer periods of time like years. If an issue doesn't exist for a topic, that topic doesn't exist within the organization.\n\nA while back I declared in a meeting that \"Issues are cheap\". The context around the comment was such that I was suggesting that we can just create more issues when we need to surface a topic. If that issue goes away (closed, lost, whatever) and the topic surfaces again, create another issue. It's cheap...all you have to do is a bit of typing.\n\nMy comment was met with the statement that, in fact, issues are not cheap. At first I was taken aback. My little world was being shaken by this statement. How could an issue not be cheap? It took mere minutes to create one. We don't get charged for disk space or issue count by Github. The person that made that comment went on to clarify their position by saying that \"Ideas are cheap\".\n\nI forget what the rest of that meeting was about. My mind was racing to rationalize this distinction. In my world issues and ideas were one and the same. But in this person's world they were clearly different. And it turns out they are right.\n\nIdeas are the nascent concept that is represented within an issue. The issue is just the envelope that carries the message. Like a letter, the cost of writing a partially formed idea onto paper is trivial. A few minutes of your time, a pen and paper. This is cheap.\n\nUnlike the writing on the paper however, envelopes have a lifetime cost associate with them. Mail needs to be picked up, transported, sorted, transported again, and delivered. Issues have this cost too. They must be triaged, prioritized, backlogged, re-prioritized, worked on, closed and retrospected on.\n\nWhen you think about it from that standpoint, issues are *really* expensive. The thought that you can just create an issue doesn't remove the fact that they will incur a cost throughout their lifetime.\n\nProbably the most insidious type of issue is the drive-by one. You get an idea, you create an issue and you just keep right on moving. You don't spend any time curating that idea, but instead push that workload to other people. Not only are you creating an expense for your process, but you're asking other people to pay it off for you. I had a roommate like that once. I'm sure you know how that ended.\n\nIt's important to remember that there can be hidden costs in the actions that we take. This was a bit of an eye opener to me. Having ideas isn't a bad thing. Spending the time to make the well formed so that they carry value that warrants the effort that they will require as issues is a good first step.","categories":[],"tags":[{"name":"process","slug":"process","permalink":"https://westerndevs.com/tags/process/"}]},{"title":"Synchronize GitHub Repository with VSTS","authorId":"dylan_smith","slug":"Synchronize-GitHub-Repository-With-VSTS","date":"2017-03-08 19:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Synchronize-GitHub-Repository-With-VSTS/","link":"","permalink":"https://westerndevs.com/_/Synchronize-GitHub-Repository-With-VSTS/","excerpt":"Step by step guide to do an automated continuous one-way synchronization from a GitHub repository to a VSTS repository.","raw":"---\nlayout: post\ntitle: Synchronize GitHub Repository with VSTS\ntags:\n  - devops\n  - vsts\n  - github\ndate: 2017-03-08 14:00:00\nexcerpt: Step by step guide to do an automated continuous one-way synchronization from a GitHub repository to a VSTS repository.\nauthorId: dylan_smith\n---\nA fellow Western Dev - Justin Self - was telling me he uses GitHub for source control and VSTS for Work Items, and was wondering if it's possible to link the Work Items to GitHub commits.  If you're using VSTS for source control, you can do this easily by mentioning the Work Item ID in your commit message like so: Fixing Bug #123.  And VSTS will automatically create a link between the Work Item and the Git commit.  If you're using GitHub for source control, I don't believe there's an out of the box way to make this work.\n\nWe came up with an idea, what if we could synchronize the GitHub repository to a VSTS repository.  Then VSTS could see the commits and create the WI links.  So that's what we did, and this blog post explains how you can do the same thing.\n\nFirst things first, you need to create a VSTS Build that points to your GitHub repo.  That's easy enough, as support is built right into VSTS builds to do this.\n\n1. Go to Builds and Create a New Definition (Note: I'm using the new build editing experience - which you can turn on for your account if you haven't already).\n2. Choose the Empty Process option\n3. Configure the Get Sources task to use GitHub as the source, and give the connection a name\n4. Click Authorize using Oauth - at this step you may be prompted for your GitHub credentials and to Authorize VSTS to talk to GitHub\n5. Pick the GitHub repo and branch - I just left this as master, but later we will set it up to synch all branches\n6. Go to the Triggers tab and turn on Continuous Integration, and change the branch to include to * to trigger a build on commits/pushes to any branch\n7. Under Options turn on Allow Scripts to Access Oauth token - we'll use this later\n8. Save and Queue a build to check if it works\n\t\n![New Build Definition](http://imgur.com/YzvjdpQ.png)\n\n![Select Build Template](http://imgur.com/bnugm3O.png)\n\n![Configure Source Repo](http://imgur.com/ms364Zw.png)\n\n![GH Repo Configured](http://imgur.com/LSQDzg2.png)\n\n![Configure CI](http://imgur.com/w8bYFxi.png)\n\n![Configure OAuth](http://imgur.com/Z6Eei4O.png)\n\n![Test Build Output](http://imgur.com/PxGFis0.png)\n\nAlright, now we have a build that triggers on every GitHub commit/push, and will download the GH repo to the build agent.  The next step is to make it push any and all changes into the VSTS repo.  To do this I shamelessly copied a snippet of bash from StackOverflow.  There is a built-in build task to run a bash script - Shell Script - but that requires you to point it to a script inside your repo, which is more work than I wanted.  I just want to write the few lines of bash directly in the build.\n\n![Default Shell Task](http://imgur.com/OQCpY7W.png)\n\nFortunately there is a VSTS extension in the marketplace that lets us do exactly this: https://marketplace.visualstudio.com/items?itemName=tsuyoshiushio.shell-exec\n\n![Extension Shell Task](http://imgur.com/bSzWWig.png)\n\nOnce I installed that extension into my VSTS account, I can now add it as a task to my build and tell it the bash script I want it to run:\n\n![Install Extension](http://imgur.com/C1tVW5x.png)\n\n![Confirm Extension](http://imgur.com/SmKJlax.png)\n\n![Extension Installed](http://imgur.com/R8zE0ss.png)\n\n![Add New Task](http://imgur.com/wJZmhuE.png)\n\n![Configure Shell Task](http://imgur.com/ErwFNbF.png)\n\nThose 3 lines of bash using the git command-line are all it takes.  The one tricky bit to figure out was how to make sure it synchronized all branches - even newly created branches - in github into VSTS.  The trickery in line 1 and 3 does that.\n{% codeblock lang:shell %}\ngit branch -r | grep -v '\\->' | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\"; done\ngit remote add vsts https://pokerleaguemanager.visualstudio.com/DefaultCollection/_git/GitHubSync\ngit branch -r | grep -v '\\->' | while read remote; do git -c http.extraheader=\"AUTHORIZATION: bearer $SYSTEM_ACCESSTOKEN\" push -u vsts \"${remote#origin/}\"; done\n{% endcodeblock %}\n\nThat script is doing a few things:\n\n1. Loop through all branches in origin and create local branches to track them. Note: Because GH was setup as the repo for this build, VSTS has already created the git repo, setup origin to point to GH, and downloaded the repo to the build agent.\n2. Add the VSTS repo as a new remote called vsts\n3. Loop through all branches and push each one to VSTS\n\nThe stuff with $SYSTEM_ACCESSTOKEN in line 3 is accessing an environment variable that contains an Oauth token that can be used to communicate with VSTS - in a previous step where we set the option in the VSTS build to make Oauth token available to scripts, is what allows this to work.\n\nThere's one thing left to do to make this all work - we need to grant the build service account access to the VSTS repo.  We can do this in the repo security screen like so:\n\n![Configure Security](http://imgur.com/eCYpGEC.png)\n\nNow you can push some commits to GitHub and/or create a new branch, and the VSTS build should automatically trigger and synch the VSTS repo up almost immediately.  If everything is working you should see build output that looks something like this:\n\n![Successful Build Output](http://imgur.com/E3GhdJI.png)\n","categories":[],"tags":[{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"github","slug":"github","permalink":"https://westerndevs.com/tags/github/"},{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"}]},{"title":"Paper Cuts - My Review of PaperCall.io","authorId":"darcy_lussier","slug":"Paper-Cuts-My-Review-Of-PaperCall","date":"2017-03-07 16:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"conference/services/Paper-Cuts-My-Review-Of-PaperCall/","link":"","permalink":"https://westerndevs.com/conference/services/Paper-Cuts-My-Review-Of-PaperCall/","excerpt":"My experience using PaperCall.io, a conference session submission service.","raw":"---\nlayout: post\ntitle: \"Paper Cuts - My Review of PaperCall.io\"\ndate: 2017-03-07 11:30:00\ncategories:\n    - conference\n    - services\nexcerpt: My experience using PaperCall.io, a conference session submission service.\ncomments: true\nauthorId: darcy_lussier\n---\n## Conference Time\n\nMy next [Prairie Dev Con](http://www.prairiedevcon.com) is coming up June 6-7 2017, and this year I decided to use a service that's become very popular for speaker submissions called [PaperCall.io](http://www.papercall.io). For speakers this service is fantastic - for free you can create a profile and add any number of talk abstracts. Then when a call for speakers opens, you can easily with just a few clicks submit any number of talks.\n\nPeople can create events and open call-for-papers (CFP's) for free as well. Conferences are then included in their Open CFP list, with certain paid-level events getting \"featured\" treatment at the top of the list.\n\nThis creates a very attractive ecosystem for speakers and events, and removes the need for conferences to build their own system or use tools (i.e. SurveyMonkey) that weren't built specifically for conference session submission.\n\nSo that's the backdrop for the rest of this post. I went in very much looking forward to using PaperCall.io and I'm still very optimistic about the platform! But there are some crucial pain points that need to be addressed before I'd consider using it again for one of my conferences.\n\n## Issue 1 - The Grid\n\nWhen you have people submit their talks, the conference view displays them in a grid as per the image below.\n\n![\"PaperCall Grid\"](https://darcyblogimages.blob.core.windows.net/wdimages/PC_Grid.JPG)\n\nI want to point out a few things about the grid.\n\n### 20 Record Per Page Limit\n\nThere's paging, but it limits you to 20 records per page. You can't change this to see larger number or even all.\n\n### No Sorting By Speaker\n\nThe headings you see are what you get. You can't do a sort by Speaker for instance. You also can't add your own columns so if you wanted to sort based on location you're out of luck.\nThere is a search box, but that's only useful once you know what you're looking for (i.e. speaker name, location, etc.).\n\n### No Bulk Status Update for Free Tier\n\nTalks can be Submitted, Accepted, Rejected, or Waitlist (Declined is reserved for speakers who are accepted but decline after the fact I guess). Setting a talk to one of those settings via the drop list actually triggers a post back (let me check...yes, we're in 2017 here). So if you have 10 talks that you want to set as accepted, you have to manually one at a time set their drop list value to Accepted. This feature is listed in the Professional tier, but I don't know how its implemented.\n\n### No Filtering\n\nYou can't set filters on the data either.\n\n### Rating Style Doesn't Display\n\nI selected the \"Simple\" rating style (yes, maybe, no) compared to Five Star, but the Five Star is what was displayed in my grid.\n\n## Issue 2 - Downloading Submissions\n\nSo why not just download the data and manipulate it outside of PaperCall? If the service is supposed to be a one-stop-shop, then I shouldn't need to. I should be able to manage my speakers however I want within the application. But since we're limited to the grid as it is, exporting the data is the next best thing. Unfortuntaely you can only get to your data if a) you're on the paid tier, or b) you've identified talks as being Accepted. That's right - you have to go through the grid interface and assign statuses before you can export your own data (if you're using the free tier). Also note that the only download option you have is for Accepted talks, not Rejected or Waitlisted. If you're on the paid tier there's an API you can connect to...more on the pricing and features of the tiers later though.\n\n## Issue 3 - Speaker Communication\n\nI'm a fan of having an end-to-end ecosystem for speakers and conference organizers, but the speaker communication suffers from a few clunky parts.\n\n### No Email Contact\n\nUnless the speaker specifically puts their email address or Twitter handle into their profile, there's no way to communicate outside of the PaperCall \"Communications with Speaker\" mechanism. One of the problems with this is its tied ot a speaker AND a talk. So if you want to communicate with a speaker about 3 of their talks, you either pick one talk and include all the communication there (which ties it to the selected talk) or you break it up over the three talks. You get an email notification about the communication, which has a From that looks like this...\n\nreply+ASERWQREWQREWQGASGAERWFSAFDFSAGHG343243242SFAFwerwfewf223432FRSDFDSFSF23GFSGWGWGEWFEWDE2FEWGWGWGWEFEWFEWFEWGEWGEWGEWGF32432432FEWFEWFFR32R32FSF==@mg.papercall.io\n\n(that's not a real one, and its actually a longer address)\n\nI find it looks messy and unprofessional.\n\n### No Speaker Search for Organizers\n\nAs a conference organizer I have no way to search the speaker profiles. They aren't public (at least at the free tier), and unless a speaker sends you their URL there's no way to get to it. I understand that this model puts the power in the hands of the speakers and prevents conferences from blanketing speakers with harassing communication about their event - I'm on board with that. But if I have a speaker who's submitted a talk to my event, it would be great to be able to see all the talks that speaker could present. Perhaps I really like the speaker but their submitted talk isn't a right fit, or has been filled by someone else already, or whatever. It would be great to see other talk options. Currently I would have to request that via the speaker communication channel mentioned above.\n\n### Accepted/Rejected Communication Confusion\n\nOnce you've identified talks as Accepted or Rejected you can then send an email message to the speakers letting them know. There's an issue though - you're accepting or rejecting the talk, not necessarily the speaker. I could have a speaker have 2 talks accepted but 3 rejected. With how the email works, that speaker would get \"Unfortunately your talk was not picked. Thanks for applying, and I hope we see you as an attendee (that's a paraphrase to the stock verbiage sent out).\" but then they'd get \"Congratulations, your talk was picked\" as well! That's confusing. I imagine speakers get multiple emails as well, since one seems to be sent for each session submitted. UGH!\n\nThe way I avoided this was to assign rejected sessions for speakers who were actually accepted into the Waitlist group. Definitely a workaround/hack solution.\n\n### Speaker Headshots Default to Gravatar\n\nAlthough the interface seems to suggest that you can upload your own headshot, my Accepted Talk export provided a bunch of links to headshots that defaulted to the Gravatar for the person (if they don't have one, its the stock imgage). Not the worst thing in the world, but not the most ideal either.\n\n## Issue 4 - Pricing\n\nThere's three tiers for PaperCall - Community (Free), Professional ($499 USD/event), and Custom. I'll ignore Custom for now and focus on the Community and Professional.\n\n![\"PaperCall Pricing\"](https://darcyblogimages.blob.core.windows.net/wdimages/PC_Pricing.JPG)\n\nOver Community, Pro provides:\n* 15 more organizers\n* Unlimited submissions\n* Read-Only API Access + Webhooks\n* Bulk Tagging/Submission Management\n* Privately Tag Your Submissions\n* Added promotion on the homepage\n* 24 Hour email support\n\nSo for $500 I still have to use the web site to manage my data, as the API is read-only. I can add private tags, but that means I can filter by search only and not in the grid. Also I'm thinking there's no way to bulk-tag talks.\n\nLet me frame this in the context of Prairie Dev Con. I typically get under 200 sessions submitted. I'm the only organizer (even if I added more organizers, I can't see it going beyond 5). The *only* thing $500 gets me is the read-only API so I can export my data and 24 hour email support. This is a tough justification.\n\nIf the grid had a better interface, I was able to better communicate with speakers, and I could export my data whenever I wanted to work on it offline, I would absolutely pay $200 per event for the service...maybe even $300. And I think there are MANY conferences that are mid-range that would be open to that tier level. As it stands now, the mid-range conferences are likely looking to make the Free tier work and the mega conferences that are using the paid tier are getting a crazy deal.\n\n## In Conclusion\n\nI love the idea of PaperCall. Speakers are on board, and through PaperCall I've had speakers submit that normally wouldn't have even known about my conference. But its definitely in need of some improvements, especially at the price points they've listed. All of my criticisms in this post are meant to share how to make the product better because I *want* PaperCall to be better!\n\nBut let's be honest, CFP isn't a complicated business process and there's no reason a competitor could come up with a better process. It's in PaperCall's best interest to listen to their customers, especially those willing to become paying customers if only certain functionality is added. We'll see if any advancement is made come later this Spring when I open up PrDC Deliver CFP.\n","categories":[{"name":"conference","slug":"conference","permalink":"https://westerndevs.com/categories/conference/"},{"name":"services","slug":"conference/services","permalink":"https://westerndevs.com/categories/conference/services/"}],"tags":[]},{"title":"Initial Thoughts on Using Phaser","authorId":"david_wesst","slug":"new-post-Initial-Thoughts-on-Phaser","date":"2017-03-07 11:00:00+0000","updated":"2017-03-07 11:00:00+0000","comments":true,"path":"javascript/typescript/new-post-Initial-Thoughts-on-Phaser/","link":"","permalink":"https://westerndevs.com/javascript/typescript/new-post-Initial-Thoughts-on-Phaser/","excerpt":"As a side project, I started making another JavaScript-based video game and decided to go with Phaser as my framework of choice. Here are my initial thoughts about Phaser after using it on my project for the past two months.","raw":"---\nlayout: post\ntitle: Initial Thoughts on Using Phaser\ncategories:\n  - javascript\n  - typescript\ndate: 2017-03-07 06:00:00\nupdated: 2017-03-07 06:00:00\ntags:\n  - javascript\n  - typescript\n  - phaser\nexcerpt: As a side project, I started making another JavaScript-based video game and decided to go with Phaser as my framework of choice. Here are my initial thoughts about Phaser after using it on my project for the past two months.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2017/03/Intital-Thoughts-on-Using-Phaser/\n---\n\nI'm not a game dev, but have always wanted to make a video game. I've started and stopped so many project over the years, that I have seriously lost count.\n\nWith the new year and a fresh mind, I decided to take a stab at it again, this time focusing on just getting something done rather than getting something _done right_ as a sort of side project.\n\nIt's been about two months since I've started, and I have something basic working (although I'm not willing to share it yet) and I thought I would share my thoughts on Phaser, the framework I decided to use to help me build my game.\n\n## Techincal Requirements\nBefore we talk about the framework, let's talk about the game itself as you'll need to know what I'm building to understand why I chose the framework.\n\nHere's the technical rundown:\n\n* 2D\n* Top-Down Camera\n* Tile-based graphics\n* Using [Tiled](http://mapeditor.org) for maps\n* Gamepad support for player input\n* TypeScript support\n\nIf you think back to the old NES (Nintendo Entertainment System) and SNES (Super Nintendo Entertainment System) days, games like Final Fantasy, and the Legend of Zelda are good examples of the look of the game.\n\n![\"The Legend of Zelda for the NES\"](http://i.imgur.com/0LLlYoxb.png)\n\n### Where did you come up with these Requirements?\nI decided to make a game that I wanted to play. That's really about it.\n\nI have plenty of ideas floating around in my head, but I went with one that had a look and feel of what I like to play. \n\n### Why TypeScript Support?\nAlthough I'm a JavaScript nut, if find that TypeScript combined with the right tooling (Visual Studio Code) helps you learn an API thorugh code completion. Plus, it's compiler helps catch errors along the way without losing the versatility that comes with JavaScript. Since I'm learning a whole new domain (i.e. game development) I wanted to focus more on the learning practices and patterns, rather than worrying about the syntax.\n\nWe'll get into more of this later.\n\n### Wait! What about the Game Design?!\nThat is a whole other conversation and series of posts that I may share if I ever get this project done. For now, they don't really apply as we're sticking to the technical side of the project.\n\n(Although if you're interested, ping me on [Twitter](https://twitter.com/davidwesst) to let me know)\n\n## So, Why Phaser?\nI did quite a bit of research on this before going with Phaser. The two biggest contenders being [BabylonJS](http://babylonjs.com/) and the [CreateJS Suite](http://www.createjs.com/).\n\nAt the end of the day, Phaser not only did everything I needed it to do, but it  has a very strong community of support through [HTML5GameDevs](http://html5gamedevs.com/), and it does everything I need it to do. Plus, I had already tinkered with it a bit so that definitely gave it some extra points during the selection project.\n\n## The Highlights\nNow that I'm beyond the \"tinkering\" phaser, and into building a full game, I think I can weigh-in on the pros and cons I've come across thus far. I'm not far enough along to talk about performance, but for my little game project it seems to be running smoothly without fail.\n\n### Support is Amazing\nAs mentioned previously, the support from [HTML5GameDevs](http://html5gamedevs.com/) is great. The forums are active, and there is even a live chat for people that are registered.\n\nI've only asked one question so far, and it was answered very quickly. The rest of the time, I search the forums for my question and 99/100 times I'll find an answer.\n\nThe [examples section](https://phaser.io/examples) of the Phaser website gives links great code snippets that help wrap your brain around how to do things with the framework.\n\nLastly, as they prepare Phaser v3, the community has taken on Phaser v2 and has continued to release patches to the framework.\n\n### Focuses on 2D\nI'm just starting to learn game development and, on top of that, I'm doing this in my spare time.\n\nWhen I get stuck I don't know what I don't know, but I can generally find my way around questions and articles that discuss 2D games. Since Phaser focuses on 2D, it makes consuming the API much more familiar to me rather that navigating my way around 3D game API (i.e. BabylonJS) that bring a whole new vocabulary to the table that I have to learn.\n\n### Phaser-CE is What You're Looking For\nIt appears that they fixed it on the website, but when I was starting on this project there was no mention of _Phaser-CE_ on the homepage, and so I thought version 2.6.2 was the latest and greatest.\n\nIt turns out, that was incorrect.\n\nAs you'll see [here on the Phaser site](https://phaser.io/download/stable), version 2.6.2 was the last officially supported release, while v2.7.x is the community edition that is supported by those fine community members I mentioned earlier. So, when you install your dependency with `npm install phaser` you are installing an old version of the framework. You want `npm install phaser-ce`.\n\nThe change was minimal, considering it's the same framework. It just took me for a loop as I only figured this out as I tried to submit a pull request, only to find out that I was using the wrong version of Phaser.\n\n### JavaScript First, TypeScript Second\nI wanted TypeScript support up front because I knew it would help me get into the Phaser APIs. Now that I'm becoming more familiar with everything, I feel as though I should have toughed it out and stuck with JavaScript and Babel. \n\nThe TypeScript definition files are part of the library and are not available on through [`@types`](https://www.npmjs.com/package/@types/npm) on the NPM. From what I can gather, they are done by hand. This isn't a big deal considering they work fine, but it makes me question their accuracy, especially not being avaialble through `@types`.\n\n## Conclusion\nI don't regret choosing Phaser as it met all my short term goals and I'm still learning a lot about game development and Phaser (and it's community) is helping me with that.\n\nNext time around, assuming Phaser v3 isn't done yet, I would proabably go with [BabylonJS](http://babylonjs.com/) as it gives you everything Phaser does, but adds some pretty powerful tooling and 3D support along with it, and has the TypeScript support I expect.\n\nAll in all, Phaser has been a great place to start with game development. I highly recommend it.\n\n\n\n\n\n\n\n\n\n\n","categories":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/categories/javascript/"},{"name":"typescript","slug":"javascript/typescript","permalink":"https://westerndevs.com/categories/javascript/typescript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"typescript","slug":"typescript","permalink":"https://westerndevs.com/tags/typescript/"},{"name":"phaser","slug":"phaser","permalink":"https://westerndevs.com/tags/phaser/"}]},{"title":"Acceptance Testing With Legacy Databases","authorId":"amir_barylko","slug":"Acceptance-Testing-With-Legacy-Databases","date":"2017-01-22 15:04:33+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Testing/Acceptance-Testing-With-Legacy-Databases/","link":"","permalink":"https://westerndevs.com/Testing/Acceptance-Testing-With-Legacy-Databases/","excerpt":"One of the most common pain points of implementing automated acceptance testing is the interaction with the database. For greenfield projects you can plan from day one how to setup the test to easily include the database interaction but with legacy projects it is not always that easy.","raw":"---\nlayout: post\ntitle: Acceptance Testing With Legacy Databases\ncategories:\n  - Testing\ndate: 2017-01-22 10:04:33\ntags:\n  - Acceptance Testing\n  - Unit Testing\n  - Legacy code\n  - jackson\n  - WireMock\nauthorId: amir_barylko\noriginalurl: http://orthocoders.com/blog/2017/01/01/acceptance-testing-with-legacy-databases/\n---\n\nOne of the most common _pain points_ of implementing automated acceptance testing is the interaction with the database.\n\nFor greenfield projects you can plan from day one how to setup the test to easily include the database interaction but with legacy projects it is not always that easy.\n\n<!--more-->\n\n## Dealing with legacy code\n\nLetâ€™s face it: **Testing is hard**.\n\nI do not mean it is hard to understand. The complexity is not inherently attached to the concept of testing but (I found in most cases) a by-product of `tooling + environment + database`.\n\nThat is the reason why I think adding testing to a legacy system can be quite challenging. We did not choose the tooling, nor the database nor did we set up the environment to be test friendly.\n\nSo where to start with testing?\n\nOne option is to start by adding _unit tests_ into the codebase, but that could be a paramount effort considering that quite often the legacy code was not written with testability in mind. A lot of change, a lot of risk.\n\nOn the other hand _acceptance testing_ is the perfect candidate.\n\nWhy? _Acceptance Testing_ puts the focus on testing end to end. Given a certain input, run it through the system and make sure the output is what we expect to see.\n\nIt works for web applications, web apis, libraries, desktop applications, you name it. And also, in many cases we will not need to modify the code behaviour at all.\n\nAll that is fine and dandy, but what about the database? We may be able to create a local copy of the database to test, but what are we going to do with data generation, logic stored in the database, etc?\n\n## A perfect world\n\nLetâ€™s pause and imagine for just a few paragraphs that instead of using a database the _system under test_ uses an HTTP API to get all the information it needs.\n\nIf that was the case then we could implement acceptance testing very easily by doing something like the following pseudo algorithm:\n\n- Launch a fake HTTP server listening on the URI expected by the system.\n- Create some data that will work for my test case.\n- When the application does the call, return that data.\n- Validate the case worked as expected.\n- Shutdown the server.\n\nNeat right? This approach has many benefits.\n\n_First_, we keep modifications of the system under test to the bare minimum.\n\n_Second_, there are lots of tools in multiple languages that can help us with such a task. We can choose the same environment or one that is completely different. Whichever works better for our needs.\n\n_Third_, these steps can be easily automated and ran when it is convenient and useful.\n\nOk, the break is over.\n\n## Back to reality\n\nTo change all the database related code to start using some kind of web API could be a huge risk and effort.\n\nSuch amount of refactoring may cripple your project for a long time, and not even produce a positive result.\n\nHaving said that, what if we use the same idea but with a small twist?.\n\n## Leave the database code alone\n\nWell, not alone alone, but letâ€™s hide it behind a very thin wrapper.\n\nThe goal is that instead of directly hitting the database (or whichever function or class is being used) we are going to call a proxy that is sole job is to forward the call to the same code we were using before.\n\nThe main difference is that the _Proxy_ talks about the domain. If we were fetching some `Customer` object from the database, then the proxy will have a way to do so and return a `Customer` collection.\n\nSo the database interaction, _ORM_ mapping, etc, stays hidden.\n\nTo illustrate the idea with a bit of code, letâ€™s imagine a class in charge of finding customers in order to show them:\n\n{% codeblock lang:csharp %}\npublic class CustomersController {\n\n  public CustomerView index() {\n\n    String query = \"select NAME, ADDRESS, BIRTH_DATE from CUSTOMERS\";\n\n    ResultSet rs = dbConnection.createStatement().executeQuery(query);\n\n    List<Customer> customers = new ArrayList<Customer>();\n\n    while(rs.next()) { customers.add(loadCustomer(rs)); }\n\n    return new CustomerView(customers);\n  }\n\n  private Customer loadCustomer(ResultSet rs) { ...... }\n}\n{% endcodeblock %}\n\nThe first step would be to create an _interface_ and abstract the query to the database:\n\n{% codeblock lang:csharp %}\npublic interface CustomersQuery {\n  List<Customer> getCustomers();\n}\n{% endcodeblock %}\n\nAnd a default implementation that does the database query:\n\n{% codeblock lang:csharp %}\npublic class DatabaseCustomerQuery implements CustomersQuery {\n\n  public List<Customer> getCustomers() {\n\n    String query = \"select NAME, ADDRESS, BIRTH_DATE from CUSTOMERS\";\n\n    ResultSet rs = dbConnection.createStatement().executeQuery(query);\n\n    List<Customer> customers = new ArrayList<Customer>();\n\n    while(rs.next()) { customers.add(loadCustomer(rs)); }\n\n    return customers;\n  }\n\n  private Customer loadCustomer(ResultSet rs) { ...... }\n}\n\n{% endcodeblock %}\nAnd the original class now it uses the interface:\n\n{% codeblock lang:csharp %}\npublic class CustomersController {\n\n  public CustomerView index() {\n    return new CustomerView(this.customersQuery.getCustomers());\n  }\n}\n\n{% endcodeblock %}\n\n## Mock the proxy\n\nWhen testing the system, the library in charge of _proxying_ the interaction to the database could be switched to a different one that does an HTTP call to a URI and returns the result based on the response.\n\nBy using an HTTP call, then the test will pose as the expected source of data and respond based on the needs of each case.\n\nFollowing the previous example, we could implement a class that gets the customers data using an HTTP call to the test URI.\n\nThe example uses [Jackson](https://github.com/FasterXML/jackson) to load the json content.\n\n{% codeblock lang:csharp %}\npublic class HttpCustomersQuery implements CustomerQuery {\n\n  public List<Customer> getCustomers() {\n    HttpGet httpGet = new HttpGet(testUrl + \"/customers\");\n    HttpResponse response = httpclient.execute(httpGet);\n\n    ObjectMapper mapper = new ObjectMapper();\n    List<Customer> customers = mapper.readValue(response.getEntity().getContent(), new TypeReference<List<Customer>>(){});\n\n    return customers;\n  }\n}\n{% endcodeblock %}\n\nThe last step, when running the tests we will launch the HTTP server to serve the JSON customers:\n\nHere I am using [WireMock](http://wiremock.org/) to set up the response.\n\n{% codeblock lang:csharp %}\n@Test\npublic void testCustomersView()  {\n\n  List<Customer> expected = createSomeFakeCustomers();\n\n  String serializedCustomers = JSON.write(expeted);\n\n  stubFor(get(urlEqualTo(\"/customers\"))\n            .willReturn(aResponse()\n                .withStatus(200)\n                .withHeader(\"Content-Type\", \"application/json\")\n                .withBody(serializedCustomers)));\n\n  // run the system under test here\n  runSystem();\n\n  List<Customer> actual = getCustomersShown() ; // get the customers that are being shown\n\n  assertThat(expected, is(actual));\n}\n{% endcodeblock %}\n\n### Why bother with HTTP?\n\nWe could implement the â€œfakeâ€ version of the library as:\n\n{% codeblock lang:csharp %}\npublic class HttpCustomersQuery implements CustomerQuery {\n\n  public List<Customer> getCustomers() {\n    String jsonContent = loadResourceFrom(\"/resources/customers.json\");\n\n    ObjectMapper mapper = new ObjectMapper();\n    List<Customer> customers = mapper.readValue(jsonContent, new TypeReference<List<Customer>>(){});\n\n    return customers;\n  }\n}\n{% endcodeblock %}\n\nHowever this approach may limit our ability to separate completely the acceptance test implementation from the system we want to test.\n\nHaving an external server to pose as data source provides flexibility and could simplify quite a bit the test implementation because it gives us the freedom to choose any tool that we may see fit to do the actual implementation.\n\nThis technique could simplify manual testing as well. The test scenario data could be setup, then the system launched and wait for manual confirmation to ensure it works as expected.\n\n{% codeblock lang:gherkin %}\nGiven the customers are loaded             # All the customers in the JSON file are loaded\nWhen listing the customers                 # Launch the system to show the customers\nThen every customer name shows in the list # Ensure all customers are shown\n{% endcodeblock %}\n\n## Change impact\n\nThe change will be localized. Modifying a particular functionality of the system does not affect how other parts of the system work nor major refactoring effort is required.\n\nOf course there will be some code change, but hopefully very small and just to hide the database related code behind a very thin wrapper.\n\nOnce the acceptance tests start to roll, each new test will be easier and easier.\n\nNot only the system will have a new safety net that becomes larger and larger with every test, but the quality will grow as well.\n","categories":[{"name":"Testing","slug":"Testing","permalink":"https://westerndevs.com/categories/Testing/"}],"tags":[{"name":"Acceptance Testing","slug":"Acceptance-Testing","permalink":"https://westerndevs.com/tags/Acceptance-Testing/"},{"name":"Unit Testing","slug":"Unit-Testing","permalink":"https://westerndevs.com/tags/Unit-Testing/"},{"name":"Legacy code","slug":"Legacy-code","permalink":"https://westerndevs.com/tags/Legacy-code/"},{"name":"jackson","slug":"jackson","permalink":"https://westerndevs.com/tags/jackson/"},{"name":"WireMock","slug":"WireMock","permalink":"https://westerndevs.com/tags/WireMock/"}]},{"title":"ASP.NET Core Training in Calgary, Alberta","authorId":"james_chambers","slug":"ASP-NET-Core-Training-in-Calgary-Alberta","date":"2017-01-16 15:54:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Training/ASP-NET-Core-Training-in-Calgary-Alberta/","link":"","permalink":"https://westerndevs.com/Training/ASP-NET-Core-Training-in-Calgary-Alberta/","excerpt":"The first ASP.NET Monsters workshop is happening in Calgary from Feb 22-24, 2017. Attendance is mandatory","raw":"---\nlayout: post\ntitle: 'ASP.NET Core Training in Calgary, Alberta'\ncategories:\n  - Training\ndate: 2017-01-16 10:54:30\ntags:\n  - Training\n  - ASP.NET Core\nauthorId: james_chambers\nexcerpt: The first ASP.NET Monsters workshop is happening in Calgary from Feb 22-24, 2017. Attendance is mandatory\noriginalurl: http://jameschambers.com/2017/01/ASP-NET-Core-Training-in-Calgary-AB/\n---\n\nFor some folks, working independently and plowing through a book chapter-by-chapter is the best way for them to learn. If you're in that camp, I've got just the book for you.\n\n[![](https://jcblogimages.blob.core.windows.net:443/img/2017/aspnetcore_book.png)](http://click.linksynergy.com/link?id=hKA2dsKjtSk&offerid=145238.2497285&type=2&murl=http%3A%2F%2Fwww.informit.com%2Ftitle%2F9781509304066)\n<p class=\"article-more-link\">\n    <a href=\"http://click.linksynergy.com/link?id=hKA2dsKjtSk&offerid=145238.2497285&type=2&murl=http%3A%2F%2Fwww.informit.com%2Ftitle%2F9781509304066\">Buy the book</a>\n</p>\n\n## Want to get Hands-On?\n\nFor others, in-person training is the most effective way to dive into new content. Having someone in the same room who knows how to navigate a new release of software, framework and tooling and all the related changes is a powerful asset while you learn.\n\nThat said, we are pleased to announce our first Monsters workshop in Calgary, Alberta. Please join us in Calgary as we mash on changes, approaches, caveats and wins for all things in ASP.NET Core.\n\nAlready interested? You can [sign up today](https://training.aspnetmonsters.com) and join us in February from the 22nd to the 24th.\n<p class=\"article-more-link\">\n    <a href=\"https://training.aspnetmonsters.com\">Register for training</a>\n</p>\n\n## We're Going To Learn a Lot Together!\n\nMy good friends [Dave](https://twitter.com/dave_paquette), [Simon](https://twitter.com/stimms) and I have been mashing on ASP.NET Core since its inception. This workshop is the culmination of what we have learned along the way and applied in our projects, samples and through our videos on Microsoft's Channel 9. We're taking you deep into three fully-packed days that walk you through various stages of application development. Our number one priority is to equip you with the skills you need to start on a Core MVC project and transition your existing skills to the new tooling.\n\nWe expect you to be familiar with web technologies and to be comfortable in Visual Studio. Beyond that, here is some of what you can expect:\n\n- A solid foundation of ASP.NET Core with which you can build MVC applications\n- Exposure to configuration, testing and extensibility\n- Tips, samples and exercises that will guide you as you build software in the cloud\n- An understanding of data access using the latest version of Entity Framework\n- Access to the labs and source code used in the workshop\n\nBe sure to check out our training site to [view the full curriculum](https://training.aspnetmonsters.com).\n\n## Location, Location, Location\n\nCalgary and area is home to some of the most beautiful sights in Canada, with a mountain range full of winter sports about an hour away, skiing at Calgary's Olympic Park as well as NHL and WHL Hockey on the edge of downtown. There are great restaurants, museums, art exhibits and theatre, along with with a great night life including brew pubs, world-famous Canadian poutine and an assortment of comedy clubs.\n\nIf you're joining us from outside the area, we highly recommend adding on a few days to your trip so that you can explore the area. If you are from outside of Canada, <b>you will need to get a valid International Driver's Permit</b> from your country before you leave if you wish to rent a car when you're here (handy for exploring!).\n\n<b>Can't join us in Calgary?</b> No problem. Just hit the registration page and sign up for our email list to be notified of other upcoming training cities.\n\nHappy New Year, and happy coding!\n\n<p class=\"article-more-link\">\n    <a href=\"https://training.aspnetmonsters.com\">Register for training</a>\n</p>\n","categories":[{"name":"Training","slug":"Training","permalink":"https://westerndevs.com/categories/Training/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"Training","slug":"Training","permalink":"https://westerndevs.com/tags/Training/"}]},{"title":"Hour of Code Challenge - Completed","authorId":"dave_white","slug":"Hour-of-Code-Challenge-Completed","date":"2016-12-14 00:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Community/Hour-of-Code-Challenge-Completed/","link":"","permalink":"https://westerndevs.com/Community/Hour-of-Code-Challenge-Completed/","excerpt":"79 Grade 3 kids had a blast being introduced to the world of computer science!","raw":"---\nlayout: post\ntitle: Hour of Code Challenge - Completed\ncategories:\n  - Community \ndate: 2016-12-13 19:00:00\ntags:\n  - Hour of Code\n  - Community\n  - Learning\nexcerpt: 79 Grade 3 kids had a blast being introduced to the world of computer science!\nauthorId: dave_white\n---\n\nIn case you missed it, I threw down the gauntlet to my fellow WesternDev members in September, stating that [I will donate $100 CAD to Code.orgÂ®][4]\nif any of them are able to participate in [Computer Science Education Week][1].\n\nFirst, I had to step up and walk the walk, and I succeeded better than I had expected.\n\n## 2 Days - 4 Classes - 79 Grade 3 Students == Success\n\nThis year it was very easy to organize an hour of code session at my youngest son's school. We had tried 2 years ago and the school board was leary so we didn't get a chance to do it.\nLast year, the school board was aware of Hour of Code and already actively encouraging teachers to try and participate so we did 2 classes last year.\n\nThis year we were able to organize 4 classes of Hour Of Code sessions for the all of the Grade 3 kids in the school. I did 2 back-to-back on a Tuesday morning, and another 2 on the Thursday morning.\nThis ended up being a really great schedule and made running the sessions very easy because all of the Grade 3 students are in a school building together and the classrooms are side-by-side.  \n\n#### And now for the proof...\n\n![Kieran][6]\n\n![Mme Roberts helping out][7]\n\n![Mme Seychelle helping out][8]\n\n![Mme Seychelle][12] ![Mme Roberts][11]\n![Mme Lebrecque][10] ![Mme Donlevy][9] \n\n## WesternDevs Response\n\nWhen I issued the challenge to my fellow WesternDev members, I recognized it is actually a very hard task to organize and execute an #HourOfCode event. The technical aspects of #HourOfCode is very easy, \nbut the logistics of coordinating with schools/organizations to get classroom time and computers is very difficult.\n\nThat said, one of my fellow WesternDev members, [James Chambers][5] was able to get TWO (2) session organized! They will be run during the week of Dec. 19. I know this isn't technically during \n[Computer Science Education Week][1] but I'm going to let that slide and [donate $100 CAD to Code.orgÂ®][4] for [James Chambers][5] meeting the challenge. James has promised me that a blog post (with PICS!)\nwill be following in the coming weeks! I'll update this post when that happens!\n\nI'm also going to [donate $100 CAD to Code.orgÂ®][4] for myself as I think that besides my time, this is a very important cause and they need our financial support as well as our support in time and effort.\n\nThank you all for reading. I hope I've encouraged you all to run an #HourOfCode session in your local community in the near future!!\n\n\n[1]: https://csedweek.org/\n[2]: https://code.org/\n[3]: https://hourofcode.com/\n[4]: https://code.org/help/\n[5]: http://www.westerndevs.com/bios/james_chambers\n[6]: https://i.imgur.com/2GF5Ovl.png \"My son Kieran doing Minecraft\"\n[7]: https://i.imgur.com/NBCFMBM.png \"Mme Roberts and some kids\"\n[8]: https://i.imgur.com/XYc090T.png \"Mme Seychelle and some kids\"\n[9]: https://i.imgur.com/hDzW8ox.png \"Mme Donlevy\"\n[10]:https://i.imgur.com/9tG90nY.png \"Mme Lebrecque\"\n[11]: https://i.imgur.com/dvxPBLE.png \"Mme Roberts\"\n[12]: https://i.imgur.com/sHRG91s.png \"MMe Seychelle\" ","categories":[{"name":"Community","slug":"Community","permalink":"https://westerndevs.com/categories/Community/"}],"tags":[{"name":"Hour of Code","slug":"Hour-of-Code","permalink":"https://westerndevs.com/tags/Hour-of-Code/"},{"name":"Community","slug":"Community","permalink":"https://westerndevs.com/tags/Community/"},{"name":"Learning","slug":"Learning","permalink":"https://westerndevs.com/tags/Learning/"}]},{"title":"Integration Testing with Entity Framework Core and SQL Server","authorId":"dave_paquette","slug":"integration-testing-with-entity-framework-core-and-sql-server","date":"2016-11-27 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/integration-testing-with-entity-framework-core-and-sql-server/","link":"","permalink":"https://westerndevs.com/Development/integration-testing-with-entity-framework-core-and-sql-server/","excerpt":"Entity Framework Core makes it easy to write tests that execute against an in-memory store but sometimes we want to actually run our tests against a real relational database. In this post, we look at how to create an integration test that runs against a real SQL Server database.","raw":"---\nlayout: post\ntitle: Integration Testing with Entity Framework Core and SQL Server\ntags:\n  - ASP.NET Core\n  - Entity Framework\n  - Testing\ncategories:\n  - Development\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2016/11/27/integration-testing-with-entity-framework-core-and-sql-server.aspx'\ndate: 2016-11-27 17:00:00\nexcerpt: Entity Framework Core makes it easy to write tests that execute against an in-memory store but sometimes we want to actually run our tests against a real relational database. In this post, we look at how to create an integration test that runs against a real SQL Server database.\n---\nEntity Framework Core makes it easy to write tests that execute against an in-memory store. Using an in-memory store is convenient since we don't need to worry about setting up a relational database. It also ensures our unit tests run quickly so we aren't left waiting hours for a large test suite to complete.\n\nWhile Entity Framework Core's in-memory store works great for many scenarios, there are some situations where it might be better to run our tests against a real relational database. Some examples include when loading entities using raw SQL or when using SQL Server specific features that can not be tested using the in-memory provider. In this case, the tests would be considered an integration test since we are no longer testing our Entity Framework context in isolation. We are testing how it will work in the real world when connected to SQL Server.\n\n## The Sample Project\nFor this example, I used the following simple model and DbContext classes.\n\n{% codeblock lang:csharp %}\npublic class Monster\n{\n    public int Id { get; set; }\n    public string Name { get; set; }\n    public bool IsScary { get; set; }        \n    public string Colour { get; set; }\n}\n{% endcodeblock %}\n\n{% codeblock lang:csharp %}\npublic class MonsterContext : DbContext\n{\n    public MonsterContext(DbContextOptions<MonsterContext> options)\n        : base(options)\n    {\n    }\n\n    public DbSet<Monster> Monsters { get; set; }\n}\n{% endcodeblock %}\n\nIn an ASP.NET Core application, the context is configured to use SQL Server in the `Startup.ConfigureServices` method.\n\n{% codeblock lang:csharp %}\nservices.AddDbContext<MonsterContext>(options =>\n{\n    options.UseSqlServer(\"DefaultConnection\");\n});\n{% endcodeblock %}\n\nThe `DefaultConnection` is defined in `appsettings.json` which is loaded at startup.\n\n{% codeblock lang:javascript %}\n{\n    \"ConnectionStrings\": {\n        \"DefaultConnection\": \"Server=(localdb)\\\\mssqllocaldb;Database=monsters_db;Trusted_Connection=True;MultipleActiveResultSets=true\"\n    }\n}\n{% endcodeblock %}\n\nThe `MonsterContext` is also configured to use Migrations which were initialized using the `dotnet ef migrations add InitialCreate` command. For more on Entity Framework Migrations, see the [official tutorial](https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/migrations).\n\nAs a simple example, I created a query class that loads *scary* monsters from the database using a SQL query instead of querying the `Monsters` `DbSet` directly.    \n\n{% codeblock lang:csharp %}\npublic class ScaryMonstersQuery\n{\n    private MonsterContext _context;\n\n    public ScaryMonstersQuery(MonsterContext context)\n    {\n        _context = context;\n    }\n\n    public IEnumerable<Monster> Execute()\n    {\n        return _context.Monsters\n            .FromSql(\"SELECT Id, Name, IsScary, Colour FROM Monsters WHERE IsScary = {0}\", true);\n    }\n        \n}\n{% endcodeblock %}\n\nTo be clear, a better way to write this query is `_context.Monster.Where(m => m.IsScary == true)`, but I wanted a simple example. I also wanted to use `FromSql` because it is inherently difficult to unit test. The `FromSql` method doesn't work with the in-memory provider since it requires a relational database. It is also an extension method which means we can't simply mock the context using a tool like `Moq`. We could of course create a wrapper service that calls the `FromSql` extension method and mock that service but this only shifts the problem. The _wrapper_ approach would allow us to ensure that `FromSql` is called in the way we expect it to be called but it would not be able to ensure that the query will actually run successfully and return the expected results.\n\nAn integration test is a good option here since it will ensure that the query runs exactly as expected against a real SQL Server database.\n\n## The Test\nI used xunit as the test framework in this example. In the constructor, which is the setup method for any tests in the class, I configure an instance of the `MonsterContext` connecting to a localdb instance using a database name containing a random guid. Using a guid in the database name ensures the database is unique for this test. Uniqueness is important when running tests in parallel because it ensures these tests won't impact any other tests that aer currently running. After creating the context, a call to `_context.Database.Migrate()` creates a new database and applies any Entity Framework migrations that are defined for the `MonsterContext`.  \n\n{% codeblock lang:csharp %}\npublic class SimpleIntegrationTest : IDisposable\n{\n    MonsterContext _context;\n\n    public SimpleIntegrationTest()\n    {\n        var serviceProvider = new ServiceCollection()\n            .AddEntityFrameworkSqlServer()\n            .BuildServiceProvider();\n\n        var builder = new DbContextOptionsBuilder<MonsterContext>();\n\n        builder.UseSqlServer($\"Server=(localdb)\\\\mssqllocaldb;Database=monsters_db_{Guid.NewGuid()};Trusted_Connection=True;MultipleActiveResultSets=true\")\n                .UseInternalServiceProvider(serviceProvider);\n\n        _context = new MonsterContext(builder.Options);\n        _context.Database.Migrate();\n\n    }\n\n    [Fact]\n    public void QueryMonstersFromSqlTest()\n    {\n        //Add some monsters before querying\n        _context.Monsters.Add(new Monster { Name = \"Dave\", Colour = \"Orange\", IsScary = false });\n        _context.Monsters.Add(new Monster { Name = \"Simon\", Colour = \"Blue\", IsScary = false });\n        _context.Monsters.Add(new Monster { Name = \"James\", Colour = \"Green\", IsScary = false });\n        _context.Monsters.Add(new Monster { Name = \"Imposter Monster\", Colour = \"Red\", IsScary = true });\n        _context.SaveChanges();\n\n        //Execute the query\n        ScaryMonstersQuery query = new ScaryMonstersQuery(_context);\n        var scaryMonsters = query.Execute();\n\n        //Verify the results\n        Assert.Equal(1, scaryMonsters.Count());\n        var scaryMonster = scaryMonsters.First();\n        Assert.Equal(\"Imposter Monster\", scaryMonster.Name);\n        Assert.Equal(\"Red\", scaryMonster.Colour);\n        Assert.True(scaryMonster.IsScary);\n    }\n\n    public void Dispose()\n    {\n        _context.Database.EnsureDeleted();\n    }\n}\n{% endcodeblock %}\n\nThe actual test itself happens in the `QueryMonstersFromSqlTest` method. I start by adding some sample data to the database. Next, I create and execute the `ScaryMonstersQuery` using the context that was created in the setup method. Finally, I verify the results, ensuring that the expected data is returned from the query.\n\nThe last step is the `Dispose` method which in xunit is the teardown for any tests in this class. We don't want all these test databases hanging around forever so this is the place to delete the database that was created in the setup method. The database is deleted by calling `_context.Database.EnsureDeleted()`. \n\n## Use with Caution\nThese tests are slow! The very simple example above takes 13 seconds to run on my laptop. My advice here is to use this sparingly and only when it really adds value for your project. If you end up with a large number of these integration tests, I would consider splitting the integration tests into a separate test suite and potentially running them on a different schedule than my unit test suite (e.g. Nightly instead of every commit). \n\n## The Code\nYou can browse or download the source on [GitHub](https://github.com/AspNetMonsters/EntityFrameworkCoreIntegrationTest).","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"Entity Framework","slug":"Entity-Framework","permalink":"https://westerndevs.com/tags/Entity-Framework/"},{"name":"Testing","slug":"Testing","permalink":"https://westerndevs.com/tags/Testing/"}]},{"title":"Creating a New View Engine in ASP.NET Core","authorId":"dave_paquette","slug":"creating-a-new-view-engine-in-asp-net-core","date":"2016-11-22 17:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/creating-a-new-view-engine-in-asp-net-core/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/creating-a-new-view-engine-in-asp-net-core/","excerpt":"At the ASP.NET Hackathon in Redmond, we replaced the Razor view engine with Pug. It started off as a joke but it kind of worked okay so we rolled with it.","raw":"---\nlayout: post\ntitle: Creating a New View Engine in ASP.NET Core\ntags:\n  - ASP.NET Core\n  - MVC\ncategories:\n  - ASP.NET Core\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2016/11/22/creating-a-new-view-engine-in-asp-net-core.aspx'\ndate: 2016-11-22 12:00\nexcerpt: At the ASP.NET Hackathon in Redmond, we replaced the Razor view engine with Pug. It started off as a joke but it kind of worked okay so we rolled with it. \n---\n\nEarlier in November, the [ASP.NET Monsters](http://aspnetmonsters.com/2016/01/welcome/) had the opportunity to take part in the ASP.NET Core hackathon at the Microsoft MVP Summit. In past years, we have used the hackathon as an opportunity to spend some time working on GenFu. This year, we wanted to try something a little different.\n\n## The Crazy Idea\nA few months ago, we had [Taylor Mullen on The Monsters Weekly](https://channel9.msdn.com/Series/aspnetmonsters/ASPNET-Monsters-59-Razor-with-Taylor-Mullen) to chat about Razor in ASP.NET Core. At some point during that interview, it was pointed that MVC is designed in a way that a new view engine could easily be plugged into the framework. It was also noted that implementing a view engine is a really big job. This got us to thinking...what if we could find an existing view engine of some sort. How easy would it be to get actually put a new view engine in MVC?\n\nAnd so, that was our goal for the hackathon. Find a way to replace Razor with an alternate view engine in a single day of hacking.\n\n## Finding a Replacement\nWe wanted to pick something that in no way resembled Razor. Simon suggested [Pug](https://pugjs.org/api/getting-started.html) (previously known as Jade), a popular view template engine used in [Express](https://expressjs.com). In terms of syntax, Pug is about as different from Razor as it possibly could be. Pug uses whitespace to indicate nesting of elements and does away with angle brackets all together. For example, the following template:\n\n{% codeblock %}\ndiv\n    a(href='google.com') Google\n{% endcodeblock %}  \n\nwould generate this HTML:\n\n{% codeblock lang:html %}\n<div>\n    <a href=\"google.com\">Google</a>\n</div>\n{% endcodeblock %}\n\n## Calling Pug from ASP.NET Core\nThe first major hurdle for us was figuring out a way to compile pug templates from within an ASP.NET Core application. Pug is a JavaScript based template engine and we only had a single day to pull this off so a full port of the engine to C# was not feasible. \n\nOur first thought was to use Edgejs to call Pug's JavaScript compile function. Some quick prototyping showed us that this worked but Edgejs doesn't have support for .NET Core. This lead us to explore the [JavaScriptServices](https://github.com/aspnet/JavaScriptServices) packages created by the ASP.NET Core team. Specifically the [Node Services](https://github.com/aspnet/JavaScriptServices/tree/dev/src/Microsoft.AspNetCore.NodeServices#microsoftaspnetcorenodeservices) package which allows us to easily call out to a JavaScript module from within an ASP.NET Core application.\n\nTo our surpise, this not only worked, it was also easy! We created a very simple file called pugcompile.js.\n\n{% codeblock lang:javascript %}\n\nvar pug = require('pug');\n\nmodule.exports = function (callback, viewPath, model) {\n\tvar pugCompiledFunction = pug.compileFile(viewPath);\n\tcallback(null, pugCompiledFunction(model));\t\n};  \n\n{% endcodeblock %}\n\nCalling this JavaScript from C# is easy thanks to the Node Services package. Assuming `model` is the view model we want to bind to the template and `mytemplate.pug` is the name of the file containing the pug template:\n\n{% codeblock lang:csharp %}\nvar html = await _nodeServices.InvokeAsync<string>(\"pugcompile\", \"mytemplate.pug\", model);\n{% endcodeblock %}\n\nNow that we had proven this was possible, it was time to integrate this with MVC by creating a new MVC View Engine.\n\n## Creating the Pugzor View Engine\nWe decided to call our view engine Pugzor which is a combination of Pug and Razor. Of course, this doesn't really make much sense since our view engine really has nothing to do with Razor but naming is hard and we thought we were being funny.\n\nKeeping in mind our goal of implenting a view engine in a single day, we wanted to do this with the simplest way possible. After spending some time digging through the source code for MVC, we determined that we needed to implement the `IViewEngine` interface as well as implement a custom `IView`. \n\nThe `IViewEngine` is responsible for locating a view based on a `ActionContext` and a `ViewName`.  When a controller returns a `View`, it is the `IViewEngine`'s `FindView` method that is responsible for finding a view based on some convetions. The `FindView` method returns a `ViewEngineResult` which is a simple class containing a `boolean Success` property indicating whether or not a view was found and an `IView View` property containing the view if it was found. \n\n{% codeblock lang:csharp %}\n/// <summary>\n/// Defines the contract for a view engine.\n/// </summary>\npublic interface IViewEngine\n{\n    /// <summary>\n    /// Finds the view with the given <paramref name=\"viewName\"/> using view locations and information from the\n    /// <paramref name=\"context\"/>.\n    /// </summary>\n    /// <param name=\"context\">The <see cref=\"ActionContext\"/>.</param>\n    /// <param name=\"viewName\">The name of the view.</param>\n    /// <param name=\"isMainPage\">Determines if the page being found is the main page for an action.</param>\n    /// <returns>The <see cref=\"ViewEngineResult\"/> of locating the view.</returns>\n    ViewEngineResult FindView(ActionContext context, string viewName, bool isMainPage);\n\n    /// <summary>\n    /// Gets the view with the given <paramref name=\"viewPath\"/>, relative to <paramref name=\"executingFilePath\"/>\n    /// unless <paramref name=\"viewPath\"/> is already absolute.\n    /// </summary>\n    /// <param name=\"executingFilePath\">The absolute path to the currently-executing view, if any.</param>\n    /// <param name=\"viewPath\">The path to the view.</param>\n    /// <param name=\"isMainPage\">Determines if the page being found is the main page for an action.</param>\n    /// <returns>The <see cref=\"ViewEngineResult\"/> of locating the view.</returns>\n    ViewEngineResult GetView(string executingFilePath, string viewPath, bool isMainPage);\n}\n{% endcodeblock %}\n\nWe decided to use the same view location conventions as Razor. That is, a view is located in `Views/{ControllerName}/{ActionName}.pug`.\n\nHere is a simplified version of the FindView method for the `PugzorViewEngine`:\n\n{% codeblock lang:csharp %}\npublic ViewEngineResult FindView(\n    ActionContext actionContext,\n    string viewName,\n    bool isMainPage)\n{\n    var controllerName = GetNormalizedRouteValue(actionContext, ControllerKey);\n \n    var checkedLocations = new List<string>();\n    foreach (var location in _options.ViewLocationFormats)\n    {\n        var view = string.Format(location, viewName, controllerName);\n        if(File.Exists(view))\n            return ViewEngineResult.Found(\"Default\", new PugzorView(view, _nodeServices));\n        checkedLocations.Add(view);\n    }\n    return ViewEngineResult.NotFound(viewName, checkedLocations);\n}\n{% endcodeblock %}\n\nYou can view the complete implentation on [GitHub](https://github.com/AspNetMonsters/pugzor/blob/master/src/pugzor.core/PugzorViewEngine.cs).\n\nNext, we created a class called `PugzorView` which implements `IView`. The `PugzorView` takes in a path to a pug template and an instance of `INodeServices`. The MVC framework calls the `IView`'s `RenderAsync` when it is wants the view to be rendered. In this method, we call out to `pugcompile` and then write the resulting HTML out to the view context.\n\n{% codeblock lang:csharp %}\npublic class PugzorView : IView\n{\n    private string _path;\n    private INodeServices _nodeServices;\n\n    public PugzorView(string path, INodeServices nodeServices)\n    {\n        _path = path;\n        _nodeServices = nodeServices;\n    }\n\n    public string Path\n    {\n        get\n        {\n            return _path;\n        }\n    }\n\n    public async Task RenderAsync(ViewContext context)\n    {\n        var result = await _nodeServices.InvokeAsync<string>(\"./pugcompile\", Path, context.ViewData.Model);\n        context.Writer.Write(result);\n    }\n}\n{% endcodeblock %}\n\nThe only thing left was to configure MVC to use our new view engine. At first, we thought we could easy add a new view engine using the `AddViewOptions` extension method when adding MVC to the service collection.\n\n{% codeblock lang:csharp %}\nservices.AddMvc()\n        .AddViewOptions(options =>\n            {\n                options.ViewEngines.Add(new PugzorViewEngine(nodeServices));\n            });\n{% endcodeblock %}\n\nThis is where we got stuck.  We can't add a concrete instance of the `PugzorViewEngine` to the `ViewEngines` collection in the `Startup.ConfigureServices` method because the view engine needs to take part in dependency injection. The `PugzorViewEngine` has a dependency on `INodeServices` and we want that to be injected by ASP.NET Core's dependency injection framework. Luckily, the all knowning Razor master Taylor Mullen was on hand to show us the right way to register our view engine.   \n\nThe recommended approach for adding a view engine to MVC is to create a custom setup class that implements `IConfigureOptions<MvcViewOptions>`. The setup class takes in an instance of our `IPugzorViewEngine` via constructor injection. In the configure method, that view engine is added to the list of view engines in the `MvcViewOptions`.\n\n{% codeblock lang:csharp %}\npublic class PugzorMvcViewOptionsSetup : IConfigureOptions<MvcViewOptions>\n{\n    private readonly IPugzorViewEngine _pugzorViewEngine;\n\n    /// <summary>\n    /// Initializes a new instance of <see cref=\"PugzorMvcViewOptionsSetup\"/>.\n    /// </summary>\n    /// <param name=\"pugzorViewEngine\">The <see cref=\"IPugzorViewEngine\"/>.</param>\n    public PugzorMvcViewOptionsSetup(IPugzorViewEngine pugzorViewEngine)\n    {\n        if (pugzorViewEngine == null)\n        {\n            throw new ArgumentNullException(nameof(pugzorViewEngine));\n        }\n\n        _pugzorViewEngine = pugzorViewEngine;\n    }\n\n    /// <summary>\n    /// Configures <paramref name=\"options\"/> to use <see cref=\"PugzorViewEngine\"/>.\n    /// </summary>\n    /// <param name=\"options\">The <see cref=\"MvcViewOptions\"/> to configure.</param>\n    public void Configure(MvcViewOptions options)\n    {\n        if (options == null)\n        {\n            throw new ArgumentNullException(nameof(options));\n        }\n\n        options.ViewEngines.Add(_pugzorViewEngine);\n    }\n}\n{% endcodeblock %}\n\nNow all we need to do is register the setup class and view engine the `Startup.ConfigureServices` method.\n\n{% codeblock lang:chsarp %}\nservices.AddTransient<IConfigureOptions<MvcViewOptions>, PugzorMvcViewOptionsSetup>();\nservices.AddSingleton<IPugzorViewEngine, PugzorViewEngine>();\n{% endcodeblock %}\n\nLike magic, we now have a working view engine. Here's a simple example:\n\n#### Controllers/HomeController.cs\n\n{% codeblock lang:csharp %}\npublic IActionResult Index()\n{\n    ViewData.Add(\"Title\", \"Welcome to Pugzor!\");\n    ModelState.AddModelError(\"model\", \"An error has occurred\");\n    return View(new { People = A.ListOf<Person>() }); \n}\n{% endcodeblock %}\n\n#### Views/Home/Index.pug\n\n{% codeblock %}\nblock body\n\th2 Hello\n\tp #{ViewData.title} \n\ttable(class='table')\n\t\tthead\n\t\t\ttr\n\t\t\t\tth Name\n\t\t\t\tth Title\n\t\t\t\tth Age\n\t\ttbody\n\t\t\teach val in people\n\t\t\t\ttr\n\t\t\t\t\ttd= val.firstName\n\t\t\t\t\ttd= val.title\n\t\t\t\t\ttd= val.age\t\n{% endcodeblock %}\n\n#### Result\n{% codeblock lang:html %}\n<h2>Hello</h2>\n<p>Welcome to Pugzor! </p>\n<table class=\"table\">\n    <thead>\n        <tr>\n            <th>Name</th>\n            <th>Title</th>\n            <th>Age</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr><td>Laura</td><td>Mrs.</td><td>38</td></tr>\n        <tr><td>Gabriel</td><td>Mr. </td><td>62</td></tr>\n        <tr><td>Judi</td><td>Princess</td><td>44</td></tr>\n        <tr><td>Isaiah</td><td>Air Marshall</td><td>39</td></tr>\n        <tr><td>Amber</td><td>Miss.</td><td>69</td></tr>\n        <tr><td>Jeremy</td><td>Master</td><td>92</td></tr>\n        <tr><td>Makayla</td><td>Dr.</td><td>15</td></tr>\n        <tr><td>Sean</td><td>Mr. </td><td>5</td></tr>\n        <tr><td>Lillian</td><td>Mr. </td><td>3</td></tr>\n        <tr><td>Brandon</td><td>Doctor</td><td>88</td></tr>\n        <tr><td>Joel</td><td>Miss.</td><td>12</td></tr>\n        <tr><td>Madeline</td><td>General</td><td>67</td></tr>\n        <tr><td>Allison</td><td>Mr. </td><td>21</td></tr>\n        <tr><td>Brooke</td><td>Dr.</td><td>27</td></tr>\n        <tr><td>Jonathan</td><td>Air Marshall</td><td>63</td></tr>\n        <tr><td>Jack</td><td>Mrs.</td><td>7</td></tr>\n        <tr><td>Tristan</td><td>Doctor</td><td>46</td></tr>\n        <tr><td>Kandra</td><td>Doctor</td><td>47</td></tr>\n        <tr><td>Timothy</td><td>Ms.</td><td>83</td></tr>\n        <tr><td>Milissa</td><td>Dr.</td><td>68</td></tr>\n        <tr><td>Lekisha</td><td>Mrs.</td><td>40</td></tr>\n        <tr><td>Connor</td><td>Dr.</td><td>73</td></tr>\n        <tr><td>Danielle</td><td>Princess</td><td>27</td></tr>\n        <tr><td>Michelle</td><td>Miss.</td><td>22</td></tr>\n        <tr><td>Chloe</td><td>Princess</td><td>85</td></tr>\n    </tbody>\n</table>\n{% endcodeblock %}\n\nAll the features of pug work as expected, including templage inheritance and inline JavaScript code. Take a look at our [test website](https://github.com/AspNetMonsters/pugzor/tree/master/test/pugzore.website) for some examples.\n\n## Packaging it all up\nSo we reached our goal of creating an alternate view engine for MVC in a single day. We had some time left so we thought we would try to take this one step further and create a NuGet package. There were some challenges here, specifically related to including the required node modules in the NuGet package. Simon is planning to write a separate blog post on that topic.\n\nYou can give it a try yourself. Add a reference to the `pugzor.core` NuGet package then call `.AddPugzor()` after `.AddMvc()` in the `Startup.ConfigureServices` method.\n\n{% codeblock lang:chsarp %}\npublic void ConfigureServices(IServiceCollection services)\n{\n    // Add framework services.\n    services.AddMvc().AddPugzor();\n}\n{% endcodeblock %}\n\nRazor still works as the default but if no Razor view is found, the MVC framework will try using the PugzorViewEngine. If a matching pug template is found, that template will be rendered. \n\n![Pugzor](http://www.davepaquette.com/images/pugzor.png)\n\n## Wrapping it up\nWe had a blast working on this project. While this started out as a silly excercise, we sort of ended up with something that could be useful. We were really surprised at how easy it was to create a new view engine for MVC. We don't expect that Pugzor will be wildly popular but since it works we thought we would put it out there and see what people think. \n\nWe have some [open issues](https://github.com/AspNetMonsters/pugzor/issues) and some ideas for how to extend the `PugzorViewEngine`. Let us know what you think or jump in and contribute some code. We accept pull requests :-) \n\n","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"}]},{"title":"How to Blog with VSTS (Part 4)","authorId":"david_wesst","slug":"How-to-Blog-with-VSTS-Part-4","date":"2016-11-21 12:30:00+0000","updated":"2016-11-21 12:30:00+0000","comments":true,"path":"alm/How-to-Blog-with-VSTS-Part-4/","link":"","permalink":"https://westerndevs.com/alm/How-to-Blog-with-VSTS-Part-4/","excerpt":"I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 4 of 5, we setup a _Build_ script.","raw":"---\nlayout: post\ntitle: How to Blog with VSTS (Part 4)\ncategories:\n  - alm\ntags:\n  - visual studio team services\n  - vsts\n  - alm\n  - hexo\ndate: 2016-11-21 07:30:00\nupdated: 2016-11-21 07:30:00\nexcerpt: I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 4 of 5, we setup a _Build_ script.\nauthorId: david_wesst\n---\n\n_This is part 4 of 5 of my **How to Blog with VSTS** series. Links to the other parts will be added as they are made available._\n\n+ [Part 1: Setup][1]\n+ [Part 2: Code][2] \n+ [Part 3: Work][3]\n+ [Part 4: Build][4]\n+ Part 5: Release\n\n[1]: https://blog.davidwesst.com/2016/10/How-to-Blog-with-VSTS-Part-1/\n[2]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-2/\n[3]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-3/\n[4]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-4/\n[5]: #\n\n---\n\nI've been tinkering with Visual Studio Team Services off and on since its public release, but never did anything really productive with it. Over the past couple of weeks, I finally bit the bullet and decided to move my most prevelant personal project into VSTS: this blog.\n\nIn this post we are we going to create a build script in VSTS so we can generate our blog content consistently and get it ready to deploy somewhere.\n\n## Creating Our Build Script\nFirst, we need to navigate over to the _Build_ section of VSTS, which you can find in the navigation menu at the top of the page.\n\n<!-- image of build menu item -->\n![Build and Releases Menu](http://i.imgur.com/7S55XWDl.png)\n\nThis is where we're going to create our build script by hitting the \"Create New Build\" button.\n\n<!-- image of new build button -->\n![New Build Button](http://i.imgur.com/6uCoEEFl.png)\n\n## Adding Build Tasks (Hexo Edition)\nVSTS provides plenty of build tasks. Statically generated sites will have different build tasks, so I'm going to walk you through the build tasks I setup for [Hexo](https://hexo.io/).\n\nIt's pretty straight forward, but it helps to list them out in order.\n\n1. Install global npm dependencies\n2. Install local npm dependencies\n3. Other stuff that needs doing\n4. Generate site content\n5. Store site content as build asset OR Deploy!\n\nLet's go through these.\n\n### npm Global Dependencies\nIn this case, I only have one which is `hexo-cli` so that I can run the `hexo generate` command later on in our build. \n\nYou could add it's own npm task here, by adding a new task and setting the parameters of the task accordingly.\n\n<!-- npm install -g task -->\n![npm task](http://i.imgur.com/h1HFRAJl.png)\n\nPersonally, I don't have a separate task. I use my project's `package.json` and set a [_preinstall_ script](https://docs.npmjs.com/misc/scripts) which gets run before the `npm install` command.\n\nEither one works really. I just like having my source code setup be as simple as `npm install` and then `npm start` to run the site.\n\n### npm Local Dependencies\nJust like the previous, except this time we're using the default parameters. We just need the build to run `npm install` and let it do it's thing.\n\n### Other Stuff\nEvery project is a unique snowflake, and sometimes you have some extra tasks you need accomplished. In my case, I have a custom theme that I build every time. To accomplish this, I have a PowerShell script in the `\\tools` directory of my source code that gets run every time.\n\n<!-- powershell task with parameters -->\n![PowerShell Task with Parameters](http://i.imgur.com/aXmLKcml.png)\n\n#### CMD, Powershell, and Bash Tasks\nYou're not limited to just PowerShell, but you can have CMD or even bash shell scripts executed. The only caveat of running these is making sure that the build server being used to run your build has these capabilities.\n\nIf you're using the hosted server, like I do, then PowerShell and CMD are your best bets. I did try to have both PowerShell and Bash, which resulted in getting a message saying I \"didn't have a host that had these capabilities\".\n\nMaybe in time we'll have hosted servers that do both, but until then you'll need to setup your own server to handle these unique dependencies or try can conform your code to use one or the other.\n\n#### Keeping it Open Source\nIf you're looking to keep your source open on, you'll likely want to push it out of VSTS and into GitHub or somewhere where the public can get their hands and eyes on it.    \n\nThis is the job for another script task. In my case, I followed [this blog post](https://nkdagility.com/open-source-vsts-tfs-github-better-devops/) that [Dylan Smith](http://www.westerndevs.com/bios/dylan_smith/) directed me to and followed along.\n\n### Generate Content\nAgain, another script task. But this one is easier, as we're just running `hexo generate`. \n\nYou could write a whole script file for this too, but I opted to make it simple and just configure the build task itself.\n\n<!-- hexo generate build task -->\n![hexo generate build task](http://i.imgur.com/dGWjUTNl.png)\n\n### Save and Publish Site Content\nWe'll discuss this further when we get to [releases][5] but we need to save our content assets so we can publish them later. For hexo, this is usually the contents of the `public` folder.\n\nTo handle this, I use the _Copy and Publish Build Assets_ task and configure it to save the contents of the `public` folder so that it persists after the build is complete.\n\n<!-- publish assets task -->\n![Copy and Publish Build Assets](http://i.imgur.com/LJTk1wKl.png)\n\n#### ...or Deploy!\nAnother option is to just deploy it directly from the build script and skip the whole [release management][5] component. This would allow you do a deployment everytime you build, making sure the latest source code is live.\n\nAgain, it's a matter of preference. The reason I like [releases][5] over this model is to be able to manage the release of source code independenly of the build itself. \n\n## Triggering the Build\nI have two build scripts that are almost identical. One that I use for development and continuous integration. The other is scheduled to prepare a for a weekly release of my blog.\n\nVSTS accomodates both of these options, which you can see in the _Triggers_ tab of the build script.\n\n<!-- image of the triggers tab -->\n![Triggers Tab](http://i.imgur.com/YHlzH7Cl.png)\n\nFor my development build script, I trigger the build on pushes to the `master` branch of my repository. I have also configured this build script to run on a private build agent that I have setup.\n\nI won't be covering private build agents in this series of posts, but I assure you it's very easy. I setup my development machine as a private build server, and had it setup in about 15 minutes after following [these instructions](https://www.visualstudio.com/en-us/docs/build/admin/agents/v2-windows).\n\n## Next Up: Releases!\nWe're almost done, and technically you don't need the next step if you just want to do continual deployment and have a deployment step in your build script. That being said, I like having the release pipeline as it gives me a few other things to ensure my blogging goes out without a hitch.\n\nMore on that [next time][5].\n","categories":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/categories/alm/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/tags/hexo/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"}]},{"title":"What Is DevOps?","authorId":"dylan_smith","slug":"What-Is-DevOps","date":"2016-11-18 07:14:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"alm/What-Is-DevOps/","link":"","permalink":"https://westerndevs.com/alm/What-Is-DevOps/","excerpt":"My personal definition of DevOps, in the context of lean principles.","raw":"---\nlayout: post\ntitle: What Is DevOps?\ntags:\n  - devops\n  - alm\ncategories:\n  - alm\ndate: 2016-11-18 2:14:00\nexcerpt: My personal definition of DevOps, in the context of lean principles.\nauthorId: dylan_smith\n---\n\nI'm sure there's hundreds of posts out there trying to define DevOps, this is my $0.02 on the topic.\n\nIf you subscribe to the lean software development way of thinking, you think about a pipeline of value that results in working software.  For example this might be: Analysis -> Dev -> Test -> Deploy -> Monitor\n\nAs with any pipeline, there is likely a bottleneck somewhere that restricts the flow of value.  Lean is all about identifying and attacking these bottlenecks.  10 years ago - before Agile - the bottleneck was probably Analysis, or maybe Test.  With Agile development becoming mainstream over the last decade, it has done a pretty good job of attacking those bottlenecks, resulting in analysis and test becoming more just-in-time, spread out over the course of a project, and embedded in the regular dev workflows.  They are no longer the bottleneck.\n\nA new bottleneck has arisen, that is at the boundary of the dev/test team and the operations team.  These tend to be very separate teams, with a clear handoff between them.  This results in friction, and a bottleneck in the flow of value.\n\nSo back to my definition of DevOps:\n\n### DevOps is recognizing that a bottleneck exists between Development and Operations, and doing something to address it.\n\nThis bottleneck manifests itself in a number of ways, here's a few common ones:\n\n1. Provisioning / Managing Infrastructure - Creating environments for dev/test/uat/prod, and managing or making changes to those environments.\n2. Deployment to Production - Still often done by writing word documents with deployment instructions, scheduling a time for Operations team to do the deployment, then delivering the document and deployment packages to Operations.\n3. Production Support / Monitoring - Is the application available, are there errors, what is the load on the servers.\n\nThese are common causes of friction within development organizations.  I could go into details, but I expect just reading the points above you can identify with at least some of these concerns.\n\nWhen I hear people talk about DevOps, I often hear 3 common approaches:\n\n1. Closer collaboration between the development and operations team.  Perhaps having an Ops team member actively involved with the dev team throughout the project.\n2. Increased Dev Team Responsibility - Writing automation scripts to provision / manage environments, deployment automation scripts, etc\n3. New Role: DevOps Engineer - Introducing a new role specifically to sit between dev and ops to facilitate the automation and collaboration.\n\nAlthough #2 is the approach I see most often, I believe all 3 approaches are perfectly valid ways to attack the problem.\n\nIf you want help adopting DevOps practices or technologies, my company Imaginet is always happy to help.  Check out our DevOps offerings here: [http://www.imaginet.com/devops-as-a-service/](http://www.imaginet.com/devops-as-a-service/)\n\n\n","categories":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/categories/alm/"}],"tags":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"},{"name":"devops","slug":"devops","permalink":"https://westerndevs.com/tags/devops/"}]},{"title":"How to Blog with VSTS (Part 3)","authorId":"david_wesst","slug":"How-to-Blog-with-VSTS-Part-3","date":"2016-11-14 12:30:00+0000","updated":"2016-11-14 12:30:00+0000","comments":true,"path":"alm/How-to-Blog-with-VSTS-Part-3/","link":"","permalink":"https://westerndevs.com/alm/How-to-Blog-with-VSTS-Part-3/","excerpt":"I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 3 of 5, we start to make plans and _Work_ on our blog.","raw":"---\nlayout: post\ntitle: How to Blog with VSTS (Part 3)\ntags:\n  - visual studio team services\n  - vsts\n  - alm\n  - hexo\ncategories:\n  - alm\nexcerpt: I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 3 of 5, we start to make plans and _Work_ on our blog.\nauthorId: david_wesst\ndate: 2016-11-14 07:30:00\nupdated: 2016-11-14 07:30:00\n---\n\n_This is part 3 of 5 of my **How to Blog with VSTS** series. Links to the other parts will be added as they are made available._\n\n+ [Part 1: Setup][1]\n+ [Part 2: Code][2] \n+ [Part 3: Work][3]\n+ Part 4: Build\n+ Part 5: Release\n\n[1]: https://blog.davidwesst.com/2016/10/How-to-Blog-with-VSTS-Part-1/\n[2]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-2/\n[3]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-3/\n[4]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-4/\n[5]: #\n\n---\n\nI've been tinkering with Visual Studio Team Services off and on since its public release, but never did anything really productive with it. Over the past couple of weeks, I finally bit the bullet and decided to move my most prevelant personal project into VSTS: this blog.\n\nIn this post we are going start blogging in a controlled and coordinated manner by planning and using the _Work_ tab in our VSTS project.\n\n## What is there to plan?\nPlenty. Although features and bugs are obvious use for _Work_ tab, my primary use case is post planning. I'll be focusing on using _Work_ for blog posts, but realize that you can use these same practices for adding new enhancements or logging bugs in your blog.\n\n## Post Ideas in the Backlog\nI tend to have a lot more ideas for posts than I write. Ideas are quick and easy, but writing them (at least writing them properly) is the time consuming part.\n\nRather than trying to write a post for every idea, I use the backlog to save my post ideas as they come up. In an Agile project, I create new user story for every post idea and tag it as a \"post\" so that I can easy filter them later.\n\nThe user story title is the idea for the post title, and I use the description to outline the post.\n\n##### Blogger Note\nThe title and the outline are only two things I fill out on the work item, but without both I don't even bother saving the item because I won't remember the context without the outline.\n\n## Planning my Posts\nNow that we have a post backlog, I start planning what posts I am going to write. In my case, this is just like sprint planning and VSTS makes this easy.\n\n### Setting up Iterations\nFor starters, I setup my iterations. I like planning about one month at a time, so I go with 4-week iterations and named by the month that consumes most of the iteration. This all happens in the _Work_ configuation section of the site.\n\n![Work Configuration Menu](http://i.imgur.com/E4GjQDM.png)\n\n![Creating a New Iteration](http://i.imgur.com/a27TBI4.png)\n\n### The ~~Sprint~~ Post Planning\nNow that we have iterations, I plan what posts I want to write for the month.\n\nI do this by dragging the posts onto the iteration on the right side, and adding tasks as described by the [VSTS team](https://www.visualstudio.com/en-us/docs/work/scrum/sprint-planning).\n\n![Adding a User Story to an Iteration](http://i.imgur.com/67xoqOK.gif)\n\n#### Tasks and Templates\nAlthough tasks on a blog post work item might sound excessive, my writing process has multiple steps to try and make it easy for me to blog. First I write the initial draft (text-only), followed by proof reading and filling in the images and links that I've left. Sometimes there are other tasks like updating links on old posts, like on this post which is part of a series of posts.\n\nSince the tasks are so similar, I created a quick template that I apply to a work item that automatically puts in the tags and general tasks.\n\n### Why do all this Planning?\nThe planning takes me about 15 minutes a month, but it's still a fair question.\n\nBecause my blog is something I do on my own time, I need to stay organized so that I can go from 0 to writing in 5 minutes. Between my day job, side project, family, social committments, and supporting a local user group, my time is limited. When I have 30 minutes, I try and get something done as quickly as possible.\n\nI found that without planning a lot of my time was spent trying to figure out what I did last time I blogged. Even if it only takes like 20 or 30 minutes to get myself sorted out and blogging, sometimes that is all the time I have to commit. If I spend all my time getting ready to blog, I won't have any time to _actually_ blog.\n\n## Writing a Post\nNow that we know what we're writing about, we can start writing posts. \n\nBecause we're using a [static site generator](https://www.staticgen.com/) a post is a new markdown file and possibly couple of image files. This is analagous adding a new feature to any other software project, and thus it is treated as such.\n\nFor me, I create a new [topic branch](  ) for the post. Since I host my source code in VSTS, I use the web UI to create a new branch in the work item. On my local machine, I create a new local branch that is mapped to the new remote branch and start create a new draft post with `hexo new draft \"My Post Title\"`.\n\n![Creating a new branch from the work item](http://i.imgur.com/3xZztPR.png)\n\nThis creates a new post file in the `source\\_drafts` folder of the project, and I start writing and going through the tasks. As I complete the tasks I make commits and associate them with the tasks and work items by adding the ID numbers in the messages. For example:\n\n```bash\ngit commit -m \"wrote initial draft. part of #201 closes task #212\"\n```\n\nThe `#201` and `#212` get picked up by VSTS and associate them with the cooresponding task and user story, just like it does in GitHub. Unlike GitHub, the `closes` key word doesn't actually close the work item, which I don't mind, but should be noted for those coming from GitHub like I did.\n\n![Related Work in a Work Item](http://i.imgur.com/ZO8nzTM.png)\n\nOnce I complete all my tasks and push all the commits to VSTS, I create a Pull Request and move the work item on the board to the \"Committed\" column.\n\n![A VSTS Pull Request](http://i.imgur.com/xT1tfEf.png)\n\n### Why a Pull Request?\nAgain, another good question. You're pretty good at this.\n\nSome might it's completely unnecessary for a solo project like a blog, I like to use them for a few reasons. \n\nFor starters, VSTS (just like GitHub) gives you a web-based interface for reviewing all the code you're about to merge into your branch. This gives me an opportunity to review my post and to make sure all the pieces are in place. I'm usually about a week ahead of each post, so doing a pull request\n\nSecond, it makes the merge a bit more noticeable in the project. When I have a Pull Request waiting, my Visual Studio Code plugin will highlight it in the status bar, and it will also show up in the web-based UI. \n\nLastly, and most importantly, it provides me a way to publish a post without needing to have access source code and a git command line. When I'm ready to share a post, I merge a pull request into the `master` branch which then triggers a [build][4] and eventually a [release][5], which we will cover in future posts. All of that can be done from the web UI, which means as long as I have an internet connection on a device, I can login and publish a post.\n\n#### What about Scheduling Posts?\nThere are ways to do that with Hexo and plenty of other static site generators. In my case, I use the [Build][4] and [Release][5] parts of VSTS to manage this for me. To be continued...\n\n## Ready. Set. Blog!\nNow we're saving post ideas as user stories, writing posts and committing them to their own topic branches, and triggering a publish workflow through Pull Requests which gives us an opportinity to do proof read prior to publishing.\n\nNext up, we're going to [Build][4] our blog.\n\n","categories":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/categories/alm/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/tags/hexo/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"}]},{"title":"C# Wildcard Variables","authorId":"simon_timms","slug":"Wildcard-csharp","date":"2016-11-09 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"C/Wildcard-csharp/","link":"","permalink":"https://westerndevs.com/C/Wildcard-csharp/","excerpt":"Wildcard variables in C# are up for discussion for inclusion in C# 7 or some later version. They are a useful construct taken from functional languages like Haskel.","raw":"---\nlayout: post\ntitle: C# Wildcard Variables\ncategories:\n  - C# \ndate: 2016-11-09 17:00:00\ntags:\n  - C#\nexcerpt: Wildcard variables in C# are up for discussion for inclusion in C# 7 or some later version. They are a useful construct taken from functional languages like Haskel.\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2016/11/09/c-wildcardsdiscardsignororators/\n---\n\nThere is some great discussion going on about including discard variables in C#, possibly even for the C# 7 timeframe. It is so new that the name for them is still up in the air. In Haskel it is called a wildcard. I think this is a great feature which is found in other languages but isn't well known for people who haven't done funcitonal programming. The C# language has been sneaking into being a bit more functional over the last few releases. There is support for lambdas and there has been a bunch of work on immutability. Let's take a walk through how wildcards works. \n\nLet's say that we have a function which has a number of output paramaters:\n\n```\nvoid DoSomething(out List<T> list, out int size){}\n```\n\nUgh, already I hate this method. I've never liked the out syntax because it is wordy. To use this function you would have to do\n\n```\nList<T> list = null;\nint size = 0;\nDoSomething(out list, out size);\n```\n\nThere is some hope for that syntax in C# 7 with what I would have called inline declaration of out variables but is being called \"out variables\". The syntax would look like \n\n```\nDoSomething(out List<T> list, out int size);\n```\n\nThis is obviously much nicer and you can read a bit more about it at\nhttps://blogs.msdn.microsoft.com/dotnet/2016/08/24/whats-new-in-csharp-7-0/\n\nHowever in my code base perhaps I don't care about the size parameter. As it stands right now you still need to declare some variable to hold the size even if it never gets used. For one variable this isn't a huge pain. I've taken to using the underscore to denote that I don't care about some variable.  \n\n```\nDoSomething(out List<T> list, out int _);\n//make use of list never reference the _ variable\n```\n\nThe issue comes when I have some funciton which takes many parameters I don't care about. \n\n```\nDoSomething(out List<T> list, out int _, out float __, out decimal ___);\n//make use of list never reference the _ variables\n```\n\nThis is a huge bit of uglyness because we can't overload the _ variable so we need to create a bunch more variables. It is even more so ugly if we're using tuples and a deconstructing declaration (also part of C# 7). Our funciton could be changed to look like \n\n```\n(List<T>, int, float, decimal) DoSomething() {}\n```\n\nThis is now a function which returns a tuple containing everything we previously had as out prameters. Then you can break this tuple up using a deconstructing declaration.\n\n```\n(List<T> list, int size, float fidelity, decimal cost) = DoSomething();\n```\n\nThis will break up the tuple into the fields you actually want. Except you don't care about size, fidelity and cost. With a wildcard we can write this as \n\n```\n(List<T> list, int _, float _, decimal _) = DoSomething();\n```\n\nThis beauty of this wildcard is that we can use the same wildcard for each field an not worry about them in the least. \n\nI'm really hopeful that this feature will make it to the next release. ","categories":[{"name":"C#","slug":"C","permalink":"https://westerndevs.com/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"https://westerndevs.com/tags/C/"}]},{"title":"How to Blog with VSTS (Part 2)","authorId":"david_wesst","slug":"How-to-Blog-with-VSTS-Part-2","date":"2016-11-07 14:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"alm/How-to-Blog-with-VSTS-Part-2/","link":"","permalink":"https://westerndevs.com/alm/How-to-Blog-with-VSTS-Part-2/","excerpt":"I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 2 of 5, we get our blog into source control.","raw":"---\nlayout: post\ntitle: How to Blog with VSTS (Part 2)\ntags:\n  - visual studio team services\n  - vsts\n  - alm\n  - hexo\n  - github\n  - git\ncategories:\n  - alm\ndate: 2016-11-07 09:00:00\nexcerpt: I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 2 of 5, we get our blog into source control.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-2/\n---\n\n_This is part 2 of 5 of my **How to Blog with VSTS** series. Links to the other parts will be added as they are made available._\n\n+ [Part 1: Setup][1]\n+ [Part 2: Code][2] \n+ [Part 3: Work][3]\n+ [Part 4: Build][4]\n+ Part 5: Release\n\n[1]: https://blog.davidwesst.com/2016/10/How-to-Blog-with-VSTS-Part-1/\n[2]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-2/\n[3]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-3/\n[4]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-4/\n[5]: #\n\n---\n\nI've been tinkering with Visual Studio Team Services off and on since its public release, but never did anything really productive with it. Over the past couple of weeks, I finally bit the bullet and decided to move my most prevelant personal project into VSTS: this blog.\n\nIn this post we are going to get our blog setup in source control so we can start add posts.\n\n## You Don't Need Your Code in VSTS to Use VSTS\nThis is important.\n\nVSTS is an all-in-one solution for your software project. Source control is a part of that solution, but it should be noted that _you don't **need** to use VSTS hosted source control_ to use VSTS. I had originally setup my project just to mess with builds, releases, and work items. All my [source code](https://github.com/davidwesst/dw-blog) lived exclusively on GitHub, and VSTS still worked just fine.\n\nThe only difference I've found is a few \"nice to have\" features in the Work tab, which we'll cover [in the next post][3]. Generally speaking, though, there isn't any significant differences or advantages with using VSTS to host your code except for convienience and those few \"nice to have\" features I mentioned.\n\n### But I Want to Stay Open Source!\nVSTS _does not_ provide you with a public view of your source code. This is my major criticism of the _Code_ features, and for VSTS as a whole. \n\nThat being said, if I want people to be able to view my source code (which I do) then GitHub is a much better platform for that purpose. Consider that GitHub is ubiquitous with open source software development, so much so that even Microsoft uses it to share source code. People are more likely to find my code on GitHub than the focused platform that is VSTS. \n\nStill, the visibility is a project. To solve this just use both. I use the private repositories of VSTS as my working repos where a have a ton of branches and use that for my builds, feature development and so on. When my work is done, I publish my branches to GitHub where people could make pull requests, review my code, and so on.\n\nIn a sense I use VSTS for internal development and GitHub for external/public development and feedback.\n\n### But Now My Development Isn't Transparent!\nYou're right, other than my git history, it's not.\n\nBut, as previously mentioned, you don't need to use VSTS to host your source code or your work items (as you'll see in [part 3][3]). You just need to use the parts you want, and use other tools as you see fit. If you only want to use VSTS for the builds and releases and GitHub for source control and issue tracking, then by all means go ahead and do that.\n\nIn my case, my blog is a one man show that doesn't really get a lot of people reviewing the source code. I don't need the transparency for a project this small, but I still like keeping code visible and I keep tabs on the GitHub repository by using the [GitHub Stats Widget from Yod Labs](https://marketplace.visualstudio.com/items?itemName=YodLabs.yodlabs-githubstats).\n\nIf you're still interested in using the code tab, then keep calm and carry on. If not, you can jump ahead to the [Work][3], [Builds][4], or [Releases][5] posts.\n\n## The \"Code\" Tab\n![Code is front and center in VSTS](http://i.imgur.com/LgqbXYyl.png)\n\nWhen you open up your project in VSTS, you'll see the \"Code\" link in the header. This is where we'll be living for the duration of this post. I am also assuming that did as I did in [part 1][1] and selected Git as the type of source control you want to use for your project.\n\n### The Default Project Repository\nWhen you setup your project, you're given a default project repository with the same name of the project. In my case, I have a repository called _davidwesst.com_.\n\nYou could use this to store the source code of your blog, but I tend to use this repository as a project wiki that spans all repositories. When you click on the \"Welcome\" link on your project home page, you can see a rendered version of the README.md files in each of your repositories. This repository ends up acting as the \"root\" of all documentation on the project.\n\nIf you delete it and go back to the Welcome page you get this message:\n\n![When You Don't Have a Default Repository](http://i.imgur.com/NaI2INbl.png)\n\nSince VSTS has a decicated spot for it anyway, I figure it makes sense to use it for cross-repository documentation. \n\n### Creating Your Blog Repository\nClick on the repository dropdown menu and select _New Repository_. Name it and you're ready to push source code up to VSTS. Just follow the your static site generators instructions to setup your blog, and push the code up.\n\nIf you already have your blog published somewhere else like GitHub, you can use the _Import Repository_ feature and clone the source and the history into VSTS.\n\n![Repo Dropdown Menu](http://i.imgur.com/XC8Oqnpl.png)\n\n#### For Those Using Hexo\nIf you're using Hexo as your static site generator, you can follow [these instructions](https://hexo.io/docs/setup.html) to get setup.\n\nBefore you push, make sure you add a `.gitignore` file that excludes the _public_ and _node\\_modules_ as those folders contain the generated static content and generator dependencies, which we don't need to store in source control.\n\n## Writing Blog Posts\nWe will tackle this topic again in [part 3][3] but for now I want to describe the workflow I use to write blog posts. It should be noted that the workflow I'm about to describe works in either VSTS or GitHub. I used it in GitHub for a long time, and it continues to serve me well in VSTS.\n\nHere's a summary of what I do:\n1. Create a new branch off of `master`\n2. Write the post and commit all files to the new branch\n3. Create a Pull Request to pull the post branch into `master`\n4. Approve the PR when I'm ready to publish the post\n\n### Post Branches\nWith a static site, or in this case a static blog, each post is a new file (or files if you're including images). I think of each post as a new feature and use a [_topic branch_](https://git-scm.com/book/en/v2/Git-Branching-Branching-Workflows#Topic-Branches). \n\nMy `master` branch is my most recent and stable code, and so each post is a branch off of `master`. In short, I do the following:\n\n```bash\ngit checkout -b post/my-new-post\n# ...write the post...\ngit add . --all\ngit commit -m \"wrote new post\"\ngit push -u origin post/my-new-post\n```\n\nWhen I'm done, the branch contains everything I need to publish the post. All I need to do is merge, but I generally don't do that from the command line on my machine. Rather, I use Pull Requests.\n\n### Pull Requests\nYou may have noticed the Pull Requests link in the header earlier, which is a nice way to merge a post branch without needing a computer with the project setup. In my case, I tend to write my posts in advance and want to publish them on Monday mornings. I don't necessarily want to have to open up a command line and do the merge and push from my workstation.\n\nWith Pull Requests, I can do the merge from any web browser, and it forces me to give myself a code review. I look over the spelling and grammar and make sure everything looks good. When I'm done, I click \"Complete\" and it's merged into my master branch, which is what is used to generate the site content and published to my web host.\n\n![A Pull Request](http://i.imgur.com/ZXtAeaHl.png)\n\n## But Wait, There's More!\nAbsolultely. \n\nMore specifically [Work][3] for managing issue tracking, [Builds][4] for generating the blog content, and [Releases][5] for publishing posts. So stay tuned!\n","categories":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/categories/alm/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/tags/hexo/"},{"name":"git","slug":"git","permalink":"https://westerndevs.com/tags/git/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"},{"name":"github","slug":"github","permalink":"https://westerndevs.com/tags/github/"}]},{"title":"How to Blog with VSTS (Part 1)","authorId":"david_wesst","slug":"How-to-Blog-with-VSTS-Part-1","date":"2016-10-31 12:43:31+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"alm/How-to-Blog-with-VSTS-Part-1/","link":"","permalink":"https://westerndevs.com/alm/How-to-Blog-with-VSTS-Part-1/","excerpt":"I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 1 of 5, we get things setup.","raw":"---\nlayout: post\ntitle: How to Blog with VSTS (Part 1)\ncategories:\n  - alm\ndate: 2016-10-31 08:43:31\ntags:\n  - visual studio team services\n  - vsts\n  - alm\n  - hexo\nexcerpt: I wanted to understand how to use Visual Studio Team Services (VSTS) for a \"real\" project. Being a noob, I decided to move my blog to VSTS to understand how _any_ project can benefit from ALM practices using VSTS. In part 1 of 5, we get things setup.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/10/How-to-Blog-with-VSTS-Part-1/\n---\n\n_This is part 1 of 5 of my **How to Blog with VSTS** series. Links to the other parts will be added as they are made available._\n\n+ [Part 1: Setup][1]\n+ [Part 2: Code][2]\n+ [Part 3: Work][3]\n+ [Part 4: Build][4]\n+ Part 5: Release\n\n[1]: https://blog.davidwesst.com/2016/10/How-to-Blog-with-VSTS-Part-1/\n[2]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-2/\n[3]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-3/\n[4]: https://blog.davidwesst.com/2016/11/How-to-Blog-with-VSTS-Part-4/\n[5]: #\n\n---\n\nI've been tinkering with Visual Studio Team Services off and on since its public release, but never did anything really productive with it. Over the past couple of weeks, I finally bit the bullet and decided to move my most prevelant personal project into VSTS: this blog.\n\nThis post covers the setup, and more specifically what I use to produce my lovely blog. We'll get into the thick of it in [part 2][2].\n\n### Woah. VSTS _just_ for a Blog?\nI know. Let me address this first before we continue.\n\nIt's definitely overkill. But, it provided me with a great learning experience for VSTS and the application lifecycle management tools is provides. It also was something that I'll actually use on a regular basis, as you'll see if you get through the whole series of posts.\n\nSo, yes it's like using a grenade launcher to kill an ant. That being said, it definitely gets the job done.\n\n## The Parts\nHere's what I use in my blog project, and what you'll need if you're going to follow along.\n\n### Hexo or another Static Site Generator\nIf you're not familiar with static site generators, they are great for developers looking to blog or create simple sites. I won't be going into the details on how to use one, but they all seem to provide a similar sort of experience.\n\nAll you do is add files, generally in [Markdown](https://daringfireball.net/projects/markdown/syntax), fill out the configuration file, and run the generate command. The generator then generates a series of static HTML files from your content and then you have the files you need to publish to a web server somewhere.\n\nNo server-side code, no database, just a bunch of files. Real simple.\n\nI use [Hexo](https://hexo.io/) for my [blog](https://blog.davidwesst.com), and it is used for the [Western Devs](http://www.westerndevs.com) site. It works well, and makes scripting your build and deployment ([which we'll see later][3]) much easier.\n\nIf you don't like Hexo then take a [look here](https://www.staticgen.com/) for a bunch of static site generators based in a variety of languages.\n\n### Visual Studio Team Services Account\nSign up for the free account [here](https://www.visualstudio.com/vsts-test/) and create a new team project. I used the _Agile_ process because I found it gave me the flexibility I wanted, and _Git_ for source control.\n\nIf you want more details about the project processes available, and the differences between them, take a look at [this link](https://www.visualstudio.com/en-us/docs/work/guidance/choose-process).\n\n![\"Create a New Project in VSTS\"](http://i.imgur.com/CYlb9sNm.png)\n\n### Web Host\nJust like any website, you're going to need a place to host it. Lucky for us, we're only hosting static files which makes the options pretty open.\n\nI'm using Windows Azure, which again is overkill considering it's just static files, but I like it. If you don't want to spend money, I have also used [GitHub Pages](https://pages.github.com/) which is free and works just as well for what we'll be doing.\n\nThe important part is having a place to host the files once they are generated. No more, no less.\n\n### Web Browser\nVSTS is browser-based, so get a your favourite modern browser, get it updated, and you're good to go.\n\n### Code Editor\nWe're not going to be doing much code, but it'll come in handy later on. \n\nPersonally, I jump between [Visual Studio Code](http://code.visualstudio.com/) and [Vim](http://www.vim.org/). I'm sure you have your favourite, and that'll do just fine for our purposes.\n\n## You Forgot Visual Studio\nNo I didn't. I don't use Visual Studio, nor do we need it.\n\nIt's not that I don't like it, but it's too heavy for the amount of coding I actually do for my blog. VSTS provides us with a web-based user interface that does everything we need for managing the source code, builds, and so on. \n\nIn conclusion, I don't use Visual Studio here because I don't need it. \n\n## And We're Ready\nArmed with our toolbox of goodies and our project created, we're good to go on starting to blog with VSTS. [Next up][2], we'll starting with what is familiar and getting our code into VSTS. \n","categories":[{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/categories/alm/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/tags/hexo/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"vsts","slug":"vsts","permalink":"https://westerndevs.com/tags/vsts/"},{"name":"alm","slug":"alm","permalink":"https://westerndevs.com/tags/alm/"}]},{"title":"How to Use Global NPM Packages on a VSTS Self-Hosted Build Agent","authorId":"david_wesst","slug":"How-to-Use-Global-NPM-Packages-on-a-VSTS-Self-Hosted-Build-Agent","date":"2016-10-24 12:33:01+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"development/How-to-Use-Global-NPM-Packages-on-a-VSTS-Self-Hosted-Build-Agent/","link":"","permalink":"https://westerndevs.com/development/How-to-Use-Global-NPM-Packages-on-a-VSTS-Self-Hosted-Build-Agent/","excerpt":"I setup a self-hosted build agent in Visual Studio Team Services. My build installed global NPM packages, but the tasks that used them later on in the script would fail because they were unable to use them. This post describes what I did to get them working.","raw":"---\nlayout: post\ntitle: How to Use Global NPM Packages on a VSTS Self-Hosted Build Agent\ndate: 2016-10-24 08:33:01\ntags:\n  - javascript\n  - visual studio team services\n  - nodejs\n  - npm\ncategories:\n  - development\nexcerpt: I setup a self-hosted build agent in Visual Studio Team Services. My build installed global NPM packages, but the tasks that used them later on in the script would fail because they were unable to use them. This post describes what I did to get them working.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/10/How-to-Use-Global-NPM-Packages-on-a-VSTS-Self-Hosted-Build-Agent/\n---\n\nI took a couple of weeks off of blogging to focus on a building my presentation for [Deliver](http://www.prdcdeliver.com/). In my spare time, I started tinkering with Visual Studio Team Services, where decided to start by automating the build and release of this blog.\n\nMy build script is pretty straight forward. Setup the global dependencies with NPM, setup the local dependencies with NPM, generate the content, and publish the generated assets. This worked in my hosted agent, but not my self-hosted agent.\n\nI found a few solutions, but I'll go through the one I selected for my build agent.\n\n### The Problem\nMy build script would run `npm install --global hexo-cli` and execute as expected. When the next step would try and use the `hexo generate` command, I would get the following error:\n\n```\n##[error]hexo : The term 'hexo' is not recognized as the name of a cmdlet, function, script file, or operable program. Check \nthe spelling of the name, or if a path was included, verify that the path is correct and try again.\n```\n\nEven though the install command was successful, the build script still couldn't use the tool installed.\n\nAnother symptom of this problem is that Team Services can't see global NPM packages as common capabilities, such as Bower, Gulp, and Grunt.\n\n![\"Gulp and Grunt as capabilities\"](http://i.imgur.com/pkLEzkEl.png)\n\nI setup my build agent to use the NetworkService user account, but it could be setup for any user. The problem is that the NetworkService account can't see the global packages on the machine after they are installed. The solution is to configure NPM to point to a folder that is visible to the NetworkService account.\n\nHere's how you do it.\n\n### The Solution\nI found [this solution on StackOverflow](http://stackoverflow.com/questions/38570209/making-global-npm-packages-available-to-all-users-on-windows-2012-server) which lead me in the right direction, although I didn't follow all of it.\n\nThe [`npm prefix -g`](https://docs.npmjs.com/cli/prefix) command shows us path to global prefix folder, where the global npm packages are stored. We need to point this to a directory that NetworkService can read and execute. Generally speaking, the prefix folder is usually found in the user's AppData folder.\n\nTo change the prefix, run the command `npm config set prefix C:\\\\Path\\\\To\\\\Folder\\\\AppData\\\\Roaming\\\\npm` which will change the npm prefix folder to be the one specified. Because I've set my build agent NetworkService account, I point it at the NetworkService account AppData npm folder for simplicity.\n\nThen add the folder to the PATH variable for the machine. This will let VSTS see the npm packages as capabilities so that it knows that our build server can execute Grunt, Gulp, and Bower tasks.\n\n#### Why Didn't You Reset the Prefix?\nIt makes sense to reset the prefix to the previous value after the build has complete, as described in the StackOverflow solution. In my case, I wanted to make sure that if someone were logging into the build server to add another global package, let's say something like Hexo CLI, then it would be installed in the appropriate directory.\n\nI didn't reset the prefix because I wanted to permanently configure the build agent. It's a small build server that I'm using to experiment with continuous integration and deployment. If it's good enough for StackOverflow then it's good enough for me.\n\n## A Few Alternative Solutions\nAs an alternative solution you could setup a new directory that isn't the AppData folder, add the new folder to the PATH, and then point the prefix folder at build time. You could also leverage the `npm bin` setting and setup alias in your package.json file for the global commands you're looking to use (Thanks to [Aaron Powell](http://www.aaron-powell.com/) for providing me with that one), which is another good solution that I'll revisit if I use something other than VSTS for builds.","categories":[{"name":"development","slug":"development","permalink":"https://westerndevs.com/categories/development/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"visual studio team services","slug":"visual-studio-team-services","permalink":"https://westerndevs.com/tags/visual-studio-team-services/"},{"name":"nodejs","slug":"nodejs","permalink":"https://westerndevs.com/tags/nodejs/"},{"name":"npm","slug":"npm","permalink":"https://westerndevs.com/tags/npm/"}]},{"title":"Add some spice to your life with Resharper Templates","authorId":"justin_self","slug":"Add-some-spice-to-your-life-with-resharer-templates","date":"2016-10-04 21:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Azure/Add-some-spice-to-your-life-with-resharer-templates/","link":"","permalink":"https://westerndevs.com/Azure/Add-some-spice-to-your-life-with-resharer-templates/","excerpt":"Do you use Resharper? Do you have 5 minutes? Awesome, let's change your life.","raw":"---\nlayout: post\ntitle:  Add some spice to your life with Resharper Templates\ndate: 2016-10-04T11:30:00-06:00\ncategories: Azure\ncomments: true\nauthorId: justin_self\n---\n\nDo you use Resharper? Do you have 5 minutes? Awesome, let's change your life.\n\n<!--more-->\n\nOpen Visual Studio. Click on the `Resharper` menu item, navigate to Tools > Template Explorer, then click on the new template button (see image);\n\n![Imgur](http://i.imgur.com/gkkaV52.png)\n\nIn the editor, paste this: `//TODO : $user$ $date$ $description$`\n\nOn the right, you'll now see three parameters. For `user`, click \"choose macro\", and the select \"Full user name of current user\".\n\nFor the `date` parameter, click \"Choose macro\", and then select \"Current data specified format\". In the format box, type \"MM/dd/yyyy\".\n\nUncheck the `editable` checkboxes for `user` and `date`.\n\nLastly, in the \"Shortcut\" box, type \"todo\" and name it \"todo helper\".\n\n![Imgur](http://i.imgur.com/DGqDxdb.png)\n\nOk, save and make a new one for \"hack\" comments with \"//HACK : $user$ $date$ $description$\"\n\nNow, you should be able to go into your C# class files and do this:\n\n![Imgur](http://i.imgur.com/P8OUP3A.gif)\n\nCool, huh?\n\nOk, now let's make a unit test help with this: \n\n    [Test]\n    public void $methodName$()\n    {\n\t    //Arrange\n\t    $END$\n\n\t    //Act\n\n\n\t    //Assert\n    \n    }\n\nI use the shortcut `nut` for \"nUnit Test\". If you use other testing frameworks, just modify it to suite your needs.\n\nSave it and now add tests like this: \n\n![Imgur](http://i.imgur.com/jVo9WoU.gif)\n\nPretty sweet, right? Life changing? Maybe.","categories":[{"name":"Azure","slug":"Azure","permalink":"https://westerndevs.com/categories/Azure/"}],"tags":[]},{"title":"Windows Not Required - The New Microsoft Development Story (Video)","authorId":"david_wesst","slug":"Windows-Not-Required-Video","date":"2016-10-03 14:47:31+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Video/Windows-Not-Required-Video/","link":"","permalink":"https://westerndevs.com/Video/Windows-Not-Required-Video/","excerpt":"I delivered a presentation at the Winnipeg .NET User Group last week, where I recorded it and posted it on YouTube. I go over a number of Microsoft development tools and technologies that don't require Windows and deliver the demos on my Linux machine.","raw":"---\nlayout: post\ntitle: Windows Not Required - The New Microsoft Development Story (Video)\ncategories:\n  - Video \ndate: 2016-10-03 10:47:31\ntags:\n  - .NET Core\n  - Docker\n  - PowerShell\n  - TypeScript\n  - Visual Studio Code\n  - Visual Studio Team Services\nexcerpt: I delivered a presentation at the Winnipeg .NET User Group last week, where I recorded it and posted it on YouTube. I go over a number of Microsoft development tools and technologies that don't require Windows and deliver the demos on my Linux machine.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/10/Windows-Not-Required-Video/\n---\n\nThis is a recording of my recent Winnipeg .NET User Group talk about using Microsoft tools without requiring Windows.\n\nIt's not the most technical as I go over a ton of topics, at high level, ranging from development tools, to applications. Still, I find this stuff really interesting and hopefully you enjoy it too.\n\n<figure class=\"video\">\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BlXorwQ0DSw\" frameborder=\"0\" allowfullscreen></iframe>\n</figure>\n\nYou can view the slides [here](https://davidwesst.github.io/windows-not-required) and find their source code [here on GitHub](https://github.com/davidwesst/windows-not-required).\n","categories":[{"name":"Video","slug":"Video","permalink":"https://westerndevs.com/categories/Video/"}],"tags":[{"name":".NET Core","slug":"NET-Core","permalink":"https://westerndevs.com/tags/NET-Core/"},{"name":"Docker","slug":"Docker","permalink":"https://westerndevs.com/tags/Docker/"},{"name":"PowerShell","slug":"PowerShell","permalink":"https://westerndevs.com/tags/PowerShell/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://westerndevs.com/tags/TypeScript/"},{"name":"Visual Studio Code","slug":"Visual-Studio-Code","permalink":"https://westerndevs.com/tags/Visual-Studio-Code/"},{"name":"Visual Studio Team Services","slug":"Visual-Studio-Team-Services","permalink":"https://westerndevs.com/tags/Visual-Studio-Team-Services/"}]},{"title":"Introduction to messaging primitives","authorId":"peter_ritchie","slug":"2016-9-30-Introduction-to-Messaging-Primitives","date":"2016-09-30 21:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"C/2016-9-30-Introduction-to-Messaging-Primitives/","link":"","permalink":"https://westerndevs.com/C/2016-9-30-Introduction-to-Messaging-Primitives/","excerpt":"An introduction to messaging and the Primitives library to make writing message-driven/event-oriented systems easier.","raw":"---\nlayout: post\ntitle: Introduction to messaging primitives\ncategories:\n  - C# \ndate: 2016-09-30 17:00:00\ntags:\n  - C#\n  - Messaging\nexcerpt: An introduction to messaging and the Primitives library to make writing message-driven/event-oriented systems easier.\nauthorId: peter_ritchie\noriginalurl: http://pr-blog.azurewebsites.net/2016/09/30/introduction-to-messaging-primitives/\n---\nOne of the most flexible design/architecture techniques is event-driven/message-oriented design.  It offers unparalleled ability to loosely couple, accomplish Dependency Inversion, facilitates composability, etc.  Message-oriented systems are by nature asynchronous.  This means that a message is sent and the process that sent the message continues on to do other processing without knowing whether the message was received by the other end.  Think of it like sending an email.  You send an email, close your email client, and go on to do something else.  Later, you may return to your email program and notice that the email you sent bounced.  But, you had to return to your email program to see that because you where asynchronously doing something else.\n\nI've been working on an OSS project for a while that provides a framework for simple (once you wrap your head around the degree of loose coupling and asynchronousness) and flexible message-oriented design.  This framework is based on a small set of types that I call primitives.  Before going into what you can do with the framework, this post will first cover the primitives.\n\nIf you're used to message-oriented systems, some of this may be very understandable.  For those new to message-orientation, you'll understand why I started with the primitives.\n\nThe message primitives source code is on [GitHub](https://github.com/peteraritchie/Messaging.Primitives) and can be added to your Visual Studio project/solution from [NuGet](https://www.nuget.org/packages/PRI.Messaging.Primitives/) (but you may want to wait to add this package to your project/solution until a future post, as it's a dependency of the framework, so you don't need to add this package manually (i.e. it's referenced and NuGet will automatically get it for you).\n\nThe primitives are based on the most simple messaging building blocks (or patterns, which are well-defined in the book [Enterprise Integration Patterns](http://amazon.com/o/asin/0321200683/ref=nosim/enterpriseint-20) but can also be seen on [enterpriseintegrationpatterns.com/](http://www.enterpriseintegrationpatterns.com/)) and consist of a generic [Message](http://www.enterpriseintegrationpatterns.com/patterns/messaging/Message.html), a generic message Consumer, a generic message Producer, a generic [Pipe](http://www.enterpriseintegrationpatterns.com/patterns/messaging/PipesAndFilters.html), a generic [Command](http://www.enterpriseintegrationpatterns.com/patterns/messaging/CommandMessage.html) message (a specialized Message), a generic [Event](http://www.enterpriseintegrationpatterns.com/patterns/messaging/EventMessage.html) message (a specialized Message), and a [Bus](http://www.enterpriseintegrationpatterns.com/patterns/messaging/ControlBus.html).\n\n## Message\nThe Message pattern is modeled in the Primitives via the `IMessage` interface, detailed as follows:\n```csharp\n\tpublic interface IMessage\n\t{\n\t\tstring CorrelationId { get; set; }\n\t}\n```\nThe idea is that the message you want to send/receive implements this interface and you add the properties you need to contain the data.  The interface contains a property `CorrelationId` to provide a first-class ability to correlate multiple messages together involved in a particular interaction, transaction, or saga.  But, for the most part, you should use either Command or Event instead.\n## Command\nAs mentioned above, you don't normally implement `IMessage` directly, you derive from one of two types of `IMessage`-derived interfaces.  The first interface I'll talk about is the `ICommand` interface.  This is a marker interface to add compile-time checked semantics that I'll detail in a future post, and detailed as follows:\n```csharp\n\tpublic interface ICommand : IMessage\n\t{\n\t}\n```\nThe [marker interface pattern](https://en.wikipedia.org/wiki/Marker_interface_pattern) is a pattern that allows associating metadata with a type.  In this case the metadata is the ability to differentiate at compile-time that a particular type is a Command.\nThe Command message pattern allows you to encapsulate all the information required to request a handler perform a command, or otherwise change state.  The type of command to be performed is the type of the `ICommand`-implementating class.  Message handlers should typically have no run-time state, so everything the command handler needs to perform the command should be included within the `ICommand` type.  For example, if I want to make a request to a handler to create a client, I may have a `CorrectClientAddressCommand` with the following detail:\n```csharp\n  [Serializable]\n  public class CorrectClientAddressCommand : ICommand\n  {\n    public string CorrelationId { get; set; }\n    public string ClientId { get; set; }\n    public Address PriorAddress { get; set; }\n    public Address Address { get; set; }\n  }\n```\nAs with marker interfaces, we have the ability to introduce metadata when we're defining our messages.  Just like something can be known as a Command because it implements `ICommand`, we can introduce more depth of intent in our messages.  For example, if a client moves, their address changes, but using a command like CorrectClientAddress does not include that intent.  We could create a new, identical command named \"MoveClientCommand\" that is effectively identical to `CorrectClientAddressCommand` in content (or even derive from the same base class).  And that way we can include semantic intent with the message.  Why would you want to do that?  In the address change example, when a client corrects their address they may never have received important mailings.  In the case of a correction, the organization can re-send important mailings.  In the MoveClientCommand you may not want to re-send all that information (waste of money, annoys clients, etc.) and instead send a card welcoming them to their new home and taking advantage of an opportunity to impress the clients.\n## Event\nThe second message type pattern is an Event.  And just like an event we deal with in day-to-day life: it's a moment in time and information about that moment in time.  From the standpoint of messages, we say that an event is moment of time in the *past*, otherwise known as a fact.  It's important to remember that it's a past fact and really should be considered as immutable data.  Typically events model the details about a change in state. We model events in Primitives with the `IEvent` interface, with the following details:\n```csharp\n\tpublic interface IEvent : IMessage\n\t{\n\t\tDateTime OccurredDateTime { get; set; }\n\t}\n```\nNotice that we force implementation of `IMessage` (that is, include `CorrelationId`) and add an `OccurredDateTime` property.\nWhen we want to communicate an event, we call sending an event \"publishing\" the event.  This concept is considered \"pub/sub\" (or publish/subscribe) where something that publishes an event never knows how many subscribers (or if any) are subscribed to receive an event.  When utilizing event-driven in this way, we're very loosely coupled and any number of things can subscribe to these events and extend without affecting the sender (i.e. code changes or availability).\nTypically, when we talk about what we model with events, or facts about the past, we model state changes and include information about that state change.  To correlate to the `CorrectClientAddressCommand`; upon a successful address change, the handler of that message may publish a `ClientAddressCorrectedEvent`.  Which may look like this:\n```csharp\n  [Serializable]\n  public class ClientAddressCorrectedEvent : IEvent\n  {\n    public string CorrelationId { get; set; }\n    public DateTime OccurredDateTime { get; set; }\n    public string ClientId { get; set; }\n    public Address PriorAddress { get; set; }\n    public Address Address { get; set; }\n  }\n```\nCircling back to the CorrelationId concept, the event includes the `CorrelationId` field.  If the event is published due to the processing of another message then we would copy that `CorrelationId` value into the event.  That way, when something receives the event (remember that messaging is asynchronous) it can correlate it back to another message, likely one that *it* sent.\n## Consumer\nNow that we have a grasp on some basic message types and concepts, lets talk about how we use those messages.\nThe thing that performs consumption or handling of a message is a Consumer.  It is modeled in the Primitives via the `IConsumer` interface, detailed as follows:\n```csharp\n\tpublic interface IConsumer<in T> where T:IMessage\n\t{\n\t\tvoid Handle(T message);\n\t}\n```\nNotice that it's a generic interface and the generic type must implement `IMessage`.  The implementer of `IConsumer<in T>` must also implement a Handle method that takes in the an instance of the message that the class would process.  So, if I wanted to create a class to implement a handler for the `CorrectClientAddressCommand` command, it may look like this:\n```csharp\n  public class CorrectClientAddressCommandHandler: IConsumer<CorrectClientAddressCommand>\n  {\n    public void Handle(CorrectClientAddressCommand message)\n    {\n      // ...\n    }\n  }\n```\nNow, since we're using interfaces, a class can implement more than one handler.  For example, if I also wanted to process the ClientAddressCorrectedEvent, I may update my class to be something like:\n```csharp\n  public class CorrectClientAddressCommandHandler : IConsumer<CorrectClientAddressCommand>, IConsumer<ClientAddressChangedEvent>\n  {\n    public void Handle(CorrectClientAddressCommand message)\n    {\n      // ...\n    }\n    public void Handle(ClientAddressChangedEvent message)\n    {\n      // ...\n    }\n  }\n```\nBut, as you can tell from the type of the handler class (`CorrectClientAddressCommandHandler`) that it's named very specifically to be a `CorrectClientAddressCommand` handler.  I typically use that convention and have one handler per class, which offers a greater flexibility in terms of loosely coupled and composability.  But, in the end, it's up to you what convention you'd like to use.\nTo write code to handle a particular message you simply implement `IConsumer<int T>` for one or more types of messages\n## Producer\nThe thing that performs production of a message is called a Producer, and is model in Primitives as `IProducer<out T>`, detailed as follows:\n```csharp\n\tpublic interface IProducer<out T> where T:IMessage\n\t{\n\t\tvoid AttachConsumer(IConsumer<T> consumer);\n\t}\n```\nSimilar to `IConsumer<in T>`, except for the fact the generic type is covariant instead of contravariant, the generic type must implement `IMessage`.\nNow this is where the loose coupling and composability takes us into an advanced realm.  You'll notice that the `IProducer` has a single method `AttachConsumer` that accepts an `IConsumer<in T>` where T is the same generic type as the producer.  This is probably very different from a typical imperative design that might have a method that returns a message.  We don't do it in an imperative way because 1) we have message consumption abstraction (`IConsumer`) and 2) the fundamental asynchronousness of messaging.  The production and consumption of messages does not occur in a consistent, sequential fashion such that we would know where to place a call to a method that returns a message.  Instead, we tell the producer who can consume the message and whenever the producer gets around the producing that message, it passes it right along to the consumer.\nWe may have a class that is a producer of `CorrectClientAddressCommand` and could define it as follows:\n```csharp\n  CorrectClientAddressController : IProducer<CorrectClientAddressCommand>\n  {\n    private IConsumer<CorrectClientAddressCommand> consumer;\n    \n    public void AttachConsumer(IConsumer<CorrectClientAddressCommand> consumer)\n    {\n      this.consumer = consumer;\n    }\n    \n    public void CorrectClientAddress(Client client, Address newAddress)\n    {\n      if(consumer == null)\n        throw new InvalidOperationException(\n          \"@nameof(consumer) was null during invocation of CorrectClientAddress\");\n          \n      var command = new CorrectClientAddressCommand()\n      {\n        CorrelationId = Guid.NewGuid().ToString(\"N\");\n        ClientId = client.Id;\n        PriorAddress = client.Address;\n        Address = newAddress;\n      }\n      consumer.Handle(command);\n    }\n    //...\n  }\n```\nNotice an application-/domain-specific method `CorrectClientAddress` that contains all the information required to send a `CorrectClientAddressCommand` (and the `CorrectClientAddressCommand` handler would perform the heavy lifting asynchronously and potentially in another thread/process/node, if you're looking for scalability).\nYou could use this class like this:\n```csharp\n  var controller = new CorrectClientAddressController();\n  controller.AttachConsumer(new CorrectClientAddressCommandHandler());\n```\nAnd then when the `CorrectClientAddress` method is called, the consumer is invoked.\n## Pipe\nThe last Primitive type is the Pipe.  The pipe is a general abstraction to model anything that is both a consumer and producer.  And, in fact, is just an interface that implements `IConsumer` and `IProducer`, detailed as follows:\n```csharp\n\tpublic interface IPipe<in TIn, out TOut>\n\t  : IConsumer<TIn>, IProducer<TOut>\n\t    where TIn : IMessage where TOut : IMessage\n\t{\n\t}\n```\nWith `IPipe` the consumer has a different generic type name than the producer, but, a single type could be used for both (`IPipe<MyMessage, MyMessage>`) to model a true pipe.\nTypically though, we use `IPipe` to model various other messaging patterns like [Filters](http://www.enterpriseintegrationpatterns.com/patterns/messaging/Filter.html) (something that would ignore a message based on content) or [Translators](http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html) (something that converts one message type into another message type).\n## Bus\nThe next message pattern is the Bus.  A bus is modeled with the `IBus` type.  This bus model is basically facilitates implementing a Control Bus.  The Control Bus facilitates connecting message producers with message consumers in a more loosely coupled way.\nRemember the `CorrectClientAddressController` `IProducer` example?  The code was tightly coupled to both `CorrectClientAddressController` and `CorrectClientAddressCommandHandler` and we had to new-up both in order to hook them up.  If I'm writing code that produces a message like `CorrectClientAddressCommand`, I don't want it to be directly coupled to one particular handler.  After all, we're looking for loosely coupled and asynchronous.  With tight coupling like that I might as well just do all the work in the `CorrectClientAddress` method and skip all the messaging.\nA bus allows us to build up something at runtime that does that connection.  It will keep track of a variety handlers and invoke the correct handler when it consumes a message.\nAs you probably guessed, the `IBus` is a consumer and thus implements `IConsumer<in T>`.  But, a bus can handle a variety of different messages, so it uses `IMessage` for its type parameter, as detailed:\n```csharp\n\tpublic interface IBus : IConsumer<IMessage>\n\t{\n\t\tvoid AddTranslator<TIn, TOut>(IPipe<TIn, TOut> pipe)where TIn : IMessage where TOut : IMessage;\n\t\tvoid AddHandler<TIn>(IConsumer<TIn> consumer) where TIn : IMessage;\n\t\tvoid RemoveHandler<TMessage>(IConsumer<TMessage> consumer) where TMessage : IMessage;\n\t}\n```\nAs we can see from the definition of `IBus` is also allows the connection of pipes or translators.\nSo, if I wanted the type `CorrectClientAddressCommandHandler` to handle a `CorrectClientAddressCommand` (and produce a `ClientAddressCorrectedEvent` and a `ClientAddressCorrectedEventHandler` type to handle a `ClientAddressCorrectedEvent` message,  could use a mythical bus implementation `Bus` like this:\n```csharp\n  var bus = new Bus();\n  bus.AddHandler(new CorrectClientAddressCommandHandler());\n  //...\n  bus.AddHandler(new ClientAddressCorrectedEventHandler());\n  //...\n```\nI could then send the command to the bus and have the command handler handle the message and the event handler handle event, without ever specifically attaching one handler to the producer.  Sending that command could be done as follows:\n```csharp\n  bus.Send(new CorrectClientAddressCommand());\n```\n\"So?\", you may be thinking.  Remember when I spoke about how events are subscribed to, and how there may be more than one subscriber to an event?  I have a command handler than can only attach to one consumer, how would I be able to do that?  That's one of the benefits of the Bus, it deals with that for you.  If I added another event handler, I may create my bus as follows:\n```csharp\n  var bus = new Bus();\n  bus.AddHandler(new CorrectClientAddressCommandHandler());\n  //...\n  bus.AddHandler(new ClientAddressCorrectedEventHandler());\n  //...\n  bus.AddHandler(new ClientAddressCorrectedEventHandler2());\n  ///\n```\nThe Bus would then handle forwarding the event to both of the event handlers, avoiding the need to write a different `IConsumer` implementation that would manually do that (e.g. a [Tee](https://en.wikipedia.org/wiki/Tee_(command))).\nThese are examples of composing in-memory buses. That is, they process messages within the same process (and in a specific order).\n\nThere's a couple of important concepts that relate to messaging.  Immutability and Idempotency.\n## Immutability\nI've already touched on immutability.  But, it's important to remember that due to the asychronous nature of messaging that you should consider the messages you send to be immutable.  That is, you should't change them.  For example, I could write some error-prone code like this:\n```csharp\n  var command = new CorrectClientAddressCommand();\n  //...\n  bus.Send(command);\n  command.ClientId = 0;\n```\nIn the case of a message that is physically sent to a queue (covered in a future post) that message has been serialized to the queue and has left this process.  Making a change to the `command` object cannot be seen by the consumer of that message.  If we're talking about an in-memory bus, it could be the same situation, but the time `Send` returns the message has already been processed.  If any of the message handlers are multi-threaded then the `command` object may or may not be already handled by the time `Send` returns.  In any case, it's best to view sent messages as immutable to avoid these race conditions.\n## Idempotency\nAnother concept that often comes up in messaging is [Idempotency](https://en.wikipedia.org/wiki/Idempotence).  Idempotency is the quality of a message consumer to produce the same side-effects with the same parameters, no matter how many times it is invoked.\nWhat I've detailed so far with in-memory buses is effectively Reliable Messaging.  If `IBus.Send` returns, the message was reliably processed.  When we start to include message queues and accept messages over the wire, running on multiple computers, we have to deal with the possibility that another server or process might fail and might have to re-send a message.  This typically only happens when the reliability settings of the Queue are not set to the highest level (for example \"at-least-once delivery\", where we trade performance for the potential that a message may be sent more once.  In situations like this you may want to send a message that allows the consumer be an [Idempotent Receiver](http://www.enterpriseintegrationpatterns.com/patterns/messaging/IdempotentReceiver.html).\nIn our `CorrectClientAddressCommand` we've effectively facilitated a Idempotent Receiver, no matter how many times I send a `CorrectClientAddressCommand` the resulting client address will be the same.  Other types of messages make it difficult to have an Idempotent Receiver.  For example, if I had an `IncreaseClientAgeCommand`, processing it would always increase an `Age` property of a client.  If at-least-once-delivery was configured for the queue, this could occasionally lead to incorrect ages.  You may want to either have a command like `SetClientAgeCommand` or better yet (avoid pedantry) and have a `CorrectClientBirthDateCommand`.\n\nAnd with that, we have a good introduction of messaging and an intro to the messaging primitives library.  In a future post I'll detail the implementation of these primitives: the patterns library.\n","categories":[{"name":"C#","slug":"C","permalink":"https://westerndevs.com/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"https://westerndevs.com/tags/C/"},{"name":"Messaging","slug":"Messaging","permalink":"https://westerndevs.com/tags/Messaging/"}]},{"title":"How to Fix node-gyp Error on Windows","authorId":"david_wesst","slug":"How-to-Fix-node-gyp-Error-on-Windows","date":"2016-09-26 18:51:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"JavaScript/How-to-Fix-node-gyp-Error-on-Windows/","link":"","permalink":"https://westerndevs.com/JavaScript/How-to-Fix-node-gyp-Error-on-Windows/","excerpt":"Whenever I get a new machine, I pull down a new project using the `npm install command and get an error related to python and node-gyp. This post will remember the fix for this problem that I always forget.","raw":"---\nlayout: post\ntitle: How to Fix node-gyp Error on Windows\ncategories:\n  - JavaScript \ndate: 2016-09-26 14:51:56\ntags:\n  - NodeJS\n  - JavaScript\n  - node-gyp\nexcerpt: Whenever I get a new machine, I pull down a new project using the `npm install command and get an error related to python and node-gyp. This post will remember the fix for this problem that I always forget.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/09/How-to-Fix-node-gyp-Errors-on-Windows/\n---\n\nI hit this problem once or twice a year and always seem to forget how easy it is to fix the problem. This blog post will ensure that not only I remember, but the entire Internet will remember the solution I use.\n\nHere goes.\n\n## The Problem\n\nThe problem is simple: You pull down a repository that uses NPM for package management onto your Windows machine, you type `npm install` and you get something like this:\n\n```\nnpm ERR! Failed at the <package>@<version> install script 'node-gyp rebuild'\n```\n\nor\n\n```\ngyp ERR! stack Error: Can't find Python executable \"python\", you can set the PYTHON env variable.\n```\n\nThis is because the project you're trying to build requires the [node-gyp](https://github.com/nodejs/node-gyp) package to build something natively on the platform. That package required Python 2.7 and a few other pieces too.\n\n## The Solution\n\nSimple problems sometimes get simple solutions.\n\nThanks to the node-gyp team, that is the case and they have documented it in the project [README](https://github.com/nodejs/node-gyp).\n\nUsing a _PowerShell_ CLI instance with administrative priviledges, and use the following code:\n\n```bash\nnpm install --global --production windows-build-tools\n```\n\nAnd that's it.\n\nThere is a manual option as well, but I haven't needed to use it as the first option always works for me.\n\n","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"NodeJS","slug":"NodeJS","permalink":"https://westerndevs.com/tags/NodeJS/"},{"name":"node-gyp","slug":"node-gyp","permalink":"https://westerndevs.com/tags/node-gyp/"}]},{"title":"Hour of Code Challenge","authorId":"dave_white","slug":"Hour-of-Code-Challenge","date":"2016-09-23 23:55:48+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Community/Hour-of-Code-Challenge/","link":"","permalink":"https://westerndevs.com/Community/Hour-of-Code-Challenge/","excerpt":"I'm throwing down the gauntlet! I'm challenging my fellow WD members to get involved with Hour of Code!","raw":"---\nlayout: post\ntitle: Hour of Code Challenge\ncategories:\n  - Community \ndate: 2016-09-23 19:55:48\ntags:\n  - Hour of Code\n  - Community\n  - Learning\nexcerpt: I'm throwing down the gauntlet! I'm challenging my fellow WD members to get involved with Hour of Code!\nauthorId: dave_white\n---\n\nIn case you haven't heard, there is this little thing called [Computer Science Education Week][1] which supports our industries\neffort to get more computer science education happening for kids in the K-12 grade range. Please visit the [Computer Science Education Week][1]\nfor all of the deals, or take my word that this is an important initiative!\n\nTo support this initiative and provide resources on a year-round basis, there is the [Code.org][2] organization! To take a snippet\nfrom their website:\n\n>Launched in 2013, [Code.orgÂ®][2] is a non-profit dedicated to expanding access to computer science, and increasing participation \nby women and underrepresented students of color.\n\nOk! This sounds like another good thing! A whole organization trying to expand access to computer science education! \n\nPut the two of these things together and we get [The Hour of Code][3], a movement that provides 1 hour computer programming tutorials for all age groups in 45 different languages.\nAnd during Computer Science Education Week, which run from December 5-11, 2016, [The Hour of Code][3] movement issues a challenge to reach as many children\nas possible during the week with these 1 hour tutorials!\n\nThis is where I'm laying down my challenge to all of my fellow WD members!\n\nFor the last 2 years, I've participate in the [CSED Week][1]! In year one, I spent 1 hour with 12 kids at the Boys and Girls Club of Calgary! \nLast year, I was able to get into my sons' school and spent 1 hour with a a Gr. 2 and Gr. 4 class! It was a fantastic experience and I've already started the process\nof getting organized with the schools again this year!\n\n## My Challenge to WesternDevs\n\nI am challenging all of my fellow WesternDevs to arrange 1 Hour of Code event! This can be with their kids' schools, a local kids club, or \nevent a public event inviting kids to attend! \n\nI'm issuing his challenge because there are logistics and permissions that sometimes need to be arranged and this should give my fellow WD\nmore than enough time to get these things sorted out!\n\nI'm also willing to put up a bounty! For every WD member who performs 1 or more sessions, [I will donate $100 CAD to Code.orgÂ®][4] to support\nthis organization! In order to claim the bounty, there will need to be: \n1) Blog post talking about the event\n2) Picture with the organizer (permissions allowing) \n\nSo there it is! My challenge! I hope this encourages all of the WD (and anyone who follows us) to get involved with this fantastic opportunity to give back to the community and our children.\nI'll look forward to seeing all of your posts in December!\n\n[1]: https://csedweek.org/\n[2]: https://code.org/\n[3]: https://hourofcode.com/\n[4]: https://code.org/help/\n","categories":[{"name":"Community","slug":"Community","permalink":"https://westerndevs.com/categories/Community/"}],"tags":[{"name":"Hour of Code","slug":"Hour-of-Code","permalink":"https://westerndevs.com/tags/Hour-of-Code/"},{"name":"Community","slug":"Community","permalink":"https://westerndevs.com/tags/Community/"},{"name":"Learning","slug":"Learning","permalink":"https://westerndevs.com/tags/Learning/"}]},{"title":"How to Build ReactJS with Gulp","authorId":"david_wesst","slug":"How-to-Build-ReactJS-with-Gulp","date":"2016-09-19 23:55:48+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"JavaScript/How-to-Build-ReactJS-with-Gulp/","link":"","permalink":"https://westerndevs.com/JavaScript/How-to-Build-ReactJS-with-Gulp/","excerpt":"I started to tinker with React last week and needed to do some digging on how to transpile the React JSX files using Gulp. In this post, we walkthrough my newly updated Gulp task that transpiles JSX and JavaScript files.","raw":"---\nlayout: post\ntitle: How to Build ReactJS with Gulp\ncategories:\n  - JavaScript \ndate: 2016-09-19 19:55:48\ntags:\n  - ReactJS\n  - Gulp\n  - JavaScript\n  - Babel\nexcerpt: I started to tinker with React last week and needed to do some digging on how to transpile the React JSX files using Gulp. In this post, we walkthrough my newly updated Gulp task that transpiles JSX and JavaScript files.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/09/How-to-Build-ReactJS-with-Gulp/\n---\n\nI wanted to play with ReactJS last week and figured I would just add another gulp talk to my build process, being that it's all the rage right now.\n\nIt turns out that it wasn't that simple, as it turns out that the [`gulp-react`](https://www.npmjs.com/package/gulp-react) has been \"deprecated in favor of gulp-babel\". I hadn't planned on learning [Babel](http://babeljs.io/), but it turns out it wasn't very difficult once I was able to put all the pieces together.\n\nHere's what I did.\n\nYou can find an extended version of the code in this post in my web project's [`gulpfile.js`](https://github.com/davidwesst/dw-www/blob/master/gulpfile.js). As of this writing, it can be found in the `feat/talks` branch but should be merged into the [`master`](https://github.com/davidwesst/dw-blog) branch  soon enough.\n\n## The Gulp Task\nLet's start with the gulp task and I'll walk you though it step by step.\n\n```javascript\n// packages \nvar gulp        = require('gulp'),\n\tbabel\t\t= require('gulp-babel'),\n\tsourcemaps\t= require('gulp-sourcemaps');\n    \n// build all the JavaScript things\ngulp.task('build-script', function() {\n\tvar src = [\n\t\t'./src/script/*.js',\n\t\t'./src/components/*.jsx'\n\t\t];\n\n\treturn gulp.src(src)\n\t\t\t\t.pipe(sourcemaps.init())\n\t\t\t\t.pipe(babel({\n\t\t\t\t\tpresets: [\n\t\t\t\t\t\t'es2015',\n\t\t\t\t\t\t'react'\n\t\t\t\t\t\t]\n\t\t\t\t\t}))\n\t\t\t\t.pipe(concat('dw.js'))\n\t\t\t\t.pipe(gulp.dest('./dist/script'));\n});\n```\n\nIt's not overly complex, but there is a lot going on. Let me walk you though it.\n\n### Dependencies\n\n```javascript\n// packages\nvar gulp        = require('gulp'),\n\tbabel\t\t= require('gulp-babel'),\n\tsourcemaps\t= require('gulp-sourcemaps');\n```\n\nThese are the packages I used in this solution. Here's the breakdown in case you don't feel like searching out each one:\n\n+ [gulp](https://www.npmjs.com/package/gulp) is the build tool itself\n+ [gulp-babel](https://www.npmjs.com/package/gulp-babel) is the babel plugin that builds react for us\n+ [gulp-sourcemaps](https://www.npmjs.com/package/gulp-sourcemaps) is a pluging that generates sourcemaps for us\n\nThere are a couple of unlisted packages that you'll need as well. More specifically, the Babel preset packages. I use two:\n\n+ [babel-preset-es2015](https://www.npmjs.com/package/babel-preset-es2015) because writing modern JavaScript is awesome and helps with writing ReactJS code.\n+ [babel-preset-react](https://www.npmjs.com/package/babel-preset-react) which does the ReactJS stuff we need\n\n**NOTE: You don't need the es2015 preset package, but you should use because you don't hate yourself enough to write old JavaScript**\n\n### Pipeline\n\nThings start simple with the declaration of my source files and kicking off the gulp pipe:\n\n```javascript\nvar src = [\n\t\t'./src/script/*.js',\n\t\t'./src/components/*.jsx'\n\t\t];\n\n\treturn gulp.src(src)\n```\n\nNext up, I intialize the sourcemaps plugin\n\n```javascript\n.pipe(sourcemaps.init())\n```\n\nIf you're not sure what sourcemaps are, [look them up](http://www.html5rocks.com/en/tutorials/developertools/sourcemaps/) as all the browser development tools support them and they made debugging transpiled code much easier.\n\nNow, we leverage all the Babel things and do the actual transpilation.\n\n```javascript\n.pipe(babel({\npresets: [\n    'es2015',\n\t'react'\n]\n```\n\nAt this point were piping our source code into Babel, and it applies the preset [babel plugins](https://babeljs.io/docs/plugins/) for ES2015 and React. If you didn't include the presets earlier, you would get an error at this point so make sure they're installed.\n\nNow we just write out our sourcemaps and our newly compiled JavaScript and JSX files to our output directory.\n\n```javascript\n.pipe(sourcemaps.write('.'))\n.pipe(gulp.dest('./dist/script'));\n```\n\nEt voila! Your gulp task transpiles your JSX and JS files. You can even use some [ES2015](https://babeljs.io/docs/learn-es2015/) to boot.\n\n","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"ReactJS","slug":"ReactJS","permalink":"https://westerndevs.com/tags/ReactJS/"},{"name":"Gulp","slug":"Gulp","permalink":"https://westerndevs.com/tags/Gulp/"},{"name":"Babel","slug":"Babel","permalink":"https://westerndevs.com/tags/Babel/"}]},{"title":"Career Planning","slug":"Career-Planning","date":"2016-09-19 21:11:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Career-Planning/","link":"","permalink":"https://westerndevs.com/podcasts/Career-Planning/","excerpt":"When it comes to planning your career, the Western Devs have it aaaaaaaall worked out. But when it comes to audio issues...sorry about that, westerners.","raw":"---\nlayout: podcast\ntitle: Career Planning\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-career-planning.mp3\n  length: '41:28'\n  filesize: 39801087\n  libsynId: 4659755\n  anchorFmId: Career-Planning-evqdi0\nparticipants:\n  - kyle_baley\n  - dave_paquette\n  - justin_self\n  - lori_lalonde\n  - rob_windsor\n  - tom_opgenorth\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Career path of a programmer|https://www.ctl.io/developers/blog/post/career-path-of-a-programmer/\"\n  - \"Career plan for software engineer|http://andriybuday.com/2010/08/career-plan-for-software-engineer.html\"\n  - \"How to develop your IT career plan and why you should do it|http://itmanagersinbox.com/1452/how-to-develop-your-it-career-plan-and-why-you-should-do-it/\"\ndate: 2016-09-19 17:11:22\nrecorded: 2016-06-17\nexcerpt: \"When it comes to planning your career, the Western Devs have it aaaaaaaall worked out. But when it comes to audio issues...sorry about that, westerners.\"\n---\n\n### Synopsis\n\n* The Canadian Space Agency is hiring!\n* Was software your first career choice?\n* Is a degree required?\n* Visa requirements for applying online\n* Will you spend your entire career in software?\n* What does retirement look like to you?\n* Parenthood/consulting\n* What radio station is Justin listening to that always provides such apropos background music?\n* What role does your employer play in career planning?\n* Shifting expectations with the next generation\n* Considerations for managing your career\n* Knowing when it's time for a shift\n* Recognizing opportunities\n* The importance of company culture\n* Mentoring and teaching\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"How to Compile Typescript into a Single File with AMD Modules with Gulp","authorId":"david_wesst","slug":"TypeScript-with-AMD-and-Gulp","date":"2016-09-12 12:25:01+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/TypeScript-with-AMD-and-Gulp/","link":"","permalink":"https://westerndevs.com/_/TypeScript-with-AMD-and-Gulp/","excerpt":"The lessons I learned and the steps I took to compile TypeScript modules into a single file with AMD modules using Gulp, along with how I consumed those compiled modules in my HTML application.","raw":"---\nlayout: post\ntitle: How to Compile Typescript into a Single File with AMD Modules with Gulp\ndate: 2016-09-12 08:25:01\ntags:\n  - typescript\n  - javascript\n  - amd\n  - requirejs\n  - gulp\nexcerpt: The lessons I learned and the steps I took to compile TypeScript modules into a single file with AMD modules using Gulp, along with how I consumed those compiled modules in my HTML application.  \nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/09/How-to-Compile-Typescript-into-a-Single-File-with-AMD-Modules/\n---\nI've been tinkering with TypeScript lately and was trying to setup my project to compile all of my modules into a single file, which would then be used in an HTML page. Maybe this is obvious to the more experienced TypeScript developer, but I had made a number of false assumptions while trying to get this to work.\n\nThis post will walk you through what I did to setup my build and get it working in an HTML page.\n\n**You can follow along at home with the source code which I've put up on [GitHub](https://github.com/davidwesst/ts-project-template)**\n\n## Modular TypeScript\nMy project is starting out simple, with a single module and a couple of different \"Apps\" that will use the module. Module in TypeScript is extensively documented in the [TypeScript Handbook](), so if you're not familiar with this I would suggest reading up on it as it's pretty awesome once you start using it.\n\nIn any case, here's my code:\n\n```typescript\n// ModuleOne.ts\n\nexport class ModuleOne {\n    sayHello() {\n        console.log(\"Hello from Module1!\");\n    }\n\n    sayHelloTo(who: string) {\n        console.log(\"Hello \" + who.trim() + \". This is Module1\");\n    }\n}\n```\n\n```typescript\n// ModuleTwo.ts\n\nexport class ModuleOne {\n    sayHello() {\n        console.log(\"Hello from ModuleTwo!\");\n    }\n\n    sayHelloTo(who: string) {\n        console.log(\"Hello \" + who.trim() + \". This is ModuleTwo\");\n    }\n}\n```\n\n```typescript\n// App.ts\n\nimport * as Module1 from \"./modules/Module1\";\nimport * as Module2 from \"./modules/Module2\";\n\nexport class App {\n    start() {\n        let m1 = new Module1.ModuleOne();\n        let m2 = new Module2.ModuleTwo();\n        \n        m1.sayHelloTo(\"David Wesst\");\n        m2.sayHelloTo(\"David Wesst\");\n    }\n}\n```\n\nI have a single application that is using two modules. Plain and simple. Next up, I compile my code into a single file, then reference that in my HTML, and done like dinner.\n\n...or so I thought.\n\n ## The `--outFile` Parameter\n Reading through the doucmentation about TypeScript compilation, I found the `--outFile` or `--out` parameter in the [documentation](https://www.typescriptlang.org/docs/handbook/compiler-options.html). I immediately assumed that I was done, as I would simple choose my ES target, select the type of modules I would like, and presto. \n \n That wasn't the case.\n\n Being a person who likes modern JavaScript, I had originally opted to output ES6 compatible code complete with the new modules. \n \n This is was my first mistake. \n \n I found was that when I compiled, I would get a single output file but it would be completely blank. No error, but just an empty file. This is expected behaviour, as the `--outFile` option only supports _commonjs_ and _amd_. That meant no ES6 or even ES2015 modules for my project, which is probably for the best considering how few web browsers in the wild actually support ES6 modules as of this writing.\n\n In the end, I decided to go with AMD modules as I had some experience with [RequireJS](http://requirejs.org/).\n\n Now, when I try to compile again it works as expected! One big JavaScript file ready to go.\n \n ...sort of.\n\n ## Using My Compiled TypeScript\n  \nSomewhere along the line, I assumed that whatever JavaScript file I compiled would only need me to add a `<script>` tag reference to point to it like any other JavaScript file. \n\nThis was my second mistake, albeit a pretty silly one.\n\nAMD modules have always needed RequireJS to load properly. That's the purpose for RequireJS. Maybe I had assumed the TypeScript compiler would embed this library or something, but whatever my reasoning it was wrong. I needed to include a `data-main` file, as you do with every RequireJS example.\n\nI added this to my HTML file, along with the RequireJS library in my project:\n\n```html\n<script data-main=\"main\" type=\"text/javascript\" src=\"lib/require.js\"></script>\n```\n\nThen, my `data-main` file goes something like this:\n\n```javascript\n// main.js\n  \nrequirejs.config({\n    baseUrl: 'lib',\n    paths: {\n        'App':'../app'\n    }\n});\n\nrequirejs(['App'], function(MyApp) {\n    console.log('starting application...');\n\n    var app = new MyApp.App();\n    app.start();\n});\n```\n\nI'm not going to go into the details here, but my `paths` object in the `requirejs.config` is pointing `App` to our outputted file. We use this in our main function and point our compiled modules to the `MyApp` object. We then call the `start()` function on our exported class and we are off to the races. \n\nWhen we run the application, we see the following in the JavaScript console.\n\n![What the console window should look like](http://i.imgur.com/38ngK52.png)\n\n## Details on Compilation\nI skipped that part on purpose, because I don't use the TSC compiler directly. Rather, I use [`gulp-typescript`](https://github.com/ivogabe/gulp-typescript) with a `tsconfig` file to compile my TypeScript and create sourcemaps for them. All of this is detailed on the [package page](https://www.npmjs.com/package/gulp-typescript), but I'll include my gulp task to make sure you have all the details in one place. \n\nYou're welcome. ;)\n\n```javascript\nvar tsProject = ts.createProject('tsconfig.json');\n\ngulp.task('build-ts', ()=> {\n    let tsResult = tsProject.src()\n                    .pipe(sourcemaps.init()) // using gulp-sourcemaps as prescribed by gulp-typescript\n                    .pipe(ts(tsProject));\n    \n    return tsResult\n            .js\n            .pipe(sourcemaps.write('./'))\n            .pipe(gulp.dest('./dist'))\n            .pipe(connect.reload());  // I use gulp-connect to watch and reload the page as I develop\n});\n```\n\nOh, and here's my `tsconfig.json` file too.\n\n```javascript\n{\n    \"compilerOptions\": {\n        \"module\": \"amd\",\n        \"rootDir\": \"./src/ts\",\n        \"sourceRoot\": \"./src/ts\",\n        \"outFile\": \"app.js\",\n        \"target\": \"es5\"\n    },\n    \"exclude\": [\n        \"node_modules\",\n        \"dist\"\n    ]\n}\n```\n","categories":[],"tags":[{"name":"gulp","slug":"gulp","permalink":"https://westerndevs.com/tags/gulp/"},{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"},{"name":"typescript","slug":"typescript","permalink":"https://westerndevs.com/tags/typescript/"},{"name":"amd","slug":"amd","permalink":"https://westerndevs.com/tags/amd/"},{"name":"requirejs","slug":"requirejs","permalink":"https://westerndevs.com/tags/requirejs/"}]},{"title":"Stress","slug":"Stress","date":"2016-09-09 21:11:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Stress/","link":"","permalink":"https://westerndevs.com/podcasts/Stress/","excerpt":"Chillax with the Western Devs as they relieve their stress through podcast therapy","raw":"---\nlayout: podcast\ntitle: Stress\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-stress.mp3\n  length: '40:48'\n  filesize: 39174558\n  libsynId: 4656496\n  anchorFmId: Stress-evqdhu\nparticipants:\n  - kyle_baley\n  - colin_higgins\n  - dave_white\n  - donald_belcham\n  - dylan_smith\n  - rob_windsor\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - \"Zach Holman at GitHub|https://zachholman.com/talk/firing-people\"\n  - \"Colin Higgins on Twitter|https://twitter.com/spykebytes\"\n  - \"How stressful is software development|https://www.quora.com/How-stressful-is-the-job-of-being-a-software-engineer\"\n  - \"Dealing with developer stress|https://www.airpair.com/javascript/posts/tips-for-dealing-with-developer-stress\"\ndate: 2016-09-09 17:11:22\nrecorded: 2016-05-20\nexcerpt: \"Chillax with the Western Devs as they relieve their stress through podcast therapy\"\n---\n\n### Synopsis\n\n* Is stress leave a real thing?\n* Should stress leave be paid?\n* Work-related vs. home-related stress\n* \"Mental health\" days\n* Preventing stress at worked\n* De-stigmatizing stress at work\n* What is management's role in helping staff deal with stress?\n* How can an individual make things better?\n* Alternatives to stress leave\n* What if it's not viable to leave my job?\n* Symptoms of stress\n* Ways to handle stress\n* Importance of sharing your stress\n* Other sources of stress outside workload\n* Recognizing stress in others\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"How to Use Highlight.Js with Bower and Gulp","authorId":"david_wesst","slug":"How-to-Use-Highlight-Js-with-Bower-and-Gulp","date":"2016-08-29 14:07:11+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/How-to-Use-Highlight-Js-with-Bower-and-Gulp/","link":"","permalink":"https://westerndevs.com/_/How-to-Use-Highlight-Js-with-Bower-and-Gulp/","excerpt":"One of the best libraries I've seen for syntax highlighting on the web is highlight.js, IMHO. The catch to using the library is that it takes a bit more effort to setup than just adding a script tag and being done with it. In this post, I'll walk you through the steps I took to get this up and running with Bower and Gulp.","raw":"---\nlayout: post\ntitle: How to Use Highlight.Js with Bower and Gulp\ndate: 2016-08-29 10:07:11\ntags:\n  - gulp\n  - bower\n  - highlight.js\n  - javascript\nexcerpt: One of the best libraries I've seen for syntax highlighting on the web is highlight.js, IMHO. The catch to using the library is that it takes a bit more effort to setup than just adding a script tag and being done with it. In this post, I'll walk you through the steps I took to get this up and running with Bower and Gulp.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/08/How-to-Use-HighlightJs-with-Bower-and-Gulp/ \n---\n\nOne of the challenges I faced when getting my new blog theme up and running was getting [highlight.js](https://highlightjs.org) working as I wanted it to. The library has support for a riduculous number of languages, and provides theming capabilities, so it was the clear choice when it came to syntax highlighting for the web. The challenge is that tt's not just adding simple script tag and you're done with it. They seem to expect you to know what you're doing when consuming the library. \n\nRemember: this is a _good thing_. \n\nIf you're only include code snippets for HTML and JavaScript, why would you want to bring down the stylesheets for D, Erlang, and ActionScript too? Same goes for the themes too. You're likely going to pick a single theme and go with it, and not need all 77 they include. Because of the size of highlight.js, bringing it all down to the client would have a significant impact on your site's performance.\n\nLucky for us, the folks in charge of highlight.js have given us all the tools we need to make using the library in our application nice and performant. In my case, I use [Bower](https://bower.io) as my package manager and [Gulp](http://gulpjs.com) as my build system, which worked well once I figured out what I was doing. Let me walk you though it.\n\n## Installing Highlight.js with Bower\nTo start, you'll need to add highlight.js to your project using Bower.\n\n```bash\nbower install --save highligh.js.origin\n```\n\nGenerally speaking, after you install a package from Bower you have a ready-to-use JavaScript library. With highlight.js, this is not the case. Rather, you are left with source code ready to be built as you need it. To do that, we are going to use Gulp.\n\n## Building Highlight.js with Gulp\nThe hard work has already been done by the highlight.js team, as they have already included a lovely build tool for us to use. You can read about the details [here in the building and testing documentation](http://highlightjs.readthedocs.io/en/latest/building-testing.html). \n\nThere are two parts to \"building\" highlight.js: the JavaScript and the stylesheets. We'll be creating a task for each of them.\n\n### Building highlight.js JavaScript\nLet's start with the JavaScript component.\n\nIf you read the [docs](http://highlightjs.readthedocs.io/en/latest/building-testing.html) earlier, you know that there is a NodeJS script in the `tools` directory that will do all the heavy lifting. All we need to do in our Gulp task is use that script.\n\nHere's my Gulp task:\n\n```javascript\ngulp.task('process-highlightjs-script', function(callback) {\n    let command = 'cd bower_components/highlight.js.origin'\n                    + ' && npm install' \n                    + ' && node tools/build :common';\n    exec(command, (err, stdout, stderr)=> {\n        console.log(stderr);\n        console.log(stdout);\n\n        callback(err);\n    });\n\n    return;\n});\n```\n\nIt looks a little strange for a gulp task, but all I'm doing is executing a shell command using the [`exec`](https://nodejs.org/api/child_process.html#child_process_child_process_exec_command_options_callback) in NodeJS to spawn a child process. The important part is in the `command` variable. Let's walk through it.\n\n\n```bash\ncd bower_components/highlight.js.origin\n```\n\nFirst, I'm moving into the root directory of the highligh.js.origin package we previously installed using Bower.\n\n```bash\nnpm install\n```\n\nThen, I'm installing the dependencies that are needed by the provided build tool.\n\n```bash\nnode tools/build :common\n```\n\nFinally, I'm running the build command as specified in the documentation. This part should be changed to make sure it includes the languages you want to display in your application. Details are in their [documentation](http://highlightjs.readthedocs.io/en/latest/building-testing.html).\n\nThe rest is just outputting the standard out and standard error streams to the console, and calling the Gulp callback to make sure we come back to our original process, as per the [Gulp documentation](https://github.com/gulpjs/gulp/blob/master/docs/API.md#async-task-support). \n\nIf everything is done correctly, you should have a new `build` folder in the `highlight.js.origin` folder that contains your newly build library.\n\n## Building Highlight.js Themes for SASS\nI suppose this part is optional, but it shouldn't be.\n\nTo make optimize your site, you should be minimizing the number of requests that the client browser needs to make to get all the required resources for your application. For styling, that involves concatenanting all of your styles and style libraries into a single file.\n\nTo do this, I use [SASS](http://sass-lang.com) or, more specifically, [gulp-sass](https://www.npmjs.com/package/gulp-sass) to build all of my stylesheets and combine them into a single CSS file as described by the [SASS documentation](http://sass-lang.com/guide#topic-5). I'm going to assume that you're doing the same, or at least something similar in your application that will be using highlight.js.\n\nBecause SASS only handles combining other `scss` files, I copy the theme stylesheet into a newly named `scss` file. Here's my gulp task for that:\n\n```javascript\ngulp.task('process-highlightjs-style', function() {\n    let stylesheets = [\n        './bower_components/highlight.js.origin/src/styles/atom-one-dark.css'\n    ];\n\n    return gulp.src(stylesheets)\n            .pipe(rename('highlight.js.scss'))\n            .pipe(gulp.dest('./bower_components/highlight.js.origin/scss'));\n});\n```\n\nThen in my main stylesheet, I include:\n\n```scss\n@import '../../bower_components/highlight.js.origin/scss/highlight.js';\n```\n\nThis will bring in the `highlight.js.scss` file when I build my styles using `gulp-sass` in my application stylesheet build task.\n\nAnd now highlight.js ready to Use in our application.\n\n<figure class=\"image\">\n    ![Highlight.js in Action](http://i.imgur.com/1cUniu9.png)\n    <figcaption>Highlight.js in Action</figcaption>\n</figure>\n\n#### Caveat About Using Highlight.js with Markdown\nMy post assumes that for any blocks of text where you want syntax highlighting you're comfortable using the default [highlight.js usage](https://highlightjs.org/usage/) behaviour of wrapping the code with `<pre><code>` elements.\n\nAlthough I blog using Markdown, at the time of this writing I still wrap my text with these elements, unlike [GitHub flavoured markdown](https://help.github.com/articles/creating-and-highlighting-code-blocks/) that lets define the language using regular Markdown syntax.\n\nThe reason for this, is that my blog engine renders the Markdown syntax with a bunch of extra HTML woven throughout the code to display it without the need for a library like highlight.js. \n\nIt might not be a big deal for you or your project, but I thought it was something you should be aware of when if you're planning on using highlight.js in your application.\n\n","categories":[],"tags":[{"name":"gulp","slug":"gulp","permalink":"https://westerndevs.com/tags/gulp/"},{"name":"bower","slug":"bower","permalink":"https://westerndevs.com/tags/bower/"},{"name":"highlight.js","slug":"highlight-js","permalink":"https://westerndevs.com/tags/highlight-js/"},{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/tags/javascript/"}]},{"title":"You're using HttpClient wrong and it is destabilizing your software","authorId":"simon_timms","slug":"httpclientwrong","date":"2016-08-28 21:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Deployment/httpclientwrong/","link":"","permalink":"https://westerndevs.com/Deployment/httpclientwrong/","excerpt":"I've been using HttpClient wrong for years and it finally came back to bite me. My site was unstable and my clients furious, with a simple fix performance improved greatly and the instability disapeared. At the same time I actually improved the performance of the application through more efficient socket usage.","raw":"---\nlayout: post\ntitle: You're using HttpClient wrong and it is destabilizing your software\ntags:\n  - .net\ncategories:\n  - Deployment   \nauthorId: simon_timms\ndate: 2016-08-28 17:36:36\n---\n\nI've been using HttpClient wrong for years and it finally came back to bite me. My site was unstable and my clients furious, with a simple fix performance improved greatly and the instability disapeared. \n\n![Imgur](http://i.imgur.com/EctiaBj.jpg)\n\nAt the same time I actually improved the performance of the application through more efficient socket usage.\n\n<!-- more -->\n\nMicroservices can be a bear to deal with. As more services are added and monoliths are broken down there tends to be more communication paths between services. There are many options for communicating, but HTTP is an ever popular option. If the microservies are built in C# or any .NET language then chances are you've made use of `HttpClient`. I know I did. \n\nThe typical usage pattern looked a little bit like this: \n\n```\nusing(var client = new HttpClient())\n{\n    //do something with http client\n}\n```\n\n## Here's the Rub\nThe `using` statement is a C# nicity for dealing with disposable objects. Once the `using` block is complete then the disposable object, in this case `HttpClient`, goes out of scope and is disposed. The `dispose` method is called and whatever resources are in use are cleaned up. This is a very typical pattern in .NET and we use it for everything from database connections to stream writers. Really any object which has external resources that must be clean up uses the `IDisposable` interface. \n\nAnd you can't be blamed for wanting to wrap it with the using. First of all, it's considered good practice to do so. In fact, the [official docs](https://msdn.microsoft.com/en-ca/library/yh598w02.aspx) for `using` state:\n\n> As a rule, when you use an IDisposable object, you should declare and instantiate it in a using statement. \n\nSecondly, all code you may have seen since...the inception of `HttpClient` would have told you to use a `using` statement block, including recent docs on the [ASP.NET site itself](http://www.asp.net/web-api/overview/advanced/calling-a-web-api-from-a-net-client). The internet is generally [in agreement as well](http://stackoverflow.com/questions/212198/what-is-the-c-sharp-using-block-and-why-should-i-use-it).\n\nBut `HttpClient` is different. Although it implements the `IDisposable` interface it is actually a shared object. This means that under the covers it is [reentrant](https://en.wikipedia.org/wiki/Reentrancy_(computing)) and thread safe. Instead of creating a new instance of `HttpClient` for each execution you should share a single instance of `HttpClient` for the entire lifetime of the application. Let's look at why.\n\n## See For Yourself\nHere is a simple program written to demonstrate the use of `HttpClient`:\n\n```\n\nusing System;\nusing System.Net.Http;\n\nnamespace ConsoleApplication\n{\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            Console.WriteLine(\"Starting connections\");\n            for(int i = 0; i<10; i++)\n            {\n                using(var client = new HttpClient())\n                {\n                    var result = client.GetAsync(\"http://aspnetmonsters.com\").Result;\n                    Console.WriteLine(result.StatusCode);\n                }\n            }\n            Console.WriteLine(\"Connections done\");\n        }\n    }\n}\n\n```\n\nThis will open up 10 requests to one of the best sites on the internet [http://aspnetmonsters.com](http://aspnetmonsters.com) and do a `GET`. We just print the status code so we know it is working. The output is going to be: \n\n```\nC:\\code\\socket> dotnet run\nProject socket (.NETCoreApp,Version=v1.0) will be compiled because inputs were modified\nCompiling socket for .NETCoreApp,Version=v1.0\n\nCompilation succeeded.\n    0 Warning(s)\n    0 Error(s)\n\nTime elapsed 00:00:01.2501667\n\n\nStarting connections\nOK\nOK\nOK\nOK\nOK\nOK\nOK\nOK\nOK\nOK\nConnections done\n```\n\n## But Wait, There's More!\nAll work and everything is right with the world. Except that it isn't. If we pull out the `netstat` tool and look at the state of sockets on the machine running this we'll see: \n\n```\nC:\\code\\socket>NETSTAT.EXE\n...\n  Proto  Local Address          Foreign Address        State\n  TCP    10.211.55.6:12050      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12051      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12053      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12054      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12055      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12056      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12057      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12058      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12059      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12060      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12061      waws-prod-bay-017:http  TIME_WAIT\n  TCP    10.211.55.6:12062      waws-prod-bay-017:http  TIME_WAIT\n  TCP    127.0.0.1:1695         SIMONTIMMS742B:1696    ESTABLISHED\n...\n```\n\nHuh, that's weird...the application has exited and yet there are still a bunch of these connections open to the Azure machine which hosts the ASP.NET Monsters website. They are in the `TIME_WAIT` state which means that the connection has been closed on one side (ours) but we're still waiting to see if any additional packets come in on it because they might have been delayed on the network somewhere. Here is a diagram of TCP/IP states I stole from https://www4.cs.fau.de/Projects/JX/Projects/TCP/tcpstate.html.\n\n![Imgur](http://i.imgur.com/rXxnIA8.png)\n\nWindows will hold a connection in this state for 240 seconds (It is set by `[HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\\TcpTimedWaitDelay]`). There is a limit to how quickly Windows can open new sockets so if you exhaust the connection pool then you're likely to see error like: \n\n```\nUnable to connect to the remote server\nSystem.Net.Sockets.SocketException: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n```\n\nSearching for that in the Googles will give you some terrible advice about decreasing the connection timeout. In fact, decreasing the timeout can lead to other detrimental consequences when applications that properly use `HttpClient` or similar constructs are run on the server. We need to understand what \"properly\" means and fix the underlying problem instead of tinkering with machine level variables.\n\n## The Fix is In\nI really must thank [Harald S. Ulrksen](https://twitter.com/hsulriksen) and [Darrel Miller](https://twitter.com/darrel_miller) for pointing me to [The Patterns and Practices documents](https://t.co/bewSxPqlps) on this.\n\nIf we share a single instance of `HttpClient` then we can reduce the waste of sockets by reusing them:\n\n```\nusing System;\nusing System.Net.Http;\n\nnamespace ConsoleApplication\n{\n    public class Program\n    {\n        private static HttpClient Client = new HttpClient();\n        public static void Main(string[] args)\n        {\n            Console.WriteLine(\"Starting connections\");\n            for(int i = 0; i<10; i++)\n            {\n                var result = Client.GetAsync(\"http://aspnetmonsters.com\").Result;\n                Console.WriteLine(result.StatusCode);\n            }\n            Console.WriteLine(\"Connections done\");\n            Console.ReadLine();\n        }\n    }\n}\n```\n\nNote here that we have just one instance of `HttpClient` shared for the entire application. Eveything still works like it use to (actually a little faster due to socket reuse). Netstat now just shows: \n\n```\nTCP    10.211.55.6:12254      waws-prod-bay-017:http  ESTABLISHED\n```\n\nIn the production scenario I had the number of sockets was averaging around 4000, and at peak would exceed 5000, effectively crushing the available resources on the server, which then caused services to fall over. After implementing the change, the sockets in use dropped from an average of more than 4000 to being consistently less than 400, and usually around 100.\n\nThis is a chunk of a graph from our monitoring tools and shows what happened after we deployed a limited proof of the fix to a select number of microservices. \n\n![Imgur](http://i.imgur.com/0QVdLMT.png)\n\nThis is dramatic. If you have any kind of load at all you need to remember these two things:\n \n 1. Make your `HttpClient` static.\n 2. Do _not_ dispose of or wrap your `HttpClient` in a using unless you explicitly are looking for a particular behaviour (such as causing your services to fail).\n\n## Wrapping Up\nThe socket exhaustion problems we had been struggling with for months disapeared and our client threw a virtual parade. I cannot understate how unobvious this bug was. For years we have been conditioned to dispose of objects that implement `IDisposable` and many refactoring tools like R# and CodeRush actually warn if you don't. In this case disposing of `HttpClient` was the wrong thing to do. It is unfortunate that `HttpClient` implements `IDisposable` and encourages the wrong behaviour","categories":[{"name":"Deployment","slug":"Deployment","permalink":"https://westerndevs.com/categories/Deployment/"}],"tags":[{"name":".net","slug":"net","permalink":"https://westerndevs.com/tags/net/"}]},{"title":"Mobile First Design Tips","authorId":"david_wesst","slug":"Mobile-First-Design-Tips","date":"2016-08-16 12:55:17+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Mobile-First-Design-Tips/","link":"","permalink":"https://westerndevs.com/_/Mobile-First-Design-Tips/","excerpt":"When redesigning my personal website and blog, I started from scratch and attempted to apply a mobile-first design approach. Here are a few tips that I picked up along the way while creating and implementing my new design that might prevent some mistakes on your own sites.","raw":"---\nlayout: post\ntitle: Mobile First Design Tips\ndate: 2016-08-16 08:55:17\ntags:\n  - css\n  - mobile\n  - responsive\nexcerpt: When redesigning my personal website and blog, I started from scratch and attempted to apply a mobile-first design approach. Here are a few tips that I picked up along the way while creating and implementing my new design that might prevent some mistakes on your own sites.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/08/Mobile-First-Design-Tips/\n---\n\nLast weekend I released an updated version of my blog and website that share a similar design. I decided to go with a mobile-first approach when it came to the design, which is a tried and tested way to create responsive web applications. Along the way, I tripped up and made a few mistakes that cost me some time and rework, but could have been easily corrected had I had these tips beforehand.\n\n### Don't Forget the Viewport meta tag\nMore specifically, something like this:\n\n```html\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n```\n\nI forgot about the first week of implementing my stylesheet. I won't say that all my work was lost after adding it, but it certainly tossed all the detail I had put into the design implementation out the window.\n\nAll modern designs have this header tag. You can [read more about it on the MDN](https://developer.mozilla.org/en/docs/Mozilla/Mobile/Viewport_meta_tag) but, to summarize, it defines the width of your application viewport and the scale that it loads up in. Read through the section entitled [A Pixel is Not a Pixel](https://developer.mozilla.org/en/docs/Mozilla/Mobile/Viewport_meta_tag#A_pixel_is_not_a_pixel) in the previous MDN article to get a better understand of this.\n\nThe important thing to know is that without this, you will feel pain.\n\n### Dev Tool Emulators are Your Friends\nEdge, Chrome, Opera, Firefox, and Safari all have some kind of responsive design emulator tool to help make testing your code easy. Use it from the start, by selecting a pre-set responsive profile for the smallest device you care about for your web application.\n\nThis doesn't replace testing on an real small-screen device, but it definitely gets you moving on the development front.\n\nUsing the built-in emulators from the get-go enforces the mobile-first approach by ensuring that you start with the minimal amount of screen real-estate when building your application. As I learned, it is easy to get caught up in adding extra content because you think you'll have the space, which brings me to the next tip. \n\n![Responsive Tools in Internet Explorer](http://i.imgur.com/5JsR2Icl.png)\n\n### Start without CSS\n\nNo really, start without any styling.\n\nYou might wonder why, but it really helps with understanding what \"content\" you're displaying. If you have a large number of elements on the screen that don't actually convey any information, you are probably making your application less accessible. \n\nFor example, if you have a lot of icons that are just there for asthetic purposes, you'll notice them a lot more in the plain HTML version of your site. \n\nNot using any CSS also helps with the next tip.\n\n![My site without CSS](http://i.imgur.com/TCDTcPGl.png)\n\n### Show _ONLY_ The Bare Minimum\nAs you work on a page or a view, make sure you know what the bare minimum amount of content can go on the page to make sure it is still usable. Being more of a usability problem, it might be up to your designer or user experience expert to solve this problem, but it'll be up to you to develop and you'll be responsible for \"fixing\" the page when it's too cluttered if you don't have this information up front.\n\nIn my case, I wanted to display a list of blog posts. There is a lot of metadata for each post, but I needed to boil it down the bare minimum. From there, I was able to start placing the elements on screen and start sizing them. As I increased the screen size to test the responsiveness of my design, I added or changed the styling of certain elements to take advantage of the larger screen.\n\n> But how do you pick which screen sizes should be considered \"larger\"? There are so many devices with so many different screen sizes! _-- You_\n\nGreat question (and segway) into my next tip!\n\n### Steal Media Query Breakpoints\nAlthough every project is a unique flower, at the end of the day the majority of our users are not. At least in the device sense. For this reason, I decided to start with the breakpoints [defined in Bootstrap 3](http://getbootstrap.com/css/#responsive-utilities) and ended up using only a few them for what I needed.\n\nI used a few of the queries defined [in this forum post](https://teamtreehouse.com/community/are-there-standard-media-query-break-points) which helped me get my simple, yet responsive, design working on extra small screens and beyond.\n\n```css\n>/*==========  Mobile First Method  ==========*/\n\n/* Custom, iPhone Retina */ \n@media only screen and (min-width : 320px) {\n\n}\n\n/* Extra Small Devices, Phones */ \n@media only screen and (min-width : 480px) {\n\n}\n\n/* Small Devices, Tablets */\n@media only screen and (min-width : 768px) {\n\n}\n\n/* Medium Devices, Desktops */\n@media only screen and (min-width : 992px) {\n\n}\n\n/* Large Devices, Wide Screens */\n@media only screen and (min-width : 1200px) {\n\n}\n```","categories":[],"tags":[{"name":"css","slug":"css","permalink":"https://westerndevs.com/tags/css/"},{"name":"mobile","slug":"mobile","permalink":"https://westerndevs.com/tags/mobile/"},{"name":"responsive","slug":"responsive","permalink":"https://westerndevs.com/tags/responsive/"}]},{"title":"The Code Review Blues","authorId":"lori_lalonde","slug":"the-code-review-blues","date":"2016-08-03 20:13:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/the-code-review-blues/","link":"","permalink":"https://westerndevs.com/_/the-code-review-blues/","excerpt":"Code reviews. Some people really enjoy participating in them. Others find it worse than a visit to the dentist. I've been in both camps.","raw":"---\nlayout: post\ntitle: The Code Review Blues\ndate: 2016-08-03 16:13:00\ntags:\n  - code reviews\nexcerpt: Code reviews. Some people really enjoy participating in them. Others find it worse than a visit to the dentist. I've been in both camps.\nauthorId: lori_lalonde\noriginalurl: http://solola.ca/the-code-review-blues/\n---\n\nCode reviews. When incorporated as part of the development process, they may be carried out by a senior level professional (i.e. an architect, team lead, or dev manager), or by multiple developers on your team.  Sometimes you may go through rounds of peer reviews before the code is reviewed by a senior member for final approval.\n\nOften reviews have been conducted with the developer and reviewer sitting side by side, having a discussion about the areas that could use improvement, identifying any use cases or logic that was missing.\n\nNowadays, just as with everything else, this communication has shifted to an online system, such as GitHub, which enables reviewers to highlight code and enter comments against it. Developers can reply to those comments if further explanations are needed to justify a coding approach. Additionally, they can make the requested changes and check it back in for another round of reviews.\n\nSome people really enjoy participating in code reviews. Others find it worse than a visit to the dentist. I've been in both camps.  \n\n## When it's done wellâ€¦\n\nI have participated in many code reviews during my career, both as code author and reviewer. In the majority of those situations, it was a positive experience. I was fortunate enough to interact with reviewers that were supportive and encouraging. Often they provided constructive feedback on how I could improve the code. When that happened, I walked away from the process feeling more confident in myself and my skills because I learned something new that I would apply to my coding practice going forward.\n\nIn situations where I was reviewing someone else's code, the developer was receptive to the feedback, and appreciated the encouraging remarks I would make about his/her coding efforts as well. These positive experiences compounded over time which brought the team together, and strengthened the working relationships among team members. It resulted in a happy, cohesive, and productive team.\n\n## When things go Southâ€¦\n\nThere have been the rare occasions where the process was mired in friction, creating a rift on the team. In some cases, code reviews were used as a tool for the reviewer to chastise developers, exert authority, or cater to an obsessive compulsive nature over code styles. In other scenarios, the code author was simply unwilling to participate in the code review process in a constructive manner.\n\nLet me explain.\n\n### Code Shaming\n\nIn one case, I worked on a team with an architect who used the code review process to scold developers for coding practices that did not align with his views. He decided to impose his will on the team if anyone disagreed or countered his code review suggestions, regardless of how sound the counter argument was. The more comments he added during the review process, the more his comments became aggressive in nature. Oftentimes, the comments were just repetitive pointing out the same \"misdeeds\" over and over again.\n\nIn this situation, there is no sense of collaboration, togetherness, team work, or respect for any other team member's views. Team members that are subject to this form of code shaming end up feeling deflated and their self-confidence shaken. This is a situation that eventually drives down the team morale, and overall productivity suffers. Eventually, the team may experience a high turnover rate.\n\n### Extreme Code Formatting\n\nIn another scenario, a senior team member's obsessive compulsive nature resulted in busy work for his team members. His main pet peeves were the use of tabs instead of spaces, opening brackets being on the same line as the method or property signature, and other non-trivial items that did not contribute to code quality. I understand the need for uniformity, but these are areas that waste the developer's time. The good news here is that this problem is easily solved through the use of code formatting tools that will get the job done automatically when saving changes to your solution files.\n\nIf you are working with team members who strive for code formatting perfection, then I recommend that each member of the team make use of one of the many available IDE extensions available, such as [Code Maid](http://www.codemaid.net/) or [Resharper](http://www.jetbrains.com/resharper/features/code_formatting.html).\n\n### Unreceptive to Feedback\n\nAs a code reviewer, I have also encountered negative experiences from a team member who was not open to receiving feedback. The code review process seemed to be a source of stress and annoyance for him. In one instance, I highlighted a potential audit issue as a result of his commit, and asked him to correct it.\n\nWhat was the issue?\n\nRather than making a simple code change to an existing class, he decided it was more efficient to copy the code into a new class, make the necessary changes, and delete the original class. This resulted in the deletion of the source code history on that file. When I advised him that deleting source code history was a potential audit issue, he did not receive that feedback with an open mind.\n\nInstead of participating in the code review process in a constructive manner, he shrugged off my feedback with responses like \"Why does it matter?\", \"Nobody cares about keeping the history on that one class anyway\", \"You just want to be right\", and so on and so forth.\n\nConsidering the work being done was for a large financial institution, where audit trails are a high priority, this was a red flag and I could not let this one slide. Eventually, he conceded to make the change, but this event was the catalyst for another situation I was not expecting.\n\n### Merge Conflict Avoidance\n\nOn that same project, we were required to perform peer code reviews by commenting on each other's pull requests. Once the code passed the review process, the reviewer would be responsible for merging the pull request to master.\n\nThis was an opportunity that one team member leveraged to his advantage in order to place the burden of dealing with any merge conflicts on the remaining team members. He refused to merge other pull requests unless his pull request was merged first. There was an instance where he insisted on his pull request being merged first when he did not even have a pull request submitted at the time. When I pointed that out, he tried to save face with a reply of: \"It's coming.\" A day and a half later, he finally had a pull request to review, while the other pull request sat in limbo.\n\nAs the project progressed, this type of behaviour drove the morale of the team down to an all-time low and productivity stagnated.\n\n## The Problems (and some suggested solutions)\n\nYes, that's problem<strong><em>s</em></strong>, with an \"s\". There isn't one factor that contributes to the Code Review Blues. It's a combination of many.\n\n### Different Expectations\n\nThe main source of friction during the code review process stems from team members' differing expectations on the code review's intended purpose and end result. At a minimum, code reviewers should verify the code is functional, scalable, extensible, maintainable, and secure.\n\nThe following [tweet](https://twitter.com/bliss_ai/status/753635662792982530) from [@bliss_ai](https://twitter.com/bliss_ai) is a good starting point on questions you should ask yourself, and discuss with your team, as both code author and reviewer:\n\n![](http://solola.ca/wp-content/uploads/2016/08/CodeReviewTweet_Bliss_Ai.png)\n\n\n\nWhen introducing a code review process to your organization, it is ideal to gather your development team into a room to discuss how this will impact the team. This is also an opportunity to educate the team on effective code review practices, and provide proper training on how the code review tool should be used.\n\nThis is also an excellent opportunity to answer the following questions:\n\n1. What is the main purpose and desired end result for the code review process?\n2. What is the minimum set of criteria that must be met in order to pass a code review?\n3. How will the code review process change each team member's day-to-day task assignments?\n4. What percentage of time will be allocated into the team's project plan for code reviews?\n5. Will their participation as code reviewers be held to the same standard as their other deliverables (i.e. will the value they bring to the code review process be rewarded and recognized)?\n\nThis training should also be included in the new hire on-boarding process.\n\n### Poor Communication and/or Interpersonal Skills\n\nSenior members that are in a position of authority may have been promoted through the ranks due to strong technical skills, but they might be a little rough around the edges when it comes to communication and/or interpersonal skills.  If code reviews with a specific member become a source of contention for the entire team, it might be time to recommend soft skills training for that team member.\n\nIf you're not convinced that soft skills are worth investing in, take the time to read this short article, \"[Why Soft Skills Matter â€“ Making Your Hard Skills Shine](https://www.mindtools.com/pages/article/newCDV_34.htm)\" by MindTools.\n\n### Misinterpretation\n\nIt's never easy to critique someone else's work nor to be on the receiving end of the type of scrutiny code reviews require.\n\nIf the feedback you provided is valid and communicated with good intentions, but not well received by the code author, this may be the cause of a deeper, underlying issue. A team member may be resistant to feedback on their code changes because they perceive it to be a personal attack.\n\nIf you find yourself getting into a heated exchange with someone who takes offense to the feedback provided, take a step back and analyze the form of communication being used. Think about the following points before you proceed with the review:\n\n1. Are you relying on an online code review tool as the sole mechanism to provide feedback? If so, is it possible that the tone of the feedback is being misconstrued by the code author?\n2. Do you start or end the review process with face-to-face communication?\n3. Do you also highlight areas that were done really well or is the feedback only focused on what needs to be changed?\n\nIn this situation, you may need to experiment with different communication and feedback styles to find an approach to which your colleague is open and receptive. If all else fails, seek advice from a senior level manager.\n\n## Final Thoughts\n\nCode reviews are intended to start a conversation about code quality, where all parties emerge from the other side having learned something â€“ whether it is about improving a section of code, adopting better coding practices, or reaching an understanding on the current approach taken.\n\nEveryone should feel confident once the code review process has come to a close that the code submitted performs what it was intended while meeting a defined level of quality.\n\nThe code author and reviewer(s) should feel good about the work they are producing, should feel encouraged by the support they receive from each member on the team, and should come out of it with their self-respect and dignity intact.\n\n\n\n_*Special thanks to [Donald Belcham](https://twitter.com/dbelcham) and [Shane Courtrille](https://twitter.com/shanecourtrille) for reviewing and providing valuable feedback which contributed to the overall quality of this post._\n","categories":[],"tags":[{"name":"code reviews","slug":"code-reviews","permalink":"https://westerndevs.com/tags/code-reviews/"}]},{"title":"Exploratory Data Analysis with Matthew Renze","slug":"Exploratory-Data-Analysis-with-Matthew-Renze","date":"2016-07-27 14:58:44+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Exploratory-Data-Analysis-with-Matthew-Renze/","link":"","permalink":"https://westerndevs.com/podcasts/Exploratory-Data-Analysis-with-Matthew-Renze/","excerpt":"Kyle hijacks Matthew Renze at Prairie Dev Con 2016 to talk about exploratory data analysis and the increasingly popular programming language, R.","raw":"---\nlayout: podcast\ntitle: Exploratory Data Analysis with Matthew Renze\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-data-analysis-matthew-renze.mp3\n  length: '21:57'\n  filesize: 21069439\n  libsynId: 4546929\n  anchorFmId: Exploratory-Data-Analysis-with-Matthew-Renze-evqdhi\nparticipants:\n  - kyle_baley\n  - matthew_renze\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Matthew''s website|http://www.matthewrenze.com/'\n  - 'R language|https://www.r-project.org/'\n  - 'Tableau visualization|https://www.tableau.com/'\n  - 'Edward Tufte|https://www.edwardtufte.com/tufte/'\n  - 'Stephen Few|https://www.perceptualedge.com/blog/'\n  - 'Nathan Yau|http://flowingdata.com/'\n  - 'D3 Library|https://d3js.org/'\n  - 'Revolution Analytics|http://www.revolutionanalytics.com/'\n  - 'Beginning Data Visualization with R (Pluralsight)|https://www.pluralsight.com/courses/r-data-visualization-beginner'\n  - 'Exploratory Data Analysis with R (Pluralsight)|https://www.pluralsight.com/courses/r-data-analysis'\n  - 'How to get started programming in R|https://www.pluralsight.com/blog/data-professional/how-to-get-started-programming-in-r'\ndate: 2016-07-27 10:58:44\nrecorded: 2016-04-12\nexcerpt: Kyle hijacks Matthew Renze at Prairie Dev Con 2016 to talk about exploratory data analysis and the increasingly popular programming language, R.\n---\n\n### Synopsis\n\n* What is exploratory data analysis?\n* Compared to machine learning and data mining\n* Numerical analysis and data visualization\n* Choosing a visualization\n* The psychology of choosing a visualization\n* Lying with statistics\n* Pitching exploratory analysis to developers\n* Using R for analysis\n* R in the Microsoft ecosystem\n* Evolution of R in the community\n* Impact of data analysis\n* Making sense of the flood of data\n* Academic data analysis vs commercial data analysis\n* Importance of having a statistical background\n* Machine learning and artificial intelligence\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Keeping Your Edge on an Extended Break","authorId":"david_wesst","slug":"keeping-your-edge","date":"2016-07-18 14:30:33+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/keeping-your-edge/","link":"","permalink":"https://westerndevs.com/_/keeping-your-edge/","excerpt":"A few quick tips on how I kept up-to-date in the professional world while taking a four month hiatus for parental leave _without_ giving up time with my daughter.","raw":"---\nlayout: post\ntitle: Keeping Your Edge on an Extended Break\ndate: 2016-07-18 10:30:33\ntags:\n  - parenting\nexcerpt: A few quick tips on how I kept up-to-date in the professional world while taking a four month hiatus for parental leave _without_ giving up time with my daughter.\nauthorId: david_wesst\noriginalurl: https://blog.davidwesst.com/2016/07/Keeping-Your-Edge-on-an-Extended-Break/\n---\n\nI recently came off of four months of parental leave where I had a fantastic time bonding with my new daughter. Although four months doesn't seem like much of a hiatus, I learned very quickly that being a full time parent doesn't leave much time to keeping my professional edge.\n\nThey might seem obvious, but it took me a while to find a balance to make sure spending time with my daughter _didn't_ play second fiddle to my profession passion and/or obsession. I also didn't want to give up the little bit of sleep I've managed to get, which bring me to the three things that really helped keep up up-to-date.\n\n![](https://blog.davidwesst.com/2016/07/Keeping-Your-Edge-on-an-Extended-Break/office-daycare.jpg)\n\n### Late Night Podcasts\nThere were plenty of nights where I was awake trying to calm down an unhappy baby, or just sitting there awake unable to sleep for whatever reason. Instead of just sitting there being alone with my frustration, I would pop in on earbud and listing to one of the many development podcasts (like the [WesternDevs podcast](http://www.westerndevs.com/podcasts/) *hint hint*) or the [Bithell Games podcast](https://www.youtube.com/channel/UCkDkgK59ygwHHMDSiW0M3-g) where they talk the business of video game development.\n\nGiven the latter podcast isn't my specialty, it is defintely something I am very interested and am able to draw parallels between regular software development and video game software development.\n\n...which leads me to my next point.\n\n### Revisiting Hobby Projects \nWe all have a million ideas, some of which have become abandoned repository in the bowels of GitHub or somewhere on our hard drives. Now that time is even more scarce, it makes sense to revisit some of these and see if there are any that can be broken down into very small parts that you can tinker with once and a while.\n\nFor me, I redid my [website](https://github.com/davidwesst/dw-www), started another new video game project, and upgraded the theme on [my blog](https://github.com/davidwesst/dw-blog). It wasn't just coding though, it involved some actual planning on my part where I looked at all the projects and found ones that could be broken up into small, bite-sized tasks which lead to a feeling of accomplishment when I acutally found time to work on something.\n\nIt helped keep my coding, but more importantly, it let me practice my software planning skills. Breaking projects down into small and consumable tasks, and deciding what is \"good enough\" for any project to be released is an important skill to have in the software development world. It's especially difficult when you are your own client. \n\n### User Groups and Online Meetups\nIf you're lucky enough to have a support system in place and have a babysitter willing to come by to give you a break for a few hours, you might want to check out whatever local user groups are in your area. If not, there are always events happening online that you can attend virtually complete with a chat window.\n\nThis was something I didn't get to do a lot of, but when I did it was really nice. Meeting up with a community of like minded professionals and take a break from diapers and the like was really enjoyable and really helped keep me in the loop.\n\nI managed to get to my local .NET user group a few times, but I made a point to attend online conferences like [\\\\build](https://build.microsoft.com/) and[Microsoft Edge Web Summit](https://channel9.msdn.com/events/WebPlatformSummit/edgesummit2016) to keep myself in the loop and up-to-date. Plus, being an online streamed conference, the content ends up available on demand very shortly after it's broadcasted just in case you have to stop watching early because of a grumpy baby.\n\n## In Conclusion\nThis post might be filled with obvious tips and tricks, but I wanted to take a moment to share. Balancing \"real life\" with your professional life can be challenging regardless of what is happening, and sometimes we need to take a break from our professional passions. Hopefully this post shows you that taking an extended break doesn't mean you have to give up your professional edge. \n\n##### Photo Credits\n* [Anita Peppers | office-daycare.jpg](https://morguefile.com/creative/anitapeppers)\n","categories":[],"tags":[{"name":"parenting","slug":"parenting","permalink":"https://westerndevs.com/tags/parenting/"}]},{"title":"An Intro to NGINX for Kestrel","authorId":"simon_timms","slug":"nginx","date":"2016-07-17 21:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Deployment/nginx/","link":"","permalink":"https://westerndevs.com/Deployment/nginx/","excerpt":"Kestrel is a server capable of serving up ASP.NET Core applications on any platform but in production you need to run it behind another server.","raw":"---\nlayout: post\ntitle: An Intro to NGINX for Kestrel\ntags:\n  - nginx\n  - kestrel\n  - deployment\ncategories:\n  - Deployment   \nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2016/07/17/an-intro-to-nginx-for-kestrel/\ndate: 2016-07-17 17:36:36\nexcerpt: Kestrel is a server capable of serving up ASP.NET Core applications on any platform but in production you need to run it behind another server.\n---\n\nKestrel is a light weight web server for hosting ASP.NET Core applications on really any platform. It is based on a library called libuv which is an eventing library and it, actually, the same one used by nodejs. This means that it is an event driven asynchronous I/O based server. \n\nWhen I say that Kestrel is light weight I mean that it is lacking a lot of the things that an ASP.NET web developer might have come to expect from a web server like IIS. For instance you cannot do SSL termination with Kestrel or URL rewrites or GZip compression. Some of this can be done by ASP.NET proper but that tends to be less efficient than one might like. Ideally the server would just be responsbile for running ASP.NET code.  The suggested approach not just for Kestrel but for other light weight front end web servers like nodejs is to put a web server in front of it to handle infrastructure concerns. One of the better known ones is Nginx (pronounced engine-X like racer X).\n\n![https://www.nginx.com/wp-content/themes/nginx-theme/assets/img//logo.png](https://www.nginx.com/wp-content/themes/nginx-theme/assets/img//logo.png)\n\nNginix is a basket full of interesting capabilities. You can use it as a reverse proxy; in this configuration it takes load off your actual web server by preserving a cache of data which it serves before calling back to your web server. As a proxy it can also sit in front of multiple end points on your server and make them appear to be a single end point. This is useful for hiding a number of microservices behind a single end point. It can do SSL termination which makes it easy to add SSL to your site without having to modify a single line of code. It can also do gzip compression and serve static files. The commercial version of Nginx adds load balancing to the equation and a host of other things. \n\nLet's set up Nginx in front of Kestrel to provide gzip support for our web site. First we'll just create a new ASP.NET Core web application. \n\n```\nyo aspnet\n```\nSelect `Web Application` and then bring it up with \n\n```\ndotnet restore\ndotnet run\n```\n\nThis is running on port 5000 on my machine and hitting it with a web browser reports the content-encoding as regular, no gzip.\n\n![No gzip content encoding](http://i.imgur.com/3NvYQ0w.jpg)\n\nThat's no good, we want to make sure our applications are served with gzip. That will make the payload smaller and the application faster to load. \n\nLet's set up Nginx. I installed my copy through brew (I'm running on OSX) but you can just as easily download a copy from the [Nginx site](https://www.nginx.org/). There is even [support for Windows](http://nginx.org/en/download.html) although the performance there is not as good as it in on *NIX operating systems. I then set up a `nginx.conf` configuraiton file. The default config file is huge but I've trimmed it down here and annotated it.\n\n```\n#number of worker processes to spawn\nworker_processes  1;\n\n#maximum number of connections\nevents {\n    worker_connections  1024;\n}\n\n#serving http information\nhttp {\n    #set up mime types\n    include       mime.types;\n    default_type  application/octet-stream;\n\n    #set up logging\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    access_log  /Users/stimms/Projects/nginxdemo/logs/access.log  main;\n\n    #uses sendfile(2) to send files directly to a socket without buffering\n    sendfile        on;\n\n    #the length of time a connection will stay alive on the server\n    keepalive_timeout  65;\n\n    #compress the response stream with gzip\n    gzip  on;\n\n    #configure where to listen\n    server {\n        #listen over http on port 8080 on localhost\n        listen       8080;\n        server_name  localhost;\n\n        #serve static files from /Users/stimms/Projects/nginxdemo for requests for\n        #resources under /static\n        location /static {\n            root /Users/stimms/Projects/nginxdemo;\n        }\n\n        #by default pass all requests on / over to localhost on port 5000\n        #this is our Kestrel server\n        location / {\n            proxy_pass http://127.0.0.1:5000/;\n        }\n\n    }\n}\n```\nWith this file in place we can load up the server on port 8080 and test it out. \n\n`nginx -c /Users/stimms/Projects/nginxdemo/nginx.conf`\n\nI found I had to use full paths to the config file or nginx would look in its configuration directory.\n\nDon't forget to also run Kestrel. Now when pointing a web browser at port 8080 on the local host we see\n\n![Content-encoding gzip enabled](http://i.imgur.com/diRLFrA.jpg)\n\nContent-encoding now lists gzip compression. Even on this small page we can see a reduction from 8.5K to 2.6K scaled over a huge web site this would be a massive savings. \n\nLet's play with taking some more load off the Kestrel server by caching results. In the nginx configuration file we can add a new cache under the `http` configuration\n\n```\n#set up a proxy cache location\nproxy_cache_path  /tmp/cache levels=1:2 keys_zone=aspnetcache:8m max_size=1000m inactive=600m;  \nproxy_temp_path /tmp/cache/temp; \n```\n\nThis sets up a cache in /tmp/cache of size 8MB up to 1000MB which will become inactive after 600 minutes (10 hours). Then under the listen directive we'll add some rules about what to cache\n\n```\n#use the proxy to save files\nproxy_cache aspnetcache;\nproxy_cache_valid  200 302  60m;\nproxy_cache_valid  404      1m;\n```\n\nHere we cache 200 and 302 respones for 60 minutes and 404 responses for 1 minute. If we add these rules and restart the nginx server\n\n```\nnginx -c /Users/stimms/Projects/nginxdemo/nginx.conf -s reload\n```\n\nNow when we visit the site multiple times the output of the Kestrel web server shows it isn't being hit. Awesome! You might not want to cache everything on your site and you can add rules to the listen directive to just cache image files, for instance. \n\n```\n#just cache image files, if not in cache ask Kestrel\nlocation /images/ {\n    #use the proxy to save files\n    proxy_cache aspnetcache;\n    proxy_cache_valid  200 302  60m;\n    proxy_cache_valid  404      1m;\n    proxy_pass http://127.0.0.1:5000;\n}\n\n#by default pass all requests on / over to localhost on port 5000\n#this is our Kestrel server\nlocation / {\n    proxy_pass http://127.0.0.1:5000/;\n}\n```\n\nWhile Kestrel is fast it is still slower than Nginx at serving static files so it is worthwhile offloading traffix to Nginx when possible. \n\nNginx is a great deal of fun and worth playing with. We'll probably revisit it in future and talk about how to use it in conjunction with microservices. You can find the code for this post at [https://github.com/AspNetMonsters/Nginx](https://github.com/AspNetMonsters/Nginx).\n","categories":[{"name":"Deployment","slug":"Deployment","permalink":"https://westerndevs.com/categories/Deployment/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://westerndevs.com/tags/nginx/"},{"name":"kestrel","slug":"kestrel","permalink":"https://westerndevs.com/tags/kestrel/"},{"name":"deployment","slug":"deployment","permalink":"https://westerndevs.com/tags/deployment/"}]},{"title":"Loading View Components from a Class Library in ASP.NET Core MVC","authorId":"dave_paquette","slug":"loading-view-components-from-a-class-library-in-asp-net-core","date":"2016-07-16 12:36:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/View-Components/loading-view-components-from-a-class-library-in-asp-net-core/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/View-Components/loading-view-components-from-a-class-library-in-asp-net-core/","excerpt":"In today's post we take a look at how view components can be implemented in a separate class library and shared across multiple web applications.","raw":"---\nlayout: post\ntitle: Loading View Components from a Class Library in ASP.NET Core MVC\ntags:\n  - ASP.NET Core\n  - MVC\n  - View Components\ncategories:\n  - ASP.NET Core\n  - View Components\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2016/07/16/loading-view-components-from-a-class-library-in-asp-net-core.aspx'\ndate: 2016-07-16 08:36:36\nexcerpt: In today's post we take a look at how view components can be implemented in a separate class library and shared across multiple web applications.\n---\nIn a previous post we explored the new [View Component](http://www.davepaquette.com/archive/2016/01/02/goodbye-child-actions-hello-view-components.aspx) feature of ASP.NET Core MVC. In today's post we take a look at how view components can be implemented in a separate class library and shared across multiple web applications.\n\n## Creating a class library\nFirst, add a  a new .NET Core class library to your solution.\n\n![Add class library](http://www.davepaquette.com/images/external_view_components/create_new_class_library.png)\n\nThis is the class library where we will add our view components but before we can do that we have to add a reference to the MVC and Razor bits.\n\n{% codeblock lang:javascript %}\n    \"dependencies\": {\n        \"NETStandard.Library\": \"1.6.0\",\n        \"Microsoft.AspNetCore.Mvc\": \"1.0.0\",\n        \"Microsoft.AspNetCore.Razor.Tools\": {\n            \"version\": \"1.0.0-preview2-final\",\n            \"type\": \"build\"\n        }\n    },\n    \"tools\": {\n        \"Microsoft.AspNetCore.Razor.Tools\": \"1.0.0-preview2-final\"\n    }\n{% endcodeblock %}\n\nNow we can add a view component class to the project. I created a simple example view component called `SimpleViewComponent`.\n\n{% codeblock lang:javascript %}\n[ViewComponent(Name = \"ViewComponentLibrary.Simple\")]\npublic class SimpleViewComponent : ViewComponent\n{\n    public IViewComponentResult Invoke(int number)\n    {\n        return View(number + 1);\n    }\n}\n{% endcodeblock %}\n\nBy convention, MVC would have assigned the name `Simple` to this view component. This view component is implemented in a class library with the intention of using it across multiple web apps which opens up the possibility of naming conflicts with other view components. To avoid naming conflicts, I overrode the name using the `[ViewComponent]` attribute and prefixed the name with the name of my class library.\n\nNext, I added a `Default.cshtml` view to the `ViewComponentLibrary` in the `Views\\Shared\\Components\\Simple` folder. \n\n{% codeblock lang:csharp %}\n@model Int32\n\n<h1>\n    Hello from an external View Component!\n</h1>\n<h3>Your number is @Model</h3>\n{% endcodeblock %} \n\nFor this view to be recognized by the web application, we need to include the `cshtml` files as embedded resources in the class library. Currently, this is done by adding the following setting to the `project.json` file.\n\n{% codeblock lang:javascript %}\n\"buildOptions\": {\n    \"embed\": \"Views/**/*.cshtml\"\n}\n{% endcodeblock %} \n\n## Referencing external view components\nThe first step in using the external view components in our web application project is to add a reference to the class library. Once the reference is added, we need tell the Razor view engine that views are stored as resources in the external view library. We can do this by adding some additional configuration code to the `ConfigureServices` method in `Startup.cs`. The additional code creates a new `EmbeddedFileProvider` for the class library then adds that file provider to the `RazorViewEngineOptions`.\n\n{% codeblock lang:csharp %}\npublic void ConfigureServices(IServiceCollection services)\n{\n    // Add framework services.\n    services.AddApplicationInsightsTelemetry(Configuration);\n\n    services.AddMvc();\n\n    //Get a reference to the assembly that contains the view components\n    var assembly = typeof(ViewComponentLibrary.ViewComponents.SimpleViewComponent).GetTypeInfo().Assembly;\n\n    //Create an EmbeddedFileProvider for that assembly\n    var embeddedFileProvider = new EmbeddedFileProvider(\n        assembly,\n        \"ViewComponentLibrary\"\n    );\n\n    //Add the file provider to the Razor view engine\n    services.Configure<RazorViewEngineOptions>(options =>\n    {                \n        options.FileProviders.Add(embeddedFileProvider);\n    });\n}\n{% endcodeblock %}\n\nNow everything is wired up and we can invoke the view component just like we would for any other view component in our ASP.NET Core MVC application.\n\n{% codeblock lang:csharp %}\n<div class=\"row\">\n    @await Component.InvokeAsync(\"ViewComponentLibrary.Simple\", new { number = 5 })\n</div>\n{% endcodeblock %}\n\n## Wrapping it up\nStoring view components in a separate assembly allows them to be shared across multiple projects. It also opens up the possibility of creating a simple plugin architecture for your application. We will explore the plugin idea in more detail in a future post.\n\nYou can take a look at the full source code on [GitHub](https://github.com/AspNetMonsters/ExternalViewComponents).","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"},{"name":"View Components","slug":"ASP-NET-Core/View-Components","permalink":"https://westerndevs.com/categories/ASP-NET-Core/View-Components/"}],"tags":[{"name":"View Components","slug":"View-Components","permalink":"https://westerndevs.com/tags/View-Components/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"}]},{"title":"End to end testing for your saga","authorId":"simon_timms","slug":"NSBSagaTimeouts","date":"2016-07-15 18:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Tools/NSBSagaTimeouts/","link":"","permalink":"https://westerndevs.com/Tools/NSBSagaTimeouts/","excerpt":"Looking to do end to end testing of your saga? I strugged.","raw":"---\ntitle: End to end testing for your saga\nlayout: post\ntags:\n  - NserviceBus\ncategories:\n  - Tools\nauthorId: simon_timms\ndate: 2016-07-15 14:56:56 \nexcerpt: \"Looking to do end to end testing of your saga? I strugged.\"\n\n---\n\n**Disclaimer**\n\nIn this article I'm going to use the term \"saga\" because that's what NServiceBus calls it. Don't take this as acceptance of this definition of \"saga\" in general. What NServiceBus call \"saga\" would be better called a \"workflow\" or \"process manager\". Kellabyte has a [great article](http://kellabyte.com/tag/saga/) on it.\n\nI've recently been playing with sagas in NServiceBus. A saga is a tool for coordinating a number of messages across time. Generally there will be one or messages which start a saga and then the saga will listen for new messages to wake up and perform action. A saga is stateful which means that you can put all sorts of useful information in the saga data to allow making decisions later on. A very useful feature of sagas is that you can set a timeout to be fired at some point in the future. So for example you could start a shopping cart saga and schedule a timeout 24 hours in the future. When that timeout is reached the saga is woken up again and you could send a reminder e-mail to the owner of cart to check out. There are countless business processes which have some requirement to do something in the future even if that something is checking to make sure that some action has occured and compensating if not.\n\nAs you can imagine when testing a saga in an end to end test you don't really want to wait 24 hours for something to timeout. The usual advice around this is to make the timeout configurable in some fashion and just set it to a low value. This is difficult to do in an end to end test and still have confidence that you're not hiding some broken functionality. This is why we don't like to have special constructors just for unit testing. In my case I'm using SQL persistance to save timeout information to a database. This means that the database can be hacked to allow the manual execution of timeouts. Let's do it.\n\nThe `TimeoutEntity` table is the one we want to alter. It contains a column called `Time` which is the time at which the timeout will occur. In my case I knew an Id from the saga so I joined against the specific saga data table to find the approprate timeout to update. I only schedule one timeout at a time with this saga so I don't have to worry about finding a timeout in a multitude of them for that saga.     \n\n{% codeblock lang:sql %}\nupdate te\nset Time=GETUTCDATE()\nfrom nservicebus.dbo.TimeoutEntity te inner join \n        nservicebus.dbo.SockFinderSagaData sfsd on te.SagaId = sfsd.Id\nwhere SockId=@sockId\n{% endcodeblock %}\n\nYou'll note here that I set the time to the current time. It is important that you don't set the date to some time in the past. I initially did that and found timeouts weren't firing. The reason is that the query NServiceBus uses to find the timeout looks for timeout entries since the last polling event to now. This query would miss things scheduled way in the past. Here are the queries the NHibernate persistence uses\n\n```\n2016-07-15 16:17:29,197 DEBUG [15][NT AUTHORITY\\SYSTEM] SELECT this_.Id as y0_, this_.Time as y1_ FROM TimeoutEntity this_ WHERE this_.Endpoint = @p0 and (this_.Time >= @p1 and this_.Time <= @p2) ORDER BY this_.Time asc;@p0 = 'AdverseActions' [Type: String (4000)], @p1 = 7/15/2016 3:47:42 PM [Type: DateTime (0)], @p2 = 7/15/2016 4:17:29 PM [Type: DateTime (0)]\n2016-07-15 16:17:29,197 DEBUG [15][NT AUTHORITY\\SYSTEM] SELECT this_.Id as Id6_0_, this_.Destination as Destinat2_6_0_, this_.SagaId as SagaId6_0_, this_.State as State6_0_, this_.Time as Time6_0_, this_.Headers as Headers6_0_, this_.Endpoint as Endpoint6_0_ FROM TimeoutEntity this_ WHERE this_.Endpoint = @p0 and this_.Time > @p1 ORDER BY this_.Time asc OFFSET 0 ROWS FETCH FIRST @p2 ROWS ONLY;@p0 = 'AdverseActions' [Type: String (4000)], @p1 = 7/15/2016 4:17:29 PM [Type: DateTime (0)], @p2 = 1 [Type: Int32 (0)]\n ```\n\n Polling of the timeout table happens at least once per minute. So this still means that your tests need to wait 60 seconds from hacking the table to checking the result of the timeout. This is still a bit painful but these are integraiton tests and likely you're running a bunch in parallel so the hit isn't horrific. With this code in place I was able to reliably simulate the state of the saga in the future. I'm like a time traveler and you can be too. ","categories":[{"name":"Tools","slug":"Tools","permalink":"https://westerndevs.com/categories/Tools/"}],"tags":[{"name":"NserviceBus","slug":"NserviceBus","permalink":"https://westerndevs.com/tags/NserviceBus/"}]},{"title":"Functional Programming and F# with Rachel Reese","slug":"FSharp-with-Rachel-Reese","date":"2016-07-09 22:01:53+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/FSharp-with-Rachel-Reese/","link":"","permalink":"https://westerndevs.com/podcasts/FSharp-with-Rachel-Reese/","excerpt":"The Western Devs kidnap Rachel Reese at Prairie Dev Con and force her to speak on functional programming and F# in exchange for empanadas","raw":"---\nlayout: podcast\ntitle: \"Functional Programming and F# with Rachel Reese\"\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-functional-programming-rachel-reese.mp3\n  length: '38:15'\n  filesize: 36722849\n  libsynId: 4488105\n  anchorFmId: Functional-Programming-and-F-with-Rachel-Reese-evqdi5\nparticipants:\n  - simon_timms\n  - amir_barylko\n  - rachel_reese\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Rachel Reese''s blog|http://rachelree.se/'\n  - 'Rachel on Twitter|https://twitter.com/rachelreese'\n  - 'F#|http://fsharp.org/'\n  - 'Clojure|https://clojure.org/'\n  - 'Haskell|https://www.haskell.org/'\n  - 'John Harrop on Twitter|https://twitter.com/jonharrop'\n  - 'Stack Overflow Developer Survey 2016|http://stackoverflow.com/research/developer-survey-2016'\n  - 'WebSharper (web framework in F#)|http://websharper.com/'\n  - 'Paket (dependency management)|https://fsprojects.github.io/Paket/'\n  - 'Fake (build automation)|http://fsharp.github.io/FAKE/'\n  - 'Suave (web development library)|https://suave.io/'\n  - 'FsCheck (property-based testing)|https://github.com/fscheck/FsCheck'\n  - 'Canopy (UI testing framework)|https://lefthandedgoat.github.io/canopy/'\n  - 'F# for fun and profit|https://fsharpforfunandprofit.com'\n  - 'F# mentoring|http://fsharp.org/mentorship/'\ndate: 2016-07-09 18:01:53\nrecorded: 2016-04-12\nexcerpt: The Western Devs kidnap Rachel Reese at Prairie Dev Con and force her to speak on functional programming and F# in exchange for empanadas\n---\n\n### Synopsis\n\nSpecial guest: Rachel Reese\n\n* Why functional programming? Why now?\n* Myths of functional programming\n* This ain't your mother's functional programming\n* Can I find a job with F#?\n* Can companies find F# developers?\n* WTF is a monad?\n* Useful F# libraries\n* Scripting with F#\n* Type providers\n* Options\n* Piping\n* Discriminated unions\n* Records vs. tuples vs. structs\n* Getting started with F#\n* F# in the community\n* F# in Xamarin, Mono, and .NET Core\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Mobile Apps and Xamarin","slug":"Mobile-apps","date":"2016-07-02 18:13:23+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Mobile-apps/","link":"","permalink":"https://westerndevs.com/podcasts/Mobile-apps/","excerpt":"The Western Devs discuss the ins and outs of mobile development and Xamarin","raw":"---\nlayout: podcast\ntitle: Mobile Apps and Xamarin\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-mobile-apps-xamarin.mp3\n  length: '41:48'\n  filesize: 40120834\n  libsynId: 4488015\n  anchorFmId: Mobile-Apps-and-Xamarin-evqdht\nparticipants:\n  - kyle_baley\n  - tom_opgenorth\n  - lori_lalonde\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Xamarin|https://www.xamarin.com/'\n  - 'dot42 (discontinued)|http://www.dot42.com/'\n  - 'phonegap|http://phonegap.com/'\n  - 'cordova|https://cordova.apache.org/'\n  - 'Xamarin UI.Test|https://developer.xamarin.com/guides/testcloud/uitest/'\n  - 'Android app testing with Intel|https://software.intel.com/en-us/android/app-testing'\n  - 'Testmunk - mobile app testing|https://testmunk.com/'\n  - 'Xamarin University|https://www.xamarin.com/university'\n  - 'mvvmcross (.NET MVVM framework)|https://github.com/MvvmCross/MvvmCross'\n  - 'MVVM Light Toolkit (.NET MVVM framework)|https://mvvmlight.codeplex.com/'\n  - 'James Montemagno on GitHub|https://github.com/jamesmontemagno'\n  - 'Xamarin Evolve|https://evolve.xamarin.com/'\n\ndate: 2016-07-02 14:13:23\nrecorded: 2016-03-18\nexcerpt: The Western Devs discuss the ins and outs of mobile development and Xamarin\n---\n\n### Synopsis\n\n* Xamarin: platform for building cross-platform applications that happens to be in .NET\n* Scenarios for Xamarin forms\n* What to know when building for OSX or IOS\n* Xamarin Studio vs Visual Studio\n* Alternatives to Xamarin for cross-platform development\n* You can't avoid learning about the platform API you're developing for\n* Design considerations for mobile apps\n* Designing for tablets vs. phones vs. watches vs. cars\n* Architectural concerns for mobile apps\n* Designing for slow/poor connectivity\n* Native vs hybrid apps\n* Distinguishing between hybrid apps and responsive web apps\n* Testing on mobile\n* Versioning mobile apps\n* Analytics in mobile apps\n* The dangers of drinking with Joseph Hill\n* Existing resources\n* Importance of testing on actual devices\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Extensibility In Message Based Systems With NServiceBus","authorId":"justin_self","slug":"extensibility-in-message-systems-with-nservicebus","date":"2016-06-29 07:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"nservicebus/extensibility-in-message-systems-with-nservicebus/","link":"","permalink":"https://westerndevs.com/nservicebus/extensibility-in-message-systems-with-nservicebus/","excerpt":"One of my favorite things about message based systems is the natural points of extensibility you can gain. Though, you don't get it for free if you aren't setting yourself up for it.","raw":"---\nlayout: post\ntitle:  Extensibility In Message Based Systems With NServiceBus\ndate: 2016-06-28T21:30:00-06:00\ncategories: nservicebus\ncomments: true\nauthorId: justin_self\noriginalurl: http://www.justinself.com/extensibility-in-message-based-systems-with-nservicebus/\n---\nOne of my favorite things about message based systems is the natural points of extensibility you can gain. Though, you don't get it for free if you aren't setting yourself up for it.\n\n<!--more-->\n\nLet's say you work for a company that sells dog shoes online. Thinking about it, that's a dramatically under served market.\n\nCurrently, your company's website allows for users to pay with their credit card and then, hopefully within a few days, receive their shoes. So let's take a look at some sample code for the handler that processes the message for payment.\n\n\n    public class ProcessPaymentHandler : IHandleMessage<ProcessPayment>\n    {\n        public void Handle(ProcessPayment message)\n        {\n            paymentProviderClient.Charge(message.paymentData);\n            //save and log response\n        }\n    }\n\n\nOk, obviously a simple example but you get the idea.\n\nEverything is working well and your client's canine pals are having their paws covered in stylish footware.\n\n## The New Requirement\n\nNow your product owner comes to you with a new requirement: send a confirmation email once the payment has been processed. This is easy enough. Let's just have the ProcessPaymentHandler send a command to send the confirmation email.\n\n    public class ProcessPaymentHandler : IHandleMessage<ProcessPayment>\n    {\n        public void Handle(ProcessPayment message)\n        {\n            paymentProviderClient.Charge(message.paymentData);\n            //save and log response\n\n            var emailAddress = customerRepo.GetEmailByOrderId(message.OrderId);\n            bus.Send(new SendConfirmationEmail(emailAddress));\n        }\n    }\n\nThis is a common, albiet naive, approach to the problem. It will work, assuming we have a handler for the SendConfirmationEmail message to be received by, but there are some problems with it.\n\n## Problems\n\n### Dependency\n\nThe first problem is now the code that handles processing a payment has a dependency on the process that sends emails. This single line of code may not seem like a dependency problem, but maintaining code and clean architecture is a lot about managing dependencies. Introducing the command here forces the host of this handler to know about the location of the email handler.\n\n There's also a deployment dependency. We now have to keep this handler in sync with the current version of the email handler.\n\n If the message interface changes because the handler was expanded or for any other of a multitude of reasons, we now have to come back and change code that handles processing a payment because some other code related to an email has changed (admittedly, though, we could effectively manage different versions in messages). Which leads us to the next problem...\n\n\n### Single Responsibility\n\nIt's a violation of the Single Responsibility Principle (SRP) which basically means that a piece of code should have only one reason to change. The class is currently supporting two requirements (processing a payment and sending an email) therefore has two reasons to change.\n\n### Open/Closed\n\nIn order for us to add a new feature, we had to modify existing code. Sometimes that's inevitable, but sometimes it's a sign of a series of preceeding bad design choices. When that happens, it is a violation of the open/closed principle. This principle states that you should be able to extend functionality without having to modify the internals of existing code.\n\nWhat we want is the ability to complete the feature for sending the email without having to modify the existing code.\n\n## The Solution - Events\n\nIf the ProcessPayment Handler published an event once it was done, then the Email Handler could subscribe to the event and take the appropriate action. This allows the payment processor to continue on its merry way being none the wiser that any process cares about it.\n\nHere's the code for that:\n\n    public class ProcessPaymentHandler : IHandleMessage<ProcessPayment>\n    {\n        public void Handle(ProcessPayment message)\n        {\n            paymentProviderClient.Charge(message.paymentData);\n            //save and log response\n\n            bus.Publish(new PaymentProcessed(message.OrderId));\n        }\n    }\n\nIn this code, we removed the line getting the email address and the code to send a new SendConfirmationEmail command.\n\nIt's pretty clear why the first line was removed. Since we aren't sending the command, we don't need to find the email address.\n\nThe second line, however, has some subtleties that could be missed.\n\n### Publish\nThe command was \"sent\" while the event is \"published\". Commands can be sent from N number of hosts but they are \"sent\" to a location because that location is always known. If a service has the contract and the correct queue, it can send any command it wants to. This means, however, that the service is now coupled to the processor of that command; being aware of its very existence is a coupling.\n\nHowever, events are published from one and only one logical host but can be received by N number of hosts. Other services can subscribe to those events without the publishing service being aware of it. This inverts the coupling the other direction. The service that needs to do the action is now coupled to the service that publishes the event. The coupling here makes sense. In our case, the email service wants to know when it needs to send the confirmation email. So, we can allow it to couple to the PaymentProcesssor service.\n\nIf you are still not quite groking events vs commands, try this:\n\n* Commands are like email. You know who is going to read it and you know where it is going. You send the email to one person with the expectation that they will read it and act on it.\n\n* Events are like this blog post. I have no idea if anyone will read it, who that person is or where they are located. I put it out in case anyone is interested in my data.\n\nI want to reiterate something really quickly: Anyone can send a command, but there must be only **ONE** service that handles it. Anyone can subscribe to an event but there must be only **ONE** service that publishes it.\n\n### Naming\nThe event is named as a past tense version of the command it was being published from. This is a convention I pretty much always use when naming commands and events. The commands are imperative. They represent actions your services can do and generally found in your ubiquitous language. The events are past tense. If your command name is \"DeleteAccount\" the event would be \"AccountDeleted\".\n\nHere's some sample code for handling the event:\n\n    public class PaymentProcessedHandler : IHandleMessage<PaymentProcessed>\n    {\n        public void Handle(ProcessPayment message)\n        {\n            bus.Send(new SendConfirmationEmail());\n        }\n    }\n\nYou may have noticed I'm sending a command from this event handler instead of just doing the work. There is a reason and I'll get to why I did that in another post.\n\n## Extensibility\n\nUp to now, all we've really done is changed a command to an event and moved some logic to the event handler, which then delegates to another command handler. So where's the power in that?\n\n*Cue the Product Owner*\n\nNow we have some new requirements. Once a payment has been successfully processed, if this is a first time customer then the company wants to send out a special dog treat to the customer to give to their canine companion as a thank you for their business. So let's add that capability.\n\nIf we didn't have events, we would need to modify the existing code for processing a payment and have another command sent (which introduce more of the three problems from earlier). However, since we have events, all we need to do is let this catalogue service subscribe to the PaymentProcessed event and do its thing. This means we don't have to modify ANY code in the Payment Processor.\n\n    public class PaymentProcessedHandler : IHandleMessage<PaymentProcessed>\n    {\n        public void Handle(ProcessPayment message)\n        {\n            bus.Send(new SendPhysicalCatalouge());\n        }\n    }\n\nWe just extended the application without modifying any existing code. That's the power of using events. If the company decides they also want to add the customer to a list for someone to call and thank them personally, we could subscribe to the event again. If the company decided they no longer wanted to send dog treats, then we simply unsubscribe to the event.\n\nAll of this is done without redeploying the current, existing code (PaymentProcessor).\n\n## Under The Hood\nWhen you add a subscription to a host, NServiceBus actually sends a message from the subscribing host to the publishing host. This informs the publishing host that the subscribing host wants a copy of the event when it is published. This gets stored in whatever persistence you previously chose: (Azure Storage, SQL, MSMQ, etc). This is true for all persistences except when you are using Azure ServiceBus or RabbitMQ because they both native pub/sub capabilities and hold onto the subscription data.\n\n## TL;DR\nIn order to allow for extensibility and prepare for future features, every command should have a corresponding event to go with it. With NServiceBus, if no one has subscribed to the event, then nothing will happen so there's no overhead of adding the events to the handler.\n","categories":[{"name":"nservicebus","slug":"nservicebus","permalink":"https://westerndevs.com/categories/nservicebus/"}],"tags":[]},{"title":"O365 Custom Domains - Configuring It For A Website","authorId":"darcy_lussier","slug":"O365DomainsForWebsites","date":"2016-06-20 04:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"O365/Azure/O365DomainsForWebsites/","link":"","permalink":"https://westerndevs.com/O365/Azure/O365DomainsForWebsites/","excerpt":"Here's the situation - you've registered a custom domain and you've gone ahead and set it up to work with Office 365 so you can have yourname@yourdomain.com for emails. Fantastic!","raw":"---\ntitle: O365 Custom Domains - Configuring It For A Website\nlayout: post\ncategories:\n  - O365\n  - Azure\nauthorId: darcy_lussier\ndate: 2016-06-20\n---\nHere's the situation - you've registered a custom domain and you've gone ahead and set it up to work with\nOffice 365 so you can have yourname@yourdomain.com for emails. Fantastic!\n\n<!-- more -->\n\nNow you've created a website and you'd like to use that same custom domain for it. That's whre I found myself tonight\nand as you'll read below the process isn't as straightforward as you'd think.\n\nI'm hosting my website in Azure, and the first step there is to set up a CNAME record on the domain. I logged in to\nmy registrar's website (where I purchsaed the domain) but it informed me that I'd have to point my domain DNS *back*\nto theirs in order to make the change...which seemed silly. Or, they said, you can change it wherever I had moved the\ndomain to. In this case, it was to Microsoft's domain servers which I did when I set up Office 365.\n\nSo the question is...how do I now manage my domain when its hosted by Microsoft? Where is the domain administration tool?\n\nThe answer is...Office 365. Here's how you can access your domain's settings to add new entries like CNAME.\n\nStep 1 - Log in to your Office 365 Administration portal, go down to Settings and select Domains.\n\n![http://i.imgur.com/8E4aIG4.png](http://i.imgur.com/8E4aIG4.png)\n\nStep 2 - Select the domain from the list that you want to add a setting to.\n\n![http://i.imgur.com/bTxyHar.png](http://i.imgur.com/bTxyHar.png)\n\nStep 3 - You'll be given three options. While DNS Management may seem like the likely stop, its not. Press the Check DNS\nbutton instead.\n\n![http://i.imgur.com/DsuXUrK.png](http://i.imgur.com/DsuXUrK.png)\n\nStep 4 - The system will verify that \"All DNS records are correct, no errors found\" and will enable a DNS Settings area underneath.\nClick it.\n\n![http://i.imgur.com/VSeo2Yv.png](http://i.imgur.com/VSeo2Yv.png)\n\nStep 5 - Under the DNS Settings options, expand the Custom Records and then click the New Custom Record button.\n\n![http://i.imgur.com/4yW4SZg.png](http://i.imgur.com/4yW4SZg.png)\n\nStep 6 - Now we can select the type of record we want to add. Fill it out and hit Save.\n\n![http://i.imgur.com/OOS19NM.png](http://i.imgur.com/OOS19NM.png)\n\nStep 7 - You'll now see a listing of the custom records entered for this domain.\n\n![http://i.imgur.com/ZJeVxaf.png](http://i.imgur.com/ZJeVxaf.png)\n\nIn my case, just needed to set the CNAME up with the settings Microsoft Azure instructed me to, and do some final configuration\nin the Azure portal to enable the custom domain for my website. You may have different instructions based on where you're hosting\nyour website at.\n\nSo just to recap: If you've set up a custom domain with Office 365, then you'll need to mangae your domain from the Office 365\nadministration console going forward and not yourr domain registrar.\n\nAny questions or clarifications needed, please leave a comment!\n\nThanks,\n\nD\n","categories":[{"name":"O365","slug":"O365","permalink":"https://westerndevs.com/categories/O365/"},{"name":"Azure","slug":"O365/Azure","permalink":"https://westerndevs.com/categories/O365/Azure/"}],"tags":[]},{"title":"Building APIs","slug":"Building-APIs","date":"2016-06-13 13:58:48+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Building-APIs/","link":"","permalink":"https://westerndevs.com/podcasts/Building-APIs/","excerpt":"The Western Devs discuss the ins and outs of building APIs","raw":"---\nlayout: podcast\ntitle: Building APIs\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-building-apis.mp3\n  length: '39:28'\n  filesize: 37892683\n  libsynId: 4435502\n  anchorFmId: Building-APIs-evqdig\nparticipants:\n  - kyle_baley\n  - amir_barylko\n  - dave_white\n  - dylan_smith\n  - simon_timms\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Your API versioning is wrong|https://www.troyhunt.com/your-api-versioning-is-wrong-which-is/'\n  - 'Stripe API|https://stripe.com/docs/api'\n  - 'Azure ARM Template API|https://azure.microsoft.com/en-us/documentation/articles/resource-group-authoring-templates/'\n  - 'How Microsoft Lost the API War|http://www.joelonsoftware.com/articles/APIWar.html'\n  - 'HTTPS as a ranking signal|https://webmasters.googleblog.com/2014/08/https-as-ranking-signal.html'\n  - 'AppVeyor|https://www.appveyor.com/'\n  - 'SignalR|http://www.asp.net/signalr'\n  - 'IFTTT|https://ifttt.com'\n  - 'Zapier|https://zapier.com'\n  - 'Token-based authentication|https://scotch.io/tutorials/the-ins-and-outs-of-token-based-authentication'\n  - 'OAuth2|http://oauth.net/2/'\n  - 'Authenticating in TFS with personal access tokens|https://www.visualstudio.com/en-us/docs/setup-admin/team-services/use-personal-access-tokens-to-authenticate'\ndate: 2016-06-13 09:58:48\nrecorded: 2016-03-11\nexcerpt: \"The Western Devs discuss the ins and outs of building APIs\"\n---\n\n### Synopsis\n\n* What is an API?\n* Clarifying \"public\" with respect to APIs\n* Why build an API?\n* APIs as a means of breaking down components and compartmentalizing data\n* API design considerations\n* The effect of microservices\n* Versioning an API\n  * Maintaining multiple versions\n  * Using a message upgrade service\n  * Only adding to the API\n  * The cost of maintaining backward compatibility\n  * Using speed as an incentive to get customers to upgrade to a later version\n* SignalR/Websockets\n* Balancing data between an API and a webhook\n* Services that integrate with APIs (e.g. IFTTT, Zapier)\n* Native integration with 3rd party services\n* Authentication\n  * Token-based\n  * OAuth2\n  * Application-specific tokens\n  * Expiring tokens\n  * Applying permissions to tokens\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"How I fixed OneDrive like Mark Russinovich","authorId":"simon_timms","slug":"MarkRussinovich","date":"2016-06-11 22:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"debugging/MarkRussinovich/","link":"","permalink":"https://westerndevs.com/debugging/MarkRussinovich/","excerpt":"Even with compiled applications it is possible to debug issues","raw":"---\ntitle: How I fixed OneDrive like Mark Russinovich\nlayout: post\ncategories:\n  - debugging\nauthorId: simon_timms\ndate: 2016-06-11 18:56:56\nexcerpt: \"Even with compiled applications it is possible to debug issues\"\n---\n\nFellow [Monster David Paquette](http://aspnetmonsters.com) sent me a link to a shared OneDrive folder today with some stuff in it. Clicking on the link I was able to add it to my OneDrive. The dialog told me files would appear on my machine soon. So I waited.\n\nAfter an outrageously long time, 37 seconds, the files weren't there and I went hunting to find out why. As it turns out OneDrive wasn't even running. That's suppose to be a near impossiblity in Windows 10 so I hopped on the Interwebernets to find out why. Multiple sources suggested solutions like clearing my credentials and running `OneDrive.exe /reset`. Of course none of them worked.\n\nSomething was busted.\n\nRunning the OneDrive executable didn't bring up the UI it didn't do any of the things the Internet told me it should. My mind went back to when I was setting up my account on this computer and how I fat fingered `stimm` instead of `stimms` as my user name. Could it be the OneDrive was trying to access some files that didn't exist?\n\nChanneling my inner Mark Russinovich I opened up `ProcessMonitor` a fantastic tool which monitors file system and registry access. You can grab your own copy for free from [https://technet.microsoft.com/en-us/sysinternals/bb896645.aspx](https://technet.microsoft.com/en-us/sysinternals/bb896645.aspx).  \n\nIn the UI I added filters for any process with the word \"drive\" in it and then filtered out \"google\". I did this because I wasn't sure if the rename from skydrive to onedrive had missed anything. Then I ran the command line to start up OneDrive again.\n\nProcess monitor found about 300 results before the process exited. Sure enough as I went through the file accesses I found\n![http://i.imgur.com/soAh4PR.png](http://i.imgur.com/soAh4PR.png)\nSure enough OneDrive is trying to create files inside of a directory which doesn't exist. Scrolling further up I was able to find some references to values in the registry under `HKCU\\SOFTWARE\\Microsoft\\OneDrive` which, when I opened them up, contained the wrong paths. I corrected them\n![http://i.imgur.com/arhWYgt.png](http://i.imgur.com/arhWYgt.png)\nAnd with that in place was able to start up OneDrive successfully again and sync down the pictures of cats that David had sent me.\n\nThe story here is that it is possible, and even easy, to figure out why a compiled application on your machine isn't working. By examining the file and registry accesses it is making you might be able to suss out what's going wrong and fix it.\n","categories":[{"name":"debugging","slug":"debugging","permalink":"https://westerndevs.com/categories/debugging/"}],"tags":[]},{"title":"Maybe null is not an Option","authorId":"amir_barylko","slug":"maybe-null-is-not-an-option","date":"2016-06-09 02:28:25+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Fsharp/Functional-programming/maybe-null-is-not-an-option/","link":"","permalink":"https://westerndevs.com/Fsharp/Functional-programming/maybe-null-is-not-an-option/","excerpt":"Tony Hoare calls null references his billion dollar mistake. Using null values (NULL, Null, nil, etc) makes code harder to maintain and to understand. But what can we do about it? To start let's review the meaning of null values ...","raw":"---\nlayout: post\ntitle: \"Maybe null is not an Option\"\ndate: 2016-06-08 22:28:25\ntags:\n  - fsharp\n  - functional\n  - design\ncategories:\n  - Fsharp\n  - Functional programming\nauthorId: amir_barylko\noriginalurl: http://orthocoders.com/blog/2016/06/08/maybe-null-is-not-an-option/\n---\n\n[Tony Hoare](http://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare) calls _null references_ his **billion dollar mistake**. Using `null` values (`NULL`, `Null`, `nil`, etc) makes code harder to maintain and to understand. \n\nBut what can we do about it? To start let's review the meaning of `null` values ...\n\n<!-- more -->\n\n## Modeling optional results\n\n### Finding Customers\n\nFinding operations are very common. Given a `Customer` class, a `Find` method could look like this:\n\n``` csharp\npublic Customer Find(Query query) ;\n```\n\nNow what happens when the customer can not be found? Possible options are:\n\n* Throw an exception: Why though? Where is the exceptional case? Trying to find a `Customer` has the possible scenario of not beign found.\n\n* Return `null` to mean nothing was found. But are we positive we are getting a `null` for that reason and not other? And what can we do with the `null` value after?\n\n* Return a `NullObject` that represents `Customer` null value. That could work to show some of the customer's data, if we expect strings or something similar. But for most cases this won't be enough.\n\n### Parsing Integers\n\nIn _C#_ you can use `Int32.parse` or `Int32.tryParse` to do the job. The former throws an `Exception` when the string can not be parsed into an int and the later returns a `Bool` indicating if the operation succeeded using an `out` parameter for the value.\n\nThe first approach is not that intuitive. I want to get a result, not to catch an exception. \n\nThe second one with the boolean result seems to go in the right direction but having an `out` parameter complicates things, and makes it hard to understand and hard to pass to another function, etc.\n\n### Optional values\n\n_F#_ has a very simple way to deal with this by creating a [Discriminated Union](https://fsharpforfunandprofit.com/posts/discriminated-unions/) with only two values:\n\n``` fsharp\ntype Option<'T> = \n  | Some of 'T \n  | None\n```\n\nNow finding customers has a very clear interface:\n\n``` fsharp\nlet tryFindCustomer (q: query) : Option<Customer>\n```\n\nParse clearly can succeed giving the parsed result or fail, therefore the result is `Optional`.\n\n``` fsharp\nlet tryParseInt (s:string) =\n  match Int32.TryParse(s) with\n  | true, result -> Some result\n  | false, _     -> None\n\n```\n\nNOTE: out parameters are converted tuples in F#.\n\nAs a convention is common to call functions `trySomeAction` when the action can fail and return an _optional_ value.\n\n## Working with optional values\n\nModeling optional values is a great start. Using an optional value makes very clear the fact that the caller has the _responsibility_ to handle the possiblity of having `None` as a result.\n\nHaving clear meaning improves clarity, intent, error handling, and there is no `null` that can cause problems.\n\nHowever, in terms of handling the result, are we that much better than before?\n\nOf course we could check always for `Some` or `None` and handle the result, but where is the `fun` in that?\n\nThe key to create a great abstraction is _usage_. After seeing the same bit of code used again and again we could be confident that _abstracting_ the behaviour is going to be really useful.\n\nTo avoid doing a `match` on `Option` types luckily we have a series of helper functions that address most common scenarios.\n\nMany of these functions live in the [FSharpx](https://github.com/fsprojects/FSharpx.Extras) library.\n\n### Using defaults\n\nTo obtain the value from an `Option` we can use `Option.get`:\n\n``` fsharp\nlet get = \n  function\n  | Some a -> a\n  | None   -> failwith \"Nothing to get!\"\n\n```\n\n(what's that `function`? Here is an [explanation about pattern maching function](http://fsharpforfunandprofit.com/posts/match-expression/))\n\nNotice that `get` throws an exception when there is nothing to get.\n\nThrowing exception (and catching it) is ok, but makes hard to compose the result or transform it.\n\nInstead why not use a default value when there is `None`? (Taken from [FSharpx](https://github.com/fsprojects/FSharpx.Extras/blob/master/src/FSharpx.Extras/ComputationExpressions/Monad.fs)):\n\n``` fsharp\nlet getOrElse v = \n  function\n  | Some a -> a\n  | None   -> v\n```\n\nKnowing that it can fail, and having a default value help us write code that can use a default value and keep going:\n\n``` fsharp\nlet doWebRequest countStr =\n  countStr\n  |> tryParse\n  |> Option.getOrElse 10\n  |> processRequest\n```\n\n### Applying functions\n\nAnother common scenario is to apply a function when we get a result or just do nothing otherwise:\n\nFor example, the following code will only print the message when `tryParse` returns `Some`:\n\n``` fsharp\nlet doWebRequest param =\n  param\n  |> tryParse\n  |> Option.iter (printf \"The result is %d\")\n```\n\nAnd the implementation:\n\n``` fsharp\nlet iter f = \n  function\n  | Some a -> f a\n  | None   -> unit\n```\n\n\n### Shortcircuit `None`\n\n`iter` is useful to execute a function when there is `Some` value returned, but is common to use the value somehow and transform it into something else.\n\nFor that we can use the `map` function:\n\n``` fsharp\nlet map f = \n  function\n  | Some a -> f a |> Some\n  | None   -> None\n```\n\nThis is a bit different. Not only the function passed as parameter is applied after _unboxing_ the value, but the result is _boxed_ back into an `Option`. Once an `Option` always an `Option`.\n\nImagine a [railway](http://fsharpforfunandprofit.com/rop/) and a train that once hits the value `None` switches to a `None` railway and bypasses any operation that comes after. Thus the _shortcircuit_.\n\n``` fsharp\nlet queryCustomers quantity = [] // silly implementation\n\nlet doWebRequest req =\n  req.tryGetParam \"customers\"\n  |> Option.map queryCustomers         // Shortcircuit with None\n  |> Option.getOrElse invalidRequest   // uses the default\n```\n\n### Mapping to `Option`\n\nAnother common case, very similar to `map`, is to use a function that transforms the boxed value and returns an `Option`.\n\n``` fsharp\nlet bind f = \n  function\n  | Some a -> f a \n  | None   -> None\n```\n\nFor example the parameter that represents the _customer id_ is a string, and we need to parse it into an int. Getting the parameter can return a `None` and the parse function could return a `None` as well.\n\n``` fsharp\nlet doWebRequest req =\n  req.tryGetParam \"customerId\"\n  |> Option.bind Int32.tryParse       // Shortcircuit if None\n  |> Option.map  findCUstomer         // Shortcircuit if None\n  |> Option.getOrElse invalidRequest\n```\n\n## Multiple optional values\n\nSo far so good with one parameter. But what happens with more than one parameter? The goal is to _shortcircuit_ and if one of the parameters is `None` then abort and just return `None`.\n\nOne option is to use _FSharpx_ and the `MaybeBuilder`. I'm not going to discuss the details of how builders work but I will show you the practical usage to illustrate the point.\n\n``` fsharp\n  type Result<'TData> = \n    | Success of 'TData\n    | Error of string\n\n  let findCustomers count country city = ... // implement query\n\n  let invalidRequest = \"Boooo! Not all parameters are present!\"\n\n  let doWebRequest (req:Request) =\n    maybe {\n      let! strCount = req.tryGetParam \"count\"\n      let! city     = req.tryGetParam \"city\"\n      let! country  = req.tryGetParam \"country\"\n      let! count    = Int32.tryParse strCount\n      return findCustomers count country city\n    }\n    |> Option.map Success\n    |> Option.getOrElse (invalidRequest |> Error)\n```\n\nIn this scenario we have a happy path:\n\n* All parameters are present, then the result is `Success` with the output of `findCustomers`.\n\nAnd four _unhappy_ paths:\n\n* `count` is not present, then the `maybe` builder does shortcircuit to `None` and `getOrElse` returns an `Error`.\n* `city` is not present, then the `maybe` builder does shortcircuit to `None` and `getOrElse` returns an `Error`.\n* `country` is not present, then the `maybe` builder does shortcircuit to `None` and `getOrElse` returns an `Error`.\n* `count` is present, but can not be parsed then ... shortcircuit ... and `Error`.\n\nThe `let!` is doing the _unboxing_ from `Option` to the actual type, and when any of the expressions has the value `None` then the builder does the _shortcircuit_ and returns `None` as result.\n\n## Applicative Style\n\nAnother way to write the same concept (sometimes a bit more clear) is to use the _Applicative_ style by using operators that represent the operations that we already are using that also apply shortcircuit when possible. \n\nFor the functions we have used in the `Option` type the operators are:\n\n``` fsharp\n<!> // is an alias for map\n>>= // is an alias for bind\n```\n\n_(Find all the definitions [here](https://github.com/fsprojects/FSharpx.Extras/blob/master/src/FSharpx.Extras/ComputationExpressions/Monad.fs))_.\n\nTo use them let's try to read the parameters from the request.\n\n``` fsharp\n  let city    = req.tryGetParam \"city\"\n  let country = req.tryGetParam \"country\"\n```\n\nGood, now `count` needs to be parsed as well, so we can use `tryParse` that returns an `Option`. What can we use when we need to apply a function that returns also an `Option`? `Bind` of course, or `>>=`.\n\n``` fsharp\n  let count = req.tryGetParam \"count\" >>= Int32.tryParse \n```\n\nAll the parameters are parsed into `Option` and `findCustomers` can be invoked.\n\n``` fsharp\n  findCustomers <!> count ???? city ??? country\n```\n\nTo apply a function over an `Option` we can use the operator `<!>` (`map`), but what about the other two parameters?\n\nLet me rephrase, what happens when we apply a function that takes three parameters to just one parameter? Exactly! A partial application!\n\nSame happens when applying the operator `<!>` to a function that takes three parameters, the difference is that the partially applied function gets _boxed_ in an `Option`.\n\n``` fsharp\n  let count = req.tryGetParam \"count\" >>= Int32.tryParse \n\n  findCustomers <!> count ... // returns an Option<int -> int -> Customer list>\n```\n\nNow we need to apply the boxed function to a boxed value, and for that we can use the operator `<*>` that takes a boxed function and a boxed value and returns the boxed version of applying the function to the value.\n\n``` fsharp\n  findCustomers <!> count <*> city  ... // partial application of the function to city\n```\n\nIn this case we have two more parameters so the full version would be:\n\n``` fsharp\n  let findCustomers count city country = [] \n\n  let doWebRequest (req:Request) =\n    let city    = req.tryGetParam \"city\"\n    let country = req.tryGetParam \"country\"\n    let count   = req.tryGetParam \"count\" >>= Int32.tryParse \n\n    findCustomers <!> count <*> city <*> country\n    |> Option.map Success\n    |> Option.getOrElse (invalidRequest |> Error)\n```\n\n## Summary\n\nUsing an `Option` to represent when a value may not be present has many advantages. \n\nNot only is easier to deal with cases that produce no results, but also the code is clear an easy to follow.\n\nThough here the code is in F# you could implement similar features in your favourite language. \n\n(_Check out Java 8 [Optional class](https://docs.oracle.com/javase/8/docs/api/java/util/Optional.html) or this [C# implementation of Maybe](https://github.com/adamkrieger/FSBetter/blob/master/functions/0.2_Maybe.csx) by [Adam Krieger](https://twitter.com/AdamKrieger)_).\n\nAll the code can be found [here](https://github.com/amirci/option-sample). I included a series of tests that show how the builder and applicative style apply a shortcircuit when one of the parameters is missing.\n\nThanks to my good friend [Shane](https://twitter.com/Dead_Stroke) for helping me to test the code in all platforms.\n\n","categories":[{"name":"Fsharp","slug":"Fsharp","permalink":"https://westerndevs.com/categories/Fsharp/"},{"name":"Functional programming","slug":"Fsharp/Functional-programming","permalink":"https://westerndevs.com/categories/Fsharp/Functional-programming/"}],"tags":[{"name":"design","slug":"design","permalink":"https://westerndevs.com/tags/design/"},{"name":"functional","slug":"functional","permalink":"https://westerndevs.com/tags/functional/"},{"name":"fsharp","slug":"fsharp","permalink":"https://westerndevs.com/tags/fsharp/"}]},{"title":"ASP.NET Core Distributed Cache Tag Helper","authorId":"dave_paquette","slug":"ASP-NET-Core-Distributed-Cache-Tag-Helper","date":"2016-05-22 16:11:17+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/Tag-Helpers/ASP-NET-Core-Distributed-Cache-Tag-Helper/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/Tag-Helpers/ASP-NET-Core-Distributed-Cache-Tag-Helper/","excerpt":"The anxiously awaited ASP.NET Core RC2 has finally landed and with it we have a shiny new tag helper to explorer. In this post we will explore the new Distributed Cache tag helper and how it differs from the already existing Cache tag helper.","raw":"---\nlayout: post\ntitle: ASP.NET Core Distributed Cache Tag Helper\ntags:\n  - ASP.NET Core\n  - Tag Helpers\n  - MVC\ncategories:\n  - ASP.NET Core\n  - Tag Helpers\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2016/05/22/ASP-NET-Core-Distributed-Cache-Tag-Helper.aspx'\ndate: 2016-05-22 12:11:17\nexcerpt: The anxiously awaited ASP.NET Core RC2 has finally landed and with it we have a shiny new tag helper to explorer. In this post we will explore the new Distributed Cache tag helper and how it differs from the already existing Cache tag helper.\n---\n\nThe anxiously awaited ASP.NET Core RC2 has finally landed and with it we have a shiny new tag helper to explorer.\n\nWe previously talked about the [Cache Tag Helper](http://www.davepaquette.com/archive/2015/06/03/mvc-6-cache-tag-helper.aspx) and how it allows you to cache the output from any section of a Razor page. While the Cache Tag Helper is powerful and very useful, it is limited in that it uses an instance of `IMemoryCache` which stores cache entries in memory in the local process. If the server process restarts for some reason, the contents of the cache will be post. Also, if your deployment consists of multiple servers, each server would have its own cache, each potentially containing different contents.\n\n# Distributed Cache Tag Helper\n\nThe cache tag helper left people wanting more. Specifically they wanted to store the cached HTML in a distributed cache like Redis. Instead of complicating the existing Cache Tag Helper, the ASP.NET team enabled this use-case by adding a new [Distributed Cache Tag Helper](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/DistributedCacheTagHelper.cs).\n\nUsing the Distributed Cache Tag Helper is very similar to using the Cache Tag Helper:\n\n{% codeblock lang:html %}\n<distributed-cache name=\"MyCache\">\n    <p>Something that will be cached</p>\n    @DateTime.Now.ToString()\n</distributed-cache>\n{% endcodeblock %}\n\nThe name property is required and the value should be unique. It is used as a prefix for the cache key. This differs from the Cache Tag Helper which uses an automatically generated unique id based on the location of the cache tag helper in your Razor page. The auto generated approach cannot be used with a distributed cache because Razor would generate different unique ids for each server. You will need to make sure that you use a unique `name` each time you use the `distributed-cache` tag helper. If you unintentionally use the same name in multiple places, you might get the same results in 2 places.\n\nFor example, see what happens when 2 `distributed-cache` tag helpers with the same `name`:\n\n{% codeblock lang:html %}\n<distributed-cache name=\"MyCache\">\n    <p>Something that will be cached</p>\n    @DateTime.Now.ToString()\n</distributed-cache>\n\n<distributed-cache name=\"MyCache\">\n    <p>This should be different</p>\n    @DateTime.Now.ToString()\n</distributed-cache>\n{% endcodeblock %}\n\n![Accidental Cache Key Collision](http://www.davepaquette.com/images/distributed-cache-key-collision.png)\n\n_If you are really curious about the how cache keys are generated for both tag helpers, take a look at the [CacheTagKey Class](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/Cache/CacheTagKey.cs)._\n\nThe `vary-by-*` and `expires-*` attributes all work the same as the Cache Tag Helper. You can review those in my [previous post](http://www.davepaquette.com/archive/2015/06/03/mvc-6-cache-tag-helper.aspx).\n\n## Configuring the Distributed Cache\n\nUnless you specify some additional configuration, the distributed cache tag helper actually uses a local process in memory cache. This might seem a little strange but it does help with the developer workflow. As a developer, I don't need to worry about standing up a distributed cache like Redis just to run the app locally. The intention of course is that a true distributed cache would be used in a staging/production environments.\n\nThe simplest approach to configuring the distributed cache tag helper is to configure a `IDistributedCache` service in the Startup class. ASP.NET Core ships with 2 distributed cache implementations out of the box: [SqlServer](https://github.com/aspnet/Caching/blob/dev/src/Microsoft.Extensions.Caching.SqlServer/SqlServerCache.cs) and [Redis](https://github.com/aspnet/Caching/blob/dev/src/Microsoft.Extensions.Caching.Redis/RedisCache.cs).\n\nAs a simple test, let's try specifying a `SqlServerCache` in the `Startup.ConfigureServices` method:\n\n{% codeblock lang:csharp %}\nservices.AddSingleton<IDistributedCache>(serviceProvider =>\n    new SqlServerCache(new SqlServerCacheOptions()\n    {\n        ConnectionString = @\"Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=DistributedCacheTest;Integrated Security=True;\",\n        SchemaName = \"dbo\",\n        TableName = \"MyAppCache\"\n    }));\n{% endcodeblock %}\n\nOf course, the ConnectionString should be stored in a configuration file but for demonstration purposes I have in-lined it here.\n\nYou will need to create the database and table manually. Here is a script for creating the table, which I extracted from [here](https://github.com/aspnet/Caching/blob/dev/src/Microsoft.Extensions.Caching.SqlServer/SqlQueries.cs):\n\n{% codeblock lang:sql %}\nCREATE TABLE MyAppCache(            \n\tId nvarchar(449) COLLATE SQL_Latin1_General_CP1_CS_AS NOT NULL,\n\tValue varbinary(MAX) NOT NULL,\n\tExpiresAtTime datetimeoffset NOT NULL,\n\tSlidingExpirationInSeconds bigint NULL,\n\tAbsoluteExpiration datetimeoffset NULL,\n\tCONSTRAINT pk_Id PRIMARY KEY (Id))\n\nCREATE NONCLUSTERED INDEX Index_ExpiresAtTime ON MyAppCache(ExpiresAtTime)\n{% endcodeblock %}\n\nNow when I visit the page that contains the `distributed-cache` tag helper, I get the following error:\n\n`InvalidOperationException: Either absolute or sliding expiration needs to be provided.`\n\nThe SQL Server implementation requires us to specify some form of expiry. No problem, let's just add the those attributes to the tag helper:\n\n{% codeblock lang:html %}\n<distributed-cache name=\"MyCacheItem1\" expires-after=\"TimeSpan.FromHours(1)\">\n    <p>Something that will be cached</p>\n    @DateTime.Now.ToString()\n</distributed-cache>\n\n\n<distributed-cache name=\"MyCacheItem2\" expires-sliding=\"TimeSpan.FromMinutes(30)\">\n    <p>This should be different</p>\n    @DateTime.Now.ToString()\n</distributed-cache>\n{% endcodeblock %}\n\nNow the page renders properly and we can see the contents in SQL Server:\n\n![SQL Server Cache Contents](http://www.davepaquette.com/images/sql-server-cache-contents.png)\n\nNote that since the key is hashed and the value is stored in binary, the contents of the table in SQL server are not human readable.\n\nFor more details on working with a SQL Server or Redis distrubted cache, see the [official ASP.NET Docs](https://docs.asp.net/en/latest/performance/caching/distributed.html);\n\n## Even more configuration\n\n In some cases, you might want more control over how values are serialized or even how the distributed cache is used by the tag helper. In those cases, you could implement your own [IDistributedCacheTagHelperFormatter](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/Cache/IDistributedCacheTagHelperFormatter.cs) and/or [IDistributedCacheTagHelperStorage](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/Cache/IDistributedCacheTagHelperStorage.cs).\n\n In cases where you need complete control, you could implement your own [IDistributedCacheTagHelperService](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/Cache/IDistributedCacheTagHelperService.cs).\n\n I suspect that this added level of customization won't be needed by most people.\n\n # Conclusion\n\n The Distributed Cache Tag Helper provides an easy path to caching HTML fragments in a distributed cache. Out of the box, Redis and SQL Server are supported. Over time, I expect that a number of alternative distributed cache implementations will be provided by the community.\n","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"},{"name":"Tag Helpers","slug":"ASP-NET-Core/Tag-Helpers","permalink":"https://westerndevs.com/categories/ASP-NET-Core/Tag-Helpers/"}],"tags":[{"name":"Tag Helpers","slug":"Tag-Helpers","permalink":"https://westerndevs.com/tags/Tag-Helpers/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"MVC","slug":"MVC","permalink":"https://westerndevs.com/tags/MVC/"}]},{"title":"I regret nothing!","authorId":"kyle_baley","slug":"I-regret-nothing","date":"2016-05-08 17:46:59+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"career/I-regret-nothing/","link":"","permalink":"https://westerndevs.com/career/I-regret-nothing/","excerpt":"One of the Western Devs posted an article on our Slack channel on someone's regrets as a programmer. I fundamentally disagree with the sentiment of this article and the remainder of this post will be sixteen paragraphs and three quotes belabouring this point.","raw":"---\nlayout: post\ntitle: I regret nothing!\ncategories:\n  - career\ndate: 2016-05-08 13:46:59\ntags:\nauthorId: kyle_baley\n---\n\nOne of the Western Devs posted [an article](http://thecodist.com/article/my-biggest-regret-as-a-programmer) on our Slack channel on someone's regrets as a programmer. I fundamentally disagree with the sentiment of this article and the remainder of this post will be sixteen paragraphs and three quotes belabouring this point.\n\n<!-- more -->\n\nIn the article, the author describes their [Mr. Destiny](http://www.imdb.com/title/tt0100201/) moment where they wish they had gone into management rather than sticking with being \"just\" a programmer. Thus perpetuating the myth in our industry that you aren't worth anything unless you *Change The Worldâ„¢*.\n\nTo be fair, the author is careful not to be prescriptive. It's very much \"these are my regrets and thoughts\" and not \"you should do this in the same situation\". I'm thankful for that and I'll pay the same courtesy. These are my thoughts as they apply to me personally. If you can relate to it in any way, that's not my fault.\n\n{% img pull-right /images/yes-no.jpg 400 %}\n\nTwenty years ago, I suppose I was at my own crossroads though I didn't really recognize it as such until I thought about it just now. Ever since I was in grade 8, I was going to be an actuary. At the time, I was told \"you have to be good at math and you'll make a lot of money\". What else do you need to hear when you're in grade 8?\n\nSo I set about my goal and got a degree, a Bachelor of Commerce I believe. I wasn't really sure what to make of the Faculty of Management, what with their wine and cheeses and people coming to class in suits and \"networking\" but whatever, I did the work, finished my studies, and started interviewing like you were supposed to.\n\nAt some point in that first six months of interviewing, I clued in to something: programming, which I had been doing for about 10 years by that point as a hobby, was a career choice! Was it a lucrative one? Was it better than being an actuary? Who cares, it was fun and people got paid to do it! So I went back to school. That is, to my ageing memory, the sum total of the thought I put into it. (This will be a recurring theme throughout this post.)\n\nSo I did it. I went back to school, got the degree, and started working. My first job was in the corporate world, an oil and gas company. My next, with a startup where I [interviewed badly](http://kyle.baley.org/2008/01/interview-question-tell-me-about-your-mother) and they hired me anyway. The third was with a consulting company. Since then: contracting, my own startup, and employeehood in some order. Along the way, I've learned, to varying degrees, VB6, classic ASP, .NET, RPG, Livelink, Sharepoint, JavaScript, Ruby, Java, SQL Server, Azure, Google Web Toolkit, Docker, CI, CD, CQRS, CORS, CSS and more acronymed software methodologies and techniques than I care to ~~put thought into remembering~~ admit. I've blogged (clearly), co-written a book, spoken at conferences, and created a user group that was the first of its kind in the country. (It lasted less than a year.)\n\nWhile you're free to copy and paste all this into my obituary, there's a reason I list it out. I have never put much thought into my career and I don't have any intrinsic itch I've been meaning to scratch. How did I choose when and where to do all of these things? The opportunities came up and I said yes. Again, not much more thought goes into it than that.\n\nI have no regrets (including [the book](http://kyle.baley.org/2011/04/brownfield-application-development-one-year-later)). Certainly not on a macro level. Should I have pursued a career as an actuary which, in all likelihood, would have ended up more lucrative financially? Was it a waste of time learning Livelink? Should I have gone into a managerial role?\n\nDoes it matter?\n\nI am where I am now because of the choices I've made. I've often joked that the main criteria I use to choose my contracts/positions is whether I think it'll be fun. Not whether it was in a hot technology or a dynamic industry. I'm too lazy to figure out what constitutes either of those.\n\nThe net result of those criteria is that I can look back favourably on a career (and I use that term loosely) as \"just a programmer\". One that I have no intention of leaving because why would I?\n\nI am, and I can't emphasize this enough, having the time of my life.\n\nNow to be fair, I've always had the power to see things through a pair of glasses a supernatural shade of rose. But consider the language from the post I mentioned earlier:\n\n> I can only imagine how in demand I could have been.\n>\n> My sister has 10X the assets I have.\n\nAnd my favourite one:\n\n> And today I am still just a programmer. Whoâ€™s the weenie now?\n\nTo me, this speaks of a chronic problem of chasing the wrong thing. What you're _supposed_ to do versus what you _want_ to do. And I've often wondered, if you had taken that other path, would you still have no regrets?\n\nThe world is filled with people who want to leave their mark, some of whom actually will. And I think it's great that these people exist. We need them. For my part, I see no point in feeling guilt or regret doing something I love doing in an industry that is privileged enough to provide me with a good living doing it.\n","categories":[{"name":"career","slug":"career","permalink":"https://westerndevs.com/categories/career/"}],"tags":[]},{"title":"Prairie Dev Con Recap","slug":"Prairie-Dev-Con-Recap","date":"2016-05-07 17:22:31+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Prairie-Dev-Con-Recap/","link":"","permalink":"https://westerndevs.com/podcasts/Prairie-Dev-Con-Recap/","excerpt":"The Western Devs reminisce on their experience at Prairie Dev Con and offer tips to speakers, conference organizers, and attendees.","raw":"---\nlayout: podcast\ntitle: Prairie Dev Con Recap\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-prairie-dev-con-recap.mp3\n  length: '44:36'\n  filesize: 42817504\n  libsynId: 4350772\n  anchorFmId: Prairie-Dev-Con-Recap-evqdhl\nparticipants:\n  - kyle_baley\n  - darcy_lussier\n  - david_wesst\n  - dave_paquette\n  - simon_timms\n  - lori_lalonde\n  - james_chambers\n  - donald_belcham\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Prairie Dev Con|http://www.prairiedevcon.com'\n  - 'Balmoral Hall|http://balmoralhall.com'\n  - 'Sturgeon Heights Collegiate|http://sturgeonheights.sjsd.net/'\n  - 'Ben Ed (Sweet ES6)|http://benmvp.com'\n  - 'David Alpert|http://blog.spinthemoose.com/'\ndate: 2016-05-07 13:22:31\nrecorded: 2016-04-15\nexcerpt: \"The Western Devs reminisce on their experience at Prairie Dev Con and offer tips to speakers, conference organizers, and attendees.\"\n---\n\n### Synopsis\n\n* History of Prairie Dev Con\n* Tee shirts, empanadas, and luchadors\n* Speaker tip: Be specific in session titles\n* Conference organizer tip: Treat your speakers well\n* Conference organizer tip: Create a positive vibe\n* Presentation review: Docker\n* Presentation review: Staying relevant in the industry\n* Feedback stats\n* The broadening technology landscape\n* The challenge of determining what the market wants to hear\n* Benefits of covering costs for speakers\n* Conference organizer tip: Focus on all of: Attendees, Sponsors, Organizer, Speakers\n* Speaking topic: When not to push to production\n* Topic trends: Mobile is dead (at least in Winnipeg)\n* Presentation review: Dealing with difficult people\n* Law of two feet: It's okay to leave if you don't want to stay in the session\n* Presentation review: Sweet ES6\n* Record the &*%$# sessions, D'Arcy!\n* The finances of attending and running a conference\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"CI with F# SQL Type Providers","authorId":"simon_timms","slug":"CI-for-fsharp-typeproviders","date":"2016-05-06 22:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"docker/CI-for-fsharp-typeproviders/","link":"","permalink":"https://westerndevs.com/docker/CI-for-fsharp-typeproviders/","excerpt":"F# type providers are awesome but it took me a bit to figure out how to get them to work with CI","raw":"---\ntitle: CI with F# SQL Type Providers\nlayout: post\ncategories:\n  - docker\nauthorId: simon_timms\ndate: 2016-05-06 18:56:56 \nexcerpt: \"F# type providers are awesome but it took me a bit to figure out how to get them to work with CI\"\n---\n\nMy experimentation with F# continues. My latest challenge has been figuring out how to get SQL type providers to work with continuous integration. The way that SQL type providers work (and I'm speaking broadly here because there are about 4 of them) is that they examine a live database and generate types from it. On your local machine this is a perfect set up because you have the database locally to do development against. However on the build server having a database isn't as likely.\n\nIn my particular case I'm using Visual Studio Online or TFS Online or whatever the squid it is called these days. Visual studio team services that's what it is called. \n\n![Screenshot of VSTS](http://i.imgur.com/raetyHn.jpg)\n\nI'm using a hosted build agent which may or may not have a database server on it - at least not one that I really want to rely on. I was tweeting about the issue and Dmitry Morozov (who wrote the type provider I'm using - the F# community on twitter is amazing) suggested that I just make the database part of my version control. Of course I'm already doing that but in this project I was using EF migrations. The issue with that is that I need to have the database in place to build the project and I needed to build the project to run the migrations... For those who are big into graph theory you will have recognized that there is a cycle in the dependency graph and that ain't good.\n\n![Graph cycles](http://i.imgur.com/8tORskw.png)\n\nEF migrations are kind of a pain, at least that was my impression. I checked with Canada's Julie Lerman, David Paquette, to see if maybe I was just using them wrong.\n\n![Discussion with Dave Paquette](http://i.imgur.com/0O49NuU.jpg)\n\nSo I migrated to roundhouse which is a story for another post. With that in place I set up a super cheap database in azure and I hooked up the build process to update that database on every deploy. This is really nice because it catches database migration issues before the deployment step. I've been burned by migrations which locked the database before on this project and now I can catch them against a low impact database. \n\nOne of the first step in my build process is to deploy the database.\n![Build process](http://i.imgur.com/rcrX5KS.jpg)\n\nIn my F# I have a setting module which holds all the settings and it includes \n\n```\nmodule Settings = \n    [<Literal>]\n    let buildTimeConnectionString = \"Server=tcp:builddependencies.database.windows.net,1433;Database=BuildDependencies;User ID=build@builddependencies;Password=goodtryhackers;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\"\n```\nAnd this string is used throughout my code when I create the SQL based types \n```\ntype Completions = SqlProgrammabilityProvider<Settings.buildTimeConnectionString>\n```\nand \n\n```\nlet mergeCommand = new SqlCommandProvider<\"\"\"\n        merge systems as target\n        ...\"\"\", Settings.buildTimeConnectionString>(ConnectionStringProvider.GetConnection)\n```\nIn that second example you might notice that the build time connection string is different from the run time connection string which is specified as a parameter. \n\n##How I wish it worked\n\nFor the most part having a database build as part of your build process isn't a huge deal. You need it for integration tests anyway but it is a barrier for adoption. It would be cool if you could check in a serialized version of the schema and, during CI builds, point the type provider at this serialized version. This serialized version could be generated on the developer workstations then checked in. I don't think it is an ideal solution and now I've done the footwork to get the build database I don't think I would use it.\n\n","categories":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/categories/docker/"}],"tags":[]},{"title":"Running your app on Windows Server Core Containers","authorId":"simon_timms","slug":"windows-docker","date":"2016-04-27 22:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"docker/windows-docker/","link":"","permalink":"https://westerndevs.com/docker/windows-docker/","excerpt":"Getting started with NServiceBus on windows containers","raw":"---\ntitle: Running your app on Windows Server Core Containers\nlayout: post\ncategories:\n  - docker\nauthorId: simon_timms\ndate: 2016-04-27 18:56:56\nexcerpt: \"Getting started with NServiceBus on windows containers\"\n---\nMost of the day I work on an app which makes use of [NServiceBus](http://particular.net/). If you've ever talked to me about messaging, then you know that I'm all over messaging like a ferret in a sock.\n![Sock Ferret](http://i.imgur.com/oglJADJ.png)\n\nSo I'm, understandibly, a pretty big fan of NServiceBus - for the most part. The thing with architecting your solution to use SOA or Microservices or whatever we're calling it these days is that you end up with a lot of small applications. Figuring out how to deploy these can be a bit of a pain as your system grows. One solution I like is to make use of the exciting upcoming world of containers. I've deployed a few ASP.NET Core applications to a container but NServiceBus doesn't work on dotnet Core so I need to us a Windows container here.\n\nFirst up is to download the ISO for Windows Server Core 2016 from Microsoft. You can do that for free [here](https://www.microsoft.com/en-us/server-cloud/products/windows-server-2016/).  I provisioned a virtual box VM and installed Windows using the downloaded ISO. I chose to use windows server core as opposed to the version of windows which includes a full UI. The command line was good enough for Space Quest II and by gum it is good enough for me.\n\nStarting up this vm gets you this screen\n![Imgur](http://i.imgur.com/jvEsdMU.png)\n\nOkay, let's do it. Docker isn't installed by default but there is a great article on how to install it onto an existing machine [here](https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/quick-start-windows-server). In short I ran\n\n`powershell.exe`\n\nWhich started up powershell for me (weird that powershell isn't the default shell). Then\n\n```\nwget -uri https://aka.ms/tp4/Install-ContainerHost -OutFile C:\\Install-ContainerHost.ps1\n& C:\\Install-ContainerHost.ps1\n```\nI didn't specify the -HyperV flag as in the linked article because I wanted Docker containers. There are two flavours of containers on Windows at the moment. HyperV containers which are heavier weight and Docker containers which are lighter. I was pretty confident I could get away with Docker containers so I started with that. The installer took a long, long time. It had to download a bunch of stuff and for some reason it decided to use the background downloader which is super slow.\n\n![Slowwwww](http://i.imgur.com/3ce2GpA.png)\n\nBy default, the docker daemon only listens on 127.0.0.1 which means that you can only connect to it from inside the virtual machine. That's not all that useful as all my stuff is outside of the virtual machine. I needed to do a couple of things to get that working.\n\nThe first was to tell docker to listen on all interfaces. Ideally you shouldn't allow docker to bind to external interfaces without the TLS certificates installed. That was kind of a lot of work so I ignored the warning in the logs that it generates\n\n```\n/!\\\\ DONT' BIND ON ANY IP ADDRESS WITHOUT setting -tlsverify IF YOU DON'T KNOW WHAT YOU'RE DOING /!\\\\\n```\nYeah that's fine. To do this open up the docker start command and tell it to listen on the 0.0.0.0 interface.\n\n```\nnotepad c:\\programdata\\docker\\runDockerDaemon.cmd\n```\n\nNow edit the line\n\n```\ndocker daemon -D -b \"Virtual Switch\"\n```\nto read\n```\ndocker daemon -D -b \"Virtual Switch\" -H 0.0.0.0:2376\n```\n\nNow we need to relax the firewall rules or, in my case, turn off the firewall completely.\n\n```\nSet-NetFirewallProfile -name * -Enabled \"false\"\n```\n\nNow restart docker\n```\nnet stop docker\nnet start docker\n```\nWe should now be able to access docker from the host operating system. And indeed I can by specifying the host to connect to when using the docker tools. In my case on port 2376 on 192.168.0.13\n\n```\ndocker -H tcp://192.168.0.13:2376 ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS   PORTS               NAMES\n```\n\nFinally, we can actually start using docker.\n\n I hammered together a quick docker file which sucked in the output of my NSB handler's build directory.\n\n```\nFROM windowsservercore\n\nADD bin/Debug /funnel\n\nWORKDIR /funnel\n\nENTRYPOINT NServiceBus.Host.exe\n```\nThis dockerfile is based on the windowservercore image which was loaded onto the virtual machine during the setup script. You can check that using the `images` command to docker. To get the docker file running I first build the image then ask for it to be run\n\n```\ndocker -H  tcp://192.168.0.13:2376 build -t funnel1 -f .\\Dockerfile .\ndocker -H  tcp://192.168.0.13:2376 run -t -d funnel1\n```\n\nThe final command spits out a big bunch of letters and numbers which is the Id of the image. I can use that to get access to the command line output from that image\n\n```\ndocker -H  tcp://192.168.0.13:2376 logs fb0d6f23050c9039e65a106bea62a9049d9f79ce6070234472c112fed516634e\n```\n\nWhich gets me\n![Output](http://i.imgur.com/Asbigge.png)\n\nWith that I'm most of the way there. I still need to figure out some networking stuff so NSB can find my database and put our license file in there and check that NSB is actually able to talk to MSMQ and maybe find a better way to get at the logs... okay there is actually a lot still to do but this is the first step.\n","categories":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/categories/docker/"}],"tags":[]},{"title":"GitHub Authentication with ASP.NET Core","authorId":"james_chambers","slug":"github-authentication-asp-net-core","date":"2016-04-26 11:12:54+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/github-authentication-asp-net-core/","link":"","permalink":"https://westerndevs.com/Development/github-authentication-asp-net-core/","excerpt":"Authentication has changed over the years, and my take on it has surely shifted. No longer is it the scary, intimidating beastie that must be overcome on our projects. Today, we can let external providers provide the authentication mechanisms, giving the user with a streamlined experience that can give them access to our application with previuosly defined credentials. Let's have a look at what it takes to allow users to authenticate in our application using GitHub as the login source, and you can check out the Monsters video take of this on Channel 9.","raw":"---\ntitle: 'GitHub Authentication with ASP.NET Core'\nlayout: post\ntags:\n  - Visual Studio 2015\n  - Authentication\ncategories:\n  - Development\nauthorId: james_chambers\noriginalurl: 'http://jameschambers.com/2016/04/github-authentication-asp-net-core/'\ndate: 2016-04-26 07:12:54\n---\nAuthentication has changed over the years, and my take on it has surely shifted. No longer is it the scary, intimidating beastie that must be overcome on our projects. Today, we can let external providers provide the authentication mechanisms, giving the user with a streamlined experience that can give them access to our application with previuosly defined credentials.\n\n![GitHub Authentication in ASP.NET Core](https://jcblogimages.blob.core.windows.net/img/2016/04/github-auth.png)\n\nLet's have a look at what it takes to allow users to authenticate in our application using GitHub as the login source, and you can check out the Monsters video take of this on [Channel 9](https://channel9.msdn.com/Series/aspnetmonsters/Episode-26-GitHub-Authentication-in-ASPNET-Core).\n\n<!-- more -->\n\n## Background and Overview\n\nOAuth has been known as a complicated spec to adhere to, and this is further perpetuated by the fact that while much of the mechanics are the same among authentication providers, the implementation of how one retrieves information about the logged in user is different from source-to-source.\n\nThe security repo for ASP.NET gives us some pretty good options for the big, wider market plays like Facebook and Twitter, but there is aren't - nor can or should there be - packages for every provider. GitHub is appealing as a source when we target other developers, and while it lacks a package of its own, we can leverage the raw OAuth provider and implement the user profile loading details on our own.\n\nIn short, the steps are as follows:\n 1. Install the `Microsoft.AspNet.Authentication.OAuth` package\n 1. Register you application in GitHub\n 1. Configurate the OAuth parameters in your application\n 1. Enable the OAuth middleware\n 1. Retrieve the claims for the user\n\nOkay, now let's dive into the nitty gritty of it.\n\n## Install the Package\n\nFirst step is a gimme.  Just head into your `project.json` and add the package to the list of dependencies in your application.\n\n````\n  \"Microsoft.AspNet.Authentication.OAuth\": \"1.0.0-rc1-final\",\n````\n\nYou can see here that I am on RC1, so assume there may still be some changes to the naming and, obviously, the version of the package you'll want to use.\n\n## Create the App in GitHub\n\nPull down the user account menu from your avatar in the top-right corner of GitHub, then select Settings. Next, go to the OAuth Applications section and create a new application. This is pretty straightforward, but it's worth pointing out a few things.\n\n![Creating an OAuth app in GitHub](https://jcblogimages.blob.core.windows.net:443/img/2016/04/github-app.png)\n\nFirst, you'll need to note your client ID and secret, or minimally, you'll want to leave the browser window open.\n\nSecond you'll see that I have a authorization callback setup in the app as follows:\n\n`https://localhost:44363/signin-github`\n\nThis is important for two reasons:\n\n1. This will only work locally on your machine\n2. The `signin-github` bit will need to be configured in our middleware\n\nIf you want better control over how that is configured in your application, you can incorporate the appropriate settings into your configuration files, but you'll also need to update your GitHub app. This process is still relevant - you'll likely want something to test with locallying without having to deploy to test your application.\n\n## Configure Your Client ID and Secret\n  For production applications you'll be fine to set environment variables or configure application settings in Azure (which are loaded as env vars), but locally you'll want access to the config as well. You can setup [user secrets via the command line](https://channel9.msdn.com/Series/aspnetmonsters/Episode-23-Working-With-Sensitive-Data-User-Secrets), or you can just right-click on your project in Visual Studio 2015 and select \"Manage User Secrets\". From there, you set it up like so:\n\n````\n{\n  \"GitHub:ClientId\": \"your_id\",\n  \"GitHub:ClientSecret\": \"your_secret\"\n}\n````\n\n## Enable OAuth Middleware\n\nIn the above code we also wired up some code to fire during the `OnCreatingTicket` event, so let's implement that next.  To do this, we'll add the middleare to the `Configure` method in our startup class, and add a property to the class to expose our desired settings.\n\nThe middleware call is like so:\n` app.UseOAuthAuthentication(GitHubOptions);`\n\nAnd we create the property as such:\n<script src=\"https://gist.github.com/MisterJames/746331337329ca50556cbff19a0ba176.js\"></script>\n\nRemember that callback path that we setup on GitHub, you'll see it again in our settings above. You'll also note that we're retrieving our client ID and secret from our configuration, and that we're setting up a handler when the auth ticket is created so that we can go fetch additional details about the authenticating party.\n\n## Retrieve the User's Claims\nWe'll have to call back out to GitHub to get the user's details, they don't come back with the base calls for authentication. This is the part that is different for each provider, and thus you'll need to write this part for yourself if you wish to use an alternate source for authentication.\n\nWe will add two parts to this; the first will call out to get the information about the user, the second will parse the result to extract the claims. Both of these can live in your `startup.cs` class.\n\n<script src=\"https://gist.github.com/MisterJames/6a2ee9918afa9019aa3c1891f216102a.js\"></script>\n\n<script src=\"https://gist.github.com/MisterJames/c818ad44950d1c7312e2d36b93041407.js\"></script>\n\nUnfortunately the base implementation of the OAuth provider does not support allowing us to request additional fields for the user; I'll take a look at that in a future post. All you're going to get are the basics with the above - so none of the account details beyond the email addres, nor ability to work with their repos/issues/PRs.\n\n## Next Steps\n\nThere you have it. All the chops you need to start exercising your OAuth muscle, and a basic implementation that you can leverage as a starting point. Trying this out will take you about 15 minutes, start to finish, provided you already have a GitHub account.\n\n 1. [Get the latest VS 2015 and ASP.NET Core bits](https://get.asp.net/)\n 1. [Explore the ASP.NET security repo on GitHub](https://github.com/aspnet/security)\n 1. Run through the samples above.\n\nFinally, check out the Monsters' video on Channel 9 where I code this live.\n\n<iframe src=\"https://channel9.msdn.com/Series/aspnetmonsters/Episode-26-GitHub-Authentication-in-ASPNET-Core/player\" width=\"640\" height=\"360\" allowFullScreen frameBorder=\"0\"></iframe>\n\nHappy Coding!\n","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"Visual Studio 2015","slug":"Visual-Studio-2015","permalink":"https://westerndevs.com/tags/Visual-Studio-2015/"},{"name":"Authentication","slug":"Authentication","permalink":"https://westerndevs.com/tags/Authentication/"}]},{"title":"FSharp.Data.SqlClient Type Provider Having Trouble Bulk Loading","authorId":"simon_timms","slug":"FSharp.Data.SqlClient no method","date":"2016-04-24 22:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"fsharp/FSharp.Data.SqlClient no method/","link":"","permalink":"https://westerndevs.com/fsharp/FSharp.Data.SqlClient%20no%20method/","excerpt":"Uh oh: Method not found: 'Void FSharp.Data.DataTable`1.BulkCopy'","raw":"---\ntitle: FSharp.Data.SqlClient Type Provider Having Trouble Bulk Loading\nlayout: post\ncategories:\n  - fsharp\nauthorId: simon_timms\ndate: 2016-04-24 18:56:56 \nexcerpt: \"Uh oh: Method not found: 'Void FSharp.Data.DataTable`1.BulkCopy'\"\n---\n\nI really hate assembly binding and redirects and the such in .net. Any time I see a fatal error message like \n\n```\nCould not load file or assembly 'FSharp.Core, Version=4.3.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\n```\n\nI want to quit programming and become a baker. Yeah I would have to get up early but I would rarely encounter \n\n```\nCould not leven bread using yeast 1.0.4.5 or one of it's dependencies\n```\n\nAnyway while trying to use the bulk loading feature I was having a problem because my F# project was referending FSharp.Core 4.4.0 and the C# project I had consuming the F# api was using 4.3.0. I solved it by installing the correct version of the dll in the C# project\n\n```\nInstall-Package fsharp.core\n```","categories":[{"name":"fsharp","slug":"fsharp","permalink":"https://westerndevs.com/categories/fsharp/"}],"tags":[]},{"title":"FSharp.Data.SqlClient Type Provider Not Finding Tables","authorId":"simon_timms","slug":"FSharp.Data.SqlClient","date":"2016-04-23 22:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"fsharp/FSharp.Data.SqlClient/","link":"","permalink":"https://westerndevs.com/fsharp/FSharp.Data.SqlClient/","excerpt":"If the SqlProgrammabilityProvider isn't finding table names for you this might be the fix","raw":"---\ntitle: FSharp.Data.SqlClient Type Provider Not Finding Tables\nlayout: post\ncategories:\n  - fsharp\nauthorId: simon_timms\ndate: 2016-04-23 18:56:56 \nexcerpt: \"If the SqlProgrammabilityProvider isn't finding table names for you this might be the fix\"\n---\n\nThis one took me forever to figure out. I'm using the SqlProgrammabilityProvider from FSharp.Data.SqlClient to do some data work. However when I point the connection string at my databse it isn't able to generate any completions after Table\n\n{% codeblock lang:powershell %}\n [<Literal>]\n    let connectionString = @\"Data Source=localhost\\SQL2014;Initial Catalog=Completions;Integrated Security=True;MultipleActiveResultSets=True;Timeout=360\"\n\n    type Completions = SqlProgrammabilityProvider<connectionString>\n    \n    let TagExists(projectId, tagNumber) =\n        let b = new Completions.dbo.Tables. //ugh, won't complete\n{% endcodeblock %}\n\nI tried pointing it at different databases on the same server and it worked just fine. I finally remembered that one of the tables in the database made use of spatial types. I experimented by deleting the tables which made use of the coordinates and all of a sudden things started working!\n\nObviously deleting the table wasn't a great solution so I pulled in the required packages which I think are\n\n```\nInstall-Package microsoft.SqlServer.Types\nInstall-Package System.Spatial\n```\n\nI was now able to access the tables as expected.","categories":[{"name":"fsharp","slug":"fsharp","permalink":"https://westerndevs.com/categories/fsharp/"}],"tags":[]},{"title":"Prairie Dev Con 2016 Presentation Materials","authorId":"kyle_baley","slug":"Prairie-Dev-Con-2016-Presentation-Materials","date":"2016-04-18 20:14:02+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"conferences/Prairie-Dev-Con-2016-Presentation-Materials/","link":"","permalink":"https://westerndevs.com/conferences/Prairie-Dev-Con-2016-Presentation-Materials/","excerpt":"Materials from Death to the Batch Job and Docker presentations","raw":"---\nlayout: post\ntitle: \"Prairie Dev Con 2016 Presentation Materials\"\ncategories:\n  - conferences\ndate: 2016-04-18 16:14:02\nexcerpt: \"Materials from Death to the Batch Job and Docker presentations\"\nauthorId: kyle_baley\n---\n\nIf you don't know by now, [we love Prairie Dev Con](http://www.westerndevs.com/conferences/Prairie-Dev-Con-What-you-need-to-know/) at Western Devs. And if you attended last week, hopefully you saw why. It's a high quality conference in a most unlikely setting. Here are the materials from each of my presentations.\n\n### Death to the Batch Job\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/yVLz0SMbBhETVc\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/KyleBaley/death-to-the-batch-job\" title=\"Death to the Batch Job\" target=\"_blank\">Death to the Batch Job</a> </strong> from <strong><a href=\"//www.slideshare.net/KyleBaley\" target=\"_blank\">Kyle Baley</a></strong> </div>\n\nIn addition to the slides, you can find the source code for the presentation [here](https://github.com/andreasohlund/DeathToTheBatchJob). There's also a [blog post](http://particular.net/blog/death-to-the-batch-job) that expands a little on the ideas.\n\n### Docker\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/6s34SBJRwOt5nz\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/KyleBaley/docker-for-people-who-have-heard-of-docker-but-think-its-just-this-weird-linux-thing-that-doesnt-impact-me\" title=\"Docker For People Who Have Heard of Docker But Think It&#x27;s Just This Weird Linux Thing That Doesn&#x27;t Impact Me\" target=\"_blank\">Docker For People Who Have Heard of Docker But Think It&#x27;s Just This Weird Linux Thing That Doesn&#x27;t Impact Me</a> </strong> from <strong><a href=\"//www.slideshare.net/KyleBaley\" target=\"_blank\">Kyle Baley</a></strong> </div>\n\nThe two sample apps I showed during the presentation are:\n\n- [AzureCodeCamp](http://github.com/stimms/AzureCodeCamp)\n- [GenFu](https://github.com/MisterJames/GenFu)\n\nFor the first, I showed how to connect the app to PostgreSQL which can be running either locally or in Docker. For the second, I showed how to create a container for a .NET Core application running in Linux. If you want to play with this container, you can get it [here on Docker Hub](https://hub.docker.com/r/kbaley/genfu/).\n\n### Pre-game Show\n\nIn addition to the presentations, [Donald](http://localhost:4000/bios/donald_belcham/) and I (but mostly me), with the help of the Winnipeg .NET User Group, hosted a fishbowl discussion and an evening of drinks and food the night before the conference courtesy of Particular Software. During the talk, we covered ways to build a solid team, the importance of company culture, and good ways to vet people. It was a great chat and it was nice to see so many people weigh in with their experience.\n\n### I'd like to thank...\n\nIt's hard to overstate the value this \"big little conference\" has for me. It has all the benefits of a larger conference while still being accessible to both the attendees and speakers. [D'Arcy Lussier](http://localhost:4000/bios/darcy_lussier/) is a force to be reckoned with in the conference circuit.\n\nMany thanks to the [Winnipeg .NET User Group](http://www.winnipegdotnet.org/) for their help with the fishbowl event.\n\nFinally, a big thank you to those of you who came to the the conference itself and continue making it great.","categories":[{"name":"conferences","slug":"conferences","permalink":"https://westerndevs.com/categories/conferences/"}],"tags":[]},{"title":"Prairie Dev Con 2016 Wrapup","authorId":"donald_belcham","slug":"PrDC-Wrapup-DB","date":"2016-04-18 17:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"conferences/PrDC-Wrapup-DB/","link":"","permalink":"https://westerndevs.com/conferences/PrDC-Wrapup-DB/","excerpt":"Slides and other materials from PrDC 2016","raw":"---\ntitle: Prairie Dev Con 2016 Wrapup\nlayout: post\ncategories:\n  - conferences\nauthorId: donald_belcham\ndate: 2016-04-18 13:56:56 \nexcerpt: \"Slides and other materials from PrDC 2016\"\n---\n\n![](http://content.igloocoder.com/images/PrDCLogo_Small.png)  \nLast week I spoke at Prairie Dev Con in wonderful Winnipeg and while the city lived up to it's moniker of \"Winterpeg\", the conference was fantastic! \n\nI noticed that there was a lot of buzz around microservices and distributed systems at the conference this year. It seemed like there were endless conversations happening in the halls and at meals about the topic. While there were a lot of sessions that touched on the topic, or topics in the same space, there were a lot of areas that went untouched. For those that missed my sessions or those that are looking for a refresher here are the slide decks.\n\n### Slides\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/uQ4xqsIBJy8nHM\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/igloocoder/microservices-a-gentle-introduction\" title=\"Microservices: A Gentle Introduction\" target=\"_blank\">Microservices: A Gentle Introduction</a> </strong> </div>\n\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/K2ur1a17XwPhmQ\" width=\"595\" height=\"485\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/igloocoder/microservices-the-nitty-gritty\" title=\"Microservices: The Nitty Gritty\" target=\"_blank\">Microservices: The Nitty Gritty</a> </strong>  </div>\n\n### Additional materials\nIf you're interested in diving deeper into microservices I have a Github repo that contains all the articles and videos that I've absorbed in the last year. If you find materials that are missing please send me a pull request. I'd love to read/watch more on the subject. [https://github.com/dbelcham/microservice-material](https://github.com/dbelcham/microservice-material)\n","categories":[{"name":"conferences","slug":"conferences","permalink":"https://westerndevs.com/categories/conferences/"}],"tags":[]},{"title":"Thoughts on Microsoft Build 2016","slug":"MS-Build","date":"2016-04-10 00:18:38+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/MS-Build/","link":"","permalink":"https://westerndevs.com/podcasts/MS-Build/","excerpt":"The Western Devs pontificate on the latest Microsoft Build announcements","raw":"---\nlayout: podcast\ntitle: Thoughts on Microsoft Build 2016\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-ms-build.mp3\n  length: '44:51'\n  filesize: 43063681\n  libsynId: 4284045\n  anchorFmId: Thoughts-on-Microsoft-Build-2016-evqdi6\nparticipants:\n  - darcy_lussier\n  - dave_paquette\n  - lori_lalonde\n  - amir_barylko\n  - donald_belcham\n  - kyle_baley\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Microsoft Build|https://build.microsoft.com/'\n  - 'The Verge on MS Build|http://www.theverge.com/2016/3/30/11317924/microsoft-event-news-recap-hololens-windows-10-build-2016'\n  - 'Twitter bot gone wrong|http://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist'\n  - 'Mars Hololens demo|https://www.youtube.com/watch?v=DXT-ynvI3Lg'\n  - 'Case Western Reserve University and Hololens|http://case.edu/hololens/'\ndate: 2016-04-09 20:18:38\nrecorded: 2016-04-01\nexcerpt: The Western Devs pontificate on the latest Microsoft Build announcements\n---\n\n### Synopsis\n\n* Bots: The next level or glorified clippy?\n* Xamarin: Free! Mobile apps for everyone!\n* Hololens shipped\n* XBox One is now a developer kit\n* Unified store\n* Cognitive API/Project oxford\n* Azure Functions\n* Linux on Windows\n* Project Centennial\n* Inking capabilities - Anniversary update\n* Do you wish you had gone?\n* Hands on labs\n* On giving hardware away at conferences\n* Wherefor Windows Phone?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"The Monsters Weekly - Episode 20 - 'Docs and GitHub Repos'","authorId":"aspnet_monsters","slug":"The-Monsters-Weekly-Episode-20-Docs-and-GitHub-Repos","date":"2016-04-02 15:58:36+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Monsters-Weekly/The-Monsters-Weekly-Episode-20-Docs-and-GitHub-Repos/","link":"","permalink":"https://westerndevs.com/Monsters-Weekly/The-Monsters-Weekly-Episode-20-Docs-and-GitHub-Repos/","excerpt":"Join the ASP.NET Monsters for a quick tour of the Documentation and GitHub repos for ASP.NET Core.","raw":"---\nlayout: post\ntitle: \"The Monsters Weekly - Episode 20 - 'Docs and GitHub Repos'\"\ncategories:\n  - Monsters Weekly\ndate: 2016-04-02 11:58:36\ntags: \n  - ASP.NET Core\n  - GitHub\nauthorId: aspnet_monsters\noriginalurl: http://aspnetmonsters.com/2016/03/monsters-weekly/ep20/\n---\n\nJoin the ASP.NET Monsters for a quick tour of the Documentation and GitHub repos for ASP.NET Core.\n\n<!--more-->\n<iframe src=\"https://channel9.msdn.com/Series/aspnetmonsters/Episode-20-Docs-and-GitHub-Repos/player\" width=\"640\" height=\"360\" allowFullScreen frameBorder=\"0\"></iframe>\n\nIn this episode, Dave shows us how to browse the code base, where to log issues and how to engage with the ASP.NET development team on GitHub [github.com/aspnet](https://github.com/aspnet). We take a quick spin through the fancy new Read the Docs site 90 [docs.asp.net](http://docs.asp.net).","categories":[{"name":"Monsters Weekly","slug":"Monsters-Weekly","permalink":"https://westerndevs.com/categories/Monsters-Weekly/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"},{"name":"GitHub","slug":"GitHub","permalink":"https://westerndevs.com/tags/GitHub/"}]},{"title":"Diabetes technology","slug":"Diabetes-technology","date":"2016-03-27 21:01:07+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Diabetes-technology/","link":"","permalink":"https://westerndevs.com/podcasts/Diabetes-technology/","excerpt":"Special guest, Jake Belcham, joins the Western Devs as they review the state of diabetes technology today and what's coming in the future","raw":"---\nlayout: podcast\ntitle: \"Diabetes technology\"\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-diabetes-technology.mp3\n  length: '39:14'\n  filesize: 37658214\n  libsynId: 4251678\n  anchorFmId: Diabetes-technology-evqdhm\nparticipants:\n  - donald_belcham\n  - james_chambers\n  - kyle_baley\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Night scout|http://www.nightscout.info/'\n  - 'JDRF|http://www.jdrf.ca/'\n  - 'Open source diabetes|http://www.opensourcediabetes.org/'\ndate: 2016-03-27 17:01:07\nrecorded: 2016-02-12\nexcerpt: Special guest, Jake Belcham, joins the Western Devs as they review the state of diabetes technology today and what's coming in the future\n---\n\n### Synopsis\n\n* Type 1 vs. Type 2\n* Determining your blood glucose level\n* Insulin pump vs. insulin injections\n* The joys of sleeping in and being hungry\n* Early insulin pumps\n* Dealing with statistical variability\n* Continuous glucose meters\n* Trends vs. point in time\n* Communication (or lack thereof) between meters and pumps\n* Factoring in other measures (e.g. heart rate)\n* Artificial pancreas: more mashup than moonshot\n* Remote monitoring\n* The importance of intuition\n* Using machine learning with the data\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Idiomatic Iterative Design","authorId":"amir_barylko","slug":"idiomatic-iterative-design","date":"2016-03-23 20:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Design/idiomatic-iterative-design/","link":"","permalink":"https://westerndevs.com/Design/idiomatic-iterative-design/","excerpt":"Lately I have been having fun solving the AdventOfCode. I mainly used Haskell to solve each day so I can learn a bit about Haskell and as a byproduct VIM as well. In the last Ruby Meetup we used Day 7 to illustrate how to use Rantly for properties testing. It was my first try to solve Day 7 using Ruby, and I wanted to find an elegant, idiomatic, short way to do it...","raw":"---\ntitle: Idiomatic Iterative Design\nlayout: post\ntags:\n  - design\n  - ruby\n  - functional\ncategories:\n  - Design\nauthorId: amir_barylko\ndate: 2016-03-23 16:00:00 \noriginalurl: http://orthocoders.com/blog/2016/02/16/idiomatic-iterative-design/\n---\n\nLately I have been having fun solving the [AdventOfCode](http://adventofcode.com). I mainly used _Haskell_ to solve each day so I can learn a bit about _Haskell_ and as a byproduct _VIM_ as well.\n\nIn the last [Ruby Meetup](http://winnipegrb.org) we used [Day 7](http://adventofcode.com/day/7) to illustrate how to use [Rantly](https://github.com/abargnesi/rantly) for properties testing.\n\nIt was my first try to solve _Day 7_ using [Ruby](https://github.com/amirci/adventofcode_rb), and I wanted to find an _elegant_, _idiomatic_, _short_ way to do it...\n\n<!-- more -->\n\n## Before we start\n\nFirst I want to give a very big shout out to [Eric Wastl](http://was.tl/) for creating the [Advent Of Code](http://adventofcode.com/).\n\nTry it out, and if you like it let [Eric](https://twitter.com/ericwastl) know!\n\n## Exploring the problem\n\nSPOILER ALERT: Yes, we are going to talk about _Day 7_ and how to implement the solution. Feel free to do it on your own first.\n\nThe problem describes a circuit board with instructions to apply signals to circuits using bitwise logic operations.\n\nThe operations are:\n\n    - 123 -> x means that the signal 123 is provided to wire x.\n    - x AND y -> z means that the bitwise AND of wire x and wire y \n      is provided to wire z.\n    - p LSHIFT 2 -> q means that the value from wire p is left-shifted by 2 \n      and then provided to wire q.\n    - NOT e -> f means that the bitwise complement of the value from \n      wire e is provided to wire f.\n\n    Other possible gates include OR (bitwise OR) and RSHIFT (right-shift).\n\nI implemented the solution in [Haskell](https://github.com/amirci/adventofcode_hs) and found out that not only the parsing was the challenge but also the fact that you get a list of instructions that can be in any order, so some instructions when evaluated may result in having no signal (no value).\n\nFor example, let's consider the following sequence of instructions:\n\n    lx -> a\n    456 -> lx\n\nWhen evaluating the first instruction, the wire `lx` has no signal yet, so it has to be reevaluated later.\n\nOf course you could create an evaluation tree but that seemed a bit too much for the problem at hand. So I decided to do the following:\n\n1. Parse instructions into commands\n2. Repeat evaluating commands until all pass\n3. Return the value of wire \"a\" from the board\n\n## The _classy_ way\n\nAs soon as I read about _instructions_ my mind started to race thinking about _parsing_ and _patterns_.\n\nMy first thought was I could use the [Interpreter Pattern](https://en.wikipedia.org/wiki/Interpreter_pattern) to build a hierarchy of classes to evaluate expressions. But it seemed like an overkill.\n\nTherefore, I decided to use classes to represent each command:\n\n``` ruby\nclass AndCmd    ; end\nclass OrCmd     ; end\nclass LShiftCmd ; end\nclass RShiftCmd ; end\nclass NotCmd    ; end\nclass EvalCmd   ; end\n```\n\nEach _class_ has two methods with clear responsibilities:\n\n- `parse(token1, token2, token3..., wire)` Class factory method that parses the command and returns an instance of the command.\n\n- `wireIt(board)` Instance method that evaluates the command to assign the result of the operation to the target wire.\n\nThe constructor of the class stores the operands and target wire.\n\n``` ruby\nclass AndCmd\n  def initialize(lhs, rhs, wire) ; @lhs, @rhs, @wire = [lhs, rhs, wire] end\n  def wireIt(board) \n    # asign the @lhs & @rhs to the board\n  end\n  def self.parse(....)\n    # parse the string to match the AND command and return a new instance\n  end\nend\n```\n\nThe `parse` factory method receives the expected tokens. If the `cmd` token matches the string `\"AND\"` then returns an instance of `AndCmd`.\n\n``` ruby\ndef self.parse(x, cmd, y, arrow, z)\n  AndCmd.new(x, y, z) if cmd == \"AND\"\nend\n```\n\n### Abstracting the board\n\nEvaluating the command was more complex because the values could be undefined. Some kind of validation was necessary.\n\nI started using a `Hash` as a _CircuitBoard_ and then checking if the values were defined. It got a bit more complicated when I realized I had to parse values, because I could get `lx AND lb` and also `1 AND ll`.\n\nInspired by my _Haskell_ solution I thought that a class that implements a _short circuit_ evaluation could be very useful. That way, if one of the involved values was not defined the whole command was undefined.\n\nTo simplify board access and evaluation I created a `Board` class that handles the assignment plus the lookup.\n\nThe `assign` method takes a block that gets evaluated if all the values are defined, otherwise it is ignored.\n\n``` ruby\nclass Board\n  attr_reader :wires                                           \n  def initialize ; @wires = {} end                             \n  def [](y) ; @wires[y] end\n\n  def assign(wire, *exprs)                                     \n    values = exprs.map { |exp| value exp }                     \n    return nil if values.any?(&:nil?)Â·                         \n    @wires[wire] = block_given? ? yield(*values) : values[0]   \n  end                                                          \n\n  privateÂ·                                                     \n  def value(exp)                                               \n    /\\d+/.match(exp) ? exp.to_i : @wires[exp]                  \n  end                                                          \nend                                                            \n```\n\nThis simplifies things quite a bit. Now the `wireIt` implementation only applies the operation when all the values are defined:\n\n``` ruby\ndef wireIt(board)\n  board.assign(@wire, @lhs, @rhs) { |l, r| l & r }\nend\n```\n\n### Parsing instructions into commands\n\nTo parse the string I created a `parse` method that splits the instruction into tokens (words) and finds a parsing factory method with the same amount of parameters that returns an actual instance of the command. \n\nThis is similar to a [chain of responsibility](https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern) where the parser tries to parse the instruction. If the parser cannot do it then the parser passes the instruction to the next parser in the chain until one of them succeeds.  If no parser succeeds, `nil` is returned.\n\n\n``` ruby\ndef parse(instruction)                                              \n  tokens = instruction.split                                        \n  [AssignCmd, AndCmd, OrCmd, RightShiftCmd, LeftShiftCmd, NotCmd]   \n    .map    { |k| k.method(:parse) }                                \n    .select { |m| m.parameters.length == tokens.length }            \n    .map    { |m| m.call(*tokens) }                                 \n    .find   { |cmd| cmd }Â·                                          \nend                                                                 \n```\n\n### The main loop\n\nThe last bit of the exercise is to keep evaluating all commands until there are no failing commands left.\n\n``` ruby\ndef wire(instructions)                                  \n  cmds = instructions.map(&method(:parse))              \n  board = Board.new                                     \n  while !cmds.empty?                                    \n    cmds = cmds.select { |cmd| cmd.wireIt(board).nil? } \n  end                                                   \n  board                                                 \nend                                                     \n```\n\nYou can see the complete source [here](https://github.com/amirci/adventofcode_rb/blob/master/lib/day7_v2.rb).\n\n## The _Ruby_ way\n\nAfter finishing the implementation using classes I started to wonder what would be more idiomatic to _Ruby_.\n\nClasses are fine, but I wanted to explore the _dynamic_ aspect of _Ruby_ and use `eval`.\n\n### Parsing instructions\n\nInstead of having a `Class` factory method to parse, I declared `parse_xxx` functions that return a string to be evaluated later for the expected command.\n\n``` ruby\ndef board(exp) ; \"board['#{exp}']\" end\n\ndef expr(exp) ; /\\d+/.match(exp) ? exp : board(exp) end\n\ndef parse_and(x, cmd, y, arrow, z) \n  cmd == \"AND\" && \"#{board z} = #{expr x} & #{board y}\" \nend\n```\n\nThe main `parse` function now uses the `parse_xxx` functions instead. The parse chooses the function that has the same amount of parameters as the tokens in the instructions and also returns a string with the expression to be evaluated.\n\n``` ruby\ndef parse(instruction)\n  tokens = instruction.split\n  [:parse_assign, :parse_and, :parse_or, :parse_rshift, :parse_lshift, :parse_not]\n  .map    { |s| method(s) }\n  .select { |m| m.parameters.length == tokens.length }                           \n  .map    { |m| m.call(*tokens) }\n  .find   { |cmd| cmd }\nend\n```\n\n### The main loop\n\nThe main loop is very similar to the main loop of the _classy version_ with the difference that to get the actual value, `eval` is called for every command. If the command succeeds, the evaluation will return some kind of number.\n\n``` ruby\ndef wire(instructions)                                                     \n  cmds = instructions.map { |line| self.parse line }                       \n  board = {}                                                               \n  while !cmds.empty?Â·                                                      \n    cmds = cmds.reject { |cmd| (eval(cmd) rescue nil).kind_of? Fixnum }    \n  end                                                                      \n  board                                                                    \nend                                                                        \n```\n\n### The result\n\nThe solution seems simpler than using classes. The strings make each command quite transparent.\n\nProbably, even the parsing could be combined into one function and reduce the amount of functions in general.\n\nYou can see the complete solution [here](https://github.com/amirci/adventofcode_rb/blob/master/lib/day7_v3.rb).\n\n\n## The _functional_ way\n\nAfter the _Classy_ and _Ruby_ way I wanted to see if I could be a bit more functional.\n\nThe approach this time was to parse the instruction into a `lambda` that will evaluate the actual command.\n\nI decided to reuse the `Board` class from the first solution to make the value evaluation easier and implement a short circuit when a value is not yet defined.\n\n### Parsing instructions into commands\n\nThird timeâ€™s the charm! I reduced the amount of parsing functions by having a binary parsing function that uses a hash to decide which operation to apply.\n\n``` ruby\ndef parse_bin(x, cmd, y, arrow, wire)                                 \n  op = {\"AND\" => :&, \"OR\" => :|, \"LSHIFT\" => :<<, \"RSHIFT\" => :>>}[cmd]\n  op && -> (board) { board.assign(wire, x, y, &op) }\nend\n```\n\nThe hash lookup  decides which operation to use. \n\nThe general `parse` function is similar to the _Ruby_ way:\n\n``` ruby\ndef parse(instruction)\n tokens = instruction.split\n [:parse_assign, :parse_bin, :parse_not]\n   .map    { |s| method(s) }\n   .select { |m| m.parameters.length == tokens.length }\n   .map    { |m| m.call(*tokens) }\n   .find   { |cmd| cmd }\nend\n```\n\n### The main loop\n\nInstead of using `eval`, the `call` method is used on each `lambda` to evaluate the command.\n\n``` ruby\ndef wire(instructions)\n  cmds = instructions.map { |line| parse line }\n  board = Board.new\n  while !cmds.empty?\n    cmds = cmds.select { |cmd| cmd.call(board).nil? }\n  end\n  board\nend\n```\n\nYou can see the entire solution [here](https://github.com/amirci/adventofcode_rb/blob/master/lib/day7_v4.rb).\n\n## The verdict\n\nThe version I like the most is the `eval` version because it is simple and straightforward.\n\nThe second best option, in my opinion, is the _functional_ version because it uses  the bitwise logic operators as functions.\n\nLast but not least is the _classy_ version. Using instances of classes does not seem to be necessary for this exercise because all parameters can be captured when creating the `lambda` closure for each instruction.\n\nWhich one is would __you__ choose?\n\n\n","categories":[{"name":"Design","slug":"Design","permalink":"https://westerndevs.com/categories/Design/"}],"tags":[{"name":"design","slug":"design","permalink":"https://westerndevs.com/tags/design/"},{"name":"ruby","slug":"ruby","permalink":"https://westerndevs.com/tags/ruby/"},{"name":"functional","slug":"functional","permalink":"https://westerndevs.com/tags/functional/"}]},{"title":"Certificates for Everyone! Let's Encrypt in Azure with ASP.NET Core","authorId":"dave_white","slug":"Lets-Encrypt-ASPNET-Core","date":"2016-03-22 20:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Tools/Lets-Encrypt-ASPNET-Core/","link":"","permalink":"https://westerndevs.com/Tools/Lets-Encrypt-ASPNET-Core/","excerpt":"This may have been one of the most exciting things (from a web site owner's perspective) to happen in quite a while...","raw":"---\ntitle: Certificates for Everyone! Let's Encrypt in Azure with ASP.NET Core\nlayout: post\ntags:\n  - asp.net\n  - let's encrypt\n  - azure\ncategories:\n  - Tools\nauthorId: dave_white\ndate: 2016-03-22 16:00:00 \nexcerpt: \"This may have been one of the most exciting things (from a web site owner's perspective) to happen in quite a while...\"\n---\n\nThis may have been one of the most exciting things (from a web site owner's perspective) to happen in quite a while, and something I think the industry\nhas been hoping would happen for a long time. Free SSL certificates for everyone! With the creation of [letsencrypt.org][1], an organization whose goal is to \nmake certificate usage free and easy, we are finally freed from the shackles of organizations like GoDaddy or Verisign who charge (sometimes exorbitant) fees\njust so that an individual or organization can have the ability encrypt their web traffic.\n\n<!-- more -->\n\nNow the tricky thing with Let's Encrypt is the automation part. Because of how Let's Encrypt uses automation to provision certificates, it was originally built to\nwork with Apache/Linux based web servers. This made anyone who used Azure or IIS feel like the kid on the outside looking in. Luckily, nothing that Let's Encrypt\ndoes is voodoo magic and it didn't take long for the Azure community to build the necessary parts to get this working in Azure.\n\nThere is a fantastic post from [Nik Molnar][2] that describes\nhow to get this working on Azure. I definitely want to talk about Let's Encrypt on Azure, but I don't want to take away from Nik's post, so go there and  check it out.\n\nWhat I'm going to add though is a bit of PowerShell goodness around some of his steps that makes it a bit easier to get Let's Encrypt up and running on your Azure\nwebsite. I'm also going to add a couple tidbits about getting it working on ASP.NET Core.\n\n#### Pre-requisites\n\nJust going to re-iterate [Nik's prerequisites][2]:\n1. Scale up your Azure Website to at least an S1 (1 dedicated IP, 5 SNI)\n2. Acquire your custom domain. The _{domain name here}_.azurewebsites.net domains are all secured with by a Microsoft certificate so if you aren't planning on using your own domain name, you can just stop now.\n3. Create a storage account for the Let's Encrypt Site extension to use\n4. Put your storage account conne   ction strings into two Application Settings on your website.\n5. Register a Service Princpal. This is the identity that is allowed to execute the web jobs the the site extension uses to do the work \n\n## PowerShell\n\nAgain, as per [Nik's post][2], you'll need the [Azure PowerShell module][3] before we get started. \n\nIn an Azure .ps1 (script) or .psm1 (module), or in PowerShell ISE, paste the function below. \n\n{% codeblock lang:powershell %}\nfunction Set-AllAzureDetailsForLetsEncrypt() {\n<# \n    .SYNOPSIS\n    Set up an Azure Website to be able to use the Let's Encrypt Site Extension\n    .DESCRIPTION\n    This function will use the information provided to create a new connection between the Let's Encrypt Site extension and your website.\n    It will then use this user/connection to setup the certificate from Let's Encrypt on your web site\n    .EXAMPLE\n    Set-AllAzureDetailsForLetsEncrypt -subscriptionId {guid} -uniqueUri {Any unique Url} -strongPassword {password} -displayName {Name of Application} -resourceGroupName {Name of ResourceGroup to work in}\n    .EXAMPLE\n    Set-AllAzureDetailsForLetsEncrypt -subscriptionId 00000000-0000-0000-0000-000000000000 -uniqueUri \"http://myapplication-091820398123\" -strongPassword \"P@ssw0rd\" -displayName \"Let's Encrypt Site Extension\" -resourceGroupName \"Default-Web-WestUS\"\n    .PARAMETER subscriptionId\n    (optional) The subscriptionId to attempt to use during the login\n    .PARAMETER uniqueUri\n    The uniqueUri of the \"app\" that we are creating in Azure Active Directory \n    .PARAMETER strongPassword\n    The strong password of the \"app\" that we are creating in Azure Active Directory \n    .PARAMETER displayName\n    (optional) The name of the application that we will be creating in Azure Active Directory (defaults to \"Let's Encrypt Site Extension\")\n    .PARAMETER resourceGroupName\n    The name of the ResourceGroup that this app lives in.\n#>\n\n    [CmdLetBinding()]\n    param(\n        [parameter(Mandatory = $false)]\n        [guid]$subscriptionId,\n        [parameter(Mandatory = $true)]\n        [string]$uniqueUri,\n        [parameter(Mandatory = $true)]\n        [string]$strongPassword,\n        [parameter(Mandatory = $false)]\n        [string]$displayName,\n        [parameter(Mandatory = $true)]\n        [string]$resourceGroupName\n    )\n    begin{\n        if([string]::IsNullOrEmpty($displayName)){ $displayName = \"Let's Encrypt Site Extension\" }\n    }\n    process{\n         # log into your Azure Account for a given subscription\n         try{\n            if($subscriptionId -eq $null){\n                $account = Login-AzureRMAccount\n            } else {\n                $account = Login-AzureRMAccount -SubscriptionId $subscriptionId\n            }\n         }catch{\n            Write-Error \"Failed to log into Azure with the credentials or SubscriptionId provided.\"\n            break             \n         }\n         \n         # create a new active directory application\n         $app = New-AzureRmADApplication -DisplayName $displayName -HomePage $uniqueUri -IdentifierUris $uniqueUri -Password $strongPassword\n         \n         # create a new Service Principal for your application\n         New-AzureRmADServicePrincipal -ApplicationId $app.ApplicationId\n         \n         # assign your Service princpal as a contributor\n         New-AzureRmRoleAssignment -RoleDefinitionName Contributor -ServicePrincipalName $app.ApplicationId\n         \n         # store these details to transfer to the extension which will use the service principal to do it's work\n         $letsEncrypt = @{}\n         $letsEncrypt.Tenant = $account.Context.Tenant.TenantId\n         $letsEncrypt.SubscriptionId = $account.Context.Subscription.SubscriptionId\n         $letsEncrypt.ClientId = $app.ApplicationId \n         $letsEncrypt.ClientSecret = $strongPassword\n         $letsEncrypt.ResourceGroupName = $resourceGroupName\n         \n         # return the letsEncrypt object with all of our details to make it easy to clip them into the website extensions fields \n         $letsEncrypt\n    }\n    end{}\n} #end function Set-AllAzureDetailsForLetsEncrypt\n{% endcodeblock %}\n\nAfter you've pasted this function into a PowerShell script file or directly into a session type in the following:\n{% codeblock lang:powershell %}\n$letsEncrypt = Set-AllAzureDetailsForLetsEncrypt -subscriptionId <Insert SubscriptionId Here> -uniqueUri \"http://myapplication-091820398123\" -strongPassword \"P@ssw0rd\" -displayName \"Let's Encrypt Site Extension\" -resourceGroupName \"Default-Web-WestUS\"\n{% endcodeblock %}\n\nYou do not need to provide the SubscriptionId if your account/login is only associated with one subscription. Also, you do not need to provide a DisplayName if you don't mind the display name being \"Let's Encrypt Site Extension\". You must provide the other parameters.\n\nOnce you have run through the script, you should have an object in the `$letsEncrypt` variable that has everything you need for entering the data in the Site Extension.\n```\n$letsEncrypt \n\nName                           Value                                                            \n----                           -----                                                            \nSubscriptionId                 11111111-2222-3333-4444-555555555555                             \nResourceGroupName              Default-WestUs-1                                                 \nTenant                         11111111-2222-3333-4444-555555555555                             \nClientSecret                   P@ssw0rd                                                         \nClientId                       00000000-0000-0000-0000-000000000000                                                                \n```\n\nNow you will be able to type `$letsEncrypt.Tenant | clip` to get the TenantId for pasting into the Site Extension fields. You can do this for each\nfield that is required for the extension to run.\n\n## ASP.NET Core\nASP.NET Core (formerly know as ASP.NET 5/MVC 6) had a different approach to handling static files, which are required for Let's Encrypt to work. Basically, \nwhat happens is the WebJob gets some files from Let's Encrypt. Let's Encrypt will then use the DNS entry and this \"well-known\" location on your site to see if the files\nare there. If they are there, you own the domain, and the certificate can be approved automatically. If you didn't own the domain, and hadn't setup the Let's Encrypt Site Extension\nto do all of this work, none of this would have worked and it could be assumed that you do not own the domain.\n\nIf you look in your Kudo dashboard after the Let's Encrypt Site Extension has run, you'll see the following you your folder structure:\n\nd:/home/site/wwwroot/.well-known/acme-challenge\n\nAnd you'll see the challenge files in the last folder\n\n![.well-known-acme-challenge](http://i.imgur.com/sQcRNFX.png)\n\n#### The Thing I had Trouble With\nI couldn't find a good example at the time of how to allow Let's Encrypt to get these .well-known files using ASP.NET Core. So I created a controller that basically returned the contents when asked for a specific route.\n\nThe URI that Let's Encrypt will look for these well-known files is http://{mydomainname}.com/.well-known/acme-challenge \n\n{% codeblock lang:csharp %}\n[Route(\".well-known\")]\npublic class WellKnownController : Controller\n{\n    public WellKnownController(IHostingEnvironment env)\n    {\n        Env = env;\n    }\n\n    private IHostingEnvironment Env { get; }\n\n    [HttpGet(\"acme-challenge/{id}\")]\n    [Produces(\"text/json\")]\n    public IActionResult Get(string id)\n    {\n        var content = string.Empty;\n        var path = Env.WebRootPath;\n        var fullPath = Path.Combine(path, @\".well-known\\acme-challenge\");\n        var dirInfo = new DirectoryInfo(fullPath);\n        var files = dirInfo.GetFiles();\n        foreach (var fileInfo in files)\n        {\n            if (fileInfo.Name == id)\n            {\n                using (var file = System.IO.File.OpenText(fileInfo.FullName))\n                {\n                    return Ok(file.ReadToEnd());\n                }\n            }\n        }\n        return Ok(content);\n    }\n}\n{% endcodeblock %}\n\n#### Update\nAt the time of this writing, I haven't had a chance to confirm if this works since the Let's Encrypt files do not have an extension.\n[Handling Static Files in ASP.NET Core][4]\n\n## That's It!\nHopefully, between [Nik's fantastic post][2] and this post, you should have a fairly good understanding of what has been happening, and you've got your \nASP.NET Core site secured and up and running on Azure using [Let's Encrypt][1]!!! \n\n[1]: https://letsencrypt.org\n[2]: https://gooroo.io/GoorooTHINK/Article/16420/Lets-Encrypt-Azure-Web-Apps-the-Free-and-Easy-Way/20047#.Vuron4-cGM9\n[3]: https://azure.microsoft.com/en-us/documentation/articles/powershell-install-configure/\n[4]: http://docs.asp.net/en/latest/fundamentals/static-files.html\n","categories":[{"name":"Tools","slug":"Tools","permalink":"https://westerndevs.com/categories/Tools/"}],"tags":[{"name":"azure","slug":"azure","permalink":"https://westerndevs.com/tags/azure/"},{"name":"asp.net","slug":"asp-net","permalink":"https://westerndevs.com/tags/asp-net/"},{"name":"let's encrypt","slug":"let-s-encrypt","permalink":"https://westerndevs.com/tags/let-s-encrypt/"}]},{"title":"Truly Ergonomic Keyboard","authorId":"donald_belcham","slug":"truly-ergonomic-review","date":"2016-03-19 21:25:28+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Ergonomics/truly-ergonomic-review/","link":"","permalink":"https://westerndevs.com/Ergonomics/truly-ergonomic-review/","excerpt":"It was time for a new keyboard and I was looking for something mechanical and ergonomic.","raw":"---\nlayout: post\ntitle: Truly Ergonomic Keyboard\ntags:\n  - Ergonomics\n  - Office\ncategories:\n  - Ergonomics\ndate: 2016-03-19 17:25:28\nauthorId: donald_belcham\nexcerpt: It was time for a new keyboard and I was looking for something mechanical and ergonomic.\n---\nBack in February I joined a bunch of the other WesternDevs to [talk about Ergonomics](http://www.westerndevs.com/podcasts/Ergonomics/) and one of the topics we touched on was ergonomic keyboards. As I said in the podcast, years ago when I worked on a factory line I got carpal tunnel really bad. Ever since I have to watch the peripherals that I use when I work for long periods on the computer. If I don't, and a good example of this is if I use a built in laptop keyboard for a two to three 8+ hour days in a row, I will get crippling pain in my fore-arm and elbow. Since I spent no less than 8 hours a day in front of a computer, choosing a keyboard and mouse combination is pretty important for me.\n\nHistorically I've used a [Microsoft Wireless Comfort Keyboard 5000](https://www.amazon.ca/Microsoft-Comfort-Blue-Track-Desktop/dp/B002DY7M66/ref=sr_1_fkmr1_1?ie=UTF8&qid=1458259452&sr=8-1-fkmr1&keywords=ms+wireless+comfort+5000) and it has served me well. Make no mistake, this is **not** an ergonomic keyboard. It simply has a few keys in the middle that are wider than normal to provide a bit of a spread between your hands.\n\nFellow WesternDev [Simon Timms](http://www.westerndevs.com/bios/simon_timms/) makes use of a clickety-clackety keyboard and swears up and down by it's [Cherry keys](http://cherryamericas.com/product-category/keyswitch/). Sure it makes him sound like he's got a teleprinter machine pumping out the news in the back of his office, but for some reason the idea of a mechanical keyboard appealed to me. I don't think I've had one since back in my 386 days.\n\nAfter a bunch of research on ergonomic mechanical keyboards I settled on getting the [Truly Ergonomic Keyboard](https://www.trulyergonomic.com) (TEK). The only real decision you have to make when ordering is choosing between two models that have slightly different key layouts (double vs single keys in the lower corners of the keyboard).\n\nThe first two things that I noticed when the keyboard arrived were the size of it and the weight. The keyboard is narrower than the main keypad on the Comfort 5000 (note that the Comfort 5000 also has a number pad that the TEK doesn't). The TEK is about 1.5 times heavier than the Comfort 5000 which not only makes it feel more substantial, it also makes it harder to push it around on your desk surface.\n\n![](http://content.igloocoder.com/images/TEK_size_comparison.jpg)\n\nGetting started with the TEK is no more difficult than plugging in the USB cord. There's no driver disk or software to install. Windows (10 in my case) recognized it straight away and I was off to the races....snail races that is.\n\nTyping on the TEK takes a bit of getting used to if you're a touch typist. The first touch typing issue that I ran into was the different orientation of the keys on the keyboard. On traditional keyboards, the main four rows of keys are arranged in columns that are angled from the bottom right to the top left. \n\n![](http://content.igloocoder.com/images/traditional_ergo_finger_movement.jpg)\n\nOn the TEK the columns of keys converge at the top middle of the keyboard, similar in pattern to an upside down V. What this means for your transition to the TEK is that you'll constantly be reaching too far to the left when you move up from the home row and too far to the right when you move down from it. For me that meant that I was constantly reaching for the `B` key and, instead, getting the `Backspace` key (more about why it was the `Backspace` key shortly). There were other keys that I had similar problems on, but this was the most obvious since I was killing letters when I didn't want to.\n\nThe second transition issue that I had was the location of many of the keys. Instead of having dead space in the middle like many ergonomic keyboards have, the TEK fills that space with a number of commonly used secondary keys. `Windows | Command`, `Delete`, `-_`, `Backspace` (which I just hit again while trying to type `B`) and `Return` all reside in the middle. When you first look at the keyboard it's a novel layout. The `Spacebar` is split in two with the `Return` in the middle of them. The `Shift` and `Ctrl` keys also move up the keyboard by one row with `Ctrl` taking the places traditionally held by `CAPS` and `Return`. Getting used to `Return` being between the `Spacebars` was one of the tougher things for me at the start. I would type away and go to hit `Return` and get nothing....because I was actually pushing the `Ctrl` key. I have to say that having the `Return` key in the bottom middle of the keyboard between the `Spacebars` is a delight. I can't imagine why this isn't part of the standard layout.\n\n![](http://content.igloocoder.com/images/TEK_finger_movement.jpg)\n\nThe layout of the TEK does have some things that didn't initially work for me. As a developer I tend to use some of the secondary and tertiary keys pretty frequently. For example `{` and `}` are almost as common as `A` and `E`. Those two keys are not located on the far right one row above the home row. Instead they're on the complete opposite side of that row. I'm still working on getting fluent with typing those keys.\n\nI also `Tab` a lot and I'm very used to using `Alt+Tab` and `Ctrl+Tab` to work my way through UIs. The `Tab` key is moved to the top left of the keyboard and is a single width key instead of the more traditional double width. Learning on a double width key made me very lazy in my finger placement when I was typing `Tab`. I learned fairly quickly to reach up a row to get to `Tab` but I couldn't gain the pinky finger control to both reach up and accurately push the right key and this really affected the speed of my keyboarding when working in code. Luckily TEK has a solution.\n\nThe TEK is fully programmable. Simply create your layout [on the website](https://www.trulyergonomic.com/store/features/fully-programmable-in-firmware--truly-ergonomic-mechanical-keyboard) and save a file that it will generate for you. Flip a DIP switch on the back of the keyboard and flash the new programming onto it. Now I have a `Tab` key in the middle of the keyboard between the `5` and `6`. Striking that 1.5x sized key with my index finger is much more accurate and fast. I reprogrammed the `CAPS` key so that it was reversed and `Insert` was the default with `Fn+CAPS` becoming the alternate. These are the only two tweaks that I've done thus far but it's nice to have this option if I notice inefficiencies in the future.\n\nSo far I'm very happy with the Truly Ergonomic Keyboard. The mechanical keys are a dream to type on and the layout feels as if it is more efficient (my typing speed is about the same as on a more traditional keyboard even though I'm still working on muscle memory). Most importantly I haven't yet noticed any hint of carpal pain which tells me that this keyboard is, at a minimum, no worse for me physically than what I was using before.\n\nThe biggest question you're all probably asking is if it's worth the (at the time of writing) $250 USD price tag. It's hard to say. I'm not disappointed with having spent that kind of money on it, but each to their own.\n\nAll images provided courtesy of [Truly Ergonomic](http://www.trulyergonomic.com)","categories":[{"name":"Ergonomics","slug":"Ergonomics","permalink":"https://westerndevs.com/categories/Ergonomics/"}],"tags":[{"name":"Ergonomics","slug":"Ergonomics","permalink":"https://westerndevs.com/tags/Ergonomics/"},{"name":"Office","slug":"Office","permalink":"https://westerndevs.com/tags/Office/"}]},{"title":"What is middleware anyway?","authorId":"simon_timms","slug":"What-is-middleware","date":"2016-03-16 17:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Tools/What-is-middleware/","link":"","permalink":"https://westerndevs.com/Tools/What-is-middleware/","excerpt":"I find middleware to be a confusing term which doesn't mean anything or perhaps means everything. Let's figure out what middleware means","raw":"---\ntitle: What is middleware anyway?\nlayout: post\ntags:\n  - asp.net\ncategories:\n  - Tools\nauthorId: simon_timms\ndate: 2016-03-16 13:56:56 \nexcerpt: \"I find middleware to be a confusing term which doesn't mean anything or perhaps means everything. Let's figure out what middleware means\"\noriginalurl: http://aspnetmonsters.com/2016/03/2016-02-28-what-is-middleware-anyway/\n---\n\nIf you spend a bit of time around the net ASP.NET Core there is a word you're going to hear thrown around a bunch and that is \"middleware\". I find middleware to be a confusing term which doesn't mean anything or perhaps means everything. Let's figure out what middleware means and what sorts of middleware we can slot into ASP.NET Core. \n\n<!-- more -->\n\nMiddleware sits between two pieces of software which talk with one another piece.  It is responsible for connecting the softwares together and may intercede to alter the communication or even intercept it. I know what you're thinking: that's a super vague definition, by that definition almost everything is middleware. Yep. See why I consider the term to be so confusing? The software we use these days is hugely abstracted and there are a lot of layers. Any of these layers in between are middleware. \n\n![Middleware as a hamburger](http://i.imgur.com/JHGMtei.png)\n\nWhen I was a kid I had this game, Spellicoptor, which you had to boot right into. As far as I know it ran right on the hardware without that heavy weight Disk Operating System getting in the way. That was probably the last piece of software I used which wasn't middleware - it was certainly the most sneakily educational. \n\nFor a web application we usually think of middleware as the software between the web server and the application responsible for returning HTML. In ASP.NET Core serving out static files such as .css files and images is performed by middleware. The prevents our, comparatively, complicated application pipeline from even running. This would be an example of middleware which intercepts requests and prevents it from even reaching the other layer.  You could also put authentication in the pipeline so that by the time a request hits you application code you can be confident that it is properly authenticated. In ASP.NET Core the middleware is implemented as a pipeline. This means that a request can pass though multiple pieces of middleware before it hits your code. In theory your code should not depend on a piece of middleware being there. However, in practice, we frequently do depend on something being there. Consider the case of authenticating a user: we frequently rely on the user name being set. However this user name could have been set by some authentication middleware or it could have been set by some mock development middleware which passes in a fake user. \n\nThe pipeline in ASP.NET Core is a bi-directional one. This means that each piece of middleware has two opportunities to intervene in the request processing: when the request comes in and when the response goes out. So your middleware can alter the data your application gets or it can alter the data coming from your application. \n\n![Incoming and outgoing](http://i.imgur.com/PR96d6W.png)\n\nWhen should you use middleware? I like to think of it as something of a cross cutting concern. If there is something you want to before or after a large number of requests and it isn't part of the core logic of the application then middleware might be the place for you. Pay attention to the \"core logic\" part of that sentence. If your application has some cross cutting concern but is really part of the logic of the application - say sending notification e-mail when anybody deletes an item (via the DELETE HTTP verb) then this shouldn't be in the middleware. However if you want to log requests then middleware could be a great place. \n\nMiddleware can take the place of what was one written as modules for IIS. Moving this functionality to middleware which knows how to talk OWIN means that your application is less coupled to IIS. It may be difficult to imagine a world where IIS is not the de facto tool for running ASP.NET applications but I suspect there are a great number of sites and applications which don't need the power of a full IIS stack behind them. \n\nI wrote ASP.NET applications for the better part of a decade and I wrote modules perhaps 3 times in all those years. I just didn't see the advantage to using them. However others such as Dave and James assure me that in the wider world modules were used quite heavily. So this paragraph was going to be about how you'll likely never need to write middleware but at this juncture I honestly don't know where everything will end up. You might be writing middleware for 90% of your code. \n\nI'm excited to see where the middleware for ASP.NET ends up. There is already some pretty nifty tooling in place providing custom error messages to help people out with very descriptive errors. I'd love to hear about your ideas for middleware in the comments below.\n","categories":[{"name":"Tools","slug":"Tools","permalink":"https://westerndevs.com/categories/Tools/"}],"tags":[{"name":"asp.net","slug":"asp-net","permalink":"https://westerndevs.com/tags/asp-net/"}]},{"title":"Western Devs â™¥ Prairie Dev Con","authorId":null,"slug":"Prairie-Dev-Con-What-you-need-to-know","date":"2016-03-07 14:56:57+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"conferences/Prairie-Dev-Con-What-you-need-to-know/","link":"","permalink":"https://westerndevs.com/conferences/Prairie-Dev-Con-What-you-need-to-know/","excerpt":"The Western Devs talk about why they love Prairie Dev Con","raw":"---\nlayout: post\ntitle: 'Western Devs â™¥ Prairie Dev Con'\ncategories:\n  - conferences\ndate: 2016-03-07 09:56:57\ntags: PrDC\nauthorId:\nexcerpt: The Western Devs talk about why they love Prairie Dev Con\n---\n\n[Prairie Dev Con](http://prairiedevcon.com/) starts in just over a month. And the Western Devs will be there [in force](http://www.westerndevs.com/speaking/). No fewer than nine of us are presenting and Dylan Smith is going a step further by putting on a workshop on [How to DevOps](http://prairiedevcon.com/Workshops). Add D'Arcy Lussier as the conference organizer and this will be the highest concentration of Western Devs in one city since our failed second season pitch on Dragon's Den. (The world is apparently not ready for spray-on luchador masks.) And based on the [sessions](http://prairiedevcon.com/Sessions) it looks to be one of the best yet.\n\nBeing the impartial and unbiased people we are, we won't outline reasons why you should go. Instead, we'll just tell you what we love about PrDC.\n\n<img style=\"float: left; margin-right: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/simon.jpg\" />\n\n<h2><a href=\"/bios/simon_timms\">Simon Timms</a></h2>\n\nI love talking at PrDC because there is always something totally new to learn about. One year it was graph databases, another year how to use EF properly and still another how to build paper airplanes. No matter what, I come away with the feeling my brain is full\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: right; margin-left: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/Lori_Lalonde.jpg\" />\n\n<h2 style=\"text-align: right\"><a href=\"/bios/lori_lalonde\">Lori Lalonde</a></h2>\n\nPrairie Dev Con was my first. The first conference I attended. The first conference I had been given the opportunity to speak at. The first conference that welcomed me as part of a broader tech community. Maybe that makes my opinion of it a little bias. Maybe not. \n\nThe atmosphere of Prairie Dev Con is one of learning but in a fun, relaxed manner. The cost of registration is the most affordable I've seen for a 2 day conference, but the value it provides in return is tenfold. This is the type of conference that I wished was available to me throughout my career. Something that would have satisfied my desire to learn, that I could afford to attend, with or without the support of my past employers. \n\nWhat I can say for certain is that I look forward to returning to Prairie Dev Con each year, because D'Arcy Lussier - conference organizer extraordinaire - knows how to put on a good show. From development to management to dev ops, it includes sessions that are suited to anyone in the technology sector, with a top notch speaker lineup to boot (and I'm not just saying that because I'm one of them). \n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: left; margin-right: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/davidwesst_400x400.jpg\" />\n\n## <a href=\"/bios/david_wesst\">David Wesst</a>\n\nPresenting at PrDC has been important to me and will continue to be as long as it is around. Every time I attend I am always introduced to new concepts, new ideas, and new development communities that allow me to expand my knowledge and grow as a professional. PrDC has helped make me into the developer I am today, and I really can't imagine where I would had I not presented and attended PrDC all those years ago when it first began.\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: right; margin-left: 20px; height: 100px; border-radius: 8px;\" src=\"https://www.gravatar.com/avatar/fadb1de2c18ab0fc42ebc0988327c90f?s=200\" />\n\n<h2 style=\"text-align: right\"><a href=\"/bios/amir_barylko\">Amir Barylko</a></h2>\n\nPrDC is always super fun! Not only as a presenter but as an attendee is always insightful and exciting. So far I have missed only one and I can say that I always meet lots of cool people, listen to interesting talks and take home all kinds of new knowledge to process the rest of the year (or until the next PrDC).\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: left; margin-right: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/Kyle_300x300.jpg\" />\n\n## <a href=\"/bios/kyle_baley\">Kyle Baley</a>\n\nPrairie Dev Con is great because I always come home with at least one new friend (figuratively speaking, of course; I don't _literally_ come home with them...anymore). Not only is the content high quality but the presenters are approachable and always willing to share tips and tricks outside of their presentations. It's the type of conference that is so good you wish everyone knew about it but are glad if they don't so you can make yourself look better.\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: right; margin-left: 20px; height: 100px; border-radius: 8px;\" src=\"https://www.gravatar.com/avatar/1ac8c631c34a86ddcfb6ae67f6a7a551?s=200\" />\n\n<h2 style=\"text-align: right\"><a href=\"/bios/james_chambers\">James Chambers</a></h2>\n\nIn addition to meeting great folks that have become even better friends (oh, and D'Arcy, too), the technical conversation at PrDC is intense, in depth and has shaped how I approach all aspects of development. The enthusiasm of attendees and other speakers makes for a wonderful venue for me to grow as a developer.\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: left; margin-right: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/DavePaquette_241x241.jpg\" />\n\n## <a href=\"/bios/dave_paquette\">Dave Paquette</a>\n\nEvery time I attend PrDC I learn at least 1 extremely useful new thing that has an immediate positive impact on my day job. Sometimes it is process related, like the year I learned that I should stop trying to estimate software projects. Other times it is very deeply technical, like the year I learned how to tune SQL Server to handle tables with 50 billion records (I also learned that my project with 50 millions records wasnâ€™t so big after all).\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: right; margin-left: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/dylan_300x300.jpg\" />\n\n<h2 style=\"text-align: right\"><a href=\"/bios/dylan_smith\">Dylan Smith</a></h2>\n\nEvery time I attend PrDC, I come away re-energized and motivated to apply the learnings to my day-to-day work.  Unlike some of the bigger conferences (Build, Ignite, etc), PrDC has a more intimate atmosphere, and you can really get to know the speakers, attendees, join in hallway conversations, and really build up your network.  And thatâ€™s not even mentioning the top-tier content and speakers that Dâ€™Arcy manages to fit into the 3 day conference.\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<img style=\"float: left; margin-right: 20px; height: 100px; border-radius: 8px;\" src=\"/images/avatars/donald_belcham_240x240.jpg\" />\n\n## <a href=\"/bios/donald_belcham\">Donald Belcham</a>\n\nPrDC is one of the few conferences that I do not want to miss. While the scheduled content quality is indisputable, there is an enormous amount of knowledge that is available for unscheduled transfer. Hallway, lunch and post event conversations are what I look forward to at PrDC. Being able to connect with people on topics that matter in their day-to-day work is important, even if the topic hasn't garnered enough interest to warrant a full conference session.\n\n<div style=\"clear:both;margin-bottom:30px;\"></div>\n\n<hr />\n\nSo if any of this motivates you to attend Prairie Dev Con, [head on](http://prairiedevcon.com/) over and register.\n","categories":[{"name":"conferences","slug":"conferences","permalink":"https://westerndevs.com/categories/conferences/"}],"tags":[{"name":"PrDC","slug":"PrDC","permalink":"https://westerndevs.com/tags/PrDC/"}]},{"title":"Addendum To My Demystifying The Cloud Talk","authorId":"darcy_lussier","slug":"Addendum-To-My-Demystifying-The-Cloud-Talk","date":"2016-03-03 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Addendum-To-My-Demystifying-The-Cloud-Talk/","link":"","permalink":"https://westerndevs.com/_/Addendum-To-My-Demystifying-The-Cloud-Talk/","excerpt":"","raw":"---\nlayout: post\ntitle: Addendum To My Demystifying The Cloud Talk\ndate: 2016-03-03\ncategories:\ncomments: true\nauthorId: darcy_lussier\n---\n\nBack in November 2015 I spoke on \"Demystifying the Cloud\" [at SDEC here in Winnipeg][7]. That session just got posted to InfoQ ([you can view it here][1]), but in four months a lot can change \nin the IT industry. So here's an addendum to what was talked about.\n\n## Microsoft Data Centers\n\nI mentioned it in the session, but wanted to point out again that Microsoft is bringing Azure data centers to Canada in 2016. [You can read the story about this here.][3]\n\n## AWS Data Centers\n\nIn January 2016 Amazon announced that they were going to open an AWS region in Montreal. From [their blog post][2]:\n\n>I am happy to announce that we will be opening an AWS region in Montreal, QuÃ©bec, Canada in the coming year. This region will be carbon-neutral and powered almost entirely by clean, renewable hydro power.\n>The planned Canada-Montreal region will give AWS partners and customers the ability to run their workloads and store their data in Canada.\n\nAs of this post I haven't read anything about concrete dates beyond \"launching in 2016\".\n\n## IBM Enters Commercial Cloud Services\n\nAt one point in my session an attendee pointed out that IBM has had cloud services in Canada already, so the Azure and Amazon announcements suggesting that the cloud \"is finally coming to Canada\" wasn't 100%. And he was right.\n\nIn 2013 IBM bought SoftLayer, a cloud platform company in Texas. In 2014 [IBM opened its first Canadian SoftLayer data center][4], providing IaaS from a Canadian footprint. Services were limited to servers, storage, and networking though - not the PaaS or SaaS offerings of Azure/Microsoft and AWS.\n\nBut in mid 2014 [IBM launched BlueMix][5], their cloud platform for developers! In typical IBM fashion, their marketing on this has been non-existent (they just can't seem to understand how consumer-based marketing works); which is unfortunate \nbecause the platform looks REALLY interesting - [who wouldn't want to try integrating Watson into their app?!][6] BlueMix gives developers an IBM version of Azure's PaaS, and a solid 3rd option (ok, 4th if you count Google). Unfortunately though\nBlueMix isn't currently available *from* a Canadian data center.\n\n## The Cloud Always Changes\nIn the same way that an actual cloud changes shape as it moves through the air, the technical cloud does the same. The rate of growth, evolution, and change in cloud computing happens at an ongoing, rapid pace that can be hard to keep up to - especially for those of us who \nblog or speak on the subject. But cloud is an important discussion to have, especially with more Canadian options enabling public and private organizations to take advantage of the cloud.\n\n\n[1]: http://www.infoq.com/presentations/cloud-2015\n[2]: https://aws.amazon.com/blogs/aws/in-the-works-aws-region-in-canada/\n[3]: http://reimagine.microsoft.ca/en-ca/\n[4]: http://www.softlayer.com/press/ibm-opens-first-softlayer-data-center-canada\n[5]: http://www.ibm.com/cloud-computing/bluemix/\n[6]: http://www.ibm.com/cloud-computing/bluemix/watson/\n[7]: http://www.sdeconf.com/\n\n\n  \n","categories":[],"tags":[]},{"title":"Using text-overflow:ellipsis with Inline Flex","authorId":"david_wesst","slug":"Using-Overflow-Ellipsis-in-Inline-Flex","date":"2016-02-29 20:58:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"css/Using-Overflow-Ellipsis-in-Inline-Flex/","link":"","permalink":"https://westerndevs.com/css/Using-Overflow-Ellipsis-in-Inline-Flex/","excerpt":"Two out of three Dave's hit this obscure CSS problem that turned out to be expected behaviour. David Wesst walks us through the reason why and how to fix it.","raw":"---\nlayout: post\ntitle: \"Using text-overflow:ellipsis with Inline Flex\"\ncategories:\n  - css\ndate: 2016-02-29 15:58:40\ntags:\n  - html\n  - css\n  - flexbox\nexcerpt: Two out of three Dave's hit this obscure CSS problem that turned out to be expected behaviour. David Wesst walks us through the reason why and how to fix it.\nauthorId: david_wesst\noriginalurl: http://www.webnotwar.ca/opensource/using-text-overflowellipsis-with-inline-flex/\n---\n\n[Dave Paquette][1], a fellow Western Dev, hit a strange CSS snag the other day. He wanted to use the `text-overflow: ellipsis` on a flexbox item that displayed text, where the ellipsis would show up if the text was too long.\n\nIt didn't work as expected, but after some digging, a solution was discovered. This post exists to document both the problem and solution with the hope that it prevents future headaches for other CSS developers.\n\n## The Problem\n\nHis code went something like this:\n\n```html\n<ul>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>Item Code 321</span>\n    <span>Descriptive Text</span>\n  </li>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>Item Code that is way too long and shoud use ellipsis</span>\n    <span>Descriptive Text</span>\n  </li>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>Item Code 123</span>\n    <span>Descriptive Text</span>\n  </li>\n</ul>\n```\n\n```css\nli span {\n  display: inline-flex;\n}\n\n.item span {\n  width: 100px;\n  margin-left: 1em;\n  white-space: nowrap;\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n```\n\nThe goal here is to have all the `<span>` elements line up, regardless of their contents. If their contents exceed the length of the `<span>` then should display an ellipsis.\n\nThis isn't the case.\n\nInstead, you get something like this:\n\n{% asset_img broken-result.png \"Not quite right\" %}\n\nWe have everything aligned, but no ellipsis. Why?\n\n## The Solution\n\nIt turns out that this is the expected behaviour (see \"But Why?\" section below).\n\nThe solution is to make sure that we apply the `text-overflow` property to a block element that lives inside a flex item. In our case, we need to wrap the text we want to have the ellipsis show up in a block element.\n\nThe working code looks like this:\n\n```html\n<ul>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>\n        <div>\n          Item Code 321\n        </div>\n    </span>\n    <span>Descriptive Text</span>\n  </li>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>\n        <div>\n          Item Code that is way too long and shoud use ellipsis\n        </div>\n    </span>\n    <span>Descriptive Text</span>\n  </li>\n  <li class=\"item\">\n    <i>Icon</i>\n    <span>\n        <div>\n          Item Code 123\n        </div>\n    </span>\n    <span>Descriptive Text</span>\n  </li>\n</ul>\n```\n\n```css\n.item span {\n  display: inline-flex;\n}\n\n.item span {\n  width: 100px;\n  margin-left: 1em;\n  white-space: nowrap;\n}\n\n.item span div {\n  overflow: hidden;\n  text-overflow: ellipsis;\n}\n```\n\nThe `<div>` we introduce is a block element that lives inside of the flex item. Now the `text-overflow` property applies, and all is good!\n\n{% asset_img working-result.png \"It Works!\" %}\n\n### The Solution in Action\n\nIn case you want to see the end result working, here you&nbsp;can check it out [here][4].\n\n<script async src=\"https://jsfiddle.net/davidwesst/fhkt9mco/5/embed/html,css,result/\"></script>\n\n## But Why?\n\nI'll try and explain this, but be aware that I'm likely oversimplifying things.\n\nAs I mentioned before, this whole issue is expected behaviour. If you look at [the spec][5] for,&nbsp;`text-overflow` you find the definition you see in the quote below\n\nThis property specifies rendering when inline content overflows its line box edge in the inline progression direction of its block container element (\"the block\") that has overflow other than visible.\n\nIt turns out `text-overflow` isn't meant to work on flex items, rather it is meant to work on block items as per the spec.\n\nThat being said, so what? I mean, can't the spec be adjusted to include `text-overflow` on flex items? Although you would think it's not a big deal to make this work for flex-items too, there are a number of considerations that need to be taken into account. More specifically, how flexbox works.\n\nIt turns out that there really isn't a clean way to do this. If you're wondering how I came to that conclusion you can stop because I didn't. Those responsible for the specification did, and you can read the full conversation that started with a [Mozilla bug report][6] and leads to a whole [mail group discussion][7] about why it should (or, in this case, should not) be implemented as part of the spec.\n\nAnd that, ladies and gentlemen, is how you get `text-overflow` working on flex items. You don't. :)\n\n[1]: http://www.westerndevs.com/bios/dave_paquette/\n[4]: https://jsfiddle.net/davidwesst/fhkt9mco/5/embed/html,css,result\n[5]: https://drafts.csswg.org/css-ui/#text-overflow\n[6]: https://bugzilla.mozilla.org/show_bug.cgi?id=912434\n[7]: http://lists.w3.org/Archives/Public/www-style/2013Sep/0070.html\n","categories":[{"name":"css","slug":"css","permalink":"https://westerndevs.com/categories/css/"}],"tags":[{"name":"css","slug":"css","permalink":"https://westerndevs.com/tags/css/"},{"name":"html","slug":"html","permalink":"https://westerndevs.com/tags/html/"},{"name":"flexbox","slug":"flexbox","permalink":"https://westerndevs.com/tags/flexbox/"}]},{"title":"Building developer tools with Igal Tabachnik","slug":"Building-developer-tools-with-Igal-Tabachnik","date":"2016-02-29 20:57:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Building-developer-tools-with-Igal-Tabachnik/","link":"","permalink":"https://westerndevs.com/podcasts/Building-developer-tools-with-Igal-Tabachnik/","excerpt":"We talk with Igal Tabachnik and discuss the finer points of building tools for Visual Studio","raw":"---\nlayout: podcast\ntitle: Building developer tools with Igal Tabachnik\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-developer-tools-with-igal-tabachnik.mp3\n  length: '27:50'\n  filesize: 26712724\n  libsynId: 4162249\n  anchorFmId: Building-developer-tools-with-Igal-Tabachnik-evqdid\nparticipants:\n  - kyle_baley\n  - donald_belcham\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'In Absentia (Igal''s blog)|http://hmemcpy.com/'\n  - 'Igal on Twitter|https://twitter.com/hmemcpy'\n  - 'OzCode|http://www.oz-code.com/'\n  - 'Debugging Tips and Tricks - Visual Studio Blog|https://blogs.msdn.microsoft.com/visualstudio/2015/05/22/debugging-tips-and-tricks'\n  - 'The .NET Compiler Platform (Roslyn)|https://github.com/dotnet/roslyn'\n  - 'Learn Roslyn Now|https://joshvarty.wordpress.com/learn-roslyn-now/'\n  - 'Visual Studio Extensibility samples|https://github.com/Microsoft/VSSDK-Extensibility-Samples'\n  - 'Project Rider - a cross-platform C# IDE by JetBrains|https://blog.jetbrains.com/dotnet/2016/01/13/project-rider-a-csharp-ide/'\ndate: 2016-02-29 15:57:40\nrecorded: 2016-01-22\nexcerpt: \"We talk with Igal Tabachnik and discuss the finer points of building tools for Visual Studio\"\n---\n\n### Synopsis\n\n* What happens under the covers in Visual Studio?\n* Finding further use of compilers, debuggers, and source code analyzers\n* TypeMock - Low level CLR to mock statics and sealed classes\n* Reverse engineering without documentation\n* OzCode - Tying into the debugger\n* Developer tools vs. LOB apps\n* Howâ€™d they do that?\n* Going past F10 and F11\n* Moving the debug point backward or forward\n* Using Roslyn to hook into the build process\n* Roslyn: opening the compiler black box. Itâ€™s not scary anymore\n* Using Roslyn for domain-specific plugins\n* ReSharper and DevExpress: Comparing a tool that isnâ€™t switching to Roslyn to one that is\n* Building tools that stay out of your way\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Developer Productivity","slug":"Developer-Productivity","date":"2016-02-23 17:31:27+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Developer-Productivity/","link":"","permalink":"https://westerndevs.com/podcasts/Developer-Productivity/","excerpt":"The Western Devs take a break from their collective workdays to discuss developer productivity","raw":"---\nlayout: podcast\ntitle: \"Developer Productivity\"\ncategories: podcasts\ncomments: true\npodcast:\n  filename: westerndevs-developer-productivity.mp3\n  length: '45:53'\n  filesize: 44047559\n  libsynId: 4168083\n  anchorFmId: Developer-Productivity-evqdi1\nparticipants:\n  - donald_belcham\n  - darcy_lussier\n  - dave_paquette\n  - kyle_baley\n  - lori_lalonde\n  - simon_timms\n  - dave_white\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Prairie Dev Con|http://www.prairiedevcon.com'\n  - 'Measuring Developer Productivity|https://blog.newrelic.com/2014/09/24/measuring-developer-productivity/'\n  - 'The myth of developer productivity|http://www.dev9.com/article/2015/1/the-myth-of-developer-productivity'\ndate: 2016-02-23 12:31:27\nrecorded: 2016-02-05\nexcerpt: \"The Western Devs take a break from their collective workdays to discuss developer productivity\"\n---\n\n### Synopsis\n\n* Can you measure developer productivity?\n* Value delivered is contextual\n* Measuring over time\n* Measuring developers vs. teams\n* Choosing appropriate metrics that measure the right things\n* Distractions/external factors\n* Collaboration and/or competition\n* What's driving the desire for measuring developer productivity?\n* Measuring beyond code: professional development, mentoring, marketing\n* Individual vs. team evaluations\n* Constant peer feedback\n* How do you manage promotions/pay raises without measuring productivity?\n* Performance review != salary review\n* _conversation devolves into a salary discussion for longer than it should have_\n* How do you allocate your budget within the context of measuring a developer's productivity?\n* Divorcing salary from productivity\n* Measuring productivity in terms of cookies\n* Employees vs. consultants\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"JavaScript Framework or JavaScript Core (Part 2)","authorId":"david_wesst","slug":"JavaScript-Framework-or-JavaScript-Core-Part-2","date":"2016-02-22 20:34:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"JavaScript/JavaScript-Framework-or-JavaScript-Core-Part-2/","link":"","permalink":"https://westerndevs.com/JavaScript/JavaScript-Framework-or-JavaScript-Core-Part-2/","excerpt":"Is it always better to use JavaScript Frameworks? David continues his analysys and shares the pros and cons of writing your JavaScript from scratch.","raw":"---\nlayout: post\ntitle: JavaScript Framework or JavaScript Core (Part 2)\ncategories:\n  - JavaScript\ndate: 2016-02-22 15:34:46\ntags:\n  - JavaScript\nexcerpt: Is it always better to use JavaScript Frameworks? David continues his analysys and shares the pros and cons of writing your JavaScript from scratch.\nauthorId: david_wesst\noriginalurl: http://www.webnotwar.ca/opensource/14136/\n---\n\n![][5]\n\nA friend and I decided to make a video game using JavaScript.\n\nWhen deciding on whether or not to use a JavaScript game framework [I went with Phaser][1], and [Chris opted to go with pure JavaScript][2].\n\nWe were both right.\n\nIn this series of posts, I want to explain how each of us answer the question: Should I use a JavaScript Framework or pure JavaScript?\n\nThis is part 2 of 2 in the series. In this post, I'll cover why I'll cover why Chris opted not to use any frameworks and stick with pure JavaScript. In [part 1][3], I covered why I opted to use a JavaScript framework rather than pure JavaScript.\n\n##### Author's Note\n\nEven though I'm talking about game development frameworks, the concepts and rationale discussed in this series of posts can be applied to any project (game, business, or otherwise) on whether or not you should use a JavaScript framework.\n\n## Framing the Question\n\nLet's frame this question in context to Chris' goals.\n\nLike me, the game is a side project for Chris which means that it is important that he learn some new things along the way&nbsp;while still having the goal of \"making a game\". We both had these general learning goals:\n\n* To learn how to draw using Canvas element\n* To learn more about \"graphics\" programming in JavaScript\n* To learn more about making games and game development\n\nChris, having less experience with JavaScript development than me, had the extra goal of:\n\n* Learning more about JavaScript itself in the context of game development\n\nThis last goal is different than mine, although I could have easily made this one of my goals too considering there is always more to learn when it comes to JavaScript. Still, it is a difference that I wanted to note as we'll touch base on that near the end of this post.\n\n## Why use Pure JavaScript?\n\nThe other side of the coin is to write your code from the ground up&nbsp;and rely on the native JavaScript APIs. In the case of Chris' game, he wrote code against the Canvas API to draw to the screen.\n\n### Pros\n\nAs always, let's begin with the reasons why this was a good decision.\n\n#### Deeper understanding of the Specification\n\nProgramming against the native API means that you'll understand the specification that everyone shares, including all of the frameworks out there. With this core knowledge, you can gain a better understand on how to fix or improve the frameworks themselves for your games moving forward.\n\nPlus, you'll understand more about why things happen the way they do than those who just consume a framework.\n\n#### Better Performance\n\n![][6]\n\nLearning the native APIs means that you're essentially programming \"against the metal\", or as close as you can come to it when you talk about JavaScript. Assuming you're writing efficient code, you are likely going to see some improved performance as compared to another game or application that is using a framework.\n\nYou will also not have to worry about Fast Food Frameworks bloating and bogging down your application on the web, as [Chris Love][4] would tell you.\n\n#### Understanding Your Full Stack\n\nSince you're writing against the native API, you will end up knowing all of your code from top to bottom because you'll have to write it all. This is an important consideration, as all software ends up with bugs. If you wrote all the code, then you'll have a good idea of where to look when it comes to triage and implementing fixes, improvements, or whatever.\n\n### Cons\n\nAs always, there are drawbacks to any solution. Let's review a few of them.\n\n#### Lack of Support and Guidance\n\nWhen you select a framework, you're relying on the experience of many other developers who have poured their knowledge into building that framework. Hypothetically, these developers wrote the framework to make something easier for their own projects.\n\nWhen you write all the code yourself, you rely on your own experience.That means&nbsp;if you don't have experience with certain architecture, patterns, or practices you're going to have to learn them quickly to figure out or possibly miss out on their benefits altogether because you simply don't know what you don't know.\n\n#### Owning all the Code\n\nWhen you own all of your code can be a bit of a double edged sword. Sure you own it all and ideally know it all inside out, but that means _you're the only one_ that actually knows it. Anyone else looking at your code, whether to provide support or contribute, is going to have a steep learning curve and rely heavily on you to learn how your code works rather than some documentation provided by a community of developers already sharing a code base.\n\nGiven, if you're a solo developer, that might not be an issue. But even in that case, you might not be able to remember the exact details about why you wrote some code a certain way 6 months after you wrote it.\n\n#### Productivity\n\nAny application, game or business apps, end up being made up of quite a bit of code to make sure all the features are implemented. Regardless of the benefits of owning all your code, the more code you have to write, the more time it's going to take before you get something up and running.\n\n## So What Should You Choose?\n\nLike I said before, Chris and I were both right in our respective decisions.\n\nThe reason for this&nbsp;is that both of us can explain the \"why\" we made the decision without having to convince ourselves that we were correct.\n\nFor your next project, you should choose what makes sense for you and your project goals. If you're looking to produce something as quickly as possible, it's probably best to pick a framework that can help you get moving faster.\n\nIf you're looking to learn or explore something new and need (or want) to know how the insides work, it's probably best to learn from the native APIs upwards, or possibly clone a framework and start modifying for your own needs.\n\nEither way, if you ask yourself \"why\" you made the decision to go one way or the other and find that you need to convince yourself that you made the right decision, maybe it's time to rethink that choice before it's too late.\n\n[1]: https://github.com/davidwesst/finder-game\n[2]: https://github.com/chrinkus/walk\n[3]: http://www.westerndevs.com/JavaScript/JavaScript-Framework-or-JavaScript-Core-Part-1/\n[4]: http://love2dev.com/#!article/Large-JavaScript-Frameworks-Are-Like-Fast-Food-Restaurants\n[5]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P2/js-framework-or-pure-js-p2.png\n[6]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P2/performance-graph.png\n","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"}]},{"title":"I squash my pull requests and you should too","authorId":"simon_timms","slug":"I-squash","date":"2016-02-18 04:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Tools/I-squash/","link":"","permalink":"https://westerndevs.com/Tools/I-squash/","excerpt":"By squashing your git commits you can make everybody's life better except for Idi Amin","raw":"---\ntitle: I squash my pull requests and you should too\nlayout: post\ntags:\n  - git\ncategories:\n  - Tools\nauthorId: simon_timms\ndate: 2016-02-17 23:56:56 \nexcerpt: \"By squashing your git commits you can make everybody's life better except for Idi Amin\"\noriginalurl: http://blog.simontimms.com/2016/02/18/i-squash-my-pull-requests-and-you-should-too/\n---\n\nA couple of weeks ago I made a change to my life. It was one of those big, earth shattering changes which ripple though everything: I started to squash the commits in my pull requests.\n\nIt was a drastic change but I think an overdue one. The reasons for my change are pretty simple: it makes looking at commit histories and maintaining long-lived branches easier. Before my pull requests would contain a lot of clutter: I'd check in small bits of work when I got them working and whoever was reviewing the pull request would have to look at a bunch of commits, some of which would later be reversed, to get an idea for what was going on. By squashing the commits down to a single commit I can focus on the key parts of the pull request without people having to see the mess I generated in between. \n\nIf you have long lived branches (I know, I know) then having a smaller number of commits during rebasing is a real help. There are fewer things that need merging so you don't end up fixing the same change over and over again. \n\nFinally the smaller number of commits in mainline give a clearer view of what has changed in the destination branch. You individual commits might just have messages like \"fixing logging\" but when squashed into a PR the commit becomes \"Adding new functionality to layout roads automatically\". Looking back that \"fixing logging\" commit isn't at all helpful once you're no longer in the thick of the feature. \n\n# What has it done for me?\n\nI've already talked about some of the great benefits in the code base but for me, individually, there were some nicities too. First off is that I don't have to worry about screwing up so much. If I go down some crazy path in the code which doesn't work then I can bail out of it easily and without worrying that other developers on my team (Hi, Nick!) will think less of me. \n\nI find myself checking in code a lot more frequently. I have a git alias to just do \n\n{% codeblock lang:bash %}\ngit wip\n{% endcodeblock %}\n\n\nand that checks in whatever I have lying around. It is a very powerful undo feature. I do a git wip whenever I find myself at the completion of some logical step be it writing a unit test or finishing some function or changing some style.  \n\n# How do you do it?\n\nIt is easy. Just work as you normally would but with the added freedoms I mentioned. When you're ready to create a pull request then you can issue\n\n{% codeblock lang:bash %}\ngit log\n{% endcodeblock %}\n\nand find the first commit in your branch. There are some suggested ways to do this automatically but they never seem to work for me so I just do it manually. \n\nNow you can rebase the commits\n\n{% codeblock lang:bash %}\ngit rebase -i 0ec9df23\n{% endcodeblock %}\n\nwhere 0ec9df23 is the sha of the last commit on the parent branch. This will open up an editor showing all the commits in chronological order. On left you'll see the word pick. \n\n{% codeblock lang:bash %}\npick 78fc6bc added PrDC session to speaking data\npick 9741725 Podcast with Yves Goeleven\n\n# Rebase 9d792c2..9741725 onto 9d792c2\n{% endcodeblock %}\n\nStarting at the bottom just change all but the first of these to `squash` or simply `s`. Save the file and exit. Git will now chug a bit and merge all the changes into one. With this complete you can push to the central repo and issue the pull request. You can add additional changes to this PR to address comments and, just before you do the merge, do another squash. You may need to push with -f but that's okay, this time. \n\nI'm a big fan of this approach and I hope you will be too. It is better for your sanity, for the teamâ€™s sanity and for the git history's sanity.\n","categories":[{"name":"Tools","slug":"Tools","permalink":"https://westerndevs.com/categories/Tools/"}],"tags":[{"name":"git","slug":"git","permalink":"https://westerndevs.com/tags/git/"}]},{"title":"Getting Started in Open Source with Adam Ralph","slug":"Getting-started-in-open-source-with-Adam-Ralph","date":"2016-02-17 23:57:13+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Getting-started-in-open-source-with-Adam-Ralph/","link":"","permalink":"https://westerndevs.com/podcasts/Getting-started-in-open-source-with-Adam-Ralph/","excerpt":"Open source addict, Adam Ralph sits down with the Western Devs to give us tips on getting started in the wacky world of open source","raw":"---\nlayout: podcast\ntitle: \"Getting Started in Open Source with Adam Ralph\"\ncategories: podcasts\ncomments: true\npodcast:\n  filename: 2016-02-15-westerndevs-open-source-with-adam-ralph.mp3\n  length: '20:19'\n  filesize: 19512125\n  libsynId: 4146532\n  anchorFmId: Getting-Started-in-Open-Source-with-Adam-Ralph-evqdhd\nparticipants:\n  - kyle_baley\n  - donald_belcham\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Adam''s blog|http://adamralph.com/'\n  - 'Adam on Twitter|https://twitter.com/adamralph'\n  - 'XBehave|https://github.com/xbehave/xbehave.net'\n  - 'FakeItEasy|https://github.com/FakeItEasy/FakeItEasy'\n  - 'ScriptCS|https://github.com/scriptcs/scriptcs'\n  - 'No pull request is too small|http://adamralph.com/2015/10/12/no-pull-request-is-too-small/'\n  - 'Pushing your project to github|http://adamralph.com/2013/05/18/oss-it-already/'\n\ndate: 2016-02-17 18:57:13\nrecorded: 2016-01-20\nexcerpt: \"Open source addict, Adam Ralph sits down with the Western Devs to give us tips on getting started in the wacky world of open source\"\n---\n\n### Synopsis\n\n- Getting Started: Launch your own or contribute\n- Itâ€™s not as scary as you think\n- Start with something you use\n- Chime in on issues\n- Taking over someone elseâ€™s project\n- Managing a project doesnâ€™t always involve a lot of time\n- The maintainer may not have all the knowledge\n- OSS already blog post\n- Donâ€™t wait for your project to be â€œready\"\n- Helping new contributors\n- The contributing.md file\n- Communicate before submitting a PR\n- Recognize that OSS is usually done on a â€œbest effortâ€ basis\n- Find a balance between too much and not enough time on OSS\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"DDWRT and logentries","authorId":"donald_belcham","slug":"DDWRT-and-logentries","date":"2016-02-17 04:10:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Networking/DDWRT-and-logentries/","link":"","permalink":"https://westerndevs.com/Networking/DDWRT-and-logentries/","excerpt":"Logging DD-WRT system entries to the cloud","raw":"---\nlayout: post\ntitle: DDWRT and logentries\ncategories:\n  - Networking  \ndate: 2016-02-16 23:10:46\ntags:\n  - Networking  \nexcerpt: \"Logging DD-WRT system entries to the cloud\"\nauthorId: donald_belcham\n---\nI've had [DD-WRT](http://www.dd-wrt.com/site/index \"dd-wrt.com\") setup on all the routers/repeaters in my house for a few years now. The platform, generally, is great. It has it's quirks, and you're not going to get my mother to install and administer it on her home network. Overall, I think it works just fine for a guy like me though.\n\nOver the last few months I've been seeing some intermittent stability issues. It seems that at the most random times the main router in the house will lock up and not respond to any pings. I have to then go downstairs, unplug the router for 30 seconds and plug it back in. By the time I'm back upstairs at my computer it's all working again.\n\nThus far the disconnections haven't happened during any meetings. When you work from home, having that happen would be a royal pain in the ass. As it is, going up and down the stairs during a train of thought is a complete pain in the ass so I've wanted to get to the bottom of it.\n\n### Logging in DD-WRT\nBy default logs for DD-WRT are stored in volatile memory. When you have to hard reset the router you, of course, lose those logs. This makes it really hard to track down the source of the problem.\n\nAn alternative is to log to the volatile memory location and then have a scheduled cron job to move those log files to an attached USB storage device. While this seems a bit better, you are never going to get the log entries that happened immediately before the router went down. There just isn't time for the cron job to run and it's on a schedule so it might not even be time for it to run.\n\nA third option is to push the logging information to a remote server. Most examples out there show a configuration where the router is pushing the log entries to a computer on the internal network. This is all well and good, but I don't want to have to guarantee that my desktop/laptop is running to receive and archive that data.\n\nBuried in [a wiki page on logging](https://www.dd-wrt.com/wiki/index.php/Logging_with_DD-WRT \"Logging with DD-WRT\") is mention of being able to log to a service called [Papertrail](https://papertrailapp.com/ \"papertrailapp.com\"). This got me thinking; I've logged from my [Arduino and Netduino](http://www.igloocoder.com/2015/04/09/Arduino-and-logging-to-the-cloud/) to [logentries.com](http://www.logentries.com \"logentries.com\") before. Why couldn't I configure DD-WRT to do the same thing?\n\n### Configuration  \nThere are three steps you need to do to get DD-WRT to log to logentries.com.  \n\n1. Setup a Log Set and Log on logentries.com  \n2. Configure DD-WRT  \n3. Restart DD-WRT  \n\n##### logentries.com\nIf you don't already have one, create an account with logentries.com for their [Free Plan](https://logentries.com/pricing/ \"logentries.com Pricing\"). Once you're logged into the website, open the Logs tab. Then click the \"+ Add New\" button and select \"Add a log\".  \n\n![](http://content.igloocoder.com/images/ddwrt-logentries-1.png)  \n\nThe will put you into the page where you select the type of logging source you're going to use. In the case of DD-WRT the logger is Syslogd so we select that one.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-2.png)\n\nIn the Configure screen you will want to add this Log to a Log Set. If you're new to logentries.com you'll want to select \"New Set\" and type a meaningful name in (i.e. Home Router). If you already have logs on logentries.com you may want to use an Existing Set.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-3.png)\n\nThe last step you need to take is to click on the \"Create Log Token\" button. This will open a bunch more information in a Configuration section of the same page.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-4.png)\n\nThe only thing you need to worry about at this time is what appears in the first command area. In my case it's telling me to use `*.* @@data.logentries.com:13630` which includes the endpoint information. This information is what you will need for configuring DD-WRT so don't ignore it.\n\nAfter you take note of the endpoint, you can click the \"Finish & View Log\" button. At this point your Log will be put into \"Discovery Mode\". You have 15 minutes to make the first call to this endpoint to activate it. That call to the logentries.com endpoint needs to happen from the IP address of your router. This will configure the Log to only accept data from that IP address.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-5.png)\n\n##### DD-WRT\nNow that you have logentries.com configured you need to quickly move through the DD-WRT setup. Remember that you only have 15 minutes to get this done. It should be easy, but don't wander off to the bowels of YouTube to watch cat videos.\n\nFirst step, log into your DD-WRT router's web interface and navigate to the Services tab.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-6.png)\n\nTowards the bottom of that page you will find the System Log section. Enable Syslogd and enter the endpoint information that you got from logentries.com earlier. You will want to ignore the `*.* @@` and only use the `data.logentries.com:XXXXX` portion, where XXXXX is the port that you were assigned.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-7.png)\n\nOnce you have this done, click Apply Settings followed by the Save button. After you've added the settings you can click the Reboot Router button and wait for your router to come back online.\n\nOnce your router has come back online and you have internet access you should be able to go to logentries.com and see that the \"Discovery on Port XXXXX\" has changed to show the IP address of your router. When I did mine it took a couple of minutes for logentries.com to process in the incoming messages and make this change. If you don't see the IP address then you weren't successful in connecting your router to logentries.com. I didn't have the happen so the only suggestion I can give you is to verify that the Syslogd settings in DD-WRT were saved and then do a reboot of the router again.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-8.png)\n\nOnce you have successfully connected the two systems you should be able to open the Log for your router and see the logging that occurs when it goes through it's startup process.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-9.png)\n\nYou could stop at this point and have all your system logging taken care of. But you can go one step further and enable firewall logging if you want. Note that when I tried this it made my router horrifically unstable and I had to completely disable it to get more than 5 consecutive minutes of uptime.  \nOpen the Security | Firewall tabs and look to the bottom of the page. There, you will see a section for Log Management. Click Enable and set up the log levels and options as you desire.\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-10.png)\n\n![](http://content.igloocoder.com/images/ddwrt-logentries-11.png)\n\n### Conclusion  \nSetting up DD-WRT and logentries.com isn't very difficult. DD-WRT can be a bit persnickety but otherwise it's a straight forward endeavour. Things you should note about this configuration:\n\n- if your internet goes down you don't get logging\n- I've not had my router freeze up since doing this so I'm not sure what, if anything, I'll capture\n- if you only configure and run the Syslogd logging you will not get much activity in the log file\n\nWhen my router craps out on me again I'll post something about what I saw, or didn't see, in the log files. ","categories":[{"name":"Networking","slug":"Networking","permalink":"https://westerndevs.com/categories/Networking/"}],"tags":[{"name":"Networking","slug":"Networking","permalink":"https://westerndevs.com/tags/Networking/"}]},{"title":"JavaScript Framework or JavaScript Core (Part 1)","authorId":"david_wesst","slug":"JavaScript-Framework-or-JavaScript-Core-Part-1","date":"2016-02-16 20:34:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"JavaScript/JavaScript-Framework-or-JavaScript-Core-Part-1/","link":"","permalink":"https://westerndevs.com/JavaScript/JavaScript-Framework-or-JavaScript-Core-Part-1/","excerpt":"Is it always better to use JavaScript Frameworks? David analyzes the pros and cons of using JavaScript frameworks versus only using core JavaScript.","raw":"---\nlayout: post\ntitle: JavaScript Framework or JavaScript Core (Part 1)\ncategories:\n  - JavaScript\ndate: 2016-02-16 15:34:46\ntags:\n  - JavaScript\nexcerpt: Is it always better to use JavaScript Frameworks? David analyzes the pros and cons of using JavaScript frameworks versus only using core JavaScript.\nauthorId: david_wesst\noriginalurl: http://www.webnotwar.ca/opensource/13996/\n---\n![js-framework-or-pure-js][1]\n\nA friend and I decided to make a video game using JavaScript.\n\nWhen deciding on whether or not to use a JavaScript game framework [I went with Phaser][2], and [Chris opted to go with pure JavaScript][3].\n\nWe were both right.\n\nIn this series of posts, I want to explain how each of us answer the question: Should I use a JavaScript Framework or pure JavaScript?\n\nThis is part 1 of 2 in the series. In this post, I'll cover why I opted to use a JavaScript framework rather than pure JavaScript. In [part 2][7], I'll cover why Chris opted not to use any frameworks and stick with pure JavaScript.\n\n##### Author's Note\n\nEven though I'm talking about game development frameworks, the concepts and rationale discussed in this series of posts can be applied to any project (game, business, or otherwise) on whether or not you should use a JavaScript framework.\n\n## Framing the Question\n\nTo better understand my decision, you should understand my goals.\n\nBeing a side project, I always make sure that I am learning something new along the way. Chris has the same perspective, outside of \"making a game\" we both have these learning goals:\n\n* To learn how to draw using Canvas element\n* To learn more about \"graphics\" programming in JavaScript\n* To learn more about making games and game development\n\nMe, being a professional application developer, I also had the goal of:\n\n* Learning to apply my application development skills to the game development world\n\nThis last goal is really important to me and is framed as a goal rather than a problem. Everything I understand about game development demonstrates that it's quite different from regular application development. In the past, I've gotten stuck thinking of a game like it's a regular application. Hopefully with this goal in mind, I'll be able to push through my tendency to build an app rather than a game.\n\n## Why use a JavaScript Framework?\n\nLet's start with the path I took for my project: to learn a new JavaScript framework.\n\n### Pros\n\nLet's start with the good stuff, shall we?\n\n#### Simplification\n\nA framework provides an abstraction over the native API's provided by JavaScript, which _should_ simplify things a little. For example, when it comes to drawing on a canvas, I create the objects that I want and give them to the engine, and voilÃ , they are drawn. I don't need to worry about the specifics on how it actually draws it, which allows me to focus on things like architecture and how to leverage the game loop itself.\n\n#### Guidance\n\nBeing new to game development, the framework provides me guidance on how to build a game rather than a business application. This has helped me immensely, as it's forced me to use a game loop and a game object hierarchy, which has helped me learn how a game works differently than a regular JavaScript application.\n\n#### Support Network &amp; Productivity\n\n![network][4]\n\nAs with any popular framework, you're going to get other experts who have already implemented things with the framework. This way, I'm not re-inventing the wheel when it comes to finding a way to implement a feature. Rather, I can learn from what others have asked, done, and documented, and apply that to my game quickly, rather than having to come up with my own solution every time something comes up.\n\nPlus, if there is an issue with one of the many functions of the framework, the community can rally behind it and push out a fix rather than just having myself to rely on for fixing any and all issues.\n\n### Cons\n\nEven with all that good, there are some shortcomings that I should note alongside the pros.\n\n#### Framework Lifespan\n\n![dead-framework][5]\n\nIn JavaScript, there are plenty of flavours of whatever framework you're using.\n\nFor every Phaser, there are at least another dozen frameworks that do the same thing, just in a different way. When it comes to picking one, there is a good chance that in a few years (or a few months in some cases) all my knowledge of the framework will be deprecated knowledge as the industry will have moved to the \"next new thing\", along with the support network I mentioned earlier.\n\n#### Performance\n\nI heard the term \"Fast Food frameworks\" for the first time from [Chris Love][6]. Now that I've gone with Phaser, my game now requires this framework to run, and its performance and compatibility with different platforms is dependent on the framework. So far, I have no issues, but as things get more complicated, I could find myself tightly coupled to the performance limitations of the framework, whatever they may be.\n\nOn top of that, even though I'm not using all of the parts of Phaser, I'll need to include the whole library, which can lead to slower load times, especially when we're talking about playing the game on the web.\n\n## To Summarize\n\nFor my needs, a framework allows me to focus on learning the higher level concepts of game development while applying my existing JavaScript and application developments. The framework also keeps me within the game development paradigm by means of forcing me down a certain implementation path (i.e. using the Phaser game loop).\n\nAll that being said, there are still great reasons not to use a JavaScript framework. We'll cover that in part 2 of this series.\n\n[1]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P1/js-framework-or-pure-js.png\n[2]: https://github.com/davidwesst/finder-game\n[3]: https://github.com/chrinkus/walk\n[4]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P1/network.png\n[5]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P1/dead-framework.png\n[6]: http://love2dev.com/#!article/Large-JavaScript-Frameworks-Are-Like-Fast-Food-Restaurants\n[7]: https://blog.davidwesst.com/2016/02/JavaScript-Framework-or-Pure-JavaScript-P2\n  \n","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"}]},{"title":"Tips for Speeding Up Visual Studio","authorId":"james_chambers","slug":"Tips-for-Speeding-Up-Visual-Studio","date":"2016-02-16 17:17:45+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Development/Tips-for-Speeding-Up-Visual-Studio/","link":"","permalink":"https://westerndevs.com/Development/Tips-for-Speeding-Up-Visual-Studio/","excerpt":"People, this is 2016. If you're waiting on your project to build or feel like your IDE is sluggish, it's time to inventory and make sure you have the optimal configuraiton for development rig. Let's talk quickly about the things that make your machine go fast (or slow) and some simple tweaks that can get your builds moving along more quickly.","raw":"---\nlayout: post\ntitle: \"Tips for Speeding Up Visual Studio\"\ncategories:\n  - Development\ndate: 2016-02-16 12:17:45\ntags:\n    - Visual Studio 2015\nauthorId: james_chambers\noriginalUrl: http://jameschambers.com/2016/02/building-your-project-should-be-pretty-quick/\n---\nPeople, this is 2016. If you're waiting on your project to build or feel like your IDE is sluggish, it's time to inventory and make sure you have the optimal configuraiton for development rig. Let's talk quickly about the things that make your machine go fast (or slow) and some simple tweaks that can get your builds moving along more quickly.\n\n![Launching VS in Safe Mode](https://jcblogimages.blob.core.windows.net:443/img/2016/02/safemode.png)\n\n<!-- more -->\n\nIn a recent ASP.NET Community Standup, the team quickly ran through a list of things that you can do to make sure that your environment is in check for building as quickly as possible and running a stable version of Visual Studio.<!-- more --> These tips included:\n - Start Visual Studio in Safe Mode\n - Run on an SSD\n - Exclude dev tools and folders from anti-virus software\n - Use a RAM disk for your code\n - Figure out _what is actually causing your grief_ \n - Disabling features\n\n_*Update*: I've gotten a few great ideas in the comments below and would love to hear more! Please feel free to add your thoughts in the comments!_\n\n## Start Visual Studio in Safe Mode\n\nThis is an easy tip to try out and has no impact on your normal dev environment. It's not as destructive as, say, resetting Visual Studio and nuking all your plugins. Just open a Visual Studio command prompt (I typically do so as admin) and launch the IDE like so:\n\n`devenv /SafeMode`\n\nMany times extensions crap out and slow you down. There are some great ones out there, so I would never suggest removing them all. I have a love-hate relationship with ReSharper and often disable it, especially when I'm travelling and can't be plugged in to step up my CPU speed.\n\n## Run on an SSD\n\nThis was a game changer for me: it was a tough pill to swallow, but going from hundreds of gigs of space down to ~40 on my first SSD was so worth it. Over the last couple of years I've upgraded along the way and currently have two that I sit on (on my two different rigs).\n\n> If you can hear your hard drive, you are going to be an unhappy individual in your life. - Scott Hanselman\n\nI recommend [this one](http://www.amazon.ca/gp/product/B00OAJ412U/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=15121&creative=330641&creativeASIN=B00OAJ412U&linkCode=as2&tag=chasthelist-20) or [this one](http://www.amazon.ca/gp/product/B00OBRFFAS/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=15121&creative=330641&creativeASIN=B00OBRFFAS&linkCode=as2&tag=chasthelist-20) and can vouch for your `_happy++;` should you make the switch. The thing I love about the Samsungs is the little Magician software they bundle with the drives so that you can easily drop your HDD and move all your data over to the new, faster kit.\n\n## Exclude Your Tooling and Code From Anti-Virus Software\n\nI want to make it perfectly clear that while I agree with this tip and do this myself, it's not one that you should take lightly as you're removing a layer of protection from your computer. So don't do it. Unless you want to run faster, in which case, exclude these guys from your real-time scan:\n\n - devenv\n - msbuild\n - your code folder\n \nI'm running McAfee, which looks like this when you drill in from the dashboard:\n\n![Excluding Tools from Virus Scans](https://jcblogimages.blob.core.windows.net:443/img/2016/02/anti-virus-exclude.PNG)\n\nPretty easy to setup, and you'll get some of your day back. But I told you not to do it.\n\n## Use a RAM Disk For Your Code\n\nThere isn't enough gain for me to recommend this one. I actually tried it a couple of years ago when I got my first SSD and the speed wasn't greatly improved. On top of that, it required slicing out RAM, running scripts to mirror or copy over the code and ran the risk of data loss if the computer freezes. If you can drop $100 or less on an SSD, it's just not worth it to run a RAM disk. Some folks argue that it's 10x the speed (or more) than an SSD, but I wasn't sold on that sales pitch.\n\n_That said_, if you're still on an HDD, I can confirm that running on a RAM disk will be a Godsend. Most recently I have used [this one](http://www.ltr-data.se/opencode.html/) but I haven't tried it on Win 10. There's also a commercial one that [a friend of mine swears by](http://www.superspeed.com/desktop/ramdisk.php). \n\n## Figure Out What is Actually Causing Your Grief\n\nThere's a great tool that most devs I know run on their machine, the much-improved version of process monitor from [sysinternals](https://technet.microsoft.com/en-us/sysinternals/processmonitor.aspx):\n\n![Process Monitor](https://jcblogimages.blob.core.windows.net:443/img/2016/02/sysinternals-procmon.PNG)\n\nSIPM will reveal everything that gets logged out by any processes that are running, from disk reads/writes to thread allocation to network activity and more. If you ever figured there wasn't a lot to do when you \"just wanted to build\", you'll be quite surprised when you build your project and see tens of thousands of events drop in milliseconds. Computers are awesome.\n\nSimply start up process monitor, then start Visual Studio and watch for events. You can start honing in and finding what is causing your grief. I find the best way to get at things is by excluding the bits that you know are not the problem, like explorer.exe, and then honing in by excluding things like reading from the registry.  \n\n## Disabling Features\n\nThings like IntelliTrace offer great benefits, but if you're in power-saver mode you're going to find yourself crying for processor cycles. I notice when travelling, when I often find myself not plugged in, that builds can drag on and debugging can be brutal if you have certain features on. Check to see if you have anything running that you don't need. \n\n## Straight from the Horse's...\n\nYou can watch the original video below (jumpt to the 36:00 mark), or hit it out on the [YouTubes](https://youtu.be/niCDYdrCOu0?t=32m6s).\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/niCDYdrCOu0\" frameborder=\"0\" allowfullscreen></iframe>\n\nA huge thanks to the leaders there on the ASP.NET team who do the weekly standup and share their insight in this area. \n\n## How About You?\n\nDo you have any tips for others using Visual Studio? Any tricks you think have helped you reach performance nirvana? Please share your thoughts below!\n\nHappy Coding!","categories":[{"name":"Development","slug":"Development","permalink":"https://westerndevs.com/categories/Development/"}],"tags":[{"name":"Visual Studio 2015","slug":"Visual-Studio-2015","permalink":"https://westerndevs.com/tags/Visual-Studio-2015/"}]},{"title":"Ergonomics","slug":"Ergonomics","date":"2016-02-13 17:35:15+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Ergonomics/","link":"","permalink":"https://westerndevs.com/podcasts/Ergonomics/","excerpt":"The Western Devs try to fight off the various aches and pains that come with chatting online all day","raw":"---\nlayout: podcast\ntitle: Ergonomics\ncategories: podcasts\ncomments: true\npodcast:\n  filename: 2016-02-13-WesternDevs-Ergonomics.mp3\n  length: '41:34'\n  filesize: 39902228\n  libsynId: 4146392\n  anchorFmId: Ergonomics-evqdhr\nparticipants:\n  - kyle_baley\n  - dave_paquette\n  - donald_belcham\n  - simon_timms\n  - dave_white\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Carpal tunnel|http://www.webmd.com/pain-management/carpal-tunnel/carpal-tunnel-syndrome-topic-overview'\n  - 'Microsoft Natural Keyboard|https://www.microsoft.com/accessories/en-us/products/keyboards/natural-ergonomic-keyboard-4000/b2m-00012'\n  - 'Ergo Dox keyboard|http://ergodox.org/'\n  - 'Kinesis Freestyle 2|http://www.kinesis-ergo.com/shop/freestyle2-for-pc-us/'\n  - 'Armrests|http://www.amazon.com/ErgoRest-330-016-BK-Articulating-Support-Black/dp/B000PSUXLS'\n  - 'Microsoft Band|https://www.microsoft.com/microsoft-band/en-us'\n  - 'Leap chair|http://www.steelcase.com/products/office-chairs/leap/'\n  - 'Reclining chairs|http://www.pcmag.com/article2/0,2817,2494027,00.asp'\n  - 'Oculus|https://www.oculus.com/en-us/'\n  - 'Hololens|https://www.microsoft.com/microsoft-hololens/en-us'\n  - 'Nintendo power glove|http://www.amazon.com/Power-Glove-Nintendo-DS/dp/B001VII7PA'\ndate: 2016-02-13 12:35:15\nrecorded: 2016-01-29\nexcerpt: The Western Devs try to fight off the various aches and pains that come with chatting online all day\n---\n\n### Synopsis\n\n* Carpal tunnel symptoms  \n* Hand position for keyboards\n* Ergonomic keyboards\n* Desk armrests\n* Desks - standing and treadmills\n* MacGyvering a treadmill desk\n* Tracking your numbers\n* Avoiding sitting down\n* Steel case chairs\n* Getting chairs on a budget\n* Ball chairs\n* Reclined position\n* Monitors: height, position, quantity\n* Oculus/Hololens\n* Is the Minority Report workstation practical?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Azure with Yves Goeleven","slug":"Azure-with-Yves-Goeleven","date":"2016-02-08 22:19:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Azure-with-Yves-Goeleven/","link":"","permalink":"https://westerndevs.com/podcasts/Azure-with-Yves-Goeleven/","excerpt":"The devs track down Microsoft MVP Yves Goeleven in a busy Thai restaurant and talk to him about Azure","raw":"---\nlayout: podcast\ntitle: \"Azure with Yves Goeleven\"\ncategories: podcasts\ncomments: true\npodcast:\n  filename: YvesGoelevenAzure.mp3\n  length: '21:56'\n  filesize: 21061896\n  libsynId: 4135263\n  anchorFmId: Azure-with-Yves-Goeleven-evqdhc\nparticipants:\n  - kyle_baley\n  - donald_belcham\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - 'Message Handler|http://www.messagehandler.net/'\n  - 'Follow Yves on Twitter|https://twitter.com/yvesgoeleven'\n  - 'Azure Marketplace|https://azure.microsoft.com/en-us/marketplace/'\n  - 'Reactive Extensions|https://msdn.microsoft.com/en-us/data/gg577609.aspx'\ndate: 2016-02-08 17:19:10\nrecorded: 2016-01-18\nexcerpt: \"The devs track down Microsoft MVP Yves Goeleven in a busy Thai restaurant and talk to him about Azure\"\n---\n\n### Synopsis\n\n* Azure for remotely monitoring cinema equipment in real-time\n* MessageHandler for IoT devices\n* The evolution of Azure services\n* The challenge of developing against a moving platform\n* Stability of total service offering vs. stability within individual services\n* Where Azure ends and third-party services begin\n* Azure Marketplace\n* Evolving away from virtual machine-based services to SaaS\n* Building for scale with IoT messages\n* The evolution of Azure from a developer platform to an IT platform\n* Why Azure?\n* The importance of support\n* Being nimble with Azure\n* Automation with ARM\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Exploring JavaScript Game Frameworks","authorId":"david_wesst","slug":"Exploring-JavaScript-Game-Frameworks","date":"2016-02-08 21:11:44+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"JavaScript/Game/Exploring-JavaScript-Game-Frameworks/","link":"","permalink":"https://westerndevs.com/JavaScript/Game/Exploring-JavaScript-Game-Frameworks/","excerpt":"David reviews a handful of JavaScript-based game frameworks he went through before selecting one for his new game.","raw":"---\nlayout: post\ntitle: Exploring JavaScript Game Frameworks\ncategories:\n    - JavaScript\n    - Game\ndate: 2016-02-08 16:11:44\ntags:\n    - JavaScript\n    - Game Development\nexcerpt: David reviews a handful of JavaScript-based game frameworks he went through before selecting one for his new game.\nauthorId: david_wesst\noriginalurl: http://www.webnotwar.ca/opensource/exploring-javascript-game-frameworks\n---\nLast month I explored a number of JavaScript-based game frameworks in an attempt to pick the perfect one for my first game.\n\nI thought I'd take a moment to share four of them that really stood out to me, along with a few honerable mentions.\n\n**Note** that I've left a bunch off the list, as that is the nature of a \"top X\" list. If I missed your favourite one, then feel free to let me know in the comments!\n\n### Phaser\n![](http://blog.davidwesst.com/2016/02/Exploring-JavaScript-Game-Frameworks/phaser.png)\n\nI'll start with the one I picked for my game. [Phaser](http://phaser.io/), developed by Richard Davey (a.k.a. [@PhotonStorm](https://twitter.com/photonstorm)) is an open source JavaScript game framework for desktop and mobile games.\n\nAfter going through [the Phaser tutorial from GameFromScratch.com](http://www.gamefromscratch.com/page/Adventures-in-Phaser-with-TypeScript-tutorial-series.aspx), here's what I liked about it:\n\n* It's just a framework, plain and simple. No special IDE or engines needed, just code.\n* It's [open source](https://github.com/photonstorm/phaser) and widely used, and has a strong community surrounding it\n* It's stable, mature, and actively developed for a JavaScript framework. Currently at vervsion 2.4.4 at the time of writing, with [v3 in the works](http://phaser.io/labs).\n\nWhat really drew me towards Phaser was it's well documented API, and the numerous tutorials and examples from the community. Even though I'm just starting out, every feature that I've looked to implement appears to have been at least partially implemented and documented in a tutorial, making the learning curve much less steep than it could have been.\n\nYou can check it out on [GitHub](https://github.com/photonstorm/phaser).\n\n### Superpowers\n![](http://blog.davidwesst.com/2016/02/Exploring-JavaScript-Game-Frameworks/superpowers.png)\n\nI think [Superpowers](http://superpowers-html5.com/index.en.html) one has a lot of potential. Seeing the [game demo](http://sparklinlabs.itch.io/discover-superpowers) they made really gave me all the proof I needed believe in what they're doing over there at Sparklin Labs.\n\nHere's the highlights that stood out to me:\n* Totally [open source](https://github.com/superpowers/superpowers), as in the engine and the tooling\n* Toolset is a browser-based IDE for game development\n* Powered by [TypeScript](http://www.typescriptlang.org/), giving you the power of JavaScript with some extras\n\nThe only drawback I found was that it was really new when I was shopping for a game framework, which means that it's more on the bleeding edge than the cutting edge IMHO. Still, this is one I'll be watching.\n\nIf you're interested in learning how to leverage Superpowers, I suggest checking out their [documentation site](http://docs.superpowers-html5.com/en/getting-started/about-superpowers) or this [GameFromScratch.com tutorial series](http://www.gamefromscratch.com/post/2016/02/01/Superpowers-Tutorial-Series-Part-One-Getting-Started.aspx).\n\n### PlayCanvas\n![](http://blog.davidwesst.com/2016/02/Exploring-JavaScript-Game-Frameworks/playcanvas.png)\n\nI discovered [PlayCanvas](https://playcanvas.com/) when I bought a Humble Bundle focused around game development tools. PlayCanvas seems to be cut from the same cloth as SuperPowers, but is a bit more mature considering it's been around longer. It focuses more on 3D content, although I'm sure you could do 2D content just fine.\n\nHere's what made PlayCanvas interesting to me:\n* Engine is [open source](https://github.com/playcanvas/engine), but tooling is not\n* Tooling runs in-browser, making it accessible to any development platform\n* Tooling was very easy to use, and I was able to start for free\n\nPlayCanvas really reminded me the [Unity3D](https://unity3d.com/) toolset, which is widely used in the industry. Given, the value is really in the tooling, which isn't open source. Still, it might be worth checking out if you are looking for a toolset to go with your game engine.\n\nHere's another [GameFromScratch tutorial](http://www.gamefromscratch.com/post/2015/04/19/A-Closer-Look-at-the-PlayCanvas-Game-Engine.aspx) that helped me get things moving.\n\n### Haxe\n![](http://blog.davidwesst.com/2016/02/Exploring-JavaScript-Game-Frameworks/haxe.png)\n\nOkay, so [Haxe](http://haxe.org/) really isn't JavaScript, but it's very similar to it and supports web development, so I'm including it. Plus, it really did stand out as a viable option to me when picking my game for a few reasons:\n\n* It's a language/toolkit that focuses on game / rich UI development\n* Very mature and has plenty of support around it for whatever games you'd like to develop\n* Is the cornerstone of the [Haxe Foundation](http://haxe.org/foundation/)\n* When targetting JavaScript, you can leverage other JavaScript libraries\n\nHaxe is something that I keep coming across in my adventures with game development, and one day I'll head back and learn more about it. Just need to find the right project, and feel a bit more comfortable with game development.\n\nYou can checkout the [source code here](https://github.com/HaxeFoundation/haxe) but I'd suggest exploring [the website](http://haxe.org/) for more information.\n\n## Honorable Mentions\nTo close things out, I wanted to highlight a few other frameworks and toolsets that I thought were pretty cool but wasn't what I was looking for with my project.\n\n* [BabylonJS](http://babylonjs.com/)\n* [ThreeJS](http://threejs.org/)\n* [PixiJS](http://www.pixijs.com/)\n\nIf I missed your favourite (which I'm sure I did) you should drop it in the comment section and share your favourite library and/or toolset. Extra points for open source repository links on the libraries.","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/categories/JavaScript/"},{"name":"Game","slug":"JavaScript/Game","permalink":"https://westerndevs.com/categories/JavaScript/Game/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://westerndevs.com/tags/JavaScript/"},{"name":"Game Development","slug":"Game-Development","permalink":"https://westerndevs.com/tags/Game-Development/"}]},{"title":"Leaving Your Work at Work (When You Work From Home)","authorId":"james_chambers","slug":"leaving-your-work-at-work-when-you-work-from-home","date":"2016-02-01 11:56:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Work-Life-Balance/leaving-your-work-at-work-when-you-work-from-home/","link":"","permalink":"https://westerndevs.com/Work-Life-Balance/leaving-your-work-at-work-when-you-work-from-home/","excerpt":"Finding success as a remote worker is pretty darn hard. Unless you're a complete natural, you will need to have the perfect combination of environment, corporate trust, family time and personal time. Failing being perfect, you can just take the route I took and try your best to follow a few practices that can help you disconnect at the end of the day.","raw":"---\ntitle: Leaving Your Work at Work (When You Work From Home)\nlayout: post\ntags:\n  - Remote Work\n  - Life Hacks\ncategories:\n  - Work Life Balance\nauthorId: james_chambers\noriginalurl: 'http://jameschambers.com/2016/01/leaving-your-work-at-work-when-you-work-from-home/'\ndate: 2016-02-01 6:56:56\n---\n\n![Leaving Your Work at Work (When You Work From Home)](https://jcblogimages.blob.core.windows.net:443/img/2016/01/shutting-down-banner.png)\n\nFinding success as a remote worker is pretty darn hard. Unless you're a complete natural, you will need to have the perfect combination of environment, corporate trust, family time and personal time. Failing being perfect, you can just take the route I took and try your best to follow a few practices that can help you disconnect at the end of the day. \n\n<!-- more -->\n\nTo that end, corporate trust is just as tricky a puzzle as the rest of the challenges for work-at-homes; however, provided you are working at or moving to a company that understands remote work and empowers you to succeed, there are things you need to be doing to build and maintain that trust. \n\n## Establish A Routine\n\nWhen you work in an office, chances are you have some form of a daily ritual that you partake in. You have things you do when get up as you prepare for your workday explicitly, and maybe some that happen subconsciously. You make some eggs for your husband or some bacon for your wife while they get the kids ready for school. Maybe you spend time in some personal study. Eventually you start to make your way to the office on foot, by bike, in your vehicle or perhaps on some form of mass public transit. Some poor saps have even parted ways with their cash to buy Segways and, sadly, they use those to get where they're going. You swing by your favorite coffee joint, or meet up with co-workers in the lobby and chat on the elevator ride and at some point you find yourself at your desk.\n\nThe point is simply this: getting from point A to point B _is part of the routine_. You start thinking about work, your schedule, what happened last week. If your transportation is \"hands off\" then you can catch up on emails or start to plan your day out. The _transition_ is more important than the destination in this case, but this is unfortunately the very thing that most remote workers will omit.\n\nFor me, I love [walking to and from work](http://jameschambers.com/2015/03/working-from-home-and-walking-to-work-surviving-remote-work/). Yes, I work from home, but I've made the walk part of my ritual. In the morning it lets me tune into my work day and in the late afternoon when I'm leaving work I can reset and get ready to enjoy my family. \n\n>Moving directly from a work context into a home context not only blurs the lines, but it discounts the fact that you need a different mindspace to manage your work than you do to manage your home.\n\nYou need a line to say, \"this is where work stops and my personal life begins.\"\n\n## Define Boundaries\n\nYou have an agreement with your employer to perform certain units of work, but it's your responsibility to [set expectations accordingly](http://jameschambers.com/2016/01/work-is-not-life/). That means that you should engage in practices that allow you to define a clean break from the work day.\n\nOne of the ways that I do this is by virtue of leaving my contact points at work. I have a Skype number that is associated with my work duties that I don't answer during evening and weekends. When I'm getting ready to leave work I change my email settings on my phone to only check for new mail on demand. \n\nAnother way to set boundaries is to actively engage in other things in the non-work times. That means dedicating time to your family, and if you must, signing your family up for things that happen during those evening and weekend hours, or getting involved with a group of friends that are active and motivated to do non-work-like things. This could be snowboarding, gaming, camping, playing music or hanging out for wings. If you make commitments that take you away from work, well, you can't work.  \n\n## Remember the Inverse\n\nLet's also note that the reverse must also hold true; if you are going to be setting limits on when you can and can't access work, you must also set limits on when you can and can't access \"home\".  I have done this in a number of ways:\n\n - I say \"good-bye\" to my wife and kids when I \"leave\" in the morning\n - I don't have a home phone in my office\n - I don't keep my personal cell phone handy\n - I don't check personal email during work time\n - I treat my time in my office as though I was in a different place than my home, as though others were working in the next office or room\n - Things like arranging child care or planning to shuttle kids between activies happens during the evening\n - My wife and kids don't re-engage with me until I come home and say \"hello\" again\n\nThere are, of course, some exceptions to these rules and there must be. My oldest son lives with Type 1 Diabetes, so when my wife is unavailable to assist, I bring the house phone into my workspace for emergencies. When I am expecting a package or someone to swing by I will indeed answer the door. But these are things that you can also make your co-workers aware of so that the disruption is not something that derails your work and they know that it may be coming.  Emergencies, on the other hand, are emergencies and I don't think those should be viewed any differently by your team and management than they would if you were in the office.\n\n## Shutting Down\n\nHere are a few tips to help you close things off at the end of the day. \n- Flag important messages or tasks that need to be accomplished when you return the next working day.\n- Be sure to log your time daily and submit your timesheet - this is a trust device for your employer.\n- Change settings on your phone to check work email on demand.\n- If you will be away for an extended period of time or even just a long weekend, set away message.\n- Close everything. All your apps. Close your email client and your browser tabs and the TPS Report you're working on (including cover pages) and everything else that is on your system.\n\nWith practices like these in place, you'll find that you actually have some peace of mind through the evening, knowing that you've tied things off at the end of the day. You don't have to worry about lost work, or missed follow-ups. \n\nBy the way, the same is also true when starting your day: you should have an easy way to get going in the morning so that as soon as you sit down at your computer you are ready to be productive and get into your daily flow. As a software developer, I actually have a script that I run that spins up my tools and opens the folders that I need so that everything I'll be working on is in front of me. Even if my computer has rebooted or applied updates, I am ready to rock out on my project in just a few seconds.  If you're interested, here's an example of a script I use to get my day started.\n\n<script src=\"https://gist.github.com/MisterJames/8a1548564202a1b96d1a.js\"></script>\n\nFinally, wind down! [I walk home](http://jameschambers.com/2015/03/working-from-home-and-walking-to-work-surviving-remote-work/) on most days to disconnect from work. I have friend who hits the gym and others for whom the timing works to break off at the end of the day and go pick up the kids.\n\n## Where Does This Leave You?\n\nIf you are currently working at home and don't have these practices in place, it will take some time to work up to them. While they mostly seem simply on the surface, some habits are hard to break, and changing expectations is even more difficult if you've previously let work creep out of your home office.\n\nAs I always say when talking with folks about this, you really need to experiment and find the things that work for you for where you're at and keep evaluating if there are tweaks or corrections you need to make along the way.\n\nAny effort to help you move away from the feeling of constantly being connected to work will help you better enjoy your evenings and weekends. I hope you find a few gems in here that encourage you to work towards that goal.\n\nHappy relaxing! (Now, go spend some time with you family or friends!)\n","categories":[{"name":"Work Life Balance","slug":"Work-Life-Balance","permalink":"https://westerndevs.com/categories/Work-Life-Balance/"}],"tags":[{"name":"Remote Work","slug":"Remote-Work","permalink":"https://westerndevs.com/tags/Remote-Work/"},{"name":"Life Hacks","slug":"Life-Hacks","permalink":"https://westerndevs.com/tags/Life-Hacks/"}]},{"title":"ProTip: Get a random date in SQL Server","authorId":"simon_timms","slug":"Random-date-in-sql","date":"2016-01-31 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"SQL-Server/Random-date-in-sql/","link":"","permalink":"https://westerndevs.com/SQL-Server/Random-date-in-sql/","excerpt":"How to fill a table with some random dates.","raw":"---\nlayout: post\ntitle: \"ProTip: Get a random date in SQL Server\"\ntags:\n  - SQL Server\ncategories:\n  - SQL Server\nauthorId: simon_timms\nexcerpt: \"How to fill a table with some random dates.\"\n---\n\nNeed to put a random date in each row of a table? Here is how to do it:\n\nI have a table which contains a RequiredCompletionDate and I wanted to give it a random date in last year so I did\n\n\n{% codeblock lang:sql %}\nupdate systems \n   set RequiredCompletionDate = DATEADD(day, \n                                        ABS(CAST(CAST(NEWID() AS VARBINARY) AS INT)) % 365, \n                                        '2015-01-01')\n{% endcodeblock %}","categories":[{"name":"SQL Server","slug":"SQL-Server","permalink":"https://westerndevs.com/categories/SQL-Server/"}],"tags":[{"name":"SQL Server","slug":"SQL-Server","permalink":"https://westerndevs.com/tags/SQL-Server/"}]},{"title":"Octopus Deploy Gotchas: 400 Error and Can't Create File When It Exists","authorId":"darcy_lussier","slug":"OctopusDeployGotchas","date":"2016-01-29 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"News/OctopusDeployGotchas/","link":"","permalink":"https://westerndevs.com/News/OctopusDeployGotchas/","excerpt":"Lessons from setting up a successful Octopus deployment.","raw":"---\nlayout: post\ntitle: \"Octopus Deploy Gotchas: 400 Error and Can't Create File When It Exists\"\ntags:\n  - Octopus\n  - TFS\n  - TFS Build\n  - OctopusDeploy\ncategories:\n  - News\nauthorId: darcy_lussier\nexcerpt: \"Lessons from setting up a successful Octopus deployment.\"\n---\nI came across two isues with setting up a deployment with TFS and Octopus that seem pretty common based on my web searches. I wanted to document the root causes and the solutions I used to get around them.\n\n### 400 Error\n\nI began seeing 400 - Bad Request error messages when queueing up a build in TFS. The weird thing is that a build done seconds before was fine, but any subsequent build after that would throw the 400.\n\nThe reason for this is because the NuGet package being pushed had the same identifier as the previous NuGet package. To avoid this, you can do a few things:\n\n1. Manually set the version number.\n2. Write some script that will set the package version number.\n3. Enable auto-incrementing on your project's asembly info.\n\nI did #3. This is as simple as going into the AssemblyInfo.cs file of any project that will be deployed as part of the build and changing the AssemblyVersion value to..\n\n[assembly: AssemblyVersion(\"1.0.*\")]\n\nOctoPack will automagically take the automatically incremented assembly version and use that for creating the NuGet package version number. The caveat here is that you must ***change code*** for this to work - by that I mean change a code file in your project which will trigger the assembly version to be incremented when a new build kicks off. For web projects, remember that ***HTML files don't get compiled*** so adding a line break in one of your views won't help because those don't get compiled and ergo don't trigger the assembly version increment.\n\nThis is not the ideal way to deal with this; its a stop gap. I shouldn't have to change a code file just to push new web files or images or whatever. I'm looking at how to implement a better solution, but for now this does work.\n\nAlso while there is docmentation that says adding \"?replace=true\" to your OctoPackPublishPackageToHttp MSBuild argument will force a NuGet package to deploy regardless if its the same version, I couldn't get that working. I have an email in to Octopus support and I'll update this post if I get it figured out.\n\n### Cannot Create a File When It Already Exists\n\nOnce Octopus does the deployment, you may see this error:\n\n***Web site is stopped. Attempting to start...***\n\n***start-webitem : Cannot create a file when that file already exists***\n\nIn my case, my deployments were completed when I manually checked them but this error forced all my deployments to show as having errored out and unsuccessful.\n\nThis issue seems to happen if you're deploying to a virtual directory in IIS and you're not using the \"IIS6+ Home Directory\" feature.\n\nIn Octopus select a project, click on Process, then click on the package you have set to deploy to IIS. Click on ***Configure Features*** (it's a hyperlink found closer to the bottom of the page. It's not obvious, so even doing a text search for it on the page can be a help).\n\nThis will bring up a dialogue of Enabled Features. Here you want to ***deselect the \"IIS Web Site and Application Pool\"*** and instead ***select \"IIS6+ Home Directory\"***. Now part of your step will have an area to specify a site/virtualdirectory path. You can only have one of the IIS-related features enabled at a time so make sure you uncheck \"IIS Web Site and Application Pool\" feature if its checked.\n\nOn a side note, but hand-in-hand with typical virtual directory deployments, if you need to have your app deployed in a specific spot (read: your virtual directory points to a folder outside of IIS) then also enable the Custom Install Directory option in Configuration Features; this will let you specify which folder the package contents should be deployed to.\n","categories":[{"name":"News","slug":"News","permalink":"https://westerndevs.com/categories/News/"}],"tags":[{"name":"Octopus","slug":"Octopus","permalink":"https://westerndevs.com/tags/Octopus/"},{"name":"TFS","slug":"TFS","permalink":"https://westerndevs.com/tags/TFS/"},{"name":"TFS Build","slug":"TFS-Build","permalink":"https://westerndevs.com/tags/TFS-Build/"},{"name":"OctopusDeploy","slug":"OctopusDeploy","permalink":"https://westerndevs.com/tags/OctopusDeploy/"}]},{"title":"Why can't you just communicate properly?","authorId":"kyle_baley","slug":"Why-can-t-you-just","date":"2016-01-27 21:09:57+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"communication/Why-can-t-you-just/","link":"","permalink":"https://westerndevs.com/communication/Why-can-t-you-just/","excerpt":"Simply follow a few rules to improve engagement","raw":"---\nlayout: post\ntitle: \"Why can't you just communicate properly?\"\ntags: communication\ncategories:\n  - communication\nexcerpt: Simply follow a few rules to improve engagement\nauthorId: kyle_baley\nalias: /category1/category2/Why-can-t-you-just/\ndate: 2016-01-27 16:09:57\n---\n\nOnline communication bugs me. Actually, _bugs_ isn't accurate. Maybe _saddens and fatigues_. When volleying with people hiding behind their keyboard shield and protected by three timezones, you have to make a conscious effort to remain optimistic. It's part of the reason I haven't taken to [Twitter](http://twitter.com/kyle_baley) as much as I probably should.\n\n{% img \"pull-left\" \"https://upload.wikimedia.org/wikipedia/en/7/79/The_Simpsons-Jeff_Albertson.png\" %}\n\nI've talked on this subject [before](http://kyle.baley.org/2009/05/and-you-opened-your-mouthwhy-or-how-to-comment-for-the-greater-good) and it's something I often have in the back of my mind when reading comments. It's come to the forefront recently with some conversations we've had at Western Devs, which led to our most recent [podcast](http://www.westerndevs.com/podcasts/Podcast-Is-Web-Development-Terrible/). I wasn't able to attend so here I am.\n\nThere are certain phrases you see in comments that automatically seem to devolve a discussion. They include:\n\n* \"Why don't you just...\"\n* \"Sorry but...\"\n* \"Can't you just...\"\n* \"It's amazing that...\"\n\nUltimately, all of these phrases can be summarized as follows:\n\n<div style=\"font-size: 18px;margin: 15px;\">I'm better than you and here's why...</div>\n\nIn my younger years, I could laugh this off amiably and say \"Oh this wacky world we live in\". But I'm turning 44 in a couple of days and it's time to start practicing my crotchety, even if it means complaining about people being crotchety.\n\nSo to that end: I'm asking, nay, _begging_ you to avoid these and similar phrases. This is for your benefit as much as the reader's. These phrases don't make you sound smart. Once you use them, it's very unlikely anyone involved will feel better about themselves, let alone engage in any form of meaningful discussion. Even if you have a valid point, who wants to be talked down to like that? Have you completely forgot what it's like to learn?\n\n<div class=\"notice\">\n\"For fuck's sake, Mom, why don't you just type the terms you want to search for in the address bar instead of typing WWW.GOOGLE.COM into Bing?\"\n</div>\n\nNow I know (from experience) it's hard to fight one's innate sense of superiority and the overwhelming desire to make it rain down on the unwashed heathen. So take it in steps. After typing your comment, remove all instances of \"just\" (except when just means \"recently\" or \"fair\", of course). The same probably goes for \"simply\". It has more of a condescending tone than a dismissive one. \"Actually\" is borderline. Rule of thumb: Don't start a sentence with it.\n\nOnce you have that little nervous tic under control, it's time to remove the negatives. Here's a handy replacement guide to get you started:\n\n| Original phrase | Replacement|\n|-----------------|------------|\n| \"Can't you\" | \"Can you\" |\n| \"Why don't you\" | \"Can you\" |\n| \"Sorry but\" | _no replacement; delete the phrase_ |\n| \"It's amazing that...\" | _delete your entire comment and have a [dandelion break](http://www.gocomics.com/bloomcounty/2008/12/03)_ |\n\n<div style=\"margin-top:15px;\"></div>\n\nSee the difference? Instead of saying _Sweet Zombie Jayzus, you must be the stupidest person on the planet for doing it this way_, you've changed the tone to _Have you considered this alternative_? In both instances, you've made your superior knowledge known but in the second, it's more likely to get acknowledged. More importantly, you're less likely to look like an idiot when the response is: _I did consider that avenue and here are legitimate reasons why I decided to go a different route_.\n\nTo be fair, sometimes the author of the work you're commenting on needs to be knocked down a peg or two themselves. I have yet to meet one of these people who respond well to _constructive_ ~~criticism~~ critique, let alone the _destructive_ type I'm talking about here. Generally, I find they feel the need to cultivate an antagonistic personality but in my experience, they usually don't have the black turtlenecks to pull it off. Usually, it ends up backfiring and their dismissive comments become too easy to dismiss over time.\n\nKyle the Inclusive\n","categories":[{"name":"communication","slug":"communication","permalink":"https://westerndevs.com/categories/communication/"}],"tags":[{"name":"communication","slug":"communication","permalink":"https://westerndevs.com/tags/communication/"}]},{"title":"Launching ASP.NET Monsters","authorId":"simon_timms","slug":"Launching-Monsters","date":"2016-01-26 19:32:21+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"WhatWeveDone/Launching-Monsters/","link":"","permalink":"https://westerndevs.com/WhatWeveDone/Launching-Monsters/","excerpt":"We're really excited about the new ASP.NET Core and the future of ASP.NET in general. So much so that we're starting a specialty site and a bi-weekly video blog all about it.","raw":"---\nlayout: post\ntitle: Launching ASP.NET Monsters\ntags:\n  - ASP.NET\ncategories:\n  - WhatWeveDone\nauthorId: simon_timms\ndate: 2016-01-26 14:32:21\nexcerpt: We're really excited about the new ASP.NET Core and the future of ASP.NET in general. So much so that we're starting a specialty site and a bi-weekly video blog all about it. \n\n---\n![http://aspnetmonsters.com/](http://aspnetmonsters.com/images/logo_579.png)\n\nOne of the best things about the Western Devs is that we all have our own areas of interest and specialty. From time to time our interests align pretty closely and when this happens great things are produced. The [ASP.NET Monsters](http://aspnetmonsters.com/) are just such a thing. \n\nSome of us are really excited about ASP.NET Core so we broke off a special interest group consisting of \n<div style=\"width: 100%\">\n    <div style=\"width: 30%; display: inline-block\">\n        <img src=\"http://aspnetmonsters.com/images/thumb_james.png\" style=\"width: 100%\"/>\n        <div style=\"text-align: center; font-weight: bold\">James Chambers</div>\n    </div>\n    <div style=\"width: 30%; display: inline-block\">\n        <img src=\"http://aspnetmonsters.com/images/thumb_dave.png\" style=\"width: 100%\"/>\n        <div style=\"text-align: center; font-weight: bold\">Dave Paquette</div>\n    </div>\n    <div style=\"width: 30%; display: inline-block\">\n            <img src=\"http://aspnetmonsters.com/images/thumb_simon.png\" style=\"width: 100%\"/>\n            <div style=\"text-align: center; font-weight: bold\">Simon Timms</div>\n    </div>\n</div>\n\nWe're going to be blogging a lot about how to use ASP.NET Core and we're even going to record an ongoing series of videos, twice a week, with tips and tutorials. These videos will be short 5-10 minute affairs so you can watch them in your lunch break. Head over to [ASP.NET Monsters](http://aspnetmonsters.com/) to check out what we've unleashed. ","categories":[{"name":"WhatWeveDone","slug":"WhatWeveDone","permalink":"https://westerndevs.com/categories/WhatWeveDone/"}],"tags":[{"name":"ASP.NET","slug":"ASP-NET","permalink":"https://westerndevs.com/tags/ASP-NET/"}]},{"title":"Is Web Development Terrible","slug":"Podcast-Is-Web-Development-Terrible","date":"2016-01-24 15:40:13+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Podcast-Is-Web-Development-Terrible/","link":"","permalink":"https://westerndevs.com/podcasts/Podcast-Is-Web-Development-Terrible/","excerpt":"The Western Devs discuss whether web development really is terrible","raw":"---\nlayout: podcast\ntitle: 'Is Web Development Terrible'\ncategories: podcasts\ncomments: true\npodcast:\n  filename: western_devs_podcast_2016-01-15.mp3\n  length: '40:55'\n  filesize: 39288276\n  libsynId: 4094859\n  anchorFmId: Is-Web-Development-Terrible-evqdjb\nparticipants:\n  - simon_timms\n  - darcy_lussier\n  - amir_barylko\n  - david_wesst\n  - james_chambers\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - Sad State of Web Development (article)|https://medium.com/@wob/the-sad-state-of-web-development-1603a861d29f#.gs36fs7zy\n  - Sad State of Entitled Web Developers (article)|https://medium.com/swlh/the-sad-state-of-entitled-web-developers-e4f314764dd#.57ww6gwmp\ndate: 2016-01-24 10:40:13\nrecorded: 2016-01-15\nexcerpt: \"The Western Devs discuss whether web development really is terrible\"\n---\n\n### Synopsis\n\n* Is web development sad?\n* Should we be critisizing software and developers?\n* What's great about web development\n* What's up with all the packages in node?\n* Is software too complex or is it just the nature of abstractions?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Submitting Your First Pull Request","authorId":"dave_paquette","slug":"Submitting-Your-First-Pull-request","date":"2016-01-24 14:32:21+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"GitHub/Submitting-Your-First-Pull-request/","link":"","permalink":"https://westerndevs.com/GitHub/Submitting-Your-First-Pull-request/","excerpt":"Over the last few years, we have seen a big shift in the .NET community towards open source. In addition to a huge number of open source community led projects, we have also seen Microsoft move major portions of the .NET framework over to GitHub. With all these packages out in the wild, the opportunities to contribute are endless. In this post I will guide you through the process of submitting your first pull request.","raw":"---\nlayout: post\ntitle: Submitting Your First Pull Request\ntags:\n  - GitHub\n  - Open Source\n  - Git\ncategories:\n  - GitHub\nauthorId: dave_paquette\ndate: 2016-01-24 09:32:21\nexcerpt: Over the last few years, we have seen a big shift in the .NET community towards open source. In addition to a huge number of open source community led projects, we have also seen Microsoft move major portions of the .NET framework over to GitHub. With all these packages out in the wild, the opportunities to contribute are endless. In this post I will guide you through the process of submitting your first pull request.\n---\n\nOriginally posted to [http://blogs.msdn.com/b/cdndevs/archive/2016/01/06/submitting-your-first-pull-request.aspx](http://blogs.msdn.com/b/cdndevs/archive/2016/01/06/submitting-your-first-pull-request.aspx \"Submitting your first Pull Request - Canadian Developer Connection\")\n\nOver the last few years, we have seen a big shift in the .NET community towards open source. In addition to a huge number of open source community led projects, we have also seen Microsoft move major portions of the .NET framework over to GitHub.\n\nWith all these packages out in the wild, the opportunities to contribute are endless. The process however can be a little daunting for first timers, especially if you are not using git in your day-to-day work. In this post I will guide you through the process of submitting your first pull request. I will show examples from my experience contributing to the [Humanitarian Toolbox][3]'s [allReady project][4]. As with all things git related, there is more than one way to do everything. This post will outline the workflow I have been using and should serve as a good starting point for most .NET developers who are interested in getting started with open source projects hosted on GitHub.\n\n## Installing GitHub for Windows\n\nThe first step is to install [GitHub for Windows][5]. GitHub's Windows desktop app is great, but the installer also installs the excellent posh-git command line tools. We will be using a combination of the desktop app and the command line tools.\n\n## Forking a Repo\n\nThe next step is to fork the repository (_repo_ for short) to which you are hoping to contribute. It is very unlikely that you will have permissions to check in code directly to the actual repo. Those permissions are reserved for project owners. The process instead is to fork the repo. A fork is a copy of the repo that you own and can do whatever you want with. Create a fork by clicking the Fork button on the repo.\n\n![Forking a repo][6]\n\nThis will create the fork for you. This is where you will be making changes and then submitting a pull request to get your changes merged in to the original repo.\n\n![Your forked repo][7]\n\nNotice on my fork's master branch where it says _This branch is even with HTBox:master_. The branch _HTBox:master_ is the master branch from the original repo and is the upstream for my master branch. When GitHub tells me my branch is even with master that means no changes have happened to HTBox:master and no changes have happened to my master branch. Both branches are identical at this point.\n\n## Cloning your fork to your local machine\n\nNext up, you will want to clone the repo to your local machine. Launch GitHub for Windows and sign in with your GitHub account if you have not already done so. Click on the + icon in the top right, select Clone and select the repo that you just forked. Click the big checkmark button at the bottom and select a location to clone the repo on your local disk.\n\n![Cloning your fork][8]\n\n## Create a local branch to do your work in\n\nYou could do all your work in your master branch, but this might be a problem if you intend to submit more than one pull request to the project. You will have trouble working on your second pull request until after your first pull request has been accepted. Instead it is best practice to create a new branch for each pull request you intend to submit.\n\n_As a side note, it is also considered best practice to submit pull requests that solve 1 issue at a time. Don't fix 10 separate issues and submit a single pull request that contains all those fixes. That makes it difficult for the project owners to review your submission._\n\nWe could use GitHub for Windows to create the branch, but we're going to drop down to the command line here instead. Using the command line to do git operations will give you a better appreciation for what is happening.\n\nTo launch the git command line, select your fork in GitHub for Windows, click on the Settings menu in the top right and select Open in Git Shell.\n\n![Open Git shell][9]\n\nThis will open a posh-git shell. From here, type the command `git checkout -b MyNewBranch`, where MyNewBranch is a descriptive name for your new branch.\n\n![Create new branch][10]\n\nThis command will create a new branch with the specified name and switch you to that branch. Notice how posh-git gives you a nice indication of what branch you are currently working on.\n\n_Advanced Learning_: Learn more about git branching with this interactive tutorial [http://pcottle.github.io/learnGitBranching/](http://pcottle.github.io/learnGitBranching/)\n\n_Pro tip:_ posh-git has auto complete. Type `git ch` + `tab` will autocomplete to `git checkout`. Press tab multiple times to cycle through available options. This is a great learning tool!\n\n## Committing and publishing your changes\n\nThe next step is to commit and publish your changes to GitHub. Make your changes just like you normally would (Enjoyâ€¦this is the part where you actually get to write code!). When you are done making your changes, you can view a list of your changes by typing the `git status` command.\n\n![git status][11]\n\nTo commit your changes, you first need to add them to your current set of changes. To add all your changes, enter the `git add â€“A` command. Note that the git add command doesn't actually do anything other than get those changes ready to commit. Once your changes have been added, you can commit your changes using the `git commit â€“m \"Your commit message\"` command.\n\n![git commit][12]\n\nIf you wanted to commit only some of the files you changed, you would need to add each of the files individually before doing the commit. This can be a little tedious. In this case, you might want to use the GitHub for Windows app. Simply select the files that you want to include, enter your commit message and click the Commit to _YourBranch_ button. This will do both the add and commit operations as a single operation. The GitHub for Windows app also shows you a diff for each file which makes it a great tool for reviewing your changes.\n\n![Review changes][13]\n\nNow your changes have been committed locally, but they have not been published to GitHub yet. To do this, you need to push your branch to a copy on GitHub. You can do this from the command line by using the `git push` command.\n\n![git push][14]\n\nNotice that git detected this branch does not exist on GitHub yet and very kindly tells me the command I need to use to create the upstream branch. Alternatively, you could simply use the Publish button in GitHub for Windows.\n\n![Publish from GitHub for Windows][15]\n\nNow the branch containing your changes should show up in your fork on the GitHub website.\n\n![Published branch][16]\n\nGitHub says my branch is 1 commit ahead of HTBox:master. That's what I want to see. I made 1 commit in my branch and no one has made any commits to HTBox:master since I created my fork. That should make my pull request clean and easy to merge. In some cases, HTBox:master will have changed since the time you started working on your branch. We'll take a look at how to handle that situation later. For now let's proceed with creating this pull request.\n\n## Creating your pull request\n\nThe next step is to create a pull request so your code can (hopefully) be merged into the original repo.\n\nTo create your pull request, click on the Compare & pull request button that is displayed when viewing your branch on the GitHub website. If for some reason that button is not visible, click the Pull Request link on your branch.\n\n![Create pull request][17]\n\nOn the Pull Request page, you can scroll down to review the changes you are submitting. For some projects, you will also see a link to guidelines for contributing. Be descriptive in your pull request. Provide information on the change you made so the project owners know exactly what you were trying to accomplish. If there is an issues you are addressing with this pull request you should reference it by number (e.g. #124) in the description of your pull request. If everything looks good, click the Create Pull Request button.\n\n![Enter pull request details][18]\n\nYour pull request has now been created and is ready for the project owners to review (and hopefully accept). Some projects will have automated checks that happen for each pull request. allReady has an AppVeyor build that compiles the application and runs unit tests. You should monitor this and ensure that all the checks pass.\n\n![Automated checks on pull requests][19]\n\nIf all goes as planned, your pull request will be accepted and you will feel a great sense of accomplishment. Of course, things don't always go as planned. Let's explore how to handle a few common scenarios.\n\n## Making changes to an existing pull request\n\nOften, the project owners will make comments on your pull request and ask you to make some changes. Don't feel bad if this happensâ€¦my first pull request to a large project had [59 comments][20] and required a fair bit of rework before it was finally merged in to the master branch. When this happens, **don't close the pull request**. Simply make your changes locally, commit them to your local branch, then push those changes to GitHub.\n\n![Push changes to an existing pull request][21]\n\nThe push can be done using the GitHub for Windows app by clicking the _Sync_ button.\n\n![Push changes to an existing pull request][22]\n\nAs soon as your changes have been pushed to GitHub the new commit will appear in the pull request. Any required checks will be re-run and the conversation with the project owners can continue. Really that's what a pull request is: An ongoing conversation about a set of proposed changes to the code base.\n\n![Pull request with multiple changes][23]\n\n## Keeping your fork up to date\n\nAnother common scenario is that your fork (and branches) become out of date. This happens any time changes are made to the original repo. You can see in this example that 4 commits have been made to HTBox:master since I created my pull request.\n\n![Branch out of date][24]\n\nIt is a good idea to make sure that your branch is not behind the branch that your pull request will be merged into (in this case HTBox:master). When you branch gets behind, you increase the chances of having merge conflicts in your pull request. Keeping your branch up to date is actually fairly simple but not entirely obvious. A common approach is to click the Update from _upstream_ button in GitHub for Windows. Clicking this button will merge the commits from master into your local branch.\n\n![Merging changes from master][25]\n\nThis works, but it's not a very clean. When using this approach, you get these strange \"merge remote tracking branch\" commits in your branch. I find this can get confusing and messy pretty quick as these additional commits make it difficult to read through your commit history to understand the changes you made in this branch. It is also strange to see a commit with your name on it that doesn't actually relate to any real changes you made to the code.\n\n![Merge commit message][26]\n\n**I find a better approach is to do a git rebase**. Don't be scared by the new terminology. A rebase is the process of rewinding the changes you made, updating the branch to include the missing commits from another branch, then replaying your commits after those. In my mind this more logically mirrors what you actually want for your pull request. This should also make your changes much easier to review.\n\nBefore you can rebase, you first need to fetch the changes from the upstream (in this case HTBox). Run `git fetch HTBox`. The fetch itself won't change your branch. It simply ensures that your local git repo has a copy of the changes from HTBox/master. Next, execute `git rebase HTBox/master`. This will rewind all your changes and then replay them after the changes that happened to HTBox/master.\n\n![git rebase][27]\n\nLuckily, we had no merge conflicts to deal with here so we can proceed with pushing our changes up to GitHub with the `git push â€“f` command.\n\n![Force push][28]\n\nNow when we look at this branch on GitHub, we can see that it is no longer behind the HTBox/master branch.\n\n![Updated branch][29]\n\nOver time, you will also want to keep your master branch up to date. The process here is the same but you usually don't need to use the force flag to push. The force flag is only necessary when you have made changes in that branch.\n\n![Updating fork][30]\n\n_**Caution**: _When you rebase, then `push â€“f`, you are rewriting the history for your branch. This normally isn't a problem if you are the only person working on your branch. It can however be a big problem if you are collaborating with another developer on your branch. If you are collaborating with others, the merge approach mentioned earlier (using the _Update from_ button in GitHub for Windows) is a safer option than the rebase option.\n\n## Dealing with Merge Conflicts\n\nDealing with conflicts is the worst part of any source control system, including git. When I run into this problem I use a combination of the command line and the git tooling built-in to Visual Studio. I like to use Visual Studio for this because the visualization used for resolving conflicts is familiar to me.\n\nIf a merge conflict occurs during a rebase, git will spew out some info for you.\n\n![Merge conflict][31]\n\nDon't panic. What happens here is the rebase stops at the commit where the merge conflict happened. It is now up to you to decide how you want to handle this merge conflict. Once you have completed the merge, you can then continue the rebase by running the `git rebase â€“continue` command. Alternatively, you can cancel everything by running the `git rebase â€“abort` command.\n\nAs I said earlier, I like to jump over to Visual Studio to handle the merge conflicts. In Visual Studio, with the solution file for the project open, open the file that has a conflict.\n\n![File with conflict][32]\n\nHere, we can see the conflicted area. You could merge it manually here, but there is a much better way. In Visual Studio, open the Team Explorer and select _Changes_.\n\n![Visual Studio Team Explorer][33]\n\nVisual Studio knows that you are in the middle of a rebase and that you have conflicts.\n\n![Visual Studio Show Conflicts][34]\n\nClick the _Conflicts_ warning and then click the Merge button to resolve merge conflicts for the conflicted file.\n\n![Open merge tool][35]\n\nThis will open the Merge window where I can select the changes I want to keep and then click the Accept Merge button.\n\n![Three way merge tool in Visual Studio][36]\n\nNow, we can continue the rebase operation with `git rebase --continue`:\n\n![git rebase --continue][37]\n\nFinally, a `git push â€“f` to push the changes to GitHub and our merge is complete! Seeâ€¦that wasn't so bad was it?\n\n## Squashing Commits\n\nSome project owners will ask you to squash your commits before they will accept your changes. Squashing is the process of combining all your commits into a single commit. Some project owners like this because it keeps the commit log on the master branch nice and clean with a single commit per pull request. Squashing is the subject of much debate but I won't get into that here. If you got through the merging you can handle this too.\n\nTo squash your commits, start by rebasing as described above. Squashing only works if all your commits are replayed AFTER all the changes in the branch that the pull request will be merged into. Next, rebase again with the `interactive (-i)` flag, specifying the number of changes you will be squashing using HEAD~x. In my case, that is 2 commits. This will open Notepad with a list of the last _x_ commits and some instructions on how to specify the commits you will be squashing.\n\n![Squashing commits][38]\n\nEdit the file, save it and close it. Git will continue the rebase process and open a second file in Notepad. This file will allow you to modify the commit messages.\n\n![Modify commit messages][39]\n\nI usually leave this file alone and close it. This completes the squashing.\n\n![Squash complete][40]\n\nFinally, run the `git push â€“f` command to push these changes to GitHub. Your branch (and associated pull request) should now show a single commit with all your changes.\n\n![Results of squashing][41]\n\n## Pull request successfully merged and closed!\n\n![Mission Accomplished][42]\n\nCongrats! You know have the tools you need to handle most scenarios you might encounter when contributing to an open source project on GitHub. It's time to impress your friends with your new found knowledge of rebasing, merging and squashing! Get out there and start contributing. If you're looking for a project to get started on, check out the list at [http://up-for-grabs.net](http://up-for-grabs.net).\n\n[1]: http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-00-60-29-metablogapi/3817.OSSMicrosoft_2D00_Banner_5F00_thumb_5F00_3B78DD00.png \"OSS&amp;Microsoft Banner\"\n[2]: http://www.microsoft.com/mvp\n[3]: http://www.htbox.org/\n[4]: https://github.com/HTBox/allReady\n[5]: https://desktop.github.com/\n[6]: http://www.davepaquette.com/images/firstpr/fork.png \"Forking a repo\"\n[7]: http://www.davepaquette.com/images/firstpr/yourfork.png \"Your fored repo\"\n[8]: http://www.davepaquette.com/images/firstpr/cloningyourfork.png \"Cloning your fork\"\n[9]: http://www.davepaquette.com/images/firstpr/opengitshell.png \"Open Git Shell\"\n[10]: http://www.davepaquette.com/images/firstpr/createnewbranch.png \"Create new branch\"\n[11]: http://www.davepaquette.com/images/firstpr/gitstatus.png \"git status\"\n[12]: http://www.davepaquette.com/images/firstpr/gitcommit.png \"git commit\"\n[13]: http://www.davepaquette.com/images/firstpr/reviewchanges.png \"Review changes\"\n[14]: http://www.davepaquette.com/images/firstpr/gitpush.png \"git push\"\n[15]: http://www.davepaquette.com/images/firstpr/publish.png \"Publish from GitHub for Windows\"\n[16]: http://www.davepaquette.com/images/firstpr/publishedbranch.png \"Published branch\"\n[17]: http://www.davepaquette.com/images/firstpr/createpullrequest.png \"Create pull request\"\n[18]: http://www.davepaquette.com/images/firstpr/enterpullrequestdetails.png \"Enter pull request details\"\n[19]: http://www.davepaquette.com/images/firstpr/pullrequestchecks.png \"Automated checks on pull requests\"\n[20]: https://github.com/aspnet/Mvc/pull/2516\n[21]: http://www.davepaquette.com/images/firstpr/pushchangestopullrequest.png \"Push changes to an existing pull request\"\n[22]: http://www.davepaquette.com/images/firstpr/sync.png \"Push changes to an existing pull request using GitHub for Windows\"\n[23]: http://www.davepaquette.com/images/firstpr/conversation.png \"Pull request with multiple changes\"\n[24]: http://www.davepaquette.com/images/firstpr/behindmaster.png \"Branch out of date\"\n[25]: http://www.davepaquette.com/images/firstpr/merging.png \"Merging changes from master\"\n[26]: http://www.davepaquette.com/images/firstpr/mergecommit.png \"Merge commit message\"\n[27]: http://www.davepaquette.com/images/firstpr/gitrebase.png \"git rebase\"\n[28]: http://www.davepaquette.com/images/firstpr/forcepush.png \"Force push\"\n[29]: http://www.davepaquette.com/images/firstpr/uptodatebranch.png \"Updated branch\"\n[30]: http://www.davepaquette.com/images/firstpr/updatingfork.png \"image\"\n[31]: http://www.davepaquette.com/images/firstpr/mergeconflict.png \"image\"\n[32]: http://www.davepaquette.com/images/firstpr/conflictedfile.png \"File with conflict\"\n[33]: http://www.davepaquette.com/images/firstpr/teamexplorer.png \"Visual Studio Team Explorer\"\n[34]: http://www.davepaquette.com/images/firstpr/showconflicts.png \"Visual Studio Show Conflicts\"\n[35]: http://www.davepaquette.com/images/firstpr/openmergetool.png \"Open merge tool\"\n[36]: http://www.davepaquette.com/images/firstpr/threewaymerge.png \"Three way merge tool in Visual Studio\"\n[37]: http://www.davepaquette.com/images/firstpr/rebasecontinue.png \"git rebase --continue\"\n[38]: http://www.davepaquette.com/images/firstpr/squash.png \"Squashing commits\"\n[39]: http://www.davepaquette.com/images/firstpr/modifycommitmessages.png \"Modify commit messages\"\n[40]: http://www.davepaquette.com/images/firstpr/squashcomplete.png \"Squash complete!\"\n[41]: http://www.davepaquette.com/images/firstpr/squashresults.png \"Results of squashing\"\n[42]: http://www.davepaquette.com/images/firstpr/accepted.png \"Mission Accomplished\"","categories":[{"name":"GitHub","slug":"GitHub","permalink":"https://westerndevs.com/categories/GitHub/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://westerndevs.com/tags/GitHub/"},{"name":"Open Source","slug":"Open-Source","permalink":"https://westerndevs.com/tags/Open-Source/"},{"name":"Git","slug":"Git","permalink":"https://westerndevs.com/tags/Git/"}]},{"title":"Strongly-Typed Configuration in ASP.NET Core MVC","authorId":"james_chambers","slug":"Strongly-Typed-Configuration-in-ASP-NET-Core-MVC","date":"2016-01-23 15:29:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/Strongly-Typed-Configuration-in-ASP-NET-Core-MVC/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/Strongly-Typed-Configuration-in-ASP-NET-Core-MVC/","excerpt":"Over the last two posts I worked through the basics of configuration in ASP.NET and how to leverage structured data in your JSON config files. Now it's time to take a deeper look at how to access relevant parts of your configuration throughout the rest of your project.","raw":"title: Strongly-Typed Configuration in ASP.NET Core MVC\nlayout: post\ntags:\n  - ASP.NET Core\ncategories:\n  - ASP.NET Core\nauthorId: james_chambers\ndate: 2016-01-23 10:29:12\noriginalurl: http://jameschambers.com/2016/01/Strongly-Typed-Configuration-in-ASP-NET-Core-MVC/\n---\n\nOver the last __[two](http://jameschambers.com/2016/01/Configuration-in-ASP-NET-Core-MVC/)__ __[posts](http://jameschambers.com/2016/01/json-config-in-aspnetcoremvc/)__ I worked through the basics of configuration in ASP.NET and how to leverage structured data in your JSON config files. Now it's time to take a deeper look at how to access relevant parts of your configuration throughout the rest of your project.\n\n![Strongly-Typed Settings Classes in ASP.NET Core](https://jcblogimages.blob.core.windows.net:443/img/2016/01/typed-settings.png)\n\n<!-- more -->\n\nI contribute to an open source project called [AllReady](https://github.com/HTBox/allReady) from the [Humanitarian Toolbox](http://htbox.org). One of the things that we do on the project is use Azure Storage Queues to send and process messages in a different execution context to keep our main application moving along nicely. In order to do this, I added some properties to the configuration file under a storage node:  \n\n````\n  \"Data\": {\n    \"DefaultConnection\": {\n      \"ConnectionString\": \"Server=(localdb)\\\\MSSQLLocalDB;Database=AllReady;Integrated Security=true;MultipleActiveResultsets=true;\"\n    },\n    \"Storage\": {\n      \"AzureStorage\": \"[storagekey]\",    \n      \"EnableAzureQueueService\": \"false\"\n    }\n  }\n ````\n\nObviously, \"`[storagekey]`\" is not a valid key to access a storage account in Azure, but you'll notice that I also have a flag in there to enable/disable the queue service. By putting this in place, we can toggle the service used at dev time and, rather than writing to the queue, we can instead write to the local console. Of course, we have the propery key set in our Azure Web App so that it's loaded and overridden at run time with the correct value. I discussed nomenclature of the keys you'd use in my post on [JSON Configuration](http://jameschambers.com/2016/01/json-config-in-aspnetcoremvc/).\n\n## Exposing Configuration Accross the Application\n\nNow, to actually put the storage settings from our config in play, we're going to create a class to contain the properties that we will need to inspect at runtime.\n\n````\n    public class AzureStorageSettings\n    {\n        public string AzureStorage { get; set; }\n        public bool EnableAzureQueueService { get; set; }\n    }\n````\n\nThis class is a one-to-one mapping of the values we put in our `Storage` section. All that's left is to get the values from our configuration in there.\n  \n## The Options Pattern for Configuration\n\nOriginally I was loading up these properties one-by-each, line after line of reading from the config and assigning the values to the instance of the `AzureStorageSettings` class. But in the fall I had the opportunity to work with [Ryan Nowak](https://github.com/rynowak) of the ASP.NET team and he showed me a much better approach with what the ASP.NET team refers to as the options pattern. It's basically closing the loop on the work we have above and giving us the ability to get at our configuration with strongy-typed objects.\n\nAs a reminder, our `Configuration` property back in `startup.cs` is an instance of an `IConfiguration`, built from the `ConfigurationBuilder` in our constructor. It contains all the data that we've added in key-value pairs, and we can now use that object to expose the information we need through our IoC container when we're configuring our services.\n\n````\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.Configure<AzureStorageSettings>(Configuration.GetSection(\"Data:Storage\"));\n        \n        // other service configuration code here...\n    }\n````\n\nWhat we have to do is call the `GetSection` method along with the corresponding path to where the object instance's properties will be loaded from. Our `Storage` information was in the `Data` property at the root of the document, so we pack it in as `Data:Storage` as the parameter to `GetSection`.\n \nNow I've got configuration in my IoC container and I've got a class that represents the slice of configuration that I'm interested in. Now I want to mux those up and use it in my service (or controller or anything that is spun up with IoC). To do that I simply inject it into my constructor like so:\n\n````\n    public QueueStorageService(IOptions<AzureStorageSettings> options)\n    {\n        AzureStorageSettings settings = options.Value;\n        \n        // work with settings\n        var cloudStorageKey = options.Value.AzureStorage;\n    }\n````\n    \nBy simply accepting a parameter of type `IOptions<AzureStorageSettings>` in the constructor of my controller, the appropriate configuration elements are parsed out and provided to me in the `Value` property as an instance of my `AzureStorageSettings` class. \n\nNote: You'll have to add a using statement to your controller or service for the `IOptions` interface: \n\n    using Microsoft.Extensions.OptionsModel;\n\n## Wrapping Up\n\nSo to review, there are a couple of things we need to do:\n \n - Create the configuration section\n - Create an object that corresponds to our configuration properties\n - Expose the settings class in our IoC container\n - Leverage `IOptions<>` to inject the settings into our constructor\n\nAs you can see, this is a powerful and efficient way to create strongly-typed configuration objects in your ASP.NET Core MVC projects. It takes a minute to wrap your head around the pieces that are in play, but we can do away with the old method of custom configuration sections and simply represent our configuration data as JSON.\n\nHappy coding!\n","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"}],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"}]},{"title":"JSON Configuration in ASP.NET Core MVC","authorId":"james_chambers","slug":"json-config-in-aspnetcoremvc","date":"2016-01-22 02:36:09+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/json-config-in-aspnetcoremvc/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/json-config-in-aspnetcoremvc/","excerpt":"Structured data in earlier versions of ASP.NET meant creating and registering custom types and configuration sections for our applications. In ASP.NET Core and in Core MVC, structured configuration is a breeze with support for JSON documents as the storage mechanism and the ability to flatten hierarchies into highly portable keys.","raw":"---\ntitle: JSON Configuration in ASP.NET Core MVC\nlayout: post\ntags:\n  - \"ASP.NET Core\"\n  - \"ASP.NET 5\"\ncategories:\n  - \"ASP.NET Core\"\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2016/01/json-config-in-aspnetcoremvc/\ndate: 2016-01-21 21:36:09\n---\n\nStructured data in earlier versions of ASP.NET meant creating and registering custom types and configuration sections for our applications. In ASP.NET Core and in Core MVC, structured configuration is a breeze with support for JSON documents as the storage mechanism and the ability to flatten hierarchies into highly portable keys.\n\n![Structured JSON Configuration](https://jcblogimages.blob.core.windows.net:443/img/2016/01/json-structured-data.png)\n\n<!-- more -->\n\nYou can see from the document snippet above, taken from the default project template, that we can easily achieve a well-structured, human-readible set of data.  Where we used to do something the the following:\n\n````\n  <appSettings>\n    <add key=\"Logging-IncludeScopes\" value=\"false\" />\n    <add key=\"Logging-Level-Default\" value=\"verbose\" />\n    <add key=\"Logging-Level-System\" value=\"Information\" />\n    <add key=\"Logging-Level-Microsoft\" value=\"Information\" />\n  </appSettings>\n````\n\nOur other option, of course, is going the custom object route, but that has always been a pain in the rear. Today we can do this:\n\n````\n  \"Logging\": {\n    \"IncludeScopes\": false,\n    \"LogLevel\": {\n      \"Default\": \"Verbose\",\n      \"System\": \"Information\",\n      \"Microsoft\": \"Information\"\n    }\n  }\n````\n\nNow the data that we have related to logging can be grouped into a logical fragment of the configuration file and can grow as required.\n\n## Exploring a Common Example\n\nThis organization is great and comes along with the benefit of being collapsable into a key-value pair.  We see evidence of this in the connection string, which is also located in the `appsettings.json` file:\n\n````\n  \"Data\": {\n    \"DefaultConnection\": {\n      \"ConnectionString\": \"Server=(localdb)\\\\mssqllocaldb;Database=aspnet5-ConfigurationSample-ad90971f-6620-4bc1-ad28-650c59478cc1;Trusted_Connection=True;MultipleActiveResultSets=true\"\n    }\n  }\n````\n\nAnd when you want to pull it out of the stored configuration, you do so like this example from `startup.cs`:\n\n````\n    services.AddEntityFramework()\n        .AddSqlServer()\n        .AddDbContext<ApplicationDbContext>(options =>\n            options.UseSqlServer(Configuration[\"Data:DefaultConnection:ConnectionString\"]));\n\n````\n\nNotice how the value of the `ConnectionString` property of the `DefaultConnection` object within the `Data` object at the root was stored as the key `Data:DefaultConnection:ConnectionString`. This is perfect for allowing overrides, such as using environment variables. This is further made handy by the fact that your settings in Azure are automatically loaded as environment variables into your application execution process at startup.\n\nIn your Azure Web App configuration, you would simply need to add a key named `Data:DefaultConnection:ConnectionString` and set the value accordingly. This means that developers can use LocalDB locally, and the app automatically lights up in the cloud with the real database.\n\n## Next Up\n\nThese key-value pairs are great, but in your application it would be a bother to have to load out each property by hand. In my next post I'm going to show you how to take a configuration section and turn it into a set of typed configuration options that can be used throughout your project.\n\nHappy coding!\n","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"}],"tags":[{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"}]},{"title":"Configuration in ASP.NET Core MVC","authorId":"james_chambers","slug":"Configuration-in-ASP-NET-Core-MVC","date":"2016-01-21 01:27:23+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"ASP-NET-Core/Configuration-in-ASP-NET-Core-MVC/","link":"","permalink":"https://westerndevs.com/ASP-NET-Core/Configuration-in-ASP-NET-Core-MVC/","excerpt":"ASP.NET Core MVC introduces a new configuration system that adds flexibility and simultaneously enables cross-platform support (in a way that makes sense on other platforms). In this post we're going to cover the basics of configuration and what you can expect as you look at the project template from File -&gt; New Project in Visual Studio 2015.","raw":"---\ntitle: \"Configuration in ASP.NET Core MVC\"\nlayout: post\ntags:\n  - \"ASP.NET Core\"\n  - \"ASP.NET 5\"\ncategories:\n  - \"ASP.NET Core\"\nauthorId: james_chambers\ndate: 2016-01-20 20:27:23\noriginalurl: http://jameschambers.com/2016/01/Configuration-in-ASP-NET-Core-MVC/\n---\n\n![Config in Startup.cs](https://jcblogimages.blob.core.windows.net:443/img/2016/01/startup-config.png)\n\nASP.NET Core MVC introduces a new configuration system that adds flexibility and simultaneously enables cross-platform support (in a way that makes sense on other platforms). In this post we're going to cover the basics of configuration and what you can expect as you look at the project template from File -> New Project in Visual Studio 2015.\n\n<!-- more -->\n\n<span class=\"side-note\">ASP.NET Core was previously called ASP.NET 5, and before that ASP.NET vNext. ASP.NET Core MVC is what was referred to as MVC 6. The tooling and the branding will change in the weeks and months ahead, but the basics of configuration I detail here should remain relatively in-tact.</span>\n\n## Configuration Happens Early\n\nIn earlier versions of MVC it is true that the configuration was loaded very early in the process. If you had values in your App.Config they got gobbled up at startup. The problem was, you didn't have a chance to really interact with the configuration system - it just was what it was. This usually meant that we would create our own systems for loading the values, we'd get creative in how we balanced config-time and run-time values and, in short, we'd have to do the heavy-lifting ourselves.\n\nASP.NET Core lets us be much more opinionated about what goes on while registering the configuration values. Sure, it does and should still load configuration pre-startup, but now we can play a role in the process. \n\n````\npublic Startup(IHostingEnvironment env)\n{\n    // Set up configuration sources.\n    var builder = new ConfigurationBuilder()\n        .AddJsonFile(\"appsettings.json\")\n        .AddJsonFile($\"appsettings.{env.EnvironmentName}.json\", optional: true);\n\n    if (env.IsDevelopment())\n    {\n        builder.AddUserSecrets();\n    }\n\n    builder.AddEnvironmentVariables();\n    Configuration = builder.Build();\n}\n````\n\nAs you can see above, the first lines of code on the first bit of code our application contains what is needed to load our configuration.  \n\n## Configuration Happens Often, if You Like\n\nMore importantly, we get a say in how and where the configuration is loaded from. A great example of this is that we can load a JSON file for the default config and then later use environment variables to overload those defaults. \n\nThat code above is the `Startup` method of the `Startup` class, and we're very certain about when the config is loaded and where from. We even get to test if we're in the development envionment.\n\nThis comes in handy when you're deploying to Azure or would like to test with your own values instead of making changes to the JSON config file that would otherwise be checked in with the project.\n\nSpeaking of which, you're going to likely need to store some values in there that you _will never want to share_, and that you'll never want to check into your repo. This would be things like API tokens for integration into third-party services and the like (think SendGrid, Twilio, PayPal and the like).\n\n## Configuration Opens Doors for Secrets\n\nAnd that brings us to user secrets. It's still not clear how these guys are going to shake down - there's still active discussion about how it should be named and stored - but the idea is straightforward and lets you work locally with sensitive data without having to modify your config. You can think of them as \"environment variables for your project\". \n\nThere is pretty basic tooling from the command line:\n\n![User Secrets](https://jcblogimages.blob.core.windows.net:443/img/2016/01/user-secret.png)\n\nThe secrets are stored in your user data here:\n\n    %APPDATA%\\microsoft\\UserSecrets\\\n    \nIf you've used secrets, there will be a sub-folder here for each project you've created. Depending on where they land, the secrets will likely be a combination of the project name and a GUID, but you can set this yourself in your project.json.\n\nI'll do a follow-up post on user secrets and demonstrate in greater detail how to leverage it in your projects.\n\n## Next Steps\n\nWe're still in an RC period (should it be called beta?) and there are naming pieces yet to come, but there is nothing stopping you from learning about the configuration system in ASP.NET Core today. Grab a copy of Visual Studio 2015 - [hey, it's free!]() - and start experimenting with the bits. Be sure to check back in the weeks ahead for more information about configuration in ASP.NET Core and Core MVC.\n\nHappy coding!\n\n","categories":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/categories/ASP-NET-Core/"}],"tags":[{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"}]},{"title":"ASP.net vNext is now ASP.net 5 is now ASP.net Core 1.0","authorId":"simon_timms","slug":"ASP-Net-5-rename","date":"2016-01-20 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"News/ASP-Net-5-rename/","link":"","permalink":"https://westerndevs.com/News/ASP-Net-5-rename/","excerpt":"What the rename was and why it didn't go far enough","raw":"---\ntitle: ASP.net vNext is now ASP.net 5 is now ASP.net Core 1.0\nlayout: post\ntags:\n  - ASP.NET 5\n  - ASP.NET Core\ncategories:\n  - News\nauthorId: simon_timms\nexcerpt: \"What the rename was and why it didn't go far enough\"\n---\n\nDid you hear the news? [ASP.NET 5 is dead](http://www.hanselman.com/blog/ASPNET5IsDeadIntroducingASPNETCore10AndNETCore10.aspx) it is now called ASP.NET Core 1.0. The name for this next generation of ASP.net(that's the capitalization I'm using because this isn't your grandmother typing in YAHOO.COM) has been up in the air for a while. We first heard the real details about ASP.net vNext at the MVP summit in 2014 and the first question on everybody's mind was \"what it was going to be called?\". At the time there wasn't a decision on that. The perception at Microsoft seemed to be that the ASP.net moniker has a long and illustrious history. There is a lot of marketing behind the name and most developers, whether they work in the technology or not, have heard of ASP.net. Thus no matter what the final name was going to be it was pretty much a certainty that it would contain \"ASP.net\" in some form or another. \n\nAnd that's the problem. \n\nSee ASP.net covers a boat load of technologies: not only does it cover ASP.net MVC but also all the garbage that is webforms. Yes the name ASP.net has a long history but it isn't one tenth as illustrious as the marketing drones at Microsoft seem to think. Let's take a second and look at the market for ASP.net. From what I can tell there are basically two types of developers who might use it\n\n1. **9-5 developers** these folks who punch the clock and aren't going to waste a whole lot of their time on new technologies unless pushed by some reason at work.\n2. **Magpies** the developers who try out the new thing and, if worthy, move to it. These folk are the reason that we have such high turnover in web frameworks and even in web languages (Elixr anybody?) \n\nASP.net Core is meant to give a sense of assurance to the 9-5 developers \"don't worry this is still ASP.net, you'll be fine it isn't any different\". Problem is that's nonsense. It is different. It is a big jump and we're just being dishonest to these folks if we say otherwise. People shouldn't be moving to your framework because of subterfuge they should be moving because it is legitimately better. ASP.net Core is legitimately better. \n\nThe 9-5 developers aren't going to be moving to Core any time soon. ASP.net 4.6 is still the path they're going to take. Let's be honest, for a lot of people, ASP.net 2 is the path they're going to take. \n\nThe rename should have been larger and should have offered the clean break that the ASP.net team have been working so hard to give us. I want to be able to go into the nodejs community, the java community whatever community and pitch them something new. Instead I'm going to be explaining why, in 2016, we're still talking about Active Server Pages and how this isn't webforms. \n\nI guess that boat has sailed now so we're stuck with a terrible name. Welcome to 2003.","categories":[{"name":"News","slug":"News","permalink":"https://westerndevs.com/categories/News/"}],"tags":[{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"}]},{"title":"ASP.net 5 is now ASP.net Core 1.0, Get Over It","authorId":"darcy_lussier","slug":"ASPNetCoreGetOverIt","date":"2016-01-20 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"News/ASPNetCoreGetOverIt/","link":"","permalink":"https://westerndevs.com/News/ASPNetCoreGetOverIt/","excerpt":"What the rename was and why it's the right one.","raw":"---\ntitle: ASP.net 5 is now ASP.net Core 1.0, Get Over It\nlayout: post\ntags:\n  - ASP.NET 5\n  - ASP.NET Core\ncategories:\n  - News\nauthorId: darcy_lussier\nexcerpt: \"What the rename was and why it's the right one.\"\n---\n\nYou know who I love? Simon Timms, that's who I love. And the thing with love is that you can disagree about something, but that doesn't mean you stop loving.\n\nSo with that said, let me tell you what's WRONG [with his recent post about the renaming of ASP.NET 5 to ASP.NET Core 1.0.](http://www.westerndevs.com/News/ASP-Net-5-rename/)\n\nActually my first beef isn't with Simon, its with Scott.[ Do we really have to be all dramatic about how \"ASP.NET 5 is DEAD\"?](http://www.hanselman.com/blog/ASPNET5IsDeadIntroducingASPNETCore10AndNETCore10.aspx) It's not dead, its just being renamed. Let's not make this more dramatic than it needs to be.\n\n### The Issue ###\n\nBack to Simon's post. The problem he seems to have is the legacy attachment to the term ASP. For those who weren't doing web development in the late 90's, ASP (Active Server Pages) was a Microsoft technology for creating dynamic web applications. You added VBScript into your web pages to make it render things, which was processed on the server. When .NET came out, Microsoft kept the ASP name for their .NET based web technology - ASP.NET was born. Over the years ASP.NET has become a catch all for all sorts of Microsoft web tech - Webforms, MVC, Ajax, etc. There's an established base of developers who know and understand what ASP.NET means.\n\n### What's in a Name? ###\n\nAnd that matters. Having a reference point to a technology for the masses that already associate Microsoft web technologies to ASP.NET matters a lot. It matters in technology adoption, it matters in approachability, it matters in how easy it is to sell new developers on something.\n\nI don't think the ASP name is kept to communicate that \"it isn't any different\" as Simon suggests. It's kept to give people a reference point; it encourages people who are attached to ASP.NET technologies to look at this new Core 1.0 offering. Consider these two conversations.\n\n****\n\n\"Hey, have you guys checked out the new web framework SimonSays?\"\n\n\"No...we do ASP.NET.\n\n\"Oh...no, it's from Microsoft!\"\n\n\"Yeah, so was Silverlight. We'll stick to our proven framework\"\n\n****\n\n\"Hey, have you guys checked out the new web framework ASP.NET 1.0 Core?\"\n\n\"Oh...we do ASP.NET...I didn't realize there was a new framework out? What's all this about?\"\n\n****\nAnd that is where the human element comes into play. People don't like change - that's why we have all these adages that \"change is hard\". Remember when Coke changed their recipe? They didn't change the name, they just said it was \"New\" Coke. Why? Because Coke as a brand was important to people...it meant something...it was a point of reference. ASP.NET is the same thing, its a brand...a point of reference.\n\nLet's look at Simon's other point: \"People shouldn't be moving to your framework because of subterfuge they should be moving because it is legitimately better. ASP.NET Core is legitimately better.\"\n\n*People should do the right thing because the right thing is the right thing.* \n\nIf this were true, we wouldn't have an entire job category called Sales and Marketing. Also, this assumes that a well architected system based on ASP.NET 4.6 is somehow null and void since ASP.NET Core 1.0 is out. Of course it isn't - the value of a system does not lie in what technology it was written in but in how much value it provides its users.\n\nPlease read that last line again. Would I ever write something new in COBOL? No. Does that invalidate reliable, running systems written in it today? Of course not.\n\nSo this leaves us with a new platform that we must now sell to developers - not in a money sense, but in a \"please try it, please learn it, please understand what issues this solves in our previous technologies\". And what better way to introduce something new than to give people a familiar frame of reference - ASP.NET.\n\n### About That 9-5 Comment... ###\n\nNot directly associated with ASP.NET but a point I'll still address is the blanket assumption of what a \"9-5\" developer is. I'm going to tell you a big secret that I've learned while being at a 9-5 public entity - the reason that most devs don't move forward with new technology is because they're busy maintaining the systems they already have. Being able to test out a new framework or a new tool comes in spurts as greenfield projects come up, not as a design choice because some vendor released something new. And if you talk to those 9-5ers you may find that while the technology might not be bleeding edge the problems they are solving are pretty impressive. ASP.NET Core 1.0 is not beyond a 9-5ers ability.\n\nWhat Simon doesn't do in his post, and what I'd love to hear, is what he thinks it SHOULD have been named. What name would have opened the door to all those new non-Microsoft developers while at the same time given existing ASP.NET devs a reason to even look its way when they have a moment to come up for air?\n\nHow about...Silverlight?\n\n","categories":[{"name":"News","slug":"News","permalink":"https://westerndevs.com/categories/News/"}],"tags":[{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"https://westerndevs.com/tags/ASP-NET-Core/"}]},{"title":"Everything You Need To Know About Microsoft Band 2 Battery Life","authorId":"james_chambers","slug":"Everything-You-Need-To-Know-About-Microsoft-Band-2-Battery-Life","date":"2016-01-18 05:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"Code-Dive/Everything-You-Need-To-Know-About-Microsoft-Band-2-Battery-Life/","link":"","permalink":"https://westerndevs.com/Code-Dive/Everything-You-Need-To-Know-About-Microsoft-Band-2-Battery-Life/","excerpt":"Including tips on how to keep yours running all week long","raw":"---\ntitle: Everything You Need To Know About Microsoft Band 2 Battery Life\nlayout: post\ntags:\n  - Microsoft Band\ncategories:\n  - Code Dive\nauthorId: james_chambers\nexcerpt: \"Including tips on how to keep yours running all week long\"\noriginalurl: http://jameschambers.com/2016/01/2106-01-17-Everything-You-Need-To-Know-About-Band-2-Battery-Life/\n---\n\n![Band 2](https://jcblogimages.blob.core.windows.net/img/2016/01/band-2.png)\n\nThe day the announcement was made for Band 2, I was watching the keynote and keeping my finger on F5, repeatedly refreshing my browser and eagerly waiting for the Band 2 \"Coming Soon\" page turn over to an \"Order Now\" page. The week before launch, I added my credit card as a saved card on the Microsoft store. As soon as the page flipped over, I pulled the trigger and reserved my new edition.\n\nI've worn it every day since November 2, 2015 and here's what I've learned about the Microsoft Band 2 battery life, including tips on how to keep yours running **all week long**.\n\n<!-- more -->\n\n## Charging the Microsoft Band 2\n\nThe Band is easily charged by unhinging the clasp and sliding the device off your wrist, then attaching the magnetized charger, which snaps automatically in place. A full charge for a fully depleted battery should run you less than two hours.\n\n![Band 2 Charging](https://jcblogimages.blob.core.windows.net/img/2016/01/band-2-charger.png)\n\n## Battery Life\n\nUnder normal usage, your Band will deplete about 30% per day. This includes getting notifications, syncing with your phone, controlling music, setting timers and alarms, using the daily heart rate tracking and buying coffee at Starbucks, should you be so inclined.\n\nAdding workouts to your day will drive the battery down a little more quickly, and I seem to burn about 10% of my battery when I go for a 30 min run with GPS turned on. \n\n## Charge Time\n\nThe \"bottom half\" of the battery seems to charge a little quicker than the \"top half\". What I mean is that going from 0%  to 50% seems to take about 40 minutes or so whereas from the 50% mark and higher, the Band 2 charges at a rate of about 1% per minute. \n \n## My Routine\n\nThe battery on my Band 2 lasts me through the week.  While it depletes every day about 30%, I also charge it every day during my daily routine and after workouts while I shower.  Here's a log from my last week of use starting with a full charge on Saturday:\n\n - Sunday AM: 72%, charged to 91%\n - Monday AM: 66%, charged to 83%\n - Tuesday AM: 51%, charged to 70%\n - Tuesday workout: 60%, charged to 79%\n - Wednesday AM: 63%, charged to 80%\n - Thursday AM: 49%, charged to 64%\n - Thursday workout: 48%, charged to 63%\n - Friday AM: 50%, charged to 65%\n - Saturday AM: 31%, charged to 100%\n \nBasically, I'm charging it when I get ready in the AM, which doesn't quite catch it up for what it lost over the previous day. However, when I workout, I typically shower afterwards and this gives me an extra 15-20 minutes to charge it again. So, interestingly, I actualy use up the Band 2 battery _less_ when I'm working out more.\n\n## Cylcing the Charge\n\nMost Saturdays I actually wear my Band until the battery warning goes off, then I plug it in until fully charged. I called into the Microsoft support line and asked if this was a good practice for using and charging the Band 2 - essentially letting it run down through the week and then giving it a complete charge on the weekend. The support technician on the call agreed that this was a good strategy and commented that she has a similar routine. In the two-and-a-half months of use, I have not seen depreciating performance on the battery life.\n\n## Charging Tips\n\nHere's a few things you can do to keep your Band 2 running all week:\n\n - Keep your charger plugged in where you get ready in the AM\n - Clean the contacts before you attach the charger, and thouroughly after a workout\n - Try to let it run down once a week and give it a full charge\n - Buy a second charger if you need one, for example if you walk or ride to work\n \nHappy fitnessing!","categories":[{"name":"Code Dive","slug":"Code-Dive","permalink":"https://westerndevs.com/categories/Code-Dive/"}],"tags":[{"name":"Microsoft Band","slug":"Microsoft-Band","permalink":"https://westerndevs.com/tags/Microsoft-Band/"}]},{"title":"Why ChakraCore OSS is Important","authorId":"david_wesst","slug":"Why-ChakraCore-OSS-is-Important","date":"2016-01-14 19:43:33+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"javascript/nodejs/chakra/Why-ChakraCore-OSS-is-Important/","link":"","permalink":"https://westerndevs.com/javascript/nodejs/chakra/Why-ChakraCore-OSS-is-Important/","excerpt":"It's pretty cool that Microsoft has released the source code of their JavaScript engine Chakra. But, why it is important and how do developers actually benefit from this?","raw":"---\nlayout: post\ntitle: Why ChakraCore OSS is Important\ncategories:\n  - javascript\n  - nodejs\n  - chakra\ndate: 2016-01-14 14:43:33\nexcerpt: It's pretty cool that Microsoft has released the source code of their JavaScript engine Chakra. But, why it is important and how do developers actually benefit from this? \nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2016/01/Why-ChakraCore-OSS-is-Important/\n---\n\nChakra is the new JavaScript engine developed by Microsoft, which was first released as part of the, [no longer supported](https://www.microsoft.com/en-ca/WindowsForBusiness/End-of-IE-support) Internet Explorer 9. This isn't a post about why Microsoft having more open source software (OSS) processes and projects is important, or how it is \"amazing\" that they are turning a new leaf.\n\nThis post answers the question that I don't see people asking: *why is having a OSS JavaScript engine important to anyone*?\n\nMaybe you already know the answer to this, or maybe you're just too shy to ask, but I'm going to take a moment and try and give you some idea about why ChakraCore being open source is really cool.\n\n![](http://blog.davidwesst.com/2016/01/Why-ChakraCore-OSS-is-Important/github-page.png)\n\n### 1. Alternative JavaScript Engine for NodeJS\n\n![](http://blog.davidwesst.com/2016/01/Why-ChakraCore-OSS-is-Important/nodejs-logo.png)\n\nThis might be the most important point for me, as NodeJS is my development platform of choice. For the first time in the history of NodeJS, there is an alternative to the V8 JavaScript engine that has been built into NodeJS from the beginning.\n\nI have no problems with V8. It's done well by me, considering without it we likely wouldn't have NodeJS at all. But having a different option provides more flexibility to the developer. Maybe Chakra can run faster than V8 when it comes to certain tasks, or maybe it can provide more JavaScript features that you want to use for your project. Now you _can_ change Node's engine, which means all kinds of opportunity for NodeJS developers.\n\nPlus, if you can change NodeJS to use ChakraCore, then why couldn't you sub in another engine like [SpiderMonkey](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey) or something else. It leads to more options, which leads to competition, and that is always healthy in my opinion.\n\n### 2. Community Support and Pull Requests\n\n![](http://blog.davidwesst.com/2016/01/Why-ChakraCore-OSS-is-Important/pull-request.png)\n\nOh no! Not another security gap in all the browsers! What ever will we do?\n\nUntil now, the only solution would have been to wait for an official release from the Microsoft team, hoping that they had all the details they needed. Now, the team can work _with_ the community directly and publically so that we know where things are at. If there is a security gap reported, people can find it, report it, *and fix it* by submitting a pull request directly to the team. \n\nNow, my browser can sleep comfortably at night.\n\n### 3. Knowing How the Sausage is Made\nAs mentioned in the in [YouTube announcement](https://youtu.be/1bfDB3YPHFI), Microsoft has decided to bet big on JavaScript as a whole which is why building a brand new JavaScript engine made sense for the business. Although it was clearly a huge improvement over Trident, we still didn't know what was going into the engine and what made it work.\n\nNow we can see how the engine actually works. No secret sauce, no magic, just code. This is great for people looking to use a JavaScript on sensitive projects, or in highly secure environments that require knowing all the insides and outs to a system before it can be considered. In those cases you can fork the project, make some changes for your environment, and continue to get support from the original team by merging their changes as they publish them.\n\n## Bottom Line\nJavaScript engines are a very specific tool, but if your application needs a scripting component or you need to interpret JavaScript directly, you're in luck because ChakraCore is out in the wild now. Fully supported by Microsoft, and is meant to be integrated with other software, it's a really great option for those that can use it.\n\nPlus, if you don't like how IE or Edge is running your JavaScript, you can get to the bottom of the problem yourself and submit a pull request. No more hiding behind the curtain. Developers have information they need to provide direct feedback on how Edge can run better. \n\nIn time, I think we'll see more than just NodeJS, but rather Chakra become the heart of a lot of cool projects. \n\n### What's Next?\nPersonally, I'm thinking of using it as an on-the-fly scripting engine where a game object has a script that needs to be interpreted during game play, like AI or some sort of behaviour. \n\nFor now I'll probably start with getting it working on my Raspberry Pi. I have one sitting on my desk and I'm itching to get something cool working on it. Either way, I'll probably start with the [Windows 10 IoT](https://dev.windows.com/en-us/iot) page for some ideas, or take a stab at [embedding it into a project](https://github.com/Microsoft/ChakraCore/wiki/Embedding-ChakraCore).\n\n# 20-Jan-2016 UPDATE: Pull Request Submitted\nIn case you're interested, [Microsoft has submitted a pull request to the NodeJS project](https://github.com/nodejs/node/pull/4765) to enable NodeJS to run with the ChakraCore engine. As I mentioned before: we can see how things are moving along publically because it's open source. No magic, just code.\n\n----\n##### EDIT\nThanks to [CaptainIncredible from Reddit](https://www.reddit.com/r/webdev/comments/41q8h2/why_chakracore_oss_is_important/) and the other commenters out there for taking the time to do the proofreading that I so desperately should have done before publishing. Feedback, even about my sloppy spelling, is always appreciated.\n","categories":[{"name":"javascript","slug":"javascript","permalink":"https://westerndevs.com/categories/javascript/"},{"name":"nodejs","slug":"javascript/nodejs","permalink":"https://westerndevs.com/categories/javascript/nodejs/"},{"name":"chakra","slug":"javascript/nodejs/chakra","permalink":"https://westerndevs.com/categories/javascript/nodejs/chakra/"}],"tags":[]},{"title":"2016 Goals","slug":"Podcast-2016-Goals","date":"2016-01-13 15:40:13+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/Podcast-2016-Goals/","link":"","permalink":"https://westerndevs.com/podcasts/Podcast-2016-Goals/","excerpt":"The Western Devs reminisce on 2015 and look to 2016","raw":"---\nlayout: podcast\ntitle: '2016 Goals'\ncategories: podcasts\ncomments: true\npodcast:\n  filename: 2016Goals.mp3\n  length: '34:23'\n  filesize: 33003394\n  libsynId: 4075337\n  anchorFmId: 2016-Goals-evqdiu\nparticipants:\n  - kyle_baley\n  - dave_paquette\n  - lori_lalonde\n  - david_wesst\nmusic:\n  - title: Doctor Man\n    artist: Johnnie Christie and the Boats\n    url: 'https://www.youtube.com/user/jwcchristie'\nlinks:\n  - Prairie Dev Con|http://www.prairiedevcon.com\n  - MS Build|http://build.microsoft.com/\n  - Ninja Gaiden|https://en.wikipedia.org/wiki/Ninja_Gaiden\n  - Xamarin Evolve|https://evolve.xamarin.com/\n  - DevTeach|http://devteach.com\n  - Chocolate Quinoa Brownies|http://www.bloorstreetmarket.ca/LCLOnline/recipes.jsp?type=details&mainIngredientId=160&recipeId=lclor42016\ndate: 2016-01-13 10:40:13\nrecorded: 2016-01-08\nexcerpt: \"The Western Devs reminisce on 2015 and look to 2016\"\n---\n\n### Synopsis\n\n* 2015 Recap\n* Getting back to code\n* Making personal projects personal again\n* Learning in public\n* Speaking/presentation goals\n* Going with the flow vs. strict adherence to plans\n* Blogging: Quality vs. Quantity\n* Tinkering with tech\n* Personal goals\n* Balancing your use of quinoa\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Your Work and Your Life are Not Intricately Intertwined","authorId":"james_chambers","slug":"Your-Work-and-Your-Life-are-Not-Intricately-Intertwined","date":"2016-01-11 15:25:47+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"work-life-balance/Your-Work-and-Your-Life-are-Not-Intricately-Intertwined/","link":"","permalink":"https://westerndevs.com/work-life-balance/Your-Work-and-Your-Life-are-Not-Intricately-Intertwined/","excerpt":"You are always setting expectations. When you answer an email at 10pm, you are setting an expectation. When let a call go to voicemail at 6pm, you are setting an expectation.","raw":"---\nlayout: post\ntitle: \"Your Work and Your Life are Not Intricately Intertwined\"\ncategories:\n  - work-life-balance\ndate: 2016-01-11 10:25:47\ntags:\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2016/01/work-is-not-life/\nalias: /category1/category2/Your-Work-and-Your-Life-are-Not-Intricately-Intertwined/\n---\n\nYou are **always** setting expectations. When you answer an email at 10pm, you are setting an expectation. When let a call go to voicemail at 6pm, you are setting an expectation.\n\n<!--more-->\n\nThese are difficult things to accept, especially if you're in competition for advancement or your employer is challenging your boundaries on a regular basis, but just remember that resetting expectations is much more difficult that setting them in the first place. Remembering that every action you take (or don't) defines how people will expect you to act in the future.  \n\nI am currently an employee and likely will be for the foreseeable future. My career aspirations do not include management despite the fact that I love to lead teams (there is a significant difference between the two, but I'll save that for a different post). I firmly believe that you can lead a work life and a personal life that are largely disconnected and be quite successful doing so.\n\nAs someone who has found incredible happiness in the _balance_ between a happy home life and spending an appropriate amount of time on my career, I recently read a post that did not sit well with me at all. In \"distilling\" everything down in to what was needed in order to be productive, the author lists a set of \"rules\" that include this:\n\n    Embrace the fact that work and life are intricately intertwined\n\nI'm going to tell you right now: there are a lot of things wrong with this rule. The rest of that post is laden with info from other sources and a ton of exercises that I can't see many people filling out, but the idea that I should somehow lose myself in dedication to work is terribly misguided.\n\nLet's break it down.\n\n## First Things First\n\nI want to add some clarity to my thought process here. The most confused aspect of the aforementioned rule is that the terms used are not well-defined. I will do that here, defining what \"work\" actually means, and how it is a fundamentally different concept than your job or your career.\n\nFirst, let's talk about your **work**. Work is the list of assigned duties that you carry out, which often vary from day-to-day and may be transient in nature; you may be asked to perform a set of work for a prescribed period of time and later be assigned different work. For some people, their work will be consistent for the entirety of their employ, others it may change from month-to-month or day-to-day. But this is a good segue as there is a difference between your work and your job.\n\nYour **job** is the collection of work and tasks you perform in exchange for a pay cheque. Your job helps you meet your financial obligations and help to contribute to your household income. The tasks assigned are usually out of your control, though many forward-thinking organizations offer some freedom over long-term assignments and let you speak into the types of tasks you take on. Your experience in other, previous jobs will open doors for you to take on greater challenges in your next job, which may be within the same organization. Again, organizations that get this will help you form a path, leading you through more complicated work and increasingly important tasks en route to helping you move to more senior roles. Admittedly, not all career paths provide this opportunity, and that's okay, too, the important thing is finding a job that supports your career.\n\nWhich leads me to the definition of **career**. Unlike the tasks you perform or the role you assume at a company, your career will never be fully articulated until you retire. That is, if retirement is indeed your endgame. If someone asks you what your career is today, your answer will likely be a point-in-time reflection of how you got to where you are. It is defined by the achievements you capture and the challenges you overcome. For some it will be a story of creativity and expression, for others it will be about dedication or service.\n\n<div class=\"notice\">    \nI'd better drop a bonus definition in here: <b>passion</b>, in the context of \"things you like to do\". Your passion is that thing that you're likely thinking about or get easily distracted by. You've likely thought, \"Someday I'd like to...\" and completed that sentence with the thing that you want to be doing <i>now</i>. It can drive you and light you up and you are excited to talk about it with others. <i>That</i> is passion.\n</div>\n\nOthers still will explain their career as the vehicle they used to chase a passion, rather than something they were passionate about. As a concrete example example of that, I've demonstrated capabilities in software development and have been on this career path for 20 years now, writing code and leading teams. However, as much as I like writing code, my _passion_ is actually learning, mentoring, sharing my work and teaching others. My career has allowed me to access my passion, and today I get to speak at conferences across North America and volunteer at the computer labs at local middle and high schools.\n\n## What You Should Really Embrace\n\nOkay, so if you're not going to embrace some kind of intertwined reality, what should you be chasing?\n\nI believe the answer lies in sorting your crap out and remembering that the three things we talked about are tangible, distinct and sometimes disconnected or misaligned. Sometimes you won't like the **work** you're doing, but you will be completely happy with your **job**. Sometimes you take a **job** because you know you will enjoy the work for a period of time, but the **job** may not be helpful in advancing your **career**.\n\nThese things are okay, at least for a time. What you need to embrace is the fact that finding the perfect combination is very difficult, especially over the long haul. As your career objectives change, the job may no longer work for you as a tool to move down your career path. Sometimes you'll have an incredible job - great employer, solid pay and awesome co-workers - but the work you're assigned isn't what you like to be doing. Some people â€“ it's happened to myself â€“ will advance through an organization based on their performance into a role that they are not suitable for and will not find be successful at (this is known as the [Peter Principle][1]).\n\n![Advancement through success can sometimes lead to failure][2]\n\nRather than thinking of things in rules, let's instead think of things in truths. It is true that:\n\n* You can have a job you love and be doing work that you don't enjoy\n* The work you are doing may be fulfilling, but doesn't give you opportunity to advance in your job\n* The job you have may serve you well today, but doesn't align with your career\n* You can (and should try to) outgrow your job\n* What is considered a promotion by your employer may permanently lead to work that is not of your liking\n* Your choice in career may change, which can lead to instability in your happiness with your work and/or job\n* There is going to be work you don't like to do in nearly every job you take\n* Your work, job and career may or may not help you operate inside your passion\n\nI don't want to constantly attend to the negative, I want to focus on the things that are going right and look for signals that I am on the correct path. These green flags tell me that I am moving in the direction that I want to be in, and help me keep a healthy balance between my personal life and what I do in my job. When pondering where I'm at with my work, job and career, I ask myself questions from this reflection list:\n\n* Do I have something to offer the people I work with and for?\n* Am I working with co-workers that can help me grow?\n* Is the work I am doing meaningful to me?\n* Does my job give me the freedom to think for myself?\n* Does the work I'm doing align with my ethics?\n* Is there a way for me to advance my career or follow my passion?\n* When I outgrow where I am, is there a place for me to go?\n* Does my job require effort within the bounds of a reasonable workday?\n* Am I still interested in the career story that is unfolding ahead of me?\n\nIf I've got positive answers for several of these, I know I'm in a good spot. On the other hand, failing to meet my criteria here on a couple of points could be a sign that there needs to be a change of season. These questions may not be exactly what you need, but the exercise is what I believe we all need to embrace; the answers are dynamic and are going to change over time, so you need to find questions that help you identify your measure of happiness.\n\n## An Employer's Role in Your Work, Job and Career\n\nIn it's simplest terms, I believe that an employer's job begins with the assignment of meaningful, relevant tasks and ends with a paycheque. In between that space there is opportunity to challenge an employee, to contribute to their growth and provide guidance on how to develop their skills in such a way that it serves the company and helps to realize the goals of the individual, wherever possible.\n\nAs employees we have to concede that an employer is concerned with generating income in greater magnitude than expense as they execute the services their clients depend on. Even when we're at a job we enjoy we will likely be tasked with actions we would not choose for ourselves. We need to be clear about our broader goals and, when appropriate, be honest when our **work** or our **job** is not checking off things from our reflection list.\n\nIf an employer is mentoring you to make your work part of your life, I boldly challenge you to push back and define strong bounds through which your work cannot cross. Someone in a mentorship role who guides employees with banter of blending work and life is clearly not interested in the career of the employee, and places higher weight on the importance of completed work than on the individual. I don't want to work _there_.\n\n## Why Work Isn't \"Intricately Intertwined\" With Your Life\n\nI will yield that the original post did not prescribe an explanation of \"work\", which is why I have above, so I'll argue this from the perspective of both \"work\" and \"job\" as I've defined in this article.\n\nThe separation of work is easy; work is a task and usually requires the context of your _work environment_. An engineer can't complete blueprints without the requisite software, a counsellor cannot complete an evaluation without a patient and someone in janitorial cannot wax the floors without the buffing machine. And the floor. These are the types of things that can easily be slotted into your work schedule and, when you are good with time management, need not spill into your personal life on any regular frequency.\n\nThe separation of job and your life is a little less trivial. To avoid carrying the stress of the day home you need to have establish some good practices around \"putting your tools down\", disconnecting from the office on your way _out_ of the office. This is going to be something different for everybody, for me it involves tearing down my workspace, closing applications and checking code in. To prevent bigger picture concerns from affecting your home life such as the economy or the sustainability of the company you work at you need to regard your employment in the correct light, namely that it is part of your career, but likely doesn't define it. And when your employer puts requirements into your job that breach your personal time, you will have some hard decisions to make.\n\nWherever you land, you need time to recharge. I agree with the sentiment from the original post that suggests multitasking can have a negative affect on you. How, then, can you intertwine work with playing with your kids? How do you answer emails when on a secluded retreat with your spouse? If getting in the zone is as easy as taking 10-15 minutes of focus, then do that _during the workday_. The important thing is to define the bounds of that workday and maintain them.\n\n## Some Well-Deserved Exceptions\n\nI know some incredible folks who have taken entirely different walks of life than I. They have found success in ways that would not work for me, as I have found success in ways that may not work for you. Here are some scenarios where your workday may bleed more frequently into your personal life.\n\n**The Self-Employed** Running your own business is hard work. You may be working across time zones, you may have travel considerations and you need to react quickly to clients in order to collect the revenues you need to stay afloat.\n\n**Family Businesses** While less common these days, the daily topics of family-run businesses will naturally find their way into conversations that happen outside of the work day. Reminders, follow-ups and even stress relief may happen when you find that every supper doubles as a staff party.\n\n**Those Without Family** I have friends who are not into the family scene and are living a single life. They have found that the cross-over tends to happen more naturally, but have also noted that they prefer to spend their time with friends or working on their career (versus servicing requests from work).\n\n**When Travel is Required** Travel is a tricky beast, but one that raises the requirement to really define when work starts and stops. As an obvious impairment to split out a normal work day, work-related travel increases the relevancy of strong boundaries when you are at home.\n\n**Crunch Times** These are realities for most folks in most fields: as a project closes, an emergency arises or a deadline approaches, a little extra effort is going to be required. You'll need to step up to successfully complete the tasks and stay in good standing with your employer.\n\nIn spite of these, I still believe that the cross-over time can be mitigated to a large degree. The important thing to do in these cases is to be more effective at communicating when the windows of work will be and ensuring that you have the support of those around you to help enforce it. Your husband or wife won't know that you are expecting a call unless you've found a way to share it with them.\n\n## What to do With All of This\n\nFirst of all, don't sell the farm. If you're finding that something in your career, your job or your work is unsettling, make sure you have set of reasonable questions you can ask yourself to find out why. Talk with your employer about ways to make it right and, if needed, start to explore alternate work arrangements inside your organization, or outside of it.\n\nOne thing you can do, immediately, is to start blocking off time for your family and for you personally. This will help to give you time to connect with those that are important to you and reflect on what is becoming of your career. When I did this, I started to see - almost immediately - how I needed the time away from work in order to concentrate when I was there. It also paved the way for positive, sweeping changes in the time I spend with my wife and kids.\n\nIt would be remiss of me to omit some of the other beliefs that I have, namely that I put my family above my job, my ethics ahead of my work and my responsibility to my family's obligations ahead of my personal interests. This means that I have had to make difficult decisions at times in order to maintain integrity and, to be honest, I suppose I'm still a long way out from knowing whether or not those were the right decisions. The best I can hope for is that hindsight reveals that the decisions were right at that time for who I was.\n\nHere is [the blog post I referenced][3], if you're interested.\n\nI usually close by saying, \"happy coding!\" but in this case, this might be more appropriate: Happy career! :)\n\n[1]: https://en.wikipedia.org/wiki/Peter_principle\n[2]: https://jcblogimages.blob.core.windows.net/img/2016/01/Peters_principle.png\n[3]: http://joelfromcanada.com/gettingshitdone/\n  ","categories":[{"name":"work-life-balance","slug":"work-life-balance","permalink":"https://westerndevs.com/categories/work-life-balance/"}],"tags":[]},{"title":"Setting Up Octopus Build Task on TFS 2015 On-Prem","authorId":"darcy_lussier","slug":"Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem","date":"2016-01-06 18:47:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"tfs/octopus/continuous-delivery/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem/","link":"","permalink":"https://westerndevs.com/tfs/octopus/continuous-delivery/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem/","excerpt":"A step-by-step guide with screenshots","raw":"---\nlayout: post\ntitle: Setting Up Octopus Build Task on TFS 2015 On-Prem\ncategories:\n  - tfs\n  - octopus\n  - continuous delivery\ndate: 2016-01-06 13:47:46\noriginalurl: 'http://geekswithblogs.net/dlussier/archive/2016/01/04/170820.aspx'\nexcerpt: \"A step-by-step guide with screenshots\"\nauthorId: darcy_lussier\n---\n\nLast week I worked on getting Octopus Deploy's Build Task installed in TFS 2015. What I found was that there was a lot of great articles that all had&nbsp;parts&nbsp;of the process, but there was nothing that brought all the steps together. So here's my list, complete with links to the various posts/blogs with the details.\n\n<!-- more -->\n\n### Step 1 - Add OctoPack To Your Project\n\nOctoPack is... \"The easiest way to package your applications from your continuous integration/automated build process is to use OctoPack.&nbsp;OctoPack adds a custom MSBuild target that hooks into the build process of your solution. When enabled, OctoPack will package your Windows Service and ASP.NET applications when MSBuild runs.\" (from [http://docs.octopusdeploy.com/display/OD/Using+OctoPack](http://docs.octopusdeploy.com/display/OD/Using+OctoPack)).\n\nOctoPack creates a nice little NuGet package out of your project which gets consumed by Octopus Deploy (which uses its own Nuget server under the hood to facilitate deployments). You can install OctoPack by adding it as a Nuget package in Visual Studio and you should add it _to all the projects you'll be deploying._ As an example, you'd add it to your web project but not to your unit test project.\n\n### Step 2 â€“ Create an API Key\n\nYou'll need this API key for setting the Visual Studio Build step's MSBuild arguments in the next step. To generate this, follow the instructions on this help doc:\n\n[http://docs.octopusdeploy.com/display/OD/How+to+create+an+API+key](http://docs.octopusdeploy.com/display/OD/How+to+create+an+API+key)\n\n### Step 3 - Modify the MSBuild Arguments in your Visual Studio Build Step\n\nOver in TFS, you need to add some arguments to the MSBuild Arguments field in your Visual Studio Build step.\n\n&nbsp;![][1]\n\nThe arguments you need to use are:\n\n    /p:RunOctoPack=true\n    /p:OctoPackPublishPackageToHttp=http://<path to octopus>/nuget/packages\n    /p:OctoPackPublishApiKey=<api key you generated in step 2>\n\n### Step 4 â€“ Create a Generic Connected Service\n\nYou need a generic service for the custom task to work. This service and your team project will share the same API key, which lets your build kick off the deployment via the service.\n\nTo create one, go into your team project and click the little gear icon in the top right corner (next to your user name). This will take you to the admin page, where you should see a number of tabs like Overview, Iterations, etc. Services will be the last one, click it.\n\n&nbsp;\n\n![][2]\n\nOn the Services page, click \"Add New Service Connection\" and select General. Fill in the values into the form that appears:  \n\nConnection Name: &nbsp;Whatever you want to name this service connection.\n\nServer URL: Where your Octopus server is\n\nUser name: Just put Octopus. Why? [Dunno, that's what this post told me to do][3].\n\nPassword/Token Key: Past in the API key that you created a few steps ago.\n\n\n### Step 5 â€“ Install Node and NPM (for TFX-CLI)\n\nInstalling the Octo Build Task for Visual Studio Online is super easy, but for on-premise it requires some extra steps. To install the task (or any task for that matter, see the links at the end of this post for more info on that) you need a tool called TFX-CLI (TFS Cross Platform Command Line Interface). This is a tool that lets you do various things with TFS from a console, but it requires Node.JS and NPM.\n\nNode is a JavaScript runtime and NPM is a package manager (think Nuget, Chocolatey, etc.).\n\nSo before you install TFX-CLI, you need Node and NPM on your system. Note that this doesn't need to be installed on your TFS server. As long as you have the proper permissions, you can install this locally on your machine and just connect to your TFS box.\n\nTo install Node, just head to their website and click on the download links. NPM should install with it by default. [https://nodejs.org/en/](https://nodejs.org/en/)\n\n### Step 6 â€“ Install TFX-CLI\n\nOnce you have Node and NPM installed, you can install TFX-CLI using NPM. Just open a command prompt and type in\n\n    npm install â€“g tfx-cli\n\nYou need to have an internet connection for this to work, as its going out and getting the latest copy.\n\n### Step 7 â€“ Enable Basic Authentication on TFS\n\nYou can connect using TFX-CLI in two ways â€“ with a personal access token (VSO) or with Basic Authentication. Microsoft NTLM isn't supported for TFX-CLI yet, so while you'll be passing in your domain credentials you'll be doing it over \"basic\" auth. Note that if you want to secure it (i.e. SSL) then you have to do that yourself manually, you don't get it out of the box.\n\nCheck out this help file from the TFX-CLI project site for a walkthrough on setting up basic authentication. [https://github.com/Microsoft/tfs-cli/blob/master/docs/configureBasicAuth.md](https://github.com/Microsoft/tfs-cli/blob/master/docs/configureBasicAuth.md)\n\nOne thing I will call out â€“ make sure you set the basic auth at the _web application_ level and not the _web site_ level, otherwise you'll just get invalid login responses in TFX-CLI.\n\n### Step 8 â€“ Clone the OctoTFS repo from Github\n\nPull down the OctoTFS project from Github. You'll need Git installed for this to work ([https://git-scm.com/](https://git-scm.com/)).\n\n&nbsp;![][4]\n\nHere I'm putting the OctoTFS project in a folder called \"OctoTFS\" in my C: drive.\n\n### Step 9 â€“ Login to TFS in TFX-CLI and Upload the Octopus Build Task\n\nFINALLY we can get the Octopus Build Task installed in TFS!\n\nFirst, log in to TFX-CLI. Open a command prompt and type...\n\n    tfx\n\nat the prompt (without the quotes). You should see this when you hit enter:\n\n&nbsp;\n\n![][5]\n\nNow you need to log in to your TFS server. Type `tfx login â€“auth-type basic`.\n\nYou'll be asked to enter in the Service URL, which is the URL to the TFS collection you want to connect to. For your username and password, enter in your domain credentials or if you have a local account on the TFS server log in with that.\n\n&nbsp;\n\n![][6]\n\nAll of this has been leading up to this next moment â€“ installing the build task! In the command line navigate to the OctopusBuildTasks folder. See the image below for the path where I installed mine earlier â€“ if you follow the same steps, it should be there for you as well. Once you're there, type...\n\n    tfx build tasks upload\n\nThis will prompt you to put in the task path, which is one level below. Type...\n\n    ./CreateOctopusRelease\n\nand hit enter. You should see green text saying that the build task was uploaded successfully!\n\n&nbsp;\n\n![][7]\n\n### Step 10 â€“ Verification\n\nHead over to your TFS instance and bring up one of the projects within the collection you specified. Edit one of the builds (or create a new one) and add a deployment step. You should now see the Octopus Deploy Build Step as an option!\n\n&nbsp;\n\n![][8]\n\n### Conclusion\n\nTFS will have to suffer through awkward deployment scenarios like this compared to the ease of installation that Visual Studio Online provides (adding this build task to VSO is automated and requires only a few button clicks to install). While it's great that we have tools like TFX-CLI, better (and updated) documentation would go a long way to help those of us trying to get as much out of our on-prem TFS.\n\nYou may be wondering why this isn't documented anywhere already. There actually is a lot of documentation out there about this process, but none that covered everything start to finish. Hopefully anyone else looking to get this working in their on-prem TFS environment can avoid some of the pain I went through (or even better, the install process for custom build tasks improves and this whole post becomes irrelevant). &nbsp;\n\nIf you're intrigued at the idea of creating your own custom build tasks now that you know how to install them, Colin Dembovsky has a great series of blog posts that will walk you through that process, definitely check it out at the links below:\n\n[http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-1](http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-1)\n\n[http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-2](http://colinsalmcorner.com/post/developing-a-custom-build-vnext-task-part-2)\n\n[1]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep2_1100442752.png \"OctopusDeployStep2_1100442752.png\"\n[2]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep4_860833856.png \"OctopusDeployStep4_860833856.png\"\n[3]: https://marketplace.visualstudio.com/items/octopusdeploy.octopus-deploy-build-release-tasks\n[4]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep8_-2114270080.png \"OctopusDeployStep8_-2114270080.png\"\n[5]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep9_-176892480.png \"OctopusDeployStep9_-176892480.png\"\n[6]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep9-2_1644126400.png \"OctopusDeployStep9-2_1644126400.png\"\n[7]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep9-3_-1831133056.png \"OctopusDeployStep9-3_-1831133056.png\"\n[8]: https://gwb.blob.core.windows.net/dlussier/Setting-Up-Octopus-Build-Task-on-TFS-2015-On-Prem_170820/OctopusDeployStep10_1810252032.png \"OctopusDeployStep10_1810252032.png\"\n\n","categories":[{"name":"tfs","slug":"tfs","permalink":"https://westerndevs.com/categories/tfs/"},{"name":"octopus","slug":"tfs/octopus","permalink":"https://westerndevs.com/categories/tfs/octopus/"},{"name":"continuous delivery","slug":"tfs/octopus/continuous-delivery","permalink":"https://westerndevs.com/categories/tfs/octopus/continuous-delivery/"}],"tags":[]},{"title":"Goodbye Child Actions, Hello View Components","authorId":"dave_paquette","slug":"goodbye-child-actions-hello-view-components","date":"2016-01-02 15:42:41+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"MVC-6/View-Components/goodbye-child-actions-hello-view-components/","link":"","permalink":"https://westerndevs.com/MVC-6/View-Components/goodbye-child-actions-hello-view-components/","excerpt":"In previous versions of MVC, we used Child Actions to build reusable components. Child Actions do not exist in MVC 6. Instead, we are encouraged to use the new View Component feature to support this use case.","raw":"---\nlayout: post\ntitle: Goodbye Child Actions, Hello View Components\ntags:\n  - ASP.NET 5\n  - MVC 6\n  - View Components\ncategories:\n  - MVC 6\n  - View Components\nauthorId: dave_paquette\noriginalurl: 'http://www.davepaquette.com/archive/2016/01/02/goodbye-child-actions-hello-view-components.aspx'\ndate: 2016-01-02 10:42:41\nexcerpt: In previous versions of MVC, we used Child Actions to build reusable components. Child Actions do not exist in MVC 6. Instead, we are encouraged to use the new View Component feature to support this use case.\n---\nIn previous versions of MVC, we used [Child Actions](http://haacked.com/archive/2009/11/18/aspnetmvc2-render-action.aspx/) to build reusable components / widgets that consisted of both Razor markup and some backend logic. The backend logic was implemented as a controller action and typically marked with a `[ChildActionOnly]` attribute. Child actions are extremely useful but as some have pointed out, it is easy to [shoot yourself in the foot](http://www.khalidabuhakmeh.com/obscure-bugs-asp-net-mvc-child-actions). \n\n**Child Actions do not exist in MVC 6**. Instead, we are encouraged to use the new View Component feature to support this use case. Conceptually, view components are a lot like child actions but they are a lighter weight and no longer involve the lifecycle and pipeline related to a controller. Before we get into the differences, let's take a look at a simple example.\n\n# A simple View Component\nView components are made up of 2 parts: A view component class and a razor view.\n\nTo implement the view component class, inherit from the base `ViewComponent` and implement an `Invoke` or `InvokeAsync` method. This class can be anywhere in your project. A common convention is to place them in a ViewComponents folder. Here is an example of a simple view component that retrieves a list of articles to display in a _What's New_ section.\n\n{% codeblock lang:csharp %}\nnamespace MyWebApplication.ViewComponents\n{\n    public class WhatsNewViewComponent : ViewComponent\n    {\n        private readonly IArticleService _articleService;\n\n        public WhatsNewViewComponent(IArticleService articleService)\n        {\n            _articleService = articleService;\n        }\n\n        public IViewComponentResult Invoke(int numberOfItems)\n        {\n            var articles = _articleService.GetNewArticles(numberOfItems);\n            return View(articles);\n        }\n    }\n}\n{% endcodeblock %}\n\nMuch like a controller action, the Invoke method of a view component simply returns a view. If no view name is explicitly specified, the default `Views\\Shared\\Components\\ViewComponentName\\Default.cshtml` is used. In this case, `Views\\Shared\\Components\\WhatsNew\\Default.cshtml`. Note there are a ton of conventions used in view components. I will be covering these in a future blog post.\n\n{% codeblock \"Views\\\\Shared\\\\Components\\\\WhatsNew\\\\Default.cshtml\" lang:html %}\n@model IEnumerable<Article>\n\n<h2>What's New</h2>\n<ul>\n@foreach (var article in Model)\n{\n    <li><a asp-controller=\"Article\" \n           asp-action=\"View\" \n           asp-route-id=\"@article.Id\">@article.Title</a></li>\n}\n</ul>\n{% endcodeblock %}\n\nTo use this view component, simply call `@Component.Invoke` from any view in your application. For example, I added this to the Home/Index view:\n{% codeblock \"Views\\\\Home\\\\Index.cshtml\" lang:html %}\n<div class=\"col-md-3\">\n    @Component.Invoke(\"WhatsNew\", 5)\n</div>\n{% endcodeblock %}\n\nThe first parameter to `@Component.Invoke` is the name of the view component. Any additional parameters will be passed to the `Invoke` method that has a matching signature. In this case, we specified a single `int`, which matches the `Invoke(int numberOfItems)` method of the `WhatsNewViewComponent` class.\n\n![What's New View Component](http://www.davepaquette.com/images/whats-new-view-component.png)\n\n## How is this different?\n\nSo far this doesn't really look any different from what we had with Child Actions. There are however some major differences here. \n\n### No Model Binding\nWith view components, parameters are passed directly to your view component when you call `@Component.Invoke()` or `@Component.InvokeAsync()` in your view. There is no model binding needed here since the parameters are not coming from the HTTP request. You are calling the view component directly using C#. No model binding means you can have overloaded `Invoke` methods with different parameter types. This is something you can't do in controllers.\n\n### No Action Filters\nView components don't take part in the controller lifecycle. This means you can't add action filters to a view component. While this might sound like a limitation, it is actually an area that caused problems for a lot of people. Adding an action filter to a child action would sometimes have unintended consequences when the child action was called from certain locations. \n\n### Not reachable from HTTP\n\nA view component never directly handles an HTTP request so you can't call directly to a view component from the client side. You will need to wrap the view component with a controller if your application requires this behaviour.\n\n## What is available?\n\n### Common Properties\nWhen you inherit from the base `ViewComponent` class, you get access to a few properties that are very similar to controllers:\n\n{% codeblock lang:csharp %}\n[ViewComponent]\npublic abstract class ViewComponent\n{\n    protected ViewComponent();\n    public HttpContext HttpContext { get; }\n    public ModelStateDictionary ModelState { get; }\n    public HttpRequest Request { get; }\n    public RouteData RouteData { get; }\n    public IUrlHelper Url { get; set; }\n    public IPrincipal User { get; }\n    \n    [Dynamic]    \n    public dynamic ViewBag { get; }\n    [ViewComponentContext]\n    public ViewComponentContext ViewComponentContext { get; set; }\n    public ViewContext ViewContext { get; }\n    public ViewDataDictionary ViewData { get; }\n    public ICompositeViewEngine ViewEngine { get; set; }\n\n    //...\n}\n{% endcodeblock %}\n\nMost notably, you can access information about the current user from the `User` property and information about the current request from the `Request` property. Also, route information can be accessed from the `RouteData` property. You also have the `ViewBag` and `ViewData`. Note that the ViewBag / ViewData are shared with the controller. If you set ViewBag property in your controller action, that property will be available in any ViewComponent that is invoked by that controller action's view.\n \n### Dependency Injection\nLike controllers, view components also take part in dependency injection so any other information you need can simply be injected to the view component. In the example above, we injected the `IArticleService` that allowed us to access articles form some remote source. Anything that you could inject into a controller can also be injected into a view component.\n\n# Wrapping it up\nView components are a powerful new feature for creating reusable widgets in MVC 6. Consider using View Components any time you have complex rendering logic that also requires some backend logic. \n \n \n","categories":[{"name":"MVC 6","slug":"MVC-6","permalink":"https://westerndevs.com/categories/MVC-6/"},{"name":"View Components","slug":"MVC-6/View-Components","permalink":"https://westerndevs.com/categories/MVC-6/View-Components/"}],"tags":[{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"MVC 6","slug":"MVC-6","permalink":"https://westerndevs.com/tags/MVC-6/"},{"name":"View Components","slug":"View-Components","permalink":"https://westerndevs.com/tags/View-Components/"}]},{"title":"Looking Forward into 2016","authorId":"david_wesst","slug":"Looking-Forward-into-2016","date":"2016-01-01 18:39:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"retrospective/Looking-Forward-into-2016/","link":"","permalink":"https://westerndevs.com/retrospective/Looking-Forward-into-2016/","excerpt":"DW documents and shares his planning session for 2016 based on what he's learned from 2015.","raw":"---\nlayout: post\ntitle: Looking Forward into 2016\ncategories:\n  - retrospective\ndate: 2016-01-01 13:39:12\nexcerpt: DW documents and shares his planning session for 2016 based on what he's learned from 2015.\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2016/01/Looking-Forward-into-2016/\n---\n\n{% img http://blog.davidwesst.com/2016/01/Looking-Forward-into-2016/sunrise.png %}\n\nThis is the spiritual sequel to my previous {% post_link 2016-01-01-Highlight-Reel-for-2015 \"Highlight Reel for 2015\" %} post. You don't need to read it previously, but it might provide some context on some of these ideas. \n\nI decided that with everything still in flux and forever changing, it's easier for me to list out the interests and ideas, rather than concrete requirements for individual projects. Not sure how they'll pan out exactly, but here's the \"what, why, and how\" for the plan for 2016.\n\n#### Focus My Content\n**What?** Any content I produce, (e.g. blog posts, YouTube videos, presentations, etc...) should be able to be categorized into one of three categories: JavaScript App Dev (Primary), JavaScript Game Dev (Secondary), Gaming (Personal).\n\n**Why?** When I go back through my blog, it feels a little too unstructured. As in, the author knows lots of stuff, but appears to be all over the place with his content. Maybe that's what people's blogs tend to be, but I'd like to focus my efforts and see if writing content in a focused way leads me into something of a specialty. \n\n**How?** The three categories listed above should act as a sort of guide before I spend time working on new content. Plus, it should allow me to plan what I'm going to write about and not about just writing anything.\n\n#### Talk About Today\n**What?** Might seem obvious, but I should be talking about what I am working on topics that I'm not familiar with, but seem to be trending or something that I want to learn \"for the next project\".\n\n**Why?** In order to get involved with conferences or to force myself to learn new subject, I would sometimes volunteer content for conferences or events on subjects that I wasn't all too familiar with. It only resulted in stress and generally content that I wouldn't call my best.\n\n**How?** When I'm working on a project, I hope to factor in some time to generate content that relates the part of the project I'm working on. For example, if I'm spending time setting up a build script for a project, document what I'm doing in blog posts or slide deck for a presentation. I could possibly review it and screen capture the steps and turn it into a series of YouTube videos. Either way, it'll involve some premeditation on my part and possibly slow down \"progress\", but I think that in time it'll become a great way to document what I'm learning, as I'm learning it.\n\n#### Get Back to Games\n**What?** I really love video games. It's a passion of mine, and I'd like to get back into sharing that side of me with the universe.\n\n**Why?** I think adding something a bit more personal to my online presence will add some personality to my content. I don't want to pollute the content I produce, so I'll have to do it in a smart way. Still, I think it's important to keep passion as part of what I think about and share with everyone. Maybe it'll inspire me with new ideas or something.\n\n**How?** Carefully and cautiously. As I said, I don't want to pollute my content stream with a bunch of random video game posts. Hopefully one of my planned projects comes together and I'll have a way for people to filter out that content as necessary. But again, we'll see how it goes.\n\n#### Become a Producer\n**What?** If I want to get structured and scheduled with my content, I need to start thinking like a producer and plan out what I will be making regardless of what it is. I plan out what the content is, schedule when it's going to get created, edited, and released, and ensure that time is available to actually make it happen.\n\n**Why?** I like sharing content, not only for some kind of self indulgence, but because it forces me to think about what I've learned and ensures that I won't lose the knowledge, and that I _actually_ understand it. To make sure that I stay on track and that the content continues to fit into everything else I'd like to accomplish in the new year.\n\n**How?** After this post, I'm going to spend some time planning out content for the year. Planning will start with looking at my projects, determining what topics I can generate content from, and what it will cost with respect to time, to generate that content. If I have more time one month, I might plan on some richer content like videos or some other ideas I have mulling around in my brain. If not, I can rely on my blogging skills.\n\n## The Point\nLike the previous post, this is more for me than it is for the audience. It is a concrete representation of my reflection on the year gone by, and how I can take the lessons learned and use them moving forward.\n\nLet's see what I can do with the upcoming year. I'll probably blog about the results in 365 days...or so.\n\nThanks for playing. ~ DW ","categories":[{"name":"retrospective","slug":"retrospective","permalink":"https://westerndevs.com/categories/retrospective/"}],"tags":[]},{"title":"Highlight Reel for 2015","authorId":"david_wesst","slug":"Highlight-Reel-for-2015","date":"2016-01-01 18:31:57+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"retrospective/Highlight-Reel-for-2015/","link":"","permalink":"https://westerndevs.com/retrospective/Highlight-Reel-for-2015/","excerpt":"David Wesst takes a moment to look back at his personal and professional development throughout 2015.","raw":"---\nlayout: post\ntitle: Highlight Reel for 2015\ncategories:\n  - retrospective\nexcerpt: David Wesst takes a moment to look back at his personal and professional development throughout 2015.\ncomments: true\nauthorId: david_wesst\noriginalurl: 'http://blog.davidwesst.com/2016/01/Highlight-Reel-for-2015/'\ndate: 2016-01-01 13:31:57\n---\n\n\n{% img http://blog.davidwesst.com/2016/01/Highlight-Reel-for-2015/thinker.png %}\n\nI just finished reading my [post about 2014](http://blog.davidwesst.com/2015/01/Highlight-Reel-for-2014/). If I were to compare my plans for 2015 to the result of the year-long sprint, I'd have to say that it appears that we are either way off track, or that the requirements have changed quite a bit.\n\n2015 has been great, with all sorts of chaos thrown throughout. In this post I'll share with you the highlights and the lessons learned from those events.\n\n### Real Life Changes\nNot long after my 2014 post, I was laid off from my position as a Systems Analyst and found out that my partner and I were finally going to have a baby. In short, my life changed dramatically throughout the year. Different income levels, new responsibilities, and just a different outlook on life.\n\nWhen I wrote that original post, I was sure that having six months to finish off all my projects was going to be plenty. But, as I mentioned before, requirements change with priorities, and with the lay off, combined with the pressure of potentially not having employment to support my larger family, resulted in plenty of non-productive days and a shift in task priority.\n\n##### Lesson 1: Personal Time is a Limited\nIt sounds lame, but it's true. \n\nAs you continue to grow as a person, you tend to take on new responsibilities, and those take up more of your personal time. Whether its a new job, or a kid, or a new house or investment. Personal time to commit to one or more of these projects is limited, so make sure that when you have the time, you're ready to jump on it.  \n\n### ZVGQ Soft Launch\nIn March I thought I had done enough to get an early release of ZVGQ, the five (now six) year long project out the door. I launched it, announced it, and started getting things moving on the site. And then it became too much to maintain, development came to a halt, and I realized the soft launch was a bust. \n\nI wouldn't say I had failed on the task execution, but rather the planning side. I underestimated what it takes to build _and maintain_ a software project, which lead the launch to fail.\n\n##### Lesson 2: Maintenance Plans are Important\nYou'd think I would have known this being a software analyst for years in both the public and private sectors. I know that I've created maintenance plans for many projects before we even started writing a single line of code.\n\nIt turns out that for my _own_ software, I tend to think like a client. I'm exempt from the rules, because it'll cost too much or whatever reason. That is where things went wrong. I cut corners to make get system live, but I didn't consider what it was going to take to maintain AND build the software in parallel. It was doable for a while, but when the real life changes started to crop up in the middle of the year, it just became too much and it slipped.\n\nI could go on about this, but I'll save it for another time.\n\n### New Job and Western Devs\nAt the end of the lay off period, my employer found a new position for me as an Application Developer, which translates to roughly an Intermediate Software Developer that focuses primarily on Java-based systems.\n\nJust as this was coming together, the [Western Devs](http://www.westerndevs.com) were starting to get organized and asking who would be interested in contributing to a community driven site filled with content from any of us willing to contribute.\n\nBeing a part of the Western Devs community and getting back into code trenches has made me realize something:\n\n##### Lesson 3: Learning New Stuff Makes it Interesting \nAfter 70 JUnit test written, 25 blog posts, and a handful of podcasts, I've relearned that the reason I love programming so much is because I'm forever going to be learning. Whether it's design principles, new methodologies, or new tools and technologies, all of it is learning and that makes it interesting to me.\n\nWith respect to my side projects, it's important that I work on things that  _I want_ to build. To keep things interesting, I need to be learning something new continually. That way I keep myself interested in the project and don't just jump to the next one because it has something cool to learn.\n\n## The Point\nNo point other than to take a moment to self-reflect and produce a concrete representation of that self-reflection. Next post will be about what I plan to do with what I've taken away from 2015 and Looking-Forward-into-2016 \"how I will take that forward into 2016\".\n\nA sort of conclusion, of sorts.\n\nThanks for playing. ~ DW \n   \n \n\n","categories":[{"name":"retrospective","slug":"retrospective","permalink":"https://westerndevs.com/categories/retrospective/"}],"tags":[]},{"title":"Complex Custom Tag Helpers in MVC 6","authorId":"dave_paquette","slug":"complex-custom-tag-helpers-in-mvc-6","date":"2015-12-28 23:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"MVC-6/Tag-Helpers/complex-custom-tag-helpers-in-mvc-6/","link":"","permalink":"https://westerndevs.com/MVC-6/Tag-Helpers/complex-custom-tag-helpers-in-mvc-6/","excerpt":"How to build complex tag helper that are made up of multiple parts in MVC 6","raw":"---\nlayout: post\ntitle: Complex Custom Tag Helpers in MVC 6\ndate: 2015-12-28 18:00:00\ncategories:\n  - MVC 6\n  - Tag Helpers\ntags:\n  - ASP.NET MVC\n  - ASP.NET 5\n  - MVC 6\n  - VS 2015\n  - Tag Helpers\nauthorId: dave_paquette\nexcerpt: How to build complex tag helper that are made up of multiple parts in MVC 6\noriginalurl: http://www.davepaquette.com/archive/2015/12/28/complex-custom-tag-helpers-in-mvc-6.aspx\n---\nIn a previous blog post we talked about how to create [a simple tag helper](http://www.davepaquette.com/archive/2015/06/22/creating-custom-mvc-6-tag-helpers.aspx) in MVC 6. In today's post we take this one step further and create a more complex tag helper that is made up of multiple parts.\n\n## A Tag Helper for Bootstrap Modal Dialogs\n\nCreating a [modal dialog](http://getbootstrap.com/javascript/#static-example) in bootstrap requires some verbose html.\n\n{% codeblock 'Bootstrap Modal' lang:html %}\n<div class=\"modal fade\" tabindex=\"-1\" role=\"dialog\">\n  <div class=\"modal-dialog\">\n    <div class=\"modal-content\">\n      <div class=\"modal-header\">\n        <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button>\n        <h4 class=\"modal-title\">Modal title</h4>\n      </div>\n      <div class=\"modal-body\">\n        <p>One fine body&hellip;</p>\n      </div>\n      <div class=\"modal-footer\">\n        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n        <button type=\"button\" class=\"btn btn-primary\">Save changes</button>\n      </div>\n    </div><!-- /.modal-content -->\n  </div><!-- /.modal-dialog -->\n</div><!-- /.modal -->\n{% endcodeblock %}\n\nUsing a tag helper here would help simplify the markup but this is a little more complicated than the Progress Bag example. In this case, we have HTML content that we want to add in 2 different places: the `<div class=\"modal-body\"></div>` element and the `<div class=\"modal-footer\"></div>` element. \n\nThe solution here wasn't immediately obvious. I had a chance to talk to [Taylor Mullen](https://twitter.com/ntaylormullen) at the MVP Summit ASP.NET Hackathon in November and he pointed me in the right direction. The solution is to use 3 different tag helpers that can communicate with each other through the `TagHelperContext`.\n\nUltimately, we want our tag helper markup to look like this:\n\n{% codeblock 'Bootstrap Modal using a Tag Helper' lang:html %}\n<modal title=\"Modal title\">\n    <modal-body>\n        <p>One fine body&hellip;</p>\n    </modal-body>\n    <modal-footer>\n        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n        <button type=\"button\" class=\"btn btn-primary\">Save changes</button>\n    </modal-footer>\n</modal>\n{% endcodeblock %}\n\nThis solution uses 3 tag helpers: `modal`, `modal-body` and `modal-footer`. The contents of the `modal-body` tag will be placed inside the `<div class=\"modal-body\"></div>` while the contents of the `<modal-footer>` tag will be placed inside the `<div class=\"modal-footer\"></div>` element. The `modal` tag helper is the one that will coordinate all this.\n\n## Restricting Parents and Children\nFirst things first, we want to make sure that `<modal-body>` and `<modal-footer>` can only be placed inside the `<modal>` tag and that the `<modal>` tag can only contain those 2 tags. To do this, we set the `RestrictChildren` attribute on the modal tag helper and the `ParentTag` property of the `HtmlTargetElement` attribute on the modal body and modal footer tag helpers:\n\n\n{% codeblock lang:csharp  %}\n[RestrictChildren(\"modal-body\", \"modal-footer\")]\npublic class ModalTagHelper : TagHelper\n{\n     //...\n}\n\n[HtmlTargetElement(\"modal-body\", ParentTag = \"modal\")]\npublic class ModalBodyTagHelper : TagHelper\n{\n    //...\n}\n\n[HtmlTargetElement(\"modal-footer\", ParentTag = \"modal\")]\npublic class ModalFooterTagHelper : TagHelper\n{\n    //...\n}\n{% endcodeblock %}\n\nNow if we try to put any other tag in the `<modal>` tag, Razor will give me a helpful error message.\n\n![Restrict children](http://www.davepaquette.com/images/restrict-children-razor-error.png \"Restricting child elements in tag helpers\")\n \n## Getting contents from the children\n\nThe next step is to create a context class that will be used to keep track of the contents of the 2 child tag helpers.\n\n{% codeblock lang:csharp %}\npublic class ModalContext\n{\n    public IHtmlContent Body { get; set; }\n    public IHtmlContent Footer { get; set; }\n}\n{% endcodeblock %}\n\nAt the beginning of the ProcessAsync method of the Modal tag helper, create a new instance of `ModalContext` and add it to the current `TagHelperContext`:\n\n{% codeblock lang:csharp %}\npublic override async Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n{\n    var modalContext = new ModalContext();\n    context.Items.Add(typeof(ModalTagHelper), modalContext);\n    //...\n}\n{% endcodeblock %}\n\nNow, in the modal body and modal footer tag helpers we will get the instance of that `ModalContext` via the `TagHelperContext`. Instead of rendering the output, these child tag helpers will set the the `Body` and `Footer` properties of the `ModalContext`.\n\n{% codeblock lang:csharp %}\n[HtmlTargetElement(\"modal-body\", ParentTag = \"modal\")]\npublic class ModalBodyTagHelper : TagHelper\n{\n    public override async Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n    {\n        var childContent = await output.GetChildContentAsync();\n        var modalContext = (ModalContext)context.Items[typeof(ModalTagHelper)];\n        modalContext.Body = childContent;\n        output.SuppressOutput();\n    }\n}\n{% endcodeblock %}\n\nBack in the modal tag helper, we call `output.GetChildContentAsync()` which will cause the child tag helpers to execute and set the properties on the `ModalContext`. After that, we just set the output as we normally would in a tag helper, placing the `Body` and `Footer` in the appropriate elements.\n\n{% codeblock \"Modal tag helper\" lang:html %}\npublic override async Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n{\n    var modalContext = new ModalContext();\n    context.Items.Add(typeof(ModalTagHelper), modalContext);\n\n    await output.GetChildContentAsync();\n\n    var template =\n$@\"<div class='modal-dialog' role='document'>\n<div class='modal-content'>\n<div class='modal-header'>\n<button type = 'button' class='close' data-dismiss='modal' aria-label='Close'><span aria-hidden='true'>&times;</span></button>\n<h4 class='modal-title' id='{context.UniqueId}Label'>{Title}</h4>\n</div>\n<div class='modal-body'>\";\n\n    output.TagName = \"div\";\n    output.Attributes[\"role\"] = \"dialog\";\n    output.Attributes[\"id\"] = Id;\n    output.Attributes[\"aria-labelledby\"] = $\"{context.UniqueId}Label\";\n    output.Attributes[\"tabindex\"] = \"-1\";\n    var classNames = \"modal fade\";\n    if (output.Attributes.ContainsName(\"class\"))\n    {\n        classNames = string.Format(\"{0} {1}\", output.Attributes[\"class\"].Value, classNames);\n    }\n    output.Attributes[\"class\"] = classNames;\n    output.Content.AppendHtml(template);\n    if (modalContext.Body != null)\n    {\n        output.Content.Append(modalContext.Body); //Setting the body contents\n    }\n    output.Content.AppendHtml(\"</div>\");\n    if (modalContext.Footer != null)\n    {\n        output.Content.AppendHtml(\"<div class='modal-footer'>\");\n        output.Content.Append(modalContext.Footer); //Setting the footer contents\n        output.Content.AppendHtml(\"</div>\");\n    }\n    \n    output.Content.AppendHtml(\"</div></div>\");\n}\n{% endcodeblock %}\n\n## Conclusion\nComposing complex tag helpers with parent / child relationships is fairly straight forward. In my opinion, the approach here is much easier to understand than the \"multiple transclusion\" approach used to solve the same problem in Angular 1. It would be easy to unit test and as always, Visual Studio provides error messages directly in the HTML editor to guide anyone who is using your tag helper.\n\nYou can check out the full source code on the [Tag Helper Samples repo](https://github.com/dpaquette/TagHelperSamples).\n ","categories":[{"name":"MVC 6","slug":"MVC-6","permalink":"https://westerndevs.com/categories/MVC-6/"},{"name":"Tag Helpers","slug":"MVC-6/Tag-Helpers","permalink":"https://westerndevs.com/categories/MVC-6/Tag-Helpers/"}],"tags":[{"name":"ASP.NET MVC","slug":"ASP-NET-MVC","permalink":"https://westerndevs.com/tags/ASP-NET-MVC/"},{"name":"ASP.NET 5","slug":"ASP-NET-5","permalink":"https://westerndevs.com/tags/ASP-NET-5/"},{"name":"MVC 6","slug":"MVC-6","permalink":"https://westerndevs.com/tags/MVC-6/"},{"name":"VS 2015","slug":"VS-2015","permalink":"https://westerndevs.com/tags/VS-2015/"},{"name":"Tag Helpers","slug":"Tag-Helpers","permalink":"https://westerndevs.com/tags/Tag-Helpers/"}]},{"title":"My Hasty Move to Hexo","authorId":"dave_paquette","slug":"my-hasty-move-to-hexo","date":"2015-12-28 22:00:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"hexo/my-hasty-move-to-hexo/","link":"","permalink":"https://westerndevs.com/hexo/my-hasty-move-to-hexo/","excerpt":"I have meant for some time now to move my blog to something a little more stable. Wordpress is a fine platform but really overkill for what I need.","raw":"---\ntitle: My Hasty Move to Hexo\ncategories:\n  - hexo\ntags:\n  - blogging\n  - azure\n  - hexo\ndate: 2015-12-28 17:00:00\nauthorId: dave_paquette\nexcerpt: I have meant for some time now to move my blog to something a little more stable. Wordpress is a fine platform but really overkill for what I need.\noriginalurl: http://www.davepaquette.com/archive/2015/12/28/my-hasty-move-to-hexo.aspx\n---\n\nAs I mentioned in my last post, I had some downtime on my blog after [my database mysteriously disappeared](http://davepaquette.com/archive/2015/12/03/the-case-of-the-disappearing-database.aspx). \n\nI have meant for some time now to move my blog to something a little more stable. Wordpress is a fine platform but really overkill for what I need. After moving all my comments to [Disqus](https://disqus.com/) earlier this year I really had no need at all for a database backend. More importantly, I found it difficult to fine-tune things in Wordpress. Not because it is necessarily difficult to do these things in Wordpress but because have absolutely no interest in learning php.\n\n## A Quick Survey\nI wanted to move to a statically generated site. I like writing my posts in Markdown and I like the simplicity of a statically generated site. I had a quick look at [this site](https://www.staticgen.com/) that provides a list of the most popular static site generators.\n\nJekyll is definitely a great option and seems to be the most popular. At the time, we were using it over at [Western Devs](http://www.westerndevs.com). The main problem I have with Jekyll is that it is a bit of a pain to get working on Windows.\n\nI noticed a handy Language filter on the site and picked .NET. There are a few options there but nothing that seems to have any great traction.\n\nNext I picked JavaScript/Node. I am reasonably proficient at JavaScript and I use Node for [front-end web dev](http://www.davepaquette.com/categories/Web-Dev/) tasks every day. In that list, Hexo seemed to be the most popular. After polling the group at Western Devs I found out that [David Wesst](http://www.westerndevs.com/bios/david_wesst/) was also using Hexo. This is great for me because Wessty is our resident Node / JavaScript expert. With an expert to fall back on in an emergency situation, I forged ahead in my move to Hexo.\n\n## Moving from Wordpress\n\nHexo provides a plugin for importing from Wordpress. All I did here was followed the steps in the [migration documentation](https://hexo.io/docs/migration.html#WordPress). All my posts came across as expected. The only thing that bothered me with the posts is that I lost syntax highlighting on my code blocks. Fixing this was a bit of a manual process, wrapping my code blocks as follows: \n\n```\n{% codeblock lang:html %}\n<div>...</div>\n{% endcodeblock %}\n```\n\nI did this for my 40 most popular blog posts which covers about 80% of the traffic to my blog. Good enough for me.\n\nNext, I needed to pull down my images. I serve my images on my blog site (I know...I should be hosting this somewhere else like blob storage or imgr). To fix this, I simply used FTP to copy the images from down from my old site and put them in the Hexo `source` folder. I my case that was the `source\\wp-content\\uploads` folder.\n\n## Deploying to Azure\n\nI decided to keep my blog hosted in Azure. To deploy to Azure using Hexo, I am using the [git deploy method](https://hexo.io/docs/deployment.html#Git). With this method, anytime I call `hexo deploy --generate`, Hexo will generate my site and then commit the generated site to a particular branch in my git repository. I then use the [Web App continuous deployment hooks](https://azure.microsoft.com/en-us/documentation/articles/web-sites-publish-source-control/) in Azure to automatically update the site whenever a change is pushed to that branch.\n\n## Some Issues with Hexo\nSince I moved my blog over, WesternDevs has also [moved to Hexo](http://www.westerndevs.com/jekyll/hexo/Migrating-from-Jekyll-to-Hexo/) as part of a big site redesign. [Kyle Baley](http://www.westerndevs.com/bios/kyle_baley/) has done a good job of documenting some of the [issues we encountered](http://www.westerndevs.com/jekyll/hexo/Migrating-from-Jekyll-to-Hexo-Part-2/) along the way.\n\nI ran into a few more specific issues. First of all, I didn't want to break all my old links so I kept the same permalinks as my old blog. The challenge with that is that each url ends in .aspx. Weird right...my old blog was Wordpress (php) but before Wordpress I was on geekswithblogs which was using some ASP.NET based blogging engine. So here I am in 2015 with a statically generated blog that is created using Node and is hosted in Azure that for some reason has `.aspx` file endings. The problem with this was that Aure uses IIS and tries to process the `aspx` files using the ASP.NET page handlers. Initially everything looked okay. The pages were still being served but some of the characters were not encoded properly. The solution here was to add a `web.config` to my Hexo `source` folder. In the `web.config` I was able to turn off the ASP.NET page handlers and tell IIS that `.aspx` pages should be treated as static content:\n\n{% codeblock lang:xml %}\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <system.webServer>\n        <handlers>\n            <remove name=\"PageHandlerFactory-ISAPI-2.0-64\" />\n            <remove name=\"PageHandlerFactory-ISAPI-2.0\" />\n            <remove name=\"PageHandlerFactory-Integrated\" />\n            <remove name=\"PageHandlerFactory-ISAPI-4.0_32bit\" />\n            <remove name=\"PageHandlerFactory-Integrated-4.0\" />\n            <remove name=\"PageHandlerFactory-ISAPI-4.0_64bit\" />\n        </handlers>\n        <staticContent>\n            <clientCache cacheControlCustom=\"public\" cacheControlMode=\"UseMaxAge\" cacheControlMaxAge=\"7.00:00:00\" /> \n            <mimeMap fileExtension=\".aspx\" mimeType=\"text/html\" />\n            <mimeMap fileExtension=\".eot\" mimeType=\"application/vnd.ms-fontobject\" />\n            <mimeMap fileExtension=\".ttf\" mimeType=\"application/octet-stream\" />\n            <mimeMap fileExtension=\".svg\" mimeType=\"image/svg+xml\" />\n            <mimeMap fileExtension=\".woff\" mimeType=\"application/font-woff\" />\n            <mimeMap fileExtension=\".woff2\" mimeType=\"application/font-woff2\" />\n        </staticContent>\n        <rewrite>\n            <rules>\n                <rule name=\"RSSRewrite\" patternSyntax=\"ExactMatch\">\n                    <match url=\"feed\" />\n                    <action type=\"Rewrite\" url=\"atom.xml\" appendQueryString=\"false\" />\n                </rule>\n                <rule name=\"RssFeedwithslash\">\n                    <match url=\"feed/\" />\n                    <action type=\"Rewrite\" url=\"atom.xml\" appendQueryString=\"false\" />\n                </rule>\n            </rules>\n        </rewrite>\n    </system.webServer>\n</configuration>\n{% endcodeblock %}\n\nIn the `web.config` I also added a rewrite rule to preserve the old RSS feed link. \n\n## Triggering a mass migration\nWhile not perfect, I have been happy with my experience migrating to Hexo. Overall, I was able to complete my initial migration within a few hours. Converting older posts to use syntax highlighting took a litte longer but I was able to do that in phases.  \n\nI talked about my experience over at Western Devs and this seems to have triggered a few of us to also move our blogs over to Hexo. Hopefully that decision does come back to bite me later...so far it is working out well.  \n\n\n","categories":[{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/categories/hexo/"}],"tags":[{"name":"blogging","slug":"blogging","permalink":"https://westerndevs.com/tags/blogging/"},{"name":"azure","slug":"azure","permalink":"https://westerndevs.com/tags/azure/"},{"name":"hexo","slug":"hexo","permalink":"https://westerndevs.com/tags/hexo/"}]},{"title":"Migrating from Jekyll to Hexo: Part 2","authorId":"kyle_baley","slug":"Migrating-from-Jekyll-to-Hexo-Part-2","date":"2015-12-28 00:43:32+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"jekyll/hexo/Migrating-from-Jekyll-to-Hexo-Part-2/","link":"","permalink":"https://westerndevs.com/jekyll/hexo/Migrating-from-Jekyll-to-Hexo-Part-2/","excerpt":"Specific issues we ran into during the migration from Jekyll to Hexo","raw":"---\nlayout: post\ntitle: 'Migrating from Jekyll to Hexo: Part 2'\ncategories:\n  - jekyll\n  - hexo\ndate: 2015-12-27 19:43:32\ntags:\nauthorId: kyle_baley\nexcerpt: Specific issues we ran into during the migration from Jekyll to Hexo\n---\n\nIn my [previous post](http://www.westerndevs.com/jekyll/hexo/Migrating-from-Jekyll-to-Hexo/), I gave some general impressions from the recent conversion of WesternDevs from Jekyll to Hexo. Here, I'll outline specific issues we tackled during the process of migrating and converting to our own theme.\n\n### Excerpts\n\nJekyll and Hexo handle excerpts very differently. In Jekyll, you can specify an excerpt in the front matter. If one isn't provided, it will use the first few characters of the post itself as the excerpt.\n\nIn Hexo, the excerpt is set in one and only one way. You need to add `<!--more-->` somewhere in the post. Everything before that is considered the excerpt. If you don't have that tag in your post, no excerpt.\n\n<div style=\"float: right; width: 250px; border: 1px solid #ddd; background-color: #eee; padding: 8px; font-size:12px; margin: 8px;\">One minor annoyance with the default handling of the excerpt in Hexo is that it escapes HTML. So if you have a link in your excerpt, it shows as &lt;a href='moo.com'&gt;My web page&lt;/a&gt; in the excerpt. Our current solution is to make sure these posts have an explicit excerpt in the front-matter.</div>\n\nLuckily, [Amir](http://www.westerndevs.com/bios/amir_barylko/) had some foresight and added code to our Jekyll site to support `<!--more-->`. Unluckily, we rarely used it. Some of our posts had excerpts, particularly the podcasts. But mostly, we relied on Jekyll to parse the post for the excerpt.\n\nIn the end, we did two things to remedy this. First we added the [https://github.com/lalunamel/hexo-front-matter-excerpt](https://github.com/lalunamel/hexo-front-matter-excerpt) package. This fixed all the posts that already had an `excerpt` in the front matter. For the rest, we went through all the remaining posts and added `<!--more-->` after the first paragraph. Ever the optimists, we also submitted a [pull request](https://github.com/lalunamel/hexo-front-matter-excerpt/pull/2) for the package to make it behave more like Jekyll.\n\n### Syntax highlighting\n\nA code block in Jekyll looks like this\n\n```\n{% highlight html%}\n<div>moo</div>\n{% endhighlight %}\n```\n\nAnd in Hexo, like this:\n\n```\n{% codeblock lang:html %}\n<div>moo</div>\n{% endcodeblock %}\n```\n\nA series of \"Find and replace in files\" actions took care of this. We also found a `syntax.styl` file to our liking. Not sure if Jekyll has built-in support for syntax highlighting. I suspect not and that it was automatically included with the [theme we chose for Jekyll](https://github.com/mmistakes/minimal-mistakes).\n\n### CSS styles\n\nJekyll uses Kramdown for parsing Markdown. One of the features it adds is the ability to specify a class on your elements like so.\n\n```\n{: .pull-right }\n![My image](http://my/image/reference)\n```\n\nThis will add the `pull-right` CSS class to the image and, as the name suggests, this will float the image on the right.\n\nAs far as I know, there's no Kramdown support in NodeJS, likely because Kramdown is a Ruby gem. So we had to modify each instance of this syntax manually. For images, we converted to the Hexo [img tag plugin](https://hexo.io/docs/tag-plugins.html) like so:\n\n```\n{% img pull-right \"http://my/image/reference\" \"My image\"}\n```\n\nThere are a few tag plugins in Hexo all ported from Octopress. Which likely means we could have used them in our Jekyll site but not much point in testing that out now...\n\nFor other instances of this kramdown-specific syntax that were applied to other elements, we just dropped down to native HTML in the Markdown:\n\n```\n<div class=\"notice\">This is a notice</div>\n```\n\n### Aliases\n\nWe changed the URL structure for our podcasts early on and to keep any links pointing to the existing ones valid, we used [the jekyll-redirect-from gem](https://github.com/jekyll/jekyll-redirect-from) to maintain the old link. We find a [suitable replacement](https://github.com/jekyll/jekyll-redirect-from) for it in Hexo. BUT that suitable replacement's official package on NPM doesn't support Hexo 3 at the time of writing so we had to reference it in our `package.json` file like so:\n\n```\n\"hexo-generator-alias\": \"git+https://github.com/hexojs/hexo-generator-alias.git\"\n```\n\nBear in mind that this is a static site. So both of these plugins, the Jekyll and the Hexo one, handle redirects by generating an HTML page at the alias location with a redirect embedded in it:\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"utf-8\">\n    <title>Redirecting...</title>\n    <link rel=\"canonical\" href=\"/podcasts/podcast-the-internet-of-things/\"><meta http-equiv=\"refresh\" content=\"0; url=/podcasts/podcast-the-internet-of-things/\">\n  </head>\n</html>\n```\n\nThis isn't quite ideal for SEO purposes but it is the recommended approach if you can't do a server-side redirect.\n\n### Empty categories\n\nThis issue was probably the source of most of our trouble. Our permalink URL is of the form `/:category/:title`. For example: `/docker/yet-another-docker-intro/`. The issue is that the vast majority of our posts are uncategorized. And Jekyll and Hexo each handle uncategorized posts very differently in their permalinks.\n\nIn the front matter for Hexo, you can define a default category which all posts will use if a category is not assigned. So if you set the default category to \"moo\", your permalink URL will be: `/moo/a-discussion-on-knockout/`.\n\nIn Jekyll, an uncategorized post appears at the root of the site. Like this: `http://www.westerndevs.com/Migrating-from-Jekyll-to-Hexo-Part-2`. But setting the default category to an empty string or to `/`, led to fully-qualified URLs that look like this: `//Migrating-from-Jekyll-to-Hexo-Part-2`. I believe this is a bug in the permalink generator. It's generating the permalink as /category/title and when there is no category, it's not going to treat it any differently.\n\nOur initial solution to this was to stick with a default category of `uncategorized` but to alias each post to the root so that existing URLs would still work but would redirect to the new one. Alas, we have Disqus comments to deal with and those are tied to a specific URL. We could have migrated them but we also would have had to contend with sucky looking URLs that have `uncategorized` in them.\n\nOur current solution: [fork Hexo](https://github.com/westerndevs/hexo). In our fork, we add some handling when generating the URL for a post so that if it starts with a //, we trim one of them. Not the most elegant solution but workable for now. And if that's not hacky enough for you, check out the solution for...\n\n### Permalinks/Disqus Comments\n\nAfter deployment, we discovered none of the existing Disqus comments were showing on our posts. The Disqus script was working because you could add new comments and it would show \"Also on Western Devs\" comments. Just no existing comments.\n\nThe culprit was, again, the permalink. Because of the leading //, Disqus thought the URL for our posts was (as an example): http://www.westerndevs.com//a-discussion-on-knockout. That double slash was enough to confuse it into not showing comments.\n\nOur interim solution is to include this in our post template:\n\n```\nvar disqus_url = '<%= page.permalink %>'.replace(\".com//\", \".com/\");\n```\n\nThat right there is some quality coding...\n\n### Feed/Sitemap/iTunes\n\nThis issue is more of a warning not to over-think things. Our feed is served up at [http://www.westerndevs.com/feed](http://www.westerndevs.com/feed). In Jekyll, we created a `feed.xml` file and assumed that it did some magic to generate a `feed` file. And we had a hell of a time trying to get Hexo to generate that same file without the .xml extension. Even to the point where we created our own package that still didn't quite work.\n\nThen someone realized that Jekyll was not actually generating a `feed` file, just a `feed.xml` file. Whatever GitHub Pages runs on allows you to access XML files without the extension. Jekyll's development server knows this. Hexo's doesn't. So when we run locally, we can't access our various XML files (for our RSS feed, our iTunes feed, and our sitemap) without specifying the extension. But on the deployed site, they work fine.\n\n### Links in Headers\n\nThere was a bug in the [default Markdown renderer](https://github.com/hexojs/hexo-renderer-marked) that caused rendering issues when you had links within headers. The bug has since been fixed but I'm including it here mostly to reiterate that you aren't locked into anything with Hexo. While we were converting, the bug was still active and all we did to fix it was to switch to [a different renderer](https://github.com/celsomiranda/hexo-renderer-markdown-it).\n\nOne philosophical point: One of the reasons for this bug is that the default renderer will automatically add bookmarks for every heading. This is a [design decision](https://github.com/hexojs/hexo-renderer-marked/issues/16) that I'm not convinced has a lot of value though it's not a position I'll defend strongly. The renderer we've switched to includes this as an option you have to explicitly turn on which I think is a better way to go.\n\n### Deployment\n\nSince GitHub Pages runs Jekyll, it's understandable that deployment with Jekyll is relatively easy. We did need to wrap it in a Rake task to accommodate working locally with a different `_config.yml` file than what is used in production but that wasn't hard to do.\n\nFor Hexo, our process is still mostly manual and has to be done locally rather than on our CI server (Travis). We generate the site, copy the static files to a folder, then check those files into the `gh-pages` branch. I imagine it [won't be hard](https://sazzer.github.io/blog/2015/05/04/Deploying-Hexo-to-Github-Pages-with-Travis/) to get our Travis process adapted to this but at present, we haven't got around to it yet. Just keep in mind, it won't be quite as easy as Jekyll.\n\n### So which one is \"better\"?\n\nI forgot to answer this in the last post. As I mentioned, we're enjoying Hexo and it's nice having everyone excited about blogging again. For the Western Devs as a group, Hexo is the better choice.\n\nBut personally, I like Jekyll a little better. It feels more polished and doesn't require you to fork the product to get what you want. Typing this up, I was looking for a way to disable line numbers for individual codeblocks but I've come up short. Plus there's a larger and more comprehensive community behind it. That said, I am _really_ enjoying the quick generation time in Hexo.\n\n---\nI believe that covers the major gotchas we encountered in our conversion. It glosses over a few things, like whether to stick with Stylus as the default CSS pre-processor or move to SASS. Or whether to use [Jade](http://jade-lang.com/) as the templating language rather than the default, EJS. Those questions are quite a bit more subjective and I wanted to keep this discussion limited to the technical hurdles we encountered.\n\nBut if you run into an issue not mentioned here, add a comment and I will update the post.\n","categories":[{"name":"jekyll","slug":"jekyll","permalink":"https://westerndevs.com/categories/jekyll/"},{"name":"hexo","slug":"jekyll/hexo","permalink":"https://westerndevs.com/categories/jekyll/hexo/"}],"tags":[]},{"title":"Merry Christmas!","slug":"Merry-Christmas","date":"2015-12-24 14:59:21+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Merry-Christmas/","link":"","permalink":"https://westerndevs.com/_/Merry-Christmas/","excerpt":"Happy holidays from the Western Devs","raw":"---\nlayout: post\ntitle: \"Merry Christmas!\"\ndate: 2015-12-24 09:59:21\nexcerpt: Happy holidays from the Western Devs\ncomments: true\n---\n\nIt's been a banner year for the [Western Devs](http://www.jibjab.com/view/gVZz5V5MSnuo1O4-Betp7w) as we emerged from the shadows of a private message board to the spotlight of [public life](http://www.jibjab.com/view/jmlenwe2RN6b-JePc97m4Q). So from all of us, we wish everyone a very Merry Christmas, [Feliz Navidad](http://www.jibjab.com/view/1Py9A_hTTLSZGMTEmEAZuA), and a very happy holiday season.\n","categories":[],"tags":[]},{"title":"Migrating from Jekyll to Hexo","authorId":"kyle_baley","slug":"Migrating-from-Jekyll-to-Hexo","date":"2015-12-22 19:00:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"jekyll/hexo/Migrating-from-Jekyll-to-Hexo/","link":"","permalink":"https://westerndevs.com/jekyll/hexo/Migrating-from-Jekyll-to-Hexo/","excerpt":"The Western Devs website has a sporty new look and a shiny new technology behind it. In this post, we'll look at the good and bad with migrating from Jekyll to Hexo","raw":"---\nlayout: post\ntitle: \"Migrating from Jekyll to Hexo\"\ndate: 2015-12-22 14:00:42\ncategories: \n    - jekyll\n    - hexo\nexcerpt: The Western Devs website has a sporty new look and a shiny new technology behind it. In this post, we'll look at the good and bad with migrating from Jekyll to Hexo\ncomments: true\nauthorId: kyle_baley\n---\n\nWesternDevs has a shiny new look thanks to graphic designer extraodinaire, [Karen Chudobiak](http://www.karenchudobiak.ca/). When implementing the design, we also decided to switch from Jekyll to Hexo. Besides having the opportunity to learn NodeJS, the other main reason was Windows. Most of us use it as our primary machine and Jekyll doesn't officially support it. There are [instructions](http://jekyll-windows.juthilo.com/) available by people who were obviously more successful at it than we were. And there are even [simpler ones](https://davidburela.wordpress.com/2015/11/28/easily-install-jekyll-on-windows-with-3-command-prompt-entries-and-chocolatey/) that I discovered during the course of writing this post and that I wish existed three months ago.\n\n{% img \"pull-right\" \"https://avatars2.githubusercontent.com/u/6375567?v=3&s=200\" %}\n\nRegardless, here we are and it's already been a positive move overall, not least because the move to Node means more of us are available to help with the maintenance of the site. But it wasn't without it's challenges. So I'm going to outline the major ones we faced here in the hopes that it will help you make your decision more informed than ours was.\n\nTo preface this, note that I'm new to Node and in fact, this is my first real project with it. That said, I'm no expert in Ruby either, which is what Jekyll is written in. And the short version of my first impressions is: Jekyll feels more like a real product but I had an easier time customizing Hexo once I dug into it. Here's the longer version\n\n### Documentation/Resources\n\nYou'll run into this very quickly. Documentation for Hexo is decent but incomplete. And once you start Googling, you'll discover many of the resources are in Chinese. I found very quickly that there is `posts` collection and that each post has a `categories` collection. But as to what these objects look like, I couldn't tell. They aren't arrays. And you can't `JSON.stringify` them because they have circular references in them. `util.inspect` works but it's not available everywhere.\n\n### Multi-author support\n\nBy default, Hexo doesn't support multiple authors. Neither does Jekyll, mind you, but we found a [pretty complete](https://github.com/mmistakes/minimal-mistakes) theme that does. In Hexo, there's a [decent package](https://www.npmjs.com/package/hexo-multiauthor) that gets you partway there. It lets you specify an author ID on a post and it will attach a bunch of information to it. But you can't, for example, get a full list of authors to list on a Who We Are page. So we created a separate data file for the authors. But we also haven't figured out how to use that file to generate a .json file to use for the Featured section on the home page. So at the moment, we have author information in three places. Our temporary solution is to disallow anyone from joining or leaving Western Devs.\n\n### Customization\n\nIf you go with Hexo and choose an [existing themes](https://hexo.io/themes/), you won't run into the same issues we did. Out of the box, it has good support for posts, categories, pagination, even things like tags and aliases with the [right plugins](https://hexo.io/plugins/).\n\nBut we started from a design *and* were migrating from an existing site with existing URLs and had to make it work. I've mentioned the challenge of multiple authors already. Another one: maintaining our URLs. Most of our posts aren't categorized. In Jekyll, that means they show up at the root of the site. In Hexo, that's not possible. At least at the moment and I suspect this is a bug. We eventually had to fork Hexo itself to maintain our existing URLs.\n\nAnother challenge: excerpts. In Jekyll, excerpts work like this: Check the front matter for an `excerpt`. If one doesn't exist, take the first few characters from the post. In Hexo, excerpts are empty by default. If you add a `<!--more-->` tag in your post, everything before that is considered an excerpt. If you specify an `excerpt` in your front matter, it's ignored because there is already an `excerpt` property on your posts.\n\nLuckily, there's a [plugin](https://github.com/lalunamel/hexo-front-matter-excerpt) to address the last point. But it still didn't address the issue of all our posts without an excerpt where we relied solely on the contents of the post.\n\nSo if you're looking to veer from the scripted path, be prepared. More on this later in the \"good parts\" section.\n\n### Overall feeling of rawness\n\nThis is more a culmination of the previous issues. It just feels like Hexo is a work-in-progress whereas Jekyll feels more like a finished product. There's a strong community behind Jekyll and plenty of help. Hexo still has bugs that suggest it's just not used much in the wild. Like [rendering headers with links in them](https://github.com/hexojs/hexo-renderer-marked/issues/16). It makes the learning process a bit challenging because with Jekyll, if something didn't work, I'd think _I'm obviously doing something wrong_. With Hexo, it's _I might be doing something wrong or there might be a bug_.\n\n---\n\n### The good parts\n\nI said earlier that the move to Hexo was positive overall and not just because I'm optimistic by nature. There are two key benefits we've gained just in the last two weeks.\n\n#### Generation time\n\nHexo is fast, plain and simple. Our old Jekyll site took six seconds to generate. Doesn't sound like much but when you're working on a feature or tweaking a post, then saving, then refreshing, then rinsing, then repeating, that six seconds adds up fast. In Hexo, a full site generation takes three seconds. But more importantly, it is smart enough to do incremental updates while you're working on it. So if you run `hexo server`, then see a mistake in your post, you can save it, and the change will be reflected almost instantly. In fact, it's usually done by the time I've switched back to the browser.\n\n#### Contributors\n\nWe had logistical challenges with Jekyll. To the point where we had two methods for Windows users that wanted to contribute (i.e. add a post). One involved a [Docker image](http://www.westerndevs.com/docker-and-western-devs/) and the other [Azure ARM](http://www.westerndevs.com/using-azure-arm-to-deploy-a-docker-container/). Neither was ideal as they took between seconds and minutes to refresh if you made changes. Granted, both methods furthered our collective knowledge in both Docker and Azure but they both kinda sucked for productivity.\n\nThat meant that realistically, only the Mac users really contributed to the maintenance of the site. And our Docker/Azure ARM processes were largely ignored as we would generally just test in production. I.e. create a post, check it in, wait for the site to deploy, make necessary changes, etc, etc.\n\nWith the switch to Hexo, we've had no fewer than five contributors to the site's maintenance already. Hexo just works on Windows. And on Mac. Best of both worlds.\n\n#### Customization\n\nThis is listed under the challenges but ever the optimist, I'm including it here as well. We've had to make some customizations for our site, including [forking Hexo](https://github.com/westerndevs/hexo) itself. And for me personally, once I got past the _why isn't this working the way I want?_ stage, it's been a ton of fun. It's crazy simple to muck around in the node modules to try stuff out. And just as simple to fork something and reference it in your project when the need arises. I mentioned an earlier issue rendering links in headers. No problem, we just swapped out the markdown renderer for [another one](https://github.com/celsomiranda/hexo-renderer-markdown-it). And if that doesn't work, we'll tweak something until it does.\n\n---\nI want to talk more on specific conversion issues we ran into as a guide to those following in our footsteps. But there are enough of them to warrant a follow up post without all this pre-amble. For now, we're all feeling the love for Hexo. So much so that no less than three other Western Devs are in the process of converting their personal blogs to it.","categories":[{"name":"jekyll","slug":"jekyll","permalink":"https://westerndevs.com/categories/jekyll/"},{"name":"hexo","slug":"jekyll/hexo","permalink":"https://westerndevs.com/categories/jekyll/hexo/"}],"tags":[]},{"title":"SQL Server Aliases","authorId":"simon_timms","slug":"sql-server-aliases","date":"2015-12-17 07:06:57+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/sql-server-aliases/","link":"","permalink":"https://westerndevs.com/_/sql-server-aliases/","excerpt":"Ever run into that problem where everybody on your team is using a different database instance name and every time you check out you have to update the config file with your instance name?","raw":"---\nlayout: post\ntitle:  SQL Server Aliases\ndate: 2015-12-16T19:06:57-07:00\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/12/17/sql-server-alias/\n---\n\nEver run into that problem where everybody on your team is using a different database instance name and every time you check out you have to update the config file with your instance name?\n\n<!--more-->\n\nBoy have I seen some complicated solutions around this involving reading from environments and having private, unversioned configuration files. One of the developers on my team recommended using SQL Server Aliases to solve the problem. I fumbled around with these for a bit because I couldn't get them to work. Eventually, with some help, I got there.\n\nLet's say that you have an instance on your machine called `sqlexpress` but that your project needs an instance called `sqlexpress2012`. The first thing is to open up the SQL Server Configuration Manager. The easiest way to do this is to run\n\n`SQLServerManager13.msc` \n\nwhere the 13 is the version number of SQL server so SQL 2014 is 12 and SQL 2016 is 13. That will give you \n\n![SQL Server Configuration Manager](http://i.imgur.com/XEwZzzl.png)\n\nThe first thing to check is that your existing instance is talking over TCP/IP. \n\n![Enable TCP/IP](http://i.imgur.com/eQRs5I5.png)\n\nThen click on the properties for TCP/IPO and in the IP Addresses tab check for the TCP Dynamic Ports setting\n\n![Dynamic ports](http://i.imgur.com/qQ4vy1y.png)\n\nMake note of that number because now we're going to jump to the alias section. \n\n![Aliases](http://i.imgur.com/pRyXu6D.png)\n\nRight click in there and add a new alias\n\n![](http://i.imgur.com/wQeGUDU.png)\n\nIn here we are going to set the alias name to the new name we want to use. The port number is what we found above, the protocol is TCP/IP and the server is the existing server name. You then have to repeat this for the 64 bit client configuration and then restart your SQL server. You should now be able to use the new name, `localhost\\sqlexpress2012` to access the server.","categories":[],"tags":[]},{"title":"Updating Sub-Collections With SQL Server's Merge","authorId":"simon_timms","slug":"updating-sub-collections-with-sql-servers-merge","date":"2015-12-16 09:52:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/updating-sub-collections-with-sql-servers-merge/","link":"","permalink":"https://westerndevs.com/_/updating-sub-collections-with-sql-servers-merge/","excerpt":"When you get to be as old as me then you start to see certain problems reappearing over and over again. I think this might be called \"experience\" but it could also be called \"not getting new experiences\".","raw":"---\nlayout: post\ntitle:  \"Updating Sub-Collections With SQL Server's Merge\"\ndate: 2015-12-15T21:52:50-07:00\ncomments: true\nexcerpt: When you get to be as old as me then you start to see certain problems reappearing over and over again. I think this might be called \"experience\" but it could also be called \"not getting new experiences\".\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/12/16/updating-sub-collections-with-sql-servers-merge/\n---\n\nWhen you get to be as old as me then you start to see certain problems reappearing over and over again. I think this might be called \"experience\" but it could also be called \"not getting new experiences\". It might be that instead of 10 years experience I have the same year of experience 10 times. Of course this is only true if you don't experiment and find new ways of doing things. Even if you're doing the same job year in and year out it is how you approach the work that determines how you will grow as a developer.\n\nOne of those problems I have encountered over the years is the problem of updating a collection of records related to one record. I'm sure you've encountered the same thing where you present the user with a table and let them edit, delete and add records.\n\n![A collection of rows](http://i.imgur.com/QCYisPG.png)\n\nNow how do you get that data back to the server? You could send each row back individually using some Ajax magic. This is kind of a pain, though, you have to keep track of a lot of requests and you're making a bunch of requests. You also need to track, behind the scenes, which rows were added and which were removed so you can send specific commands for that. It is preferable to send the whole collection at once in a single request. Now you've shifted the burden to the server. In the past I've handled this by pulling the existing collection from the database and doing painful comparisons to figure out what has changed. \n\nThere is a very useful SQL command called UPSERT which you'll find in databases such as Postgres(assuming you're on the cutting edge and you're using 9.5). Upsert is basically a command which looks at the existing table data when you modify a record. If the record doesn't exist it will be created and if it is already there the contents will be updated. This solves 2/3rds of our cases with only delete missing. Unfortunately, SQL Server doesn't support the UPSERT command - however it does support MERGE. \n\nI've always avoided MERGE because I thought it to be very complicated but in the interests of continually growing I figured it was about time that I bit the bullet and just learned how it works. I use Dapper a fair bit for talking to the database, it is just enough ORM to handle the dumb stuff while still letting me write my own SQL. It is virtually guaranteed that I write worse SQL than a full ORM but that's a cognitive dissonance I'm prepared to let ride. By writing my own SQL I have direct access to tools like merge which might, otherwise, be missed by a beefy ORM. \n\nThe first thing to know about MERGE is that it needs to run against two tables to compare their contents. Let's extend the example we have above of what appears to be a magic wand shop... that's weird and totally not related to having just watched the trailer for [Fantastic Beasts and Where to Find Them](https://www.youtube.com/watch?v=Wj1devH5JP4). Anyway our order item table looks like \n\n{% codeblock lang:sql %}\ncreate table orderItems(id uniqueidentifier,\n              orderId uniqueidentifier,\n              colorId uniqueidentifier,\n              quantity int)\n{% endcodeblock %}\n\nSo the first task is to create a temporary table to hold our records. By prefacing a table name with a `#` in SQL server we get a temporary table which is unique to our session. So other running transactions won't see the table - exactly what we want.\n\n{% codeblock lang:sql %}\nusing(var connection = GetConnection())\n{\n   connection.Execute(@\"create table #orderItems(id uniqueidentifier,\n               orderId uniqueidentifier,\n               colorId uniqueidentifier,\n               quantity int)\");\n}\n{% endcodeblock %}\nNow we'll take the items collection we have received from the web client (in my case it was via an MVC controller but I'll leave the specifics up to you) and insert each record into the new table. Remember to do this using the same session as you used to create the table. \n\n{% codeblock lang:csharp %}\nforeach(var item in orderItems)\n{\n    connection.Execute(@\"insert into #orderItems(id, \n\t\t\torderId, \n\t\t\tcolorId, \n\t\t\tquantity) \n\t\tvalues(@id, \n\t\t\t@orderId, \n\t\t\t@colorId, \n\t\t\t@quantity)\", item);\n}\n{% endcodeblock %}\n\nNow the fun part: writing the merge. \n\n{% codeblock lang:sql %}\nmerge orderItems as target\n      using #orderItems as source\n      on target.Id = source.Id \n      when matched then\n           update set target.colorId = source.colorId, \n                  target.quantity = soruce.quantity\n      when not matched by target then \n\t  insert (id, \n      \t\t  orderId, \n              colorId, \n              quantity) \n     values (source.id, \n     \t\t source.orderId, \n             source.colorId, \n             source.quantity)\n     when not matched by source \n      and orderId = @orderId then delete;\n{% endcodeblock %}\n\nWhat's this doing? Let's break it down. First we set a target table this is where the records will be inserted, deleted and updated. Next we set the source the place from which the records will come. In our case the temporary table. Both `source` and `destination` are aliases so really they can be whatever you want like `input` and `output` or `Expecto` and `Patronum`.\n\n{% codeblock lang:sql %}\nmerge orderItems as target\n      using #orderItems as source\n{% endcodeblock %}\n\nThis line instructs on how to match. Both our tables have primary ids in a single column so we'll use that.\n\n{% codeblock lang:sql %}\non target.Id = source.Id \n{% endcodeblock %}\n\nIf a record is matched the we'll update the two important target fields with the values from the source.\n\n{% codeblock lang:sql %}\nwhen matched then\n           update set target.colorId = source.colorId, \n                  target.quantity = soruce.quantity\n{% endcodeblock %}\n\nNext we give instructions as to what should happen if a record is missing in the target. Here we insert a record based on the temporary table.\n\n{% codeblock lang:sql %}\nwhen not matched by target then \n\t  insert (id, \n      \t\t  orderId, \n              colorId, \n              quantity) \n     values (source.id, \n     \t\t source.orderId, \n             source.colorId, \n             source.quantity)\n{% endcodeblock %}\n\nFinally we give the instruction for what to do if the record is in the target but not in the source - we delete it. \n\n{% codeblock lang:sql %}\nwhen not matched by source \n     and orderId = @orderId then delete;\n{% endcodeblock %}\n\nIn another world we might do a soft delete and simply update a field.\n\nThat's pretty much all there is to it. MERGE has a ton of options to do more powerful operations. There is a bunch of super poorly written documentation on this on [MSDN](https://msdn.microsoft.com/en-us/library/bb510625.aspx?f=255&MSPPError=-2147217396) if you're looking to learn a lot more.\n\n","categories":[],"tags":[]},{"title":"Barriers for Women in Technology","slug":"podcast-barriers-for-women-in-technology","date":"2015-12-14 23:12:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-barriers-for-women-in-technology/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-barriers-for-women-in-technology/","excerpt":"Special guest Rachel Thomas and the Western Devs discuss the barriers facing women when they enter and when they stay in technology","raw":"---\nlayout: podcast\ntitle:  \"Barriers for Women in Technology\"\ndate: 2015-12-14T13:12:40-05:00\nrecorded: 2015-12-11\ncategories: podcasts\nexcerpt: \"Special guest Rachel Thomas and the Western Devs discuss the barriers facing women when they enter and when they stay in technology\"\ncomments: true\npodcast:\n    filename: \"BarriersForWomenInTechnology.mp3\"\n    length: \"46:38\"\n    filesize: 44773146\n    libsynId: 4016335\n    anchorFmId: Barriers-for-Women-in-Technology-evqdhn\nparticipants:\n    - kyle_baley\n    - dylan_smith\n    - lori_lalonde\n    - dave_white\n    - james_chambers\n    - wendy_closson\n    - donald_belcham\n    - jason_row\n    - peter_ritchie\nlinks:\n    - Rachel Thomas (Twitter)|https://twitter.com/math_rachel\n    - Rachel Thomas (Medium)|https://medium.com/@racheltho\n    - It's not just a pipeline problem|https://medium.com/@racheltho/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996\n    - Hackbright Academy|https://hackbrightacademy.com/\n    - \"HBR: Women are over-mentored but under-sponsored|https://hbr.org/2010/08/women-are-over-mentored-but-un/\"\n    - \"Women in Leadership: Advice for Current and Future Leaders|https://www.linkedin.com/pulse/women-leadership-advice-current-future-leaders-cindy-fornelli\"\n    - Developing Women Leaders|https://www.linkedin.com/pulse/20130101170009-60894986-developing-women-leaders-five-essentials\n    - To get girls more interested in computer science, make classrooms less 'geeky'|http://www.washington.edu/news/2015/08/24/to-get-girls-more-interested-in-computer-science-make-classrooms-less-geeky/\nmusic:\n    - title: Doctor Man\n      artist: Johnnie Christie and the Boats\n      url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* Stats on women in tech\n* The problem with focusing solely on the pipeline\n* When teachers feed the stereotype\n* Gender bias vs. general obtuseness\n* When bias happens behind closed doors\n* Numbers and statistics vs. anecdotal evidence\n* Building trust and being observant\n* The power of stories in our youth\n* Overlap with ageism\n* The looming skills shortage\n* Benefit of a balanced workforce rather than a male-oriented work environment\n* Encouraging women to start and lead companies\n* Recognizing social differences\n* Stop asking for ninjas, gurus, and rock stars already!\n* \"We have a flexible work environment\"...do you really?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Continuous Integration With Xamarin.iOS, Visual Studio Team Services, and MacinCloud: Part 1","authorId":"lori_lalonde","slug":"continuous-integration-with-xamarin-ios-visual-studio-team-services-and-macincloud-part-1","date":"2015-12-11 01:38:11+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/continuous-integration-with-xamarin-ios-visual-studio-team-services-and-macincloud-part-1/","link":"","permalink":"https://westerndevs.com/_/continuous-integration-with-xamarin-ios-visual-studio-team-services-and-macincloud-part-1/","excerpt":"Recently, Microsoft and MacinCloud announced a partnership in which they have enabled Visual Studio Team Services (VSTS) to support continuous integration (CI) builds for Xamarin.iOS and XCode projects using a Mac build agent in the cloud.","raw":"---\nlayout: post\ntitle:  \"Continuous Integration With Xamarin.iOS, Visual Studio Team Services, and MacinCloud: Part 1\"\ndate: 2015-12-10T15:38:11-05:00\ncomments: true\nexcerpt: Recently, Microsoft and MacinCloud announced a partnership in which they have enabled Visual Studio Team Services (VSTS) to support continuous integration (CI) builds for Xamarin.iOS and XCode projects using a Mac build agent in the cloud.\nauthorId: lori_lalonde\noriginalurl: http://solola.ca/xamarin-macincloud-vsts-part1/\n---\n\nRecently, Microsoft and MacinCloud announced a partnership in which they have enabled Visual Studio Team Services (VSTS) to support continuous integration (CI) builds for Xamarin.iOS and XCode projects using&nbsp;a Mac build agent&nbsp;in the cloud. This is great news as more companies are looking to move&nbsp;towards cloud-hosted solutions for their build and deployment pipeline.\n\n<!--more-->\n\nAlthough MacinCloud indicates that the [VSTS Agent Plan setup][1] only takes a few minutes, the process is not fully automated and requires some manual steps in order to get your CI Builds working as expected. The initial setup is fairly quick, but your Xamarin.iOS CI builds will fail until you install a [Xamarin ][2]license on the MacinCloud CI Build Server. The catch? Unlike the other MacinCloud plans, the CI Build Agent Plan does not allow you to directly interact with the Build Server. Instead, you are required to contact both MacinCloud Support and Xamarin Support to complete the process. Set your expectations that it may take anywhere from&nbsp;2 â€“ 4 days before you can start using the CI Build Agent.\n\nLet's take a look at what is involved, from start to finish, to successfully configure a CI Build&nbsp;Agent for your Xamarin.iOS projects.\n\n### Step 1: Register for MacinCloud plan\n\nIn order to integrate&nbsp;Macincloud with Visual Studio Team Services, you need to register for a [CI Build Agent plan][3] at [MacinCloud.com][4]. The CI Build Agent cost is $29 US per month per build agent.\n\n### Step 2: Create a new VSTS Agent\n\nOnce you are registered, you must setup your build agent through the Dashboard, as shown below.\n\n![][5]\n\n_MacinCloud Dashboard_\n\nSelect the Edit button to configure your MacinCloud VSTS Agent.\n\n### Step 3: Associate the MacinCloud Build Agent to your VSTS Account\n\nIn the Edit VSTS Agent dialog, enter a unique agent name and a pool name. Note that the pool name must match the name of an Agent Pool in Visual Studio Team Services.\n\n![][6]\n\n_MacinCloud Edit VSTS Agent Dialog_\n\nEnter your&nbsp;VSTS URL to associate this build agent with your Team Services account.\n\n### Step 4: Generate a Microsoft Access Token in VSTS for the Build Agent\n\nNext, you will need to provide&nbsp;the&nbsp;MacinCloud Build Agent access to your VSTS account, which requires that you&nbsp;generate a Microsoft Access Token from your VSTS profile.\n\nTo do this, open a new browser tab, log into your VSTS account and navigate to the security tab for your account profile. Alternatively you can click the \"?\" button situated to the right of&nbsp;the Microsoft Access Token entry field in the Edit VSTS Agent dialog, which will launch the \"Create a personal access token\" page in your VSTS Dashboard. Provide a&nbsp;Description, and then select an Expiry term&nbsp;as well as&nbsp;the desired&nbsp;VSTS Account. Ensure **All scopes** is selected for Authorized Scopes then click the _Create Token_ button.\n\n![][7]\n\n_VSTS Dashboard â€“ Create a personal access token_\n\n### Step 5: Enter&nbsp;the&nbsp;Microsoft Access Token in the&nbsp;MacinCloud Edit VSTS Agent dialog\n\nCopy the generated access token to the clipboard and paste it in the MacinCloud Microsoft Access Token field in the Edit VSTS Agent dialog.\n\n**Important Note:** Be sure to save a copy of the generated access token in a safe location. You will not be able to retrieve the generated access token again once you navigate away from the Security page after it has been created.\n\nIf you wish, you can add your signing&nbsp;certificate&nbsp;and provisioning profile at this time, but it's not required in the initial setup.\n\nClick _Save_ to create your MacinCloud CI Build&nbsp;Agent.\n\n### Step 6: Confirm that the MacinCloud Agent appears in the&nbsp;VSTS Agent Pool\n\nNavigate to the Admin section within your VSTS Dashboard and select the Agent Pools tab. Select the Agent Pool that matches the Pool Name you entered in the&nbsp;MacinCloud&nbsp;Edit&nbsp;VSTS Agent dialog. If everything is configured properly, then the MacinCloud agent will appear in the Agents list, as shown below.\n\n![][8]\n\n_MacinCloud VSTS Agent associated&nbsp;with Default Agent Pool in VSTS_\n\n### Step 7: Install Xamarin License on the Mac&nbsp;Server&nbsp;\n\nLast but not least, you will need to install a Xamarin license on the Mac server where your CI Build Agent is hosted. Unfortunately, MacinCloud does not provide you with interactive access to the server. This requires&nbsp;some back and forth correspondence with MacinCloud Support and Xamarin Support in order to finish the setup process.\n\n### Step 7a: Request System Profile Information from MacinCloud Support\n\nFirst, you will need to submit a support request to MacinCloud to obtain the server information and system file that Xamarin will require in order to generate a license file for your CI Build Server.\n\nSend an email to [support@macincloud.com][9] with the following request:\n\n\"I am setting up a CI Build Agent to configure continuous integration&nbsp;builds in&nbsp;Visual Studio Team Services for Xamarin.iOS applications. Please send me the system profile of my CI Build Agent, which I will need to forward to Xamarin support so they can generate a License file. To retrieve the system profile, perform the following steps:\n\nâ€“ Go to: Macintosh HD -&gt; Applications -&gt; Utilities -&gt; System Information.app\n\nâ€“ Choose File &gt; Save\n\nâ€“ Select a location to save the file, such as the desktop, or Documents folder.\n\nâ€“ Name and save the file.\n\nâ€“ Please email the file to me when this is completed.\"\n\nThis initial email will generate a support ticket within the [MacinCloud Support Portal][10]. You will receive an email containing a link to the support request so you can view its status at any time.\n\nNote that response time from MacinCloud Support will vary, with the expectation that it can take anywhere from a couple of hours to 1 full business day to receive a response depending on the time of day that you submit the request.\n\nThe generated system profile information file will simply be an xml file with a _.spx_ extension.\n\n### Step 7b: Request a license from Xamarin Support\n\nOnce you receive the system profile information file from MacinCloud Support, you will need to send it to Xamarin Support. Log into your [Xamarin account][11], go to your Dashboard and select [Xamarin.iOS][12] to locate the support email address for your subscription.\n\nSend an email to the Xamarin Support using the specified email address, requesting a _**License.V2**_ file to be generated for your MacinCloud CI Build Agent. Be sure to attach the _spx_ file that you received from MacinCloud to this request.\n\nXamarin Support will provide you with the license file and instructions on where the file should be placed on the server. The Xamarin.iOS license file must be copied to the ~/Library/MonoTouch folder on the build server.\n\nI found the turnaround time on this request to be relatively quick.\n\n### Step 7c: Forward the license to MacinCloud Support\n\nForward the license file and relevant instructions to MacinCloud Support, and wait until they send a confirmation that the license has been installed on the server.\n\n**_To Be Continued..._**\n\nNow that you have the MacinCloud CI Build Agent properly&nbsp;configured, you will be able to setup&nbsp;continuous integration builds for your Xamarin.iOS projects! In the next post, we will walk through the necessary steps to create a Xamarin.iOS build definition in Visual Studio Team Services.\n\n&nbsp;\n\n&nbsp;\n\n[1]: http://www.macincloud.com/pricing/build-agent-plans/vso-build-agent-plan\n[2]: http://xamarin.com\n[3]: http://www.macincloud.com/pricing/compare\n[4]: http://www.macincloud.com/\n[5]: http://solola.ca/wp-content/uploads/2015/12/1-MIC_Dashboard-1024x219.png\n[6]: http://solola.ca/wp-content/uploads/2015/12/2-MIC_EditVSTSAgent-1024x931.png\n[7]: http://solola.ca/wp-content/uploads/2015/12/3-VSTS_Security_AccessToken-1024x617.png\n[8]: http://solola.ca/wp-content/uploads/2015/12/4-VSTS_AgentPools-1024x368.png\n[9]: mailto:support@macincloud.com\n[10]: http://support.macincloud.com/support/home\n[11]: https://xamarin.com/account/login\n[12]: https://store.xamarin.com/account/my/subscription?product=Xamarin.iOS\n  ","categories":[],"tags":[]},{"title":"The Future of Computing","slug":"podcast-the-future-of-computing","date":"2015-12-07 23:21:04+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-the-future-of-computing/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-the-future-of-computing/","excerpt":"The Western Devs pontificate on the future of computing","raw":"---\nlayout: podcast\ntitle:  \"The Future of Computing\"\ndate: 2015-12-07T13:21:04-05:00\nrecorded: 2015-12-04\ncategories: podcasts\nexcerpt: \"The Western Devs pontificate on the future of computing\"\ncomments: true\npodcast:\n    filename: \"TheFutureOfComputing.mp3\"\n    length: \"50:01\"\n    filesize: 48022352\n    libsynId: 4001183\n    anchorFmId: The-Future-of-Computing-evqdi4\nparticipants:\n    - kyle_baley\n    - dylan_smith\n    - lori_lalonde\n    - dave_white\n    - amir_barylko\n    - tom_opgenorth\n    - david_wesst\nlinks:\n    - Raspberry Pi|https://www.raspberrypi.org/\n    - Arduino|https://www.arduino.cc/\n    - Instructables|http://www.instructables.com/\n    - Tesla's robot taxis|http://www.cheatsheet.com/automobiles/will-tesla-be-the-first-car-company-to-launch-a-fleet-of-robot-taxis.html/?a=viewall\n    - Automonous tractor|https://www.youtube.com/watch?v=Ybxhvlyw-X0\n    - HIPAA|https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act\nmusic:\n    - title: Doctor Man\n      artist: Johnnie Christie and the Boats\n      url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* Desktops => Internet => Mobile => ???\n* The players: Internet of Things, Wearables/Nearables, Arduino/Raspberry Pi, 3D printing, Virtual Reality/Augmented Reality\n* The power of low cost of entry\n* Revolution or evolution?\n* The data's the thing\n* \"What do you mean I can't turn on my washing machine from my phone?\"\n* The widening technology gap\n* Job opportunities\n* Self-driving tractors (and cars too, I guess, if you're into that sort of thing)\n* When computers make ethical decisions\n* Cars as a service\n* What skills will you need?\n* Bringing a product from prototype to market\n* Building a culture of problem solving\n* Education for the future\n* The dark side: security, privacy, data ownership, and utility companies\n* How much is your privacy worth to you\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Code - The Visual Studio for Everybody","authorId":"david_wesst","slug":"vscode-the-visual-studio-for-everybody","date":"2015-12-07 17:13:06+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/vscode-the-visual-studio-for-everybody/","link":"","permalink":"https://westerndevs.com/_/vscode-the-visual-studio-for-everybody/","excerpt":"Last week I delivered three presentations: one at the Winnipeg .NET User Group and the other two at Winnipeg Code Camp. Being as awesome as we are, the user group presentation was recorded and has been published on the YouTube page.","raw":"---\nlayout: post\ntitle: \"Code - The Visual Studio for Everybody\"\ncategories:\ncomments: true\nauthorId: david_wesst\ndate: 2015-12-07 12:13:06\noriginalurl: http://blog.davidwesst.com/2015/12/Code-The-Visual-Studio-for-Everybody/\n---\n\nLast week I delivered three presentations: one at the [Winnipeg .NET User Group](http://winnipegdotnet.org/) and the other two at [Winnipeg Code Camp](http://winnipegcodecamp.com/). Being as awesome as we are, the user group presentation was recorded and has been published on the YouTube page.\n\n<!--more-->\n\nThe presentation gives you an overview of the open source, cross-platform, code editor and covers:\n\n+ The Editor Itself\n+ Debugging Experience\n+ Tasks, including integrating with Java-based task runner Gradle\n+ Extensions, including writing your own\n\t\nFeedback is always appreciated, and hopefully I'll get to see you at the next event in the new year.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aUBF5RDlvKs\" frameborder=\"0\" allowfullscreen></iframe>\n\n---\nThanks for playing. ~ DW ","categories":[],"tags":[]},{"title":"Copy Azure Blobs","authorId":"simon_timms","slug":"copy-azure-blobs","date":"2015-12-04 11:01:28+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/copy-azure-blobs/","link":"","permalink":"https://westerndevs.com/_/copy-azure-blobs/","excerpt":"Ever wanted to copy blobs from one Azure blob container to another? Me neither, until now. I had a bunch of files I wanted to use as part of a demo in a storage container and they needed to be moved over to a new container in a new resource group. It was 10 at night and I just wanted it solved so I briefly looked for a tool to do the copying for me. I failed to find anything. Ugh, time to write some 10pm style code, that is to say terrible code. Now you too can benefit from this. I put in some comments for fun.","raw":"---\nlayout: post\ntitle:  \"Copy Azure Blobs\"\ndate: 2015-12-03T23:01:28-07:00\ncomments: true\nauthorId: simon_timms\n---\n\nEver wanted to copy blobs from one Azure blob container to another? Me neither, until now. I had a bunch of files I wanted to use as part of a demo in a storage container and they needed to be moved over to a new container in a new resource group. It was 10 at night and I just wanted it solved so I briefly looked for a tool to do the copying for me. I failed to find anything. Ugh, time to write some 10pm style code, that is to say terrible code. Now you too can benefit from this. I put in some comments for fun.\n\n<!--more-->\n\n{% codeblock lang:csharp %}\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace migrateblobs\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n\t\t\t//this is the source account\n            var sourceAccount = CloudStorageAccount.Parse(\"source connection string here\");\n            var sourceClient = sourceAccount.CreateCloudBlobClient();\n            var sourceContainer = sourceClient.GetContainerReference(\"source container here\");\n\n            //destination account\n            var destinationAccount = CloudStorageAccount.Parse(\"destination connection string here\");\n            var destinationClient = destinationAccount.CreateCloudBlobClient();\n            var destinationContainer = destinationClient.GetContainerReference(\"destination container here\");\n\t\t\t\n\t\t\t//create the container here\n            destinationContainer.CreateIfNotExists();\n\n            //this token is used so the destination client can pull from the source\n            string blobToken = sourceContainer.GetSharedAccessSignature(\n                       new SharedAccessBlobPolicy\n                       {\n                           SharedAccessExpiryTime =DateTime.UtcNow.AddYears(2),\n                           Permissions = SharedAccessBlobPermissions.Read | SharedAccessBlobPermissions.Write\n                       });\n\n\n            var srcBlobList = sourceContainer.ListBlobs(useFlatBlobListing: true, blobListingDetails: BlobListingDetails.None);\n            foreach (var src in srcBlobList)\n            {\n                var srcBlob = src as CloudBlob;\n\n                // Create appropriate destination blob type to match the source blob\n                CloudBlob destBlob;\n                if (srcBlob.Properties.BlobType == BlobType.BlockBlob)\n                {\n                    destBlob = destinationContainer.GetBlockBlobReference(srcBlob.Name);\n                }\n                else\n                {\n                    destBlob = destinationContainer.GetPageBlobReference(srcBlob.Name);\n                }\n\n                // copy using src blob as SAS\n                destBlob.StartCopyFromBlob(new Uri(srcBlob.Uri.AbsoluteUri + blobToken));\n            }\n        }\n    }\n}\n{% endcodeblock %}\n\nI stole some of this code from an old post [here](http://blogs.msdn.com/b/windowsazurestorage/archive/2012/06/12/introducing-asynchronous-cross-account-copy-blob.aspx) but the API has changed a bit since then so this article is a better reference. The copy operations take place asynchronously.\n\nWe're copying between containers without copying down the the local machine so you don't incur any egress costs unless you're moving between data centers. \n\nHave fun. ","categories":[],"tags":[]},{"title":"The Case of the Disappearing Database","authorId":"dave_paquette","slug":"case-of-the-disappearing-database","date":"2015-12-03 21:29:09+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/case-of-the-disappearing-database/","link":"","permalink":"https://westerndevs.com/_/case-of-the-disappearing-database/","excerpt":"Something scary happened last week. The database backing my blog disappeared from my Azure account.","raw":"---\nlayout: post\ntitle: \"The Case of the Disappearing Database\"\ndate: 2015-12-03 16:29:09\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/12/03/the-case-of-the-disappearing-database.aspx\n---\n\nSomething scary happened last week. The database backing my blog disappeared from my Azure account.\n\n<!--more-->\n\n**Some background**: At the time, my blog was a Wordpress site hosted as an Azure Web Site with a MySQL database hosted by Azure Marketplace provider ClearDB.\n\n## Series of events\nAt approximately 12:01 PM I received an alert from Azure that my blog was returning HTTP 500 errors. I quickly checked the site to see what was happening and I was seeing the dreaded \"Error establishing a connection to the database\" message. I had seen this in the past because I was hosting on a very small MySQL instance. It was not entirely uncommon to exceed the maximum number of connections to my database. The thing is that I had recently upgraded to a larger database instance specifically to avoid this problem.\n\nSo...I logged in to the Azure Portal to investigate. To my horror, the MySql database for my blog was nowhere to be found!!! It was gone from the Azure Portal entirely and I couldn't find it on the ClearDB website either. I am the only person who has access to this Azure account and I know that I didn't delete it.\n\nI quickly opened an Azure support ticket and contacted ClearDB to see if either company could tell me what happened to my database.\n\nClearDB actually responded quickly:\n\n> Our records indicate that a remote call from Azure at Wed, 25 Nov 2015 12:00:34 -0600 was issued to us to deprovision the database\n\nUmmm WTF! I know I didn't delete the database. It seems that there is some kind of bug in the integration between Azure and ClearDB. In the mean time, Azure Support eventually replied with the following:\n\n> I have reviewed your case and have adjusted the severity to match the request. Sev A is reserved for situations that involve a system, network, server, or critical program down that severely affects production or profitability. This case is set to severity B with updates every 24 hours or sooner.\n\nAfter nearly a week, I received another update from Azure support:\n\n> I have engaged our Engineering Team already to investigate on this issue and currently waiting for an update from them. Our Engineering Team would require 5 to 7 business days to investigate the issue, I will keep you posted as soon as I hear from them.\n\nI am curious to see what the Engineering Team comes back with. I will update this post if / when I hear more.\n \n## Restoring from backup\n\nWith my database gone, my only choice was to restore from backup. This should have been an easy task. Unfortunately, my automated backup wasn't actually running as expected and my most recent backup was 7 months old. I had all my individual posts in Live Writer `wpost` files but republishing those manually would have taken me over a week.\n\n<!--more-->\n\nIn the end, ClearDB was very helpful and was able to restore my database from their internal backups. As a result, my blog was down for a little under 24 hours. \n\n## Lessons learned\n\nThese were hard lessons for me to learn because I already knew these things. Problem was that I wasn't treating my blog like the production system that it is.\n\n- Don't trust the cloud-based backups. ClearDB has automated periodic backups but I lost access to those when my database was mysteriously deleted. Have a backup held offsite. That's what my Wordpress backups were supposed to do for me, which brings me to my second point.\n\n- Test your backups periodically. I had no idea my backups weren't working until it was too late. \n\n- Complexity kills. I have a simple blog and my comments are managed by Disqus. There is really no reason I should need a relational database for this. The MySQL database here has been a constant source of failure on my blog. \n \n## Moving on\n\nOnce my blog was restored I quickly started a migration over to Hexo. I will blog more about this process shortly.\n\n\n","categories":[],"tags":[]},{"title":"Geek Gifts","slug":"podcast-geek-gifts","date":"2015-11-28 21:13:38+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-geek-gifts/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-geek-gifts/","excerpt":"Learn what the Western Devs might have got this year for Christmas if any of them were nice","raw":"---\nlayout: podcast\ntitle: \"Geek Gifts\"\ndate: 2015-11-28T11:13:38-05:00\nrecorded: 2015-11-27\ncategories: podcasts\nexcerpt: \"Learn what the Western Devs might have got this year for Christmas if any of them were nice\"\ncomments: true\npodcast:\n    filename: \"GeekGifts.mp3\"\n    length: \"1:04:35\"\n    filesize: 61995490\n    libsynId: 3981310\n    anchorFmId: Geek-Gifts-evqdik\nparticipants:\n    - kyle_baley\n    - donald_belcham\n    - dylan_smith\n    - simon_timms\n    - lori_lalonde\n    - dave_white\n    - james_chambers\n    - peter_ritchie\nmusic:\n    - title: Guitalelujah\n      artist: Josh and Dave's Metal Xmas\n      url: http://www.metalxmas.com\n    - title: Santa Lane\n      artist: Josh and Dave's Metal Xmas\n      url: http://www.metalxmas.com\n---\n\n### Health/Ergonomics:\n\n* [Ergodox DIY ergonomic mechanical keyboard](https://www.indiegogo.com/projects/ergodox-ez-an-incredible-mechanical-keyboard#/)\n* [Emperor 1510 Workstation](http://www.mwelab.com/index.php/en/products/emperor-1510)\n* [Microsoft Band 2](http://www.microsoftstore.com/store/msusa/en_US/pdp/Microsoft-Band-2/productID.324438600)\n* [Garmin vÃ­voactive](https://buy.garmin.com/en-CA/CA/p/150767)\n\n### Home Automation/Household:\n\n* [Sonos](http://www.sonos.com)\n* [August](http://august.com)\n* [Nest](https://nest.com/)\n* [Tracking Tag](http://www.uncommongoods.com/product/bluetooth-tracking-tag)\n* [Philips Hue](http://www.meethue.com/)\n* [Flic](https://flic.io)\n\n### Fun Stuff:\n\n* [Bebop Drone](https://www.parrot.com/ca/drones/parrot-bebop-2)\n* [Phantom3 Drone](http://www.dji.com/product/phantom-3-standard)\n* [FPS1000 - Low Cost High Frame Rate Camera](https://www.kickstarter.com/projects/1623255426/fps1000-the-low-cost-high-frame-rate-camera/posts/1411624)\n* [Exploding Kittens](http://www.explodingkittens.com/)\n* [Raspberry Pi](https://www.raspberrypi.org/)\n* [Hackaball](http://www.hackaball.com/)\n* [Sphere BB8](http://www.sphero.com/starwars)\n* [Loot Crate Gift Box](https://www.lootcrate.com)\n* [Scotch Advent Calendar](http://www.scotchwhiskyadvent.com/)\n\n### Other:\n\n* [Laptop bags, cables, and chargers](http://www.ianker.com/)\n* [ThingCharger](http://thingcharger.com)\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Zencastr","slug":"podcast-zencastr","date":"2015-11-25 08:47:07+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-zencastr/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-zencastr/","excerpt":"A chat with Josh Nielsen, creator of Zencastr","raw":"---\nlayout: podcast\ntitle:  \"Zencastr\"\ndate: 2015-11-24T22:47:07-05:00\nrecorded: 2015-11-20\ncategories: podcasts\nexcerpt: \"A chat with Josh Nielsen, creator of Zencastr\"\ncomments: true\npodcast:\n    filename: \"Zencastr.mp3\"\n    length: \"45:50\"\n    filesize: 43997390\n    libsynId: 3974740\n    anchorFmId: Zencastr-evqdih\n\nparticipants:\n    - kyle_baley\n    - amir_barylko\n    - donald_belcham\n    - dylan_smith\n    - dave_paquette\n    - simon_timms\n    - david_wesst\nlinks:\n    - Zencastr|https://www.zencastr.com/\n    - Josh on Twitter|https://twitter.com/joshontheweb\n    - Web Audio API spec|http://www.w3.org/TR/webaudio/\n    - Web Audio API tutorial|http://www.html5rocks.com/en/tutorials/webaudio/intro/\n    - WebRTC|http://www.webrtc.org/\n    - Lame encoder in JavaScript|https://github.com/zhuker/lamejs\n    - ORTC|http://ortc.org/\n    - 37Signals|https://basecamp.com\n\nmusic:\n    - title: Doctor Man\n      artist: Johnnie Christie and the Boats\n      url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* The tech and motivation behind Zencastr\n* Evolution of podcast logistics\n* Server architecture and the importance of keeping it small (at first)\n* Post production capabilities\n* Web RTC and ORTC standards\n* Video podcasts - technical and logistical concerns\n* Browser-based vs. desktop apps\n* Hardware for podcasts\n* Gathering feedback\n* Start-up companies: start big or start small?\n* Moving from beta to paid plans\n* Pricing plan options/add-ons\n* Limiting your beta\n* Methods for getting your first users\n* Funding\n* Deciding on features\n* Know your customer\n* Metrics\n* Doing one thing really well\n* Developing for yourself vs. for someone else\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"On Co-Location, Email, and Face-to-Face Communication","authorId":"darcy_lussier","slug":"on-co-location-email-and-face-to-face-communication","date":"2015-11-20 19:13:24+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/on-co-location-email-and-face-to-face-communication/","link":"","permalink":"https://westerndevs.com/_/on-co-location-email-and-face-to-face-communication/","excerpt":"I broke my own rule â€“ I tweeted a thought as a controversial statement.","raw":"---\nlayout: post\ntitle:  On Co-Location, Email, and Face-to-Face Communication\ndate: 2015-11-20T09:13:24-05:00\ncategories:\ncomments: true\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/11/20/168972.aspx\n---\n\n\nI broke my own rule â€“ I tweeted a thought as a controversial statement.\n\n<!--more-->\n![image][1]\n\nFrom this erupted responses from numerous people from the other point of view (mainly on the co-location piece and whether face-to-face is necessary, not so much defending email). So instead of trying to discuss this in 140 characters, here's my full train of thought on this.\n\nThe tweet was born out of two different events:\n\n1) A tweet by Steve Porter that \"email is not a collaboration tool\" (I fav'd it).\n2) My team had just worked through some design decisions in our team room, where we all sit together.\n\nI was riding a high of team collaboration mixed with crusading angst against email when I combined all that into a single tweet.\n\nNow let me talk through my thought process on all these.\n\n## Co-Location is Incredibly Important\n\nWhen I think of the teams I've been on that have succeeded the most, they all share a common theme â€“ they all worked in the same room or the same area (physical impediments (walls, hallways, different floors) are *real* barriers to teams collaborating together). Being co-located had the spin off benefit that the team felt more like a team than a group of employees assigned to the same project â€“ there's a difference. Eating together, playing board games together, building relationships with each other â€“ all of these happened, and ultimately benefited how we worked together because we were co-located.\n\nThere's a growing thought that employees don't need to be co-located, and in this wired &amp; connected world it shouldn't matter where we work. I can't (and am not trying to) argue with people's personal experiences where they feel they've had success. That's basically what I'm saying in this post â€“ when I've had the most success in projects, co-location has been a factor.\n\nConsider Mob Programming, which is somewhat of the extreme of this position and one that I'm *not* 100% sold on. Mob programming \"is very similar to Pair Programming, but the whole team works together on the same \"problem\" at the same time at the same computer.\" That's business owner, developers, testersâ€¦everyone, in one room, working on the same problem. Below is a video of a company that has adopted this as how they work everyâ€¦singleâ€¦day.\n\n<iframe height=\"315\" src=\"https://www.youtube.com/embed/p_pvslS4gEI\" frameborder=\"0\" width=\"420\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIf you talked to them, I'm sure they'd rave about how awesome this has worked for them. And really, when we talk about how to best organize a team, this speaks to the real truth: the best approach as to where team members should work â€“ remote, co-located, or on top of each other all day â€“ is best left to the individuals that make the team to decide.\n\nFor me, I will always defer to co-locating with a team I'm a part of than being remote. For me I find communication is best done face to face, with video chatting being at the very least of that. Which brings us toâ€¦\n\n## Face to Face Is Necessary\n\nI think we all agree email isn't a collaboration tool and on this point I will argue with anyone. So let's deal with the face-to-face portion of the tweet.\n\nNo, I'm not going to quote the whole \"90% of communication is non-verbal\" because that opens up a whole new can of worms about how that theory has been debunked and that you can't really set a number to something like this because of how people are different, etc. etc. But, there is a lot of research into this area that does support that non-verbal cues DO make influence how we communicate and interpret communication. If you'd like to learn more on this, [there's a number of great TED talks on the subject][2].\n\nI've also found that face to face communication removes any interpretation or guesswork that an individual has to do around inferring tone and emotion. Consider this.\n\n\"Hi Joe, please come by my office â€“ I need to discuss something with you.\"\n\nWe've probably all gotten an email like this or similar in the past. What thoughts go through your mind when you see this? Without any context this can be read as everything from getting a promotion to getting fired.\n\nNow imagine you're casually walking down the hallway and your boss passes by and says to you \"Hi Joe. Hey, can you please come by my office later? I need to discuss something with you.\" Your boss is relaxed, he even smiles when he sees you. He doesn't seem agitated or concerned. Now what goes through your mind? Probably that you just need to go talk about something with the boss and the thought that you may be in trouble doesn't even cross your mind.\n\nSeeing someone, hearing the tone in their voice, experiencing their non-verbal cues, all plays in to how we _effectively_ communicate with others, whether we're receiving or delivering the message.\n\nI have a love/hate with Slack. I've had a couple of teams that used it extensively, and for some reason I always seem to get with people who â€“ like me, admittedly â€“ love to bug and poke each other. But sometimes, without understanding the non-verbal piece of communication, those messages can be misinterpreted as being meant-spirited or even cruel. I actually took a self-imposed hiatus from Slack for a bit because it was becoming counter-productive to the blissful communication utopia the platform promises.\n\nI mentioned how effectively communicating plays in when receiving _or delivering_. Face to face communication allows us to pick up cues about the receiver and what state they're in. Are they happy, sad, frustrated? Are they in a place where a comment made would hurt them? Without non-verbal cues I have _no opportunity to alter my message to ensure its communicated effectively._\n\nWhere face to face or video isn't an option, voice is great as well. I'm also a proponent of using emoticons in emails to convey sentiment â€“ its better than forcing someone to infer what your tone is.\n\n## Let's Wrap Up\n\nEveryone is different â€“ personalities, drivers, needs, wants, etc. Everyone communicates in their own way and has their own preferences. And no solution fits everyone. Mob Programming isn't for everyone, just like an all-remote team isn't for everyone either. For me, I will always look for co-locating with my team and doing face-to-face communication as much as possible, because that's where I've seen the&nbsp; most success personally. If you're being successful in what you're doing â€“ great, keep it up! If not, look at yourself, your team, and how you're working/communicating and see if there's an opportunity to change things.\n\n[1]: https://gwb.blob.core.windows.net/dlussier/WindowsLiveWriter/OnCoLocationEmailandFaceToFaceCommunicat_6672/image_thumb.png\n[2]: https://www.google.ca/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=ted+talks+non+verbal+communication\n  ","categories":[],"tags":[]},{"title":"Inspiration From MVP Summit 2015","authorId":"david_wesst","slug":"inspiration-from-mvp-summit-2015","date":"2015-11-20 19:09:44+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/inspiration-from-mvp-summit-2015/","link":"","permalink":"https://westerndevs.com/_/inspiration-from-mvp-summit-2015/","excerpt":"I'm back from the summit and have had a week to digest everything I saw. And now I will share with the top three topics that really stuck with me.","raw":"---\nlayout: post\ntitle:  Inspiration From MVP Summit 2015\ndate: 2015-11-20T09:09:44-05:00\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/\n---\n\n![me-mvp2015.jpg][1]\n\nI'm back from the summit and have had a week to digest everything I saw. And now I will share with the top three topics that really stuck with me.\n\n<!--more-->\n\n### Visual Studio Team Services and ALM\n\nYeah, I bet you didn't think you'd read about VSO, err..I mean, VS Team Services on my blog, but you are. I'm just as surprised as you are; trust me. Why put this big, bloated piece of software on my list? Because I got to see it working with NodeJS and JVM-based projects and I have to say, what I saw was really impressive.\n\n![vso.png][2]\n\nSure, it's not perfect. It still lacks SSH support (but it's coming soon) and I'm still not 100% about why work items need to be so customizable, but it really is powerful toolset that starts you off at the right price at $0 for unlimited private repositories, and even some cloud-based build server time. The build tool is pretty cool and supports all kinds of JavaScript and Java-based systems, which really appeals to me.\n\nOne of the skills I lack on my resume is a strong understanding of Application Lifecycle Management, or ALM for short. Although I know there are plenty of tools out there, VSTS gives me everything I need for free, with some great tooling.\n\nDid they convince me to look open up my copy of Visual Studio 2015? No. No they didn't. That is staying off, but VSTS is a great stepping stone for me and it might be for you too.\n\n### Vorlon.JS is Open Sorcery\n\nNo really, it is.\n\n![vorlon.png][3]\n\nYou can triage any client-side JavaScript application from anywhere in the world by simply having the device running the application load up a page on your Vorlon server. Or, if you're looking to triage someone else's work, you can use the Vorlon proxy feature to load up any site and start going through the guts to see how it can be improved.\n\nIt's amazing, and I'm not talking more about it until I get to talk about it in depth once I get my hands dirty.\n\nOh, and did I say that it's open source? 'Cause it is.\n\n### JavaScript is Stronger Than Ever\n\nWith both ES5 and ES6 out the door, it's time to look to the future of JavaScript and where it will be going with ES7, and that's exactly what we did.\n\n![es7.png][4]\n\nThere is a lot of stuff to get excited about, but my personal favourites are [async functions][5], [class properties][6], [Array.prototype.includes][7], and String [padding][8] and [trimming][9] functions.\n\nCheck it out. Really, it's going to be cool.\n\n### Universal Windows Platform, from Raspberry Pi to Web\n\n[Project Westminster][10] is really cool. An easy way to bring your web code to a local device, but with UWP your code now works on _any_ Windows 10 device.\n\n![node-installer.png][11]\n\nPlus, with [NodeJS supporting UWP][12], me and my JavaScript code are set for pretty much anything. Oh, and I can run my NodeJS UWP application on a Raspberry Pi. That's just nuts.\n\n## The Point\n\nThe point is I'm inspired to keep on JavaScriptin' and I hope you find something in my list of cool things from MVP summit to get your brain moving.\n\nThanks for playing. ~ DW\n\n[1]: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/me-mvp2015.jpg\n[2]: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/vso.png\n[3]: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/vorlon.png\n[4]: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/es7.png\n[5]: https://tc39.github.io/ecmascript-asyncawait/\n[6]: https://github.com/jeffmo/es-class-static-properties-and-fields\n[7]: https://github.com/tc39/Array.prototype.includes\n[8]: https://github.com/tc39/proposal-string-pad-left-right\n[9]: https://github.com/sebmarkbage/ecmascript-string-left-right-trim\n[10]: http://blogs.windows.com/buildingapps/2015/07/06/project-westminster-in-a-nutshell/\n[11]: http://blog.davidwesst.com/2015/11/Inspiration-from-MVP-Summit-2015/node-installer.png\n[12]: https://github.com/Microsoft/node-uwp\n  ","categories":[],"tags":[]},{"title":"Testing With Data","authorId":"kyle_baley","slug":"testing-with-data","date":"2015-11-20 00:02:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/testing-with-data/","link":"","permalink":"https://westerndevs.com/_/testing-with-data/","excerpt":"It's not a coincidence that this is coming off the heels of Dave Paquette's post on GenFu and Simon Timms' post on source control for databases in the same way it was probably not a coincidence that Hollywood released three body-swapping movies in the 1987-1988 period (four if you include Big).","raw":"---\nlayout: post\ntitle:  \"Testing With Data\"\ndate: 2015-11-19T14:02:10-05:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\nIt's not a coincidence that this is coming off the heels of Dave Paquette's [post on GenFu](http://www.westerndevs.com/realistic-sample-data-with-genfu/) and Simon Timms' [post on source control for databases](http://www.westerndevs.com/source-control-for-sql-databases/) in the same way it was probably not a coincidence that Hollywood released three body-swapping movies in the 1987-1988 period (four if you include Big).\n\n<!-- more -->\n\nI was asked recently for some advice on generating data for use with integration and UI tests. I already have some ideas but asked the rest of the Western Devs for some elucidation. My tl;dr version is the same as what I mentioned in our [discussion on UI testing](http://www.westerndevs.com/on-ui-testing/): it's hard. But manageable. Probably.\n\nThe solution needs to balance a few factors:\n\n* Each test must start from a predictable state\n* Creating that predictable state should be fast as possible\n* Developers should be able to figure out what is going on by reading the test\n\nThe two options we discussed both assume the first factor to be immutable. That means you either clean up after yourself when the test is finished or you wipe out the database and start from scratch with each test. Cleaning up after yourself might be faster but has more moving parts. Cleaning up might mean different things depending on which step you're in if the test fails.  \n\nSo given that we will likely re-create the database from scratch before each and every test, there are two options. My _current_ favourite solution is a hybrid of the two.\n\n### Maintain a database of known data\n\nIn this option, you have a pre-configured database. Maybe it's a SQL Server .bak file that you restore before each test. Maybe it's a `GenerateDatabase` method that you execute. I've done the latter on a Google App Engine project, and it works reasonably well from an implementation perspective. We had a class for each domain aggregate and used dependency injection. So adding a new test customer to accommodate a new scenario was fairly simple. There are a number of other ways you can do it, some of which Simon touched on in his post.\n\nWe also had it set up so that we could create only the customer we needed for that particular test if we needed to. That way, we could use a step like `Given I'm logged into 'Christmas Town'` and it would set up only that data.\n\nThere are some drawbacks to this approach. You still need to create a new class for a new customer if you need to do something out of the ordinary. And if you need to do something only _slightly_ out of the ordinary, there's a strong tendency to use an existing customer and tweak its data ever so slightly to fit your test's needs, other tests be damned. With these tests falling firmly in the _long-running_ category, you don't always find out the effects of this until much later.\n\nAnother drawback: it's not obvious in the test exactly what data you need for that specific test. You can accommodate this somewhat just with a naming convention. For example, `Given I'm logged into a company from India`, if you're testing how the app works with rupees. But that's not always practical. Which leads us to the second option.\n\n### Create an API to set up the data the way you want\n\nHere, your API contains steps to fully configure your database exactly the way you want. For example:\n\n{% codeblock lang:cucumber %}\nGiven I have a company named \"Christmas Town\" owned by \"Jack Skellington\"\nAnd I have 5 product categories\nAnd I have 30 products\nAnd I have a customer\n...\n{% endcodeblock %}\n\nYou can probably see the major drawback already. This can become _very_ verbose. But on the other hand, you have the advantage of seeing exactly what data is included which is helpful when debugging. If your test data is wrong, you don't need to go mucking about in your source code to fix it. Just update the test and you're done.\n\nAlso note the lack of specifics in the steps. Whenever possible, I like to be very vague when setting up my test data. If you have a [good framework for generating test data](https://github.com/MisterJames/GenFu), this isn't hard to do. And it helps uncover issues you may not account for using hard-coded data (as anyone named D'Arcy O'Toole can probably tell you).\n\n---\nLoading up your data with a granular API isn't realistic which is why I like the hybrid solution. By default, you pre-load your database with _some_ common data, like lookup tables with lists of countries, currencies, product categories, etc. Stuff that needs to be in place for the majority of your tests.\n\nAfter that, your API doesn't need to be that granular. You can use something like `Given I have a basic company` which will create the company, add an owner and maybe some products and use that to test the process for creating an order. Under the hood, it will probably use the specific steps.\n\nOne reason I like this approach: it hides only the details you don't care about. When you say `Given I have a basic company and I change the name to \"Rick's Place\"`, that tells me, \"I don't care how the company is set up but the company name is important\". Very useful to help narrow the focus of the test when you're reading it.\n\nThis approach will understandably lead to a whole bunch of different methods for creating data of various sizes and coarseness. And for that you'll need to...\n\n### Maintain test data\n\nRegardless of your method, maintaining your test data will require constant vigilance. In my experience, there is a tremendous urge to take shortcuts when it comes to test data. You'll re-use a test company that doesn't quite fit your scenario. You'll alter your test to fit the data rather than the other way around. You'll duplicate a data setup step because your API isn't discoverable.\n\nMake no mistake, maintaining test data is work. It should be treated with the same respect and care as the rest of your code. Possibly more so since the underlying code (in whatever form it takes) technically won't be tested. Shortcuts and bad practices should _not_ be tolerated and let go because \"it's just test data\". Fight the urge to let things slide. Call it out as soon as you see it. Refactor mercilessly once you see opportunities to do so.\n\nDon't be afraid to flip over a table or two to get your point across.\n\n-- Kyle the Unmaintainable\n","categories":[],"tags":[]},{"title":"Source Control for SQL Databases","authorId":"simon_timms","slug":"source-control-for-sql-databases","date":"2015-11-19 08:40:31+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/source-control-for-sql-databases/","link":"","permalink":"https://westerndevs.com/_/source-control-for-sql-databases/","excerpt":"There are a bunch of options for migrating database schema, how can you be sure you've picked the right one?","raw":"---\nlayout: post\ntitle:  Source Control for SQL Databases\ndate: 2015-11-18T20:40:31-07:00\ncomments: true\nauthorId: simon_timms\nexcerpt: \"There are a bunch of options for migrating database schema, how can you be sure you've picked the right one?\"\noriginalurl:\n---\n\nDiscussion on the Western Devs slack channel today turned to how to manage the lifecycle of databases. This is something we've discussed in the past and today's rehash was brought to us by [D'Arcy](http://www.westerndevs.com/bios/darcy_lussier/) asking an innocent question about Entity Framework. As seems to happen on slack instead of helping five people immediately told him how wrong he was to be using EF in that way in the first place (we helped him later).  Database migrations are a hot topic and there are a lot of options in the space so I thought I'd put together a little flow chart to help people decide which option is the best for their scenario. \n\nLet's start by looking at the options. \n\n1. Explicit migrations in code\n2. Explicit migrations in SQL\n3. Desired state migrations\n\nWhat's all this mean? First let's look at explicit migrations vs desired state. In explicit migration we write out what changes we want to make to the database. So if you want to add a new column to a table then you would actually write out some form of `please add column address to the users table it is a varchar of size 50`.  These migrations stack up on each other. This means that after a few weeks of development you might have a dozen or more files with update instructions. It is very important that once you've checked in one of these migrations that you don't ever change it. Migrations are immutable. If you change your mind or make a mistake then you correct it by adding another migration. \n\n```\nmigration 1: add column addresss\n//oh drat, spelled that wrong\nmigration 2: rename column addresss to address\n```\n\nThe reason for this is that you never know when your database is going to be deployed to an environment. Typically the tools in this space keep track of the migrations which have been applied to a database. If you change a migration which has been applied then they have no way to correct the database and the migration will fail. Best not to get yourself into that situation. \n\nWith migrations you can get yourself into a mess of migrations. A project that lasts a couple of years may acquire hundreds or even thousands of migrations. For the most part this doesn't matter because the files should never change however it can slow down deployments a bit. If it does bug you and you are certain that all your database instances in the wild are current up to a certain migration then you can build a checkpoint. You would take an image of the database or generate the schema and check that in. Now you can delete all the migrations up to that point and start fresh. \n\nThese migrations can be created in code using something like entity framework migrations or using a tool like [Fluent Migrator](https://github.com/schambers/fluentmigrator) - that's option #1. Option #2 is to keep all the migrations in SQL and use something like [Roundhouse](https://github.com/chucknorris/roundhouse). Option #1 is easier to integrate with your existing ORM and you might even be able to generate some of the migrations though tools like EF's add migration which compares the previous state of your model with the new state and builds migrations (this is starting to blur the lines between options #1 and #3). However it is further away from pure SQL which a lot of people are more comfortable with. I have also found in the past that EF is easily confused by multiple people on a project building migrations at the same time. Explicit SQL migrations are a bit more work but can be cleaner. \n\nThe final option is to use a desired state migration tool. These tools look at the current state of the database and compare them with your desired state then perform whatever operations are necessary to take current to desired. You might have seen desired state configuration in other places like [puppet](https://puppetlabs.com/) or [Powershell DSC](https://technet.microsoft.com/en-us/library/dn249912.aspx) and this is pretty much the same idea. These tools are nice because you don't have to care about the current state of the database. If it is possible the tool will migrate the database. Instead of specifying what you want to change you just update the model and the desired state tooling will calculate the change. These tools tend to fall down when you have to make changes to the data in the database - they are very focused on structural changes. \n\nWe've now looked at all the options so which one should you pick? There is never going to be a 100% right answer here (unless your boss happens to be in love with one of the solutions and will fire you if you pick a different one) but there are some indicators that might point you in the right direction. \n\n1. Is your product one which has a single database instance? An example of this might be most internal corporate apps. There is only one instance and only likely to be one instance. If so then you could use any migration tool. If not then the fact that you can't properly manage multiple data migrations with SQL Server Database Projects preclude it. Code based migrations would work but tend to be a bit more difficult to set up than using pure SQL migrations. \n\n2. Do you need to create a bunch of seed data or default values? Again you might want to stay away from desired state because it is harder to get the data in. Either of the explicit migration approaches would be better. \n\n3. Is this an existing database which isn't under source control? SQL server database projects are great for this scenario. They will create a full schema from the database and properly organize it into folders. Then you can easily jump into maintaining and updating the database without a whole lot of work. \n\n4. Are there multiple slightly different versions of the database in the wild? Desired state is perfect for this. You don't need to figure out a bunch of manual migrations to set a baseline. \n\n5. Are you already using EF and have a small team unlikely to step on each other's toes? Then straight up EF migrations could be your best bet. You don't have to introduce another technology or tool. (I should mention here that you can use EF's automatic migrations to act in the same way as a desired state configuration tool so consider that. Generally the EF experts recommend against doing that in favour of explict migrations)\n\n6. Do you have a team that is very strong in SQL but not modern ORMs? Ah then SQL based migrations are likely you friend. A well-versed team may have already created a version of this. Switch to roundhouse, it will save you time in the long run. \n\nI hope that these will give you a little guidance as to which tool will work best for your project. I'm sure there are all sorts of other questions one might ask to give a hint as to which technique should be used. Please do comment on this post and I'll update it. \n\n\n![http://imgur.com/SlfjxSE](http://imgur.com/SlfjxSE.png)\n![http://imgur.com/Kq0UvYt](http://imgur.com/Kq0UvYt.png)\n![http://imgur.com/yNcJdl9](http://imgur.com/yNcJdl9.png)","categories":[],"tags":[]},{"title":"The Humanitarian Toolbox AllReady Code-a-Thon","authorId":"james_chambers","slug":"the-humanitarian-toolbox-allready-code-a-thon","date":"2015-11-18 19:31:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/the-humanitarian-toolbox-allready-code-a-thon/","link":"","permalink":"https://westerndevs.com/_/the-humanitarian-toolbox-allready-code-a-thon/","excerpt":"I was recently in Seattle for the MVP Summit, one of the best ways to connect to product teams and really smart people from around the world. Every year I get to meet more of the team that builds the tools I use daily, reconnect with peers and catch up with friends. And I eat at Magiano's. But this year, the MVP Summit was trumped in awesomeness as quickly at it came to a close as the very next morning the code-a-thon for the Humanitarian Toolbox kicked into high gear.","raw":"---\nlayout: post\ntitle:  The Humanitarian Toolbox AllReady Code-a-Thon\ndate: 2015-11-18T09:31:50-05:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/11/the-humanitarian-toolbox-allready-code-a-thon/\n---\n\nI was recently in Seattle for the MVP Summit, one of the best ways to connect to product teams and really smart people from around the world. Every year I get to meet more of the team that builds the tools I use daily, reconnect with peers and catch up with friends. And I eat at Magiano's.\n\nBut this year, the MVP Summit was trumped in awesomeness as quickly at it came to a close as the very next morning the code-a-thon for the Humanitarian Toolbox kicked into high gear.\n\n<!-- more -->\n\n> **Want to join the cause?** The easiest way to get started is to join our weekly Saturday morning call. We are online from 10AM CST to Noon CST every Saturday. [Watch Twitter â€“&gt;][1] for the link just before 10AM.\n\nThere are a lot of great projects out there. AllReady is great software with great purpose as well.\n\n## Hack for a Cause\n\nWhenever disaster strikes a community â€“ a forest fire, a tsunami, an earthquake â€“ lives are impacted. Sadly, those with the fewest resources are often the ones at most risk after the disaster.\n\nFrom November 6th to the 8th I was privileged to join in with about twenty other individuals from around the world to work on [AllReady][2], an open source project that is curated by the Humanitarian Toolbox. AllReady is software that helps communities _organize_ and _execute_ efforts in preparedness so that those who are at risk are better equipped to make it out of a disaster in the best shape possible.\n\n![image][3]\n\nThe group of us descended to the Garage at Building 27 on Microsoft Campus. We hunkered down, plowed through hundreds of commits and many dozens of issues and pull requests.\n\nIt was an amazing experience. It was a group of really smart people, supported by folks on the ASP.NET team, building software that is going to change lives.\n\n> To find out more about the awesome work that The Humanitarian Toolbox is doing, please [visit their site][4].\n\n## Hack for Yourself\n\nThere is a huge draw to dive in and help with a project that can affect so many people and thwart the negative impact of unfortunate conditions. Preparedness is so much more effective than disaster recovery.\n\n![image][5]\n\nSo...it's a good reason to get involved. But if that's not enough, check out this tech stack:\n\n* ASP.NET 5\n* MVC Framework 6\n* Azure Web Apps (Sites and Jobs)\n* Azure Storage (Tables and Queues)\n* AutoFac as the IoC container\n* MediatR as a messaging bus &amp; pub/sub provider\n* Entity Framework 7\n* GitHub and AppVeyor\n* SendGrid and Twilio\n\nI mean, _just look at that list_. That's likeâ€¦**all** the buzzwords. And jumping in to help on this project is also jumping in to learn. This is an opportunity to work with world-class developers on a project that is striving to have great architecture. It runs on the cloud in cloud-like ways and uses technology that is going to be used for the next 5-10 years and beyond.\n\n![image][6]\n\nAfter the weekend, we drew to a close by having a retrospective where we worked through the next steps and where this project is headed. It's exciting to see the momentum building as more community members come on board and start making commits.\n\n![image][7]\n\nWe've got a lot done in just a few weeks, and I'm excited to see it moving forward _daily_.&nbsp;\n\n## Join In and Start Hacking\n\nThe best part about the software is that everyone can contribute. I'm not going to lie, there are some advanced aspects of the project that will be hard to work through for junior developers. There are more aspects, still, that need the love of some senior developers. Regardless of where you are in the world or in your career, there is likely a task where you can get started.\n\nIf you have questions, reach out to me [_on Twitter_][1] and I'll help to get you started.\n\nHappy coding! ![Smile][8]\n\n[1]: http://twitter.com/CanadianJames\n[2]: http://www.htbox.org/blog/allready-project-launched-at-visual-studio-2015-release-event\n[3]: http://jameschambers.com/wp-content/uploads/2015/11/image_thumb.png \"image\"\n[4]: http://htbox.org\n[5]: http://jameschambers.com/wp-content/uploads/2015/11/image_thumb1.png \"image\"\n[6]: http://jameschambers.com/wp-content/uploads/2015/11/image_thumb2.png \"image\"\n[7]: http://jameschambers.com/wp-content/uploads/2015/11/image_thumb3.png \"image\"\n[8]: http://jameschambers.com/wp-content/uploads/2015/11/wlEmoticon-smile1.png\n  ","categories":[],"tags":[]},{"title":"Interviews and Hiring Practices","slug":"podcast-interviews-and-hiring-practices","date":"2015-11-17 23:56:08+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-interviews-and-hiring-practices/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-interviews-and-hiring-practices/","excerpt":"The definitive answer on how to hire your next dev","raw":"---\nlayout: podcast\ntitle:  \"Interviews and Hiring Practices\"\ndate: 2015-11-17T13:56:08-05:00\nrecorded: 2015-11-13\ncategories: podcasts\nexcerpt: \"The definitive answer on how to hire your next dev\"\ncomments: true\nlibsyn: 3982686\npodcast:\n    filename: \"HiringPractices.mp3\"\n    length: \"44:07\"\n    filesize: 42345205\n    libsynId: 3982686\n    anchorFmId: Interviews-and-Hiring-Practices-evqdhv\nparticipants:\n    - kyle_baley\n    - amir_barylko\n    - donald_belcham\n    - dylan_smith\n    - wendy_clossen\n    - lori_lalonde\n    - simon_timms\nlinks:\n    - Simon's coding interview|https://github.com/Pacesetter/CodingTest\n    - Codility|https://codility.com/\n    - Adaptech Solutions|http://adaptechsolutions.ca/\n    - ThoughtWorks interview process|https://www.thoughtworks.com/insights/blog/most-difficult-it-interview-ive-ever-loved\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* The purpose (and effectivness) of the initial HR screen\n* Reading from a script\n* The importance of personality fit\n* Benefits/Logistics/Cahellenges of actually working with the candidate as a step in the process\n* Interviewing is a two-way street\n* The probationary period\n* Hiring people you know\n* Interviewing senior vs. junior people\n* Techniques for evaluating technical skill\n* Involving a woman in the technical interview\n* Hiring when you aren't technical\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Realistic Sample Data With GenFu","authorId":"dave_paquette","slug":"realistic-sample-data-with-genfu","date":"2015-11-16 18:47:02+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/realistic-sample-data-with-genfu/","link":"","permalink":"https://westerndevs.com/_/realistic-sample-data-with-genfu/","excerpt":"Last week, I had the opportunity to spend some time hacking with my good friend James Chambers. One of the projects we worked on is his brainchild: GenFu","raw":"---\nlayout: post\ntitle:  Realistic Sample Data With GenFu\ndate: 2015-11-16T08:47:02-05:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/11/15/realistic-sample-data-with-genfu.aspx\n---\n\nLast week, I had the opportunity to spend some time hacking with my good friend [James Chambers][1]. One of the projects we worked on is his brainchild: [GenFu][2]\n\n<!--more-->\n\n> GenFu is a test and prototype data generation library for .NET apps. It understands different topics â€“ such as \"contact details\" or \"blog posts\" and uses that understanding to populate commonly named properties using reflection and an internal database of values or randomly created data.\n\nAs a quick sample, I attempted to replace the Sample Data Generator in the ASP.NET5 Music Store app with GenFu. With the right GenFu configuration, it worked like magic and I was able to remove over 700 lines of code!\n\nAs part of that process, it became clear that our documentation related to configuring more complex scenarios was slightly lacking. We are working on creating official project documentation. In the mean time, this post can serve as the unofficial documentation for GenFu.\n\n## Installing GenFu\n\nGenFu is available via [NuGet][3] and can be added to any .NET 4.5, 4.6, or aspnet5 project.\n\n> Install-Package GenFu\n\n## Basic Usage\n\nLet's say you have a simple Contact class as follows:\n\n{% codeblock lang:csharp %}\npublic class Contact\n{\n    public int Id { get; set; }\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n    public string EmailAdress { get; set; }\n    public string PhoneNumber { get; set; }\n    public override string ToString()\n    {\n        return $\"{Id}: {FirstName} {LastName} - {EmailAdress} - {PhoneNumber}\";\n    }\n}\n{% endcodeblock %}\n\nTo generate a list of random people using the GenFu defaults, simply call the `A.ListOf` method:\n\n{% codeblock lang:csharp %}\nvar people = A.ListOf<Contact>();\nforeach (var person in people)\n{\n    Console.WriteLine(person.ToString());\n}\n{% endcodeblock %}\n\nThis simple console app will output the following:\n\n![image][4]\n\nThat was easy, and the data generally looks pretty realistic. The default is to generate 25 objects. If you want more (or less), you can use an overload of the `ListOf` method to specify the number of objects you want. Okay, but what if the defaults aren't exactly what you wanted? That's where GenFu property filler configuration comes in.\n\n## Configuring Property Fillers\n\nGenFu has a fluent API that lets you configure exactly how your object's properties should be filled.\n\n### Manually Overriding Property Fillers\n\nLet's start with a very simple example. In the example above, the Id is populated with random values. That behaviour might be fine for you, but if you are using GenFu to generate random data to seed a database this will probably cause problems. In this case we would want the Id property to be always set to 0 so the database can automatically generate unique ids.\n\n{% codeblock lang:csharp %}\nA.Configure<Contact>()\n            .Fill(c => c.Id, 0);\n\nvar people = A.ListOf<Contact>();\n{% endcodeblock %}\n\nNow all the Ids are 0 and the objects would be safe to save to a database:\n\n![image][5]\n\nAnother option is to use a method to fill a property. This can be a delegate or any other method that returns the correct type for the property you are configuring:\n\n{% codeblock lang:csharp %}\nvar i = 1;\n\nA.Configure<Contact>()\n            .Fill(c =>; c.Id, () => { return i++; });\n{% endcodeblock %}\n\nWith that simple change, we now have sequential ids. Magic!\n\n![image][6]\n\nThere is also an option that allows you to configure a property based on other properties of the object. For example, if you wanted to create an email address that matched the first name/last name you could do the following. _Also, notice how you can chain together multiple property configurations_.\n\n{% codeblock lang:csharp %}\nA.Configure<Contact>()\n            .Fill(c => c.Id, 0)\n            .Fill(c => c.EmailAdress,\n                c => { return string.Format(\"{0}.{1}@zombo.com\", c.FirstName, c.LastName); });\n{% endcodeblock %}\n\nThis can be simplified greatly by using string interpolation in C#6.\n\n{% codeblock lang:csharp %}\nA.Configure<Contact>()\n            .Fill(c => c.Id, 0)\n            .Fill(c => c.EmailAdress,\n                  c => $\"{c.FirstName}.{c.LastName}@zombo.com\");\n{% endcodeblock %}\n\n![image][7]\n\n### Property Filler Extension Methods\n\nIn some cases, you might want to give GenFu hints about how to fill a property. For this there is a set of _With*_ and _As*_ extension methods available. For example, if you wanted an integer property to be filled with values within a particular range:\n\n{% codeblock lang:csharp %}\nA.Configure<Contact>()\n            .Fill(c => c.Age).WithinRange(18, 67);\n{% endcodeblock %}\n\nIntelliSense will show you the list of available extensions based on the type of the property you are configuring.\n\n![image][8]\n_IntelliSense showing extensions for a String property_\n\nExtensions are available for String, DateTime, Integer, Short, Decimal, Float and Double types.\n\n### WithRandom\n\nIn some situations, you might want to fill a property with a random value from a given list of values. A simple example of this might be a boolean value where you want approximately 2/3rds of the values to be true and 1/3 to be false. You could accomplish this using the WithRandom extension as follows:\n\n{% codeblock lang:csharp %}\nA.Configure<Contact>()\n            .Fill(c => c.IsRegistered)\n            .WithRandom(new bool[] { true, true, false });\n{% endcodeblock %}\n\nThe `WithRandom` method is also useful for wiring up object graphs. Imagine the following model classes:\n\n{% codeblock lang:csharp %}\npublic class IncidentReport\n{\n    public int Id { get; set; }\n    public string Description { get; set; }\n    public DateTime ReportedOn { get; set; }\n    public Contact ReportedBy { get; set; }\n}\n\npublic class Contact\n{\n    public int Id { get; set; }\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n    public string EmailAdress { get; set; }\n    public string PhoneNumber { get; set; }\n}\n{% endcodeblock %}\n\nWe could use GenFu to generate 1,000 IncidentReports that were reported by 100 different Contacts as follows:\n\n{% codeblock lang:csharp %}\nvar contacts = A.ListOf<Contact>(100);\n\nA.Configure<IncidentReport>()\n            .Fill(r => r.ReportedBy)\n            .WithRandom(contacts);\n\nvar incidentReports = A.ListOf<IncidentReport>(1000);\n{% endcodeblock %}\n\n\n## Wrapping it up\n\nThat covers the basics and you are now on your way to becoming a GenFu master. In a future post we will cover how to extend GenFu by writing your own re-usable property fillers. In the mean time, give GenFu a try and let us know what you think.\n\n[1]: http://jameschambers.com/\n[2]: https://github.com/MisterJames/GenFu\n[3]: http://nuget.org/packages/GenFu\n[4]: http://www.davepaquette.com/wp-content/uploads/2015/11/image_thumb.png \"image\"\n[5]: http://www.davepaquette.com/wp-content/uploads/2015/11/image_thumb1.png \"image\"\n[6]: http://www.davepaquette.com/wp-content/uploads/2015/11/image_thumb2.png \"image\"\n[7]: http://www.davepaquette.com/wp-content/uploads/2015/11/image_thumb3.png \"image\"\n[8]: http://www.davepaquette.com/wp-content/uploads/2015/11/image_thumb4.png \"image\"\n","categories":[],"tags":[]},{"title":"All About SmartView","slug":"podcast-all-about-smartview","date":"2015-11-11 03:13:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-all-about-smartview/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-all-about-smartview/","excerpt":"An Interview with SmartView founder Amir Barylko","raw":"---\nlayout: podcast\ntitle:  \"All About SmartView\"\ndate: 2015-11-10T15:13:26-07:00\nrecorded: 2015-10-26\ncategories: podcasts\nexcerpt: \"An Interview with SmartView founder Amir Barylko\"\ncomments: true\npodcast:\n    filename: \"SmartView.mp3\"\n    length: \"31:53\"\n    filesize: 43892503\n    libsynId: 3982695\n    anchorFmId: All-About-SmartView-evqdhf\nparticipants:\n    - simon_timms\n    - amir_barylko\nlinks:\n    - SmartView|https://smartviewapp.com/\n    - Our Kanban Podcast|http://www.westerndevs.com/podcasts/podcast-kanban/\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\nIn this podcast we talk to Amir Barylko about\n\n- Kanban\n- The impetus behind SmartView\n- Being a product owner and not a developer\n- Working with a remote team\n- Dealing with user feedback\n- Dogfooding your own product\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Encouraging and Educating Kids","slug":"podcast-encouraging-and-educating-kids","date":"2015-11-09 05:12:19+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-encouraging-and-educating-kids/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-encouraging-and-educating-kids/","excerpt":"Ways to encouraging and educate kids in the dark art of coding. With special guest Wendy Closson","raw":"---\nlayout: podcast\ntitle:  \"Encouraging and Educating Kids\"\ndate: 2015-11-08T19:12:19-05:00\nrecorded: 2015-10-30\ncategories: podcasts\nexcerpt: \"Ways to encouraging and educate kids in the dark art of coding. With special guest Wendy Closson\"\ncomments: true\npodcast:\n    filename: \"TeachingKids.mp3\"\n    length: \"45:43\"\n    filesize: 43892503\n    libsynId: 3982707\n    anchorFmId: Encouraging-and-Educating-Kids-evqdhq\nparticipants:\n    - kyle_baley\n    - amir_barylko\n    - donald_belcham\n    - dave_paquette\n    - tom_opgenorth\n    - david_wesst\n    - james_chambers\n    - dylan_smith\n    - wendy_clossen\nlinks:\n    - HALEP|https://www.bsd.ca/schools/Lindenlanes/HALEP/Pages/default.aspx\n    - Scott Hanselman on 3D printing a quad copter|http://www.hanselman.com/blog/OptimizeForTinyVictories.aspx\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\nBenefits of exposing kids to coding\nComputer classes in schools: the good, the bad, and the ugly\nCoding as a means to overcome fear of technology\nA good teacher vs. a good technologist\nCoding as a means of teaching applied logic and providing career options\nHow young is too young?\nTools to help (see section below)\nCombining interests\nCompeting with iPads and XBoxes\nShould coding be taught in schools?\nHow to search for stuff on Google\n\"Back in my day...\"\nThe fallacy of game design\nEnding a podcast is hard...\n\n### Tools to help\n* [Lego Mindstorms](http://mindstorms.lego.com)\n* [Scratch](https://scratch.mit.edu/)\n* [Kodu](http://www.kodugamelab.com/)\n* [Logo](http://turtleacademy.com/)\n* [Unity](https://unity3d.com/)\n* [Arduino](https://www.arduino.cc/)\n* [Hopscotch](https://www.gethopscotch.com/)\n* [Hour of Code](https://hourofcode.com)\n* [Code.org](https://code.org)\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Stop OSX deleting /tmp so frequently","authorId":"simon_timms","slug":"stop-osx-deleting-tmp-so-frequently","date":"2015-11-07 07:19:27+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/stop-osx-deleting-tmp-so-frequently/","link":"","permalink":"https://westerndevs.com/_/stop-osx-deleting-tmp-so-frequently/","excerpt":"Some time ago I lost a podcast recording because I stored it in /tmp. It is a bad habit but I tend to store things that I'm not going to need in the long run in /tmp. It is a throw back to my real Linux days when storage was expensive and I might not be back on that machine for a while to figure out why all the space was used.","raw":"---\nlayout: post\ntitle:  \"Stop OSX deleting /tmp so frequently\"\ncomments: true\nauthorId: simon_timms\ndate: 2015-11-06T19:19:27-07:00\nalias: /docker/stop-osx-deleting-tmp-so-frequently/\n---\n\nSome time ago I lost a podcast recording because I stored it in /tmp. It is a bad habit but I tend to store things that I'm not going to need in the long run in /tmp. It is a throw back to my real Linux days when storage was expensive and I might not be back on that machine for a while to figure out why all the space was used. \n\n<!--more-->\n  \nBy default OSX deletes the contents of /tmp when it is 3 days old, or rather it hasn't been accessed for 3 days. This is found by using `find -atime +3` (you can read all about that in the man page for find). In order to avoid losing any more important things I decided to change this from 3 days to 90 days. To do this you'll want to edit the /etc/defaults/periodic.conf file and find the variable called `daily_clean_temps_days`. In my file it looks like\n\n{% codeblock lang:bash %}\n# 110.clean-tmps\ndaily_clean_tmps_enable=\"YES\"                           # Delete stuff daily\ndaily_clean_tmps_dirs=\"/tmp\"                            # Delete under here\ndaily_clean_tmps_days=\"3\"                              # If not accessed for\ndaily_clean_tmps_ignore=\".X*-lock .X11-unix .ICE-unix .font-unix .XIM-unix\"\ndaily_clean_tmps_ignore=\"$daily_clean_tmps_ignore quota.user quota.group\"\n                                                        # Don't delete these\ndaily_clean_tmps_verbose=\"YES\"                          # Mention files deleted\n{% endcodeblock %}\n\nI went to line 4 and changed that 3 to a 90. Now <a href=\"http://www.westerndevs.com/bios/kyle_baley/\">Kyle</a> won't be disappointed in me again. Well not disappointed about this. ","categories":[],"tags":[]},{"title":"Doing Snapshots on Azure Virtual Machines","authorId":"dylan_smith","slug":"azure-snapshots","date":"2015-11-04 06:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-snapshots/","link":"","permalink":"https://westerndevs.com/_/azure-snapshots/","excerpt":"I've always been frustrated that I can't do snapshots when I'm using VM's in Azure. Especially when I'm developing some deployment automation, I like to be able to try something out - and when it inevitably screws up - reset the VM to a snapshot and try again.","raw":"---\nlayout: post\ntitle:  \"Doing Snapshots on Azure Virtual Machines\"\ndate: 2015-11-03T17:30:00-08:00\ncomments: true\nauthorId: dylan_smith\n---\n\nI've always been frustrated that I can't do snapshots when I'm using VM's in Azure. Especially when I'm developing some deployment automation, I like to be able to try something out - and when it inevitably screws up - reset the VM to a snapshot and try again. \n\n<!--more-->\n  \nWe still can't do actual snapshots, but I've written some powershell that achieves the same goal.  It will grab a copy of the VHD file (this acts as the snapshot), then when we want to restore to snapshot we'll just swap out the VHD's.  The trick here is that Azure won't let you swap out the VHD for an existing VM, so we need to actually destroy the VM, swap out the VHD's, then recreate the VM using the existing VHD.\n\n> Note: This is all done using ARM (Azure Resource Manager) style VM's.\n\nIf you want to try it out, here's a step-by-step to try it out:\n\nFirst create a VM in Azure, and be sure to select Resource Manager as the deployment model:\n\n![Create ARM VM](http://i.imgur.com/waHMXxG.png)\n\n![VM Basics](http://i.imgur.com/601dKeE.png)\n\n![Select VM Size](http://i.imgur.com/EjdOOxL.png)\n\n![VM Settings](http://i.imgur.com/05cF86c.png)\n\nAfter it's done processing we'll have a new Resource Group with 6 resources included:\n\n![Resource Group](http://i.imgur.com/QKLize8.png)\n\nOne thing I like to do - that the wizard doesn't let you specify - is to assign a DNS name to the public IP that I'll use to connect to my VM.  You can set this in the configuration page for the Public IP resource.\n\n![Set DNS](http://i.imgur.com/psVa4XE.png)\n\nLastly, I need to copy the Storage Account Access Key for use in the powershell.\n\n![Retrieve Storage Key](http://i.imgur.com/yh9cgcp.png)\n\nNow all we need to do is take the powershell below, and modify the values of the variables at the start to match the names you used when you created the VM/Resource Group.\n\nThe first time you run the script it will create the initial snapshot.  All subsequent times you run the script it will reset it to that snapshot.\n\n{% codeblock lang:powershell %}\n# Set variable values\n$resourceGroupName = \"DylanDemo\"\n$location = \"West US\"\n$vmName = \"DylanDemo\"\n$vmSize = \"Standard_D1\"\n$vnetName = \"DylanDemo\"\n$nicName = \"dylandemo665\"\n$dnsName = \"dylandemo\"\n$diskName = \"DylanDemo\"\n$storageAccount = \"dylandemo\"\n$storageAccountKey = \"Enter Storage Key Here\"\n$subscriptionName = \"MSDN MPN\"\n$publicIpName = \"DylanDemo\"\n\n$diskBlob = \"$diskName.vhd\"\n$backupDiskBlob = \"$diskName-backup.vhd\"\n$vhdUri = \"https://$storageAccount.blob.core.windows.net/vhds/$diskBlob\"\n$subnetIndex = 0\n\n# login to Azure\nAdd-AzureAccount\nSelect-AzureSubscription -SubscriptionName $subscriptionName\nSwitch-AzureMode AzureResourceManager\n\n# create backup disk if it doesn't exist\nStop-AzureVM -ResourceGroupName $resourceGroupName -Name $vmName -Force -Verbose\n\n$ctx = New-AzureStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageAccountKey\n$blobCount = Get-AzureStorageBlob -Container vhds -Context $ctx | where { $_.Name -eq $backupDiskBlob } | Measure | % { $_.Count }\n\nif ($blobCount -eq 0)\n{\n  $copy = Start-AzureStorageBlobCopy -SrcBlob $diskBlob -SrcContainer \"vhds\" -DestBlob $backupDiskBlob -DestContainer \"vhds\" -Context $ctx -Verbose\n  $status = $copy | Get-AzureStorageBlobCopyState \n  $status \n\n  While($status.Status -eq \"Pending\"){\n    $status = $copy | Get-AzureStorageBlobCopyState \n    Start-Sleep 10\n    $status\n  }\n}\n\n# delete VM\nRemove-AzureVM -ResourceGroupName $resourceGroupName -Name $vmName -Force -Verbose\nRemove-AzureStorageBlob -Blob $diskBlob -Container \"vhds\" -Context $ctx -Verbose\nRemove-AzureNetworkInterface -Name $nicName -ResourceGroupName $resourceGroupName -Force -Verbose\nRemove-AzurePublicIpAddress -Name $publicIpName -ResourceGroupName $resourceGroupName -Force -Verbose\n\n# copy backup disk\n$copy = Start-AzureStorageBlobCopy -SrcBlob $backupDiskBlob -SrcContainer \"vhds\" -DestBlob $diskBlob -DestContainer \"vhds\" -Context $ctx -Verbose\n$status = $copy | Get-AzureStorageBlobCopyState \n$status \n\nWhile($status.Status -eq \"Pending\"){\n  $status = $copy | Get-AzureStorageBlobCopyState \n  Start-Sleep 10\n  $status\n}\n\n# recreate VM\n$vnet = Get-AzurevirtualNetwork -Name $vnetName -ResourceGroupName $resourceGroupName\n\n$pip = New-AzurePublicIpAddress -Name $publicIpName -ResourceGroupName $resourceGroupName -DomainNameLabel $dnsName -Location $location -AllocationMethod Dynamic -Verbose\n$nic = New-AzureNetworkInterface -Name $nicName -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet.Subnets[$subnetIndex].Id -PublicIpAddressId $pip.Id -Verbose\n$vm = New-AzureVMConfig -VMName $vmName -VMSize $vmSize\n$vm = Add-AzureVMNetworkInterface -VM $vm -Id $nic.Id\n$vm = Set-AzureVMOSDisk -VM $vm -Name $diskName -VhdUri $vhdUri -CreateOption attach -Windows\n\nNew-AzureVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm -Verbose\n{% endcodeblock %}","categories":[],"tags":[]},{"title":"ApiController in ASP.NET 5? Nopesauce","authorId":"james_chambers","slug":"apicontroller-in-asp-net-5-nopesauce","date":"2015-11-03 05:44:15+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/apicontroller-in-asp-net-5-nopesauce/","link":"","permalink":"https://westerndevs.com/_/apicontroller-in-asp-net-5-nopesauce/","excerpt":"If you're developing in ASP.NET Web API you are familiar with the concept of inheriting from the base ApiController class. This class is still around in ASP.NET 5, but it is likely not meant for you to use. Here's why your cheese has moved.","raw":"---\nlayout: post\ntitle:  \"ApiController in ASP.NET 5? Nopesauce\"\ndate: 2015-11-02T19:44:15-05:00\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/11/apicontroller-in-asp-net-5-nopesauce/\n---\n\nIf you're developing in ASP.NET Web API you are familiar with the concept of inheriting from the base ApiController class. This class is still around in ASP.NET 5, but it is likely not meant for you to use.&nbsp; Here's why your cheese has moved.\n\n<!--more-->\n  \n> **TL;DR**: Going forward, you're going to inherit from Controller instead of ApiController, or from nothing at all.\n\n## How We Used to Do It\n\nThis is pretty much the bread and butter of a new controller in an old Web API 2.0 project:\n\n{% codeblock lang:csharp %}\npublic class ValuesController : ApiController\n{\n    [HttpGet]\n    public IEnumerable<string> Get()\n    {\n        return new string[] { \"value1\", \"value2\" };\n    }\n}\n{% endcodeblock %}\n\nNothing really too interesting here. We're inheriting from a base class so we get some methods to leverage for return types, we can access the identity of the user through an IPrincipal and we have an HttpContext available to inspect the request and modify the response.\n\n## How to Do it Now\n\nIn ASP.NET 5 we don't have the ApiController to inherit from, at least not out of the box. Instead we inherit from the Controller class.\n\n{% codeblock lang:csharp %}\npublic class ValuesController : Controller\n{\n    [HttpGet]\n    public IEnumerable<string> Get()\n    {\n        return new string[] { \"value1\", \"value2\" };\n    }\n}\n{% endcodeblock %}\n\nPretty easy, right? We actually have three less characters. Some pieces have moved around such as Request and Response objects that live as properties at the class level, and our User is now a ClaimsPrincipal instead of an IPrincipal. You'll also find that there's a host of other things that do not seem really relevant at first glance to Web API (things like the service resolver and TempData).\n\nThese extra bits are peripheral, however; the takeaway is actually that we no longer have two separate sets of classes that represent concerns like controllers or routing, and we can go about getting at the important parts of the request in the same way from both types of controllers â€“ there really is just one now.\n\n## If You Still Want to Do It Now How We Used To\n\nThere are perfectly good reasons to keep using the old format, perhaps you're at the start of a port project or some have some other reason to stay as-was. No problem, you're just going to have to pull in another package as it's not part of your project template by default. Simply edit your project.json to include the following package:\n\n    Microsoft.AspNet.Mvc.WebApiCompatShim\n\nWhile this _is_ here and you _can_ use it, it's also likely a good time to evaluate if you _need_ to use it. There are only a small set of refactorings that are required in order to use the unified interface and you can be\n\n## Next Steps\n\nMake sure you've got [Visual Studio 2015][1], you have [the latest beta installed][2] (at time of writing, beta 8), and give it a try. Happy coding ![Smile][3]\n\n[1]: https://www.visualstudio.com/?Wt.mc_id=DX_MVP4038205\n[2]: http://docs.asp.net/en/latest/getting-started/installing-on-windows.html\n[3]: http://jameschambers.com/wp-content/uploads/2015/11/wlEmoticon-smile.png\n","categories":[],"tags":[]},{"title":"Markdown in Your MVC 6 Razor Pages","authorId":"dave_paquette","slug":"markdown-in-your-mvc-6-razor-pages","date":"2015-11-03 03:02:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/markdown-in-your-mvc-6-razor-pages/","link":"","permalink":"https://westerndevs.com/_/markdown-in-your-mvc-6-razor-pages/","excerpt":"What? Markdown in your Razor code? Yeah...and it was totally easy to build too. Taylor Mullen demoed the idea of a Markdown Tag Helper idea at Orchard Harvest and I thought it would be nice to include this in my Tag Helper Samples project.","raw":"---\nlayout: post\ntitle:  Markdown in Your MVC 6 Razor Pages\ndate: 2015-11-02T17:02:42-05:00\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/11/02/markdown-in-your-mvc-6-razor-pages.aspx\n---\n\nWhat? Markdown in your Razor code? Yeah...and it was totally easy to build too.\n\n[Taylor Mullen][1] demoed the idea of a [Markdown Tag Helper][2] idea at Orchard Harvest and I thought it would be nice to include this in my [Tag Helper Samples project][3].\n\n<!--more-->\n\n## How to use it\n\nThis tag helper allows you to write Markdown directly in Razor and have that automatically converted to HTML at runtime. There are 2 options for how to use this tag helper. The first option is to use a `<markdown>` element.\n\n```\n<code><markdown>This is some _simple_ **markdown**.</markdown></code>\n```\n\nThe tag helper will take this and convert it to the following HTML:\n\n{% codeblock lang:html %}\n<p>This is some <em>simple</em> <strong>markdown</strong>.</p>\n{% endcodeblock %}\n\nThe other option is to use a &lt;p&gt; element that has the _markdown_ attribute:\n\n{% codeblock lang:html %}\n<p markdown=\"\">This is some _simple_ **markdown** in a _p_ element.</p>\n{% endcodeblock %}\n\nThe tag helper uses [MarkdownSharp][4], which supports most of the [markdown syntax supported by Stack Overflow][5].\n\n## How it works\n\nThe implementation of this tag helper is surprisingly simple. All we do is grab the contents of the tag and use MarkdownSharp to convert that to HTML.\n\n{% codeblock lang:csharp %}\n[HtmlTargetElement(\"p\", Attributes = \"markdown\")]\n[HtmlTargetElement(\"markdown\")]\n[OutputElementHint(\"p\")]\npublic class MarkdownTagHelper : TagHelper\n{\n    public async override Task ProcessAsync(TagHelperContext context, TagHelperOutput output)\n    {\n        if (output.TagName == \"markdown\")\n        {\n        output.TagName = \"p\";\n        }\n        var childContent = await context.GetChildContentAsync();\n        var markdownContent = childContent.GetContent();\n        var markdown = new MarkdownSharp.Markdown();\n        var htmlContent = markdown.Transform(markdownContent);\n        output.Content.SetContentEncoded(htmlContent);\n    }\n}\n{% endcodeblock %}\n\nYou can grab the code from [GitHub][3] or install the package using [Nuget][6].\n\n> Install-Package TagHelperSamples.Markdown\n\nGive it a try and let me know what you think.\n\n[1]: https://twitter.com/ntaylormullen\n[2]: https://www.youtube.com/watch?v=jD4H-CBab9o\n[3]: https://github.com/dpaquette/TagHelperSamples\n[4]: https://code.google.com/p/markdownsharp/\n[5]: http://stackoverflow.com/editing-help\n[6]: https://www.nuget.org/packages/TagHelperSamples.Markdown\n","categories":[],"tags":[]},{"title":"Mobile App Testing on 114 Devices in 20 Minutes","authorId":"lori_lalonde","slug":"mobile-app-testing-on-114-devices-in-20-minutes","date":"2015-11-03 02:52:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/mobile-app-testing-on-114-devices-in-20-minutes/","link":"","permalink":"https://westerndevs.com/_/mobile-app-testing-on-114-devices-in-20-minutes/","excerpt":"My day started off just like any other at the office. I plugged in my machine, launched Visual Studio and opened up the latest Xamarin.Android project I had been working on for the client. On this particular day, I had to make a layout change to ensure that the weighting of two different layouts were updated based on a set of conditions. Sounds easy enough, right?","raw":"---\nlayout: post\ntitle:  \"Mobile App Testing on 114 Devices in 20 Minutes\"\ndate: 2015-11-02T16:52:05-05:00\ncomments: true\nauthorId: lori_lalonde\noriginalurl: http://solola.ca/mobile-app-testing-xamarin-test-cloud/\n---\n\nMy day started off just like any other at the office. I plugged in my machine, launched Visual Studio and opened up the latest Xamarin.Android project I had been working on for the client. On this particular day, I had to make a layout change to ensure that the weighting of two different layouts were updated based on a set of conditions. Sounds easy enough, right?\n\n<!--more-->\n\nWell making the change was trivial. Next came the part that I dreaded, which I knew would take up the rest of my day. I walked over to the device cabinet, and grabbed a handful of devices varying in screen size, resolution and OS versions.\n\nSeated at my desk, I attempted to power on the first device. Battery drain. I plugged it in, then attempted to power on the next device. Same thing. By the time I was able to get enough charge on a device to power it on, deploy to that device, and run through the necessary tests, 10 â€“ 15 minutes had passed.&nbsp; Note that I have to repeat this process on 4 more devices. If the tests all pass the first time, that's about an hour of testing spent on a small fraction of devices for a single UI change. During my round of testing on the 4th device, I already noticed problems with the layout. This is going to be a really long day.\n\nEven if all of my tests pass, QA will perform these tests on another set of devices at random which may uncover other issues. Even worse, what happens if the QA tests passed, and the application made it to market because there were issues on devices we hadn't covered? This wasn't the type of work I wanted to deliver. Ultimately, I want to be sure the layouts look good across as many devices as possible, including the most popular ones on the market.\n\nNow what?\n\n### A Better Way\n\nBeing out of office for the next week, I wanted to get this change completed to ensure it wouldn't hold the team up in my absence, and I wanted to feel confident that it would look good on a broad range of devices. I knew this would be an impossible feat if I stuck with the manual testing route. I decided to put&nbsp;[Xamarin Test Cloud][1]&nbsp;to use to make short work of this.\n\nThanks to those Xamarin University's courses on creating Xamarin UI Tests and deploying to Test Cloud â€“ which I had already taken as a requirement for the&nbsp;Xamarin Mobile Developer Certification&nbsp;exam â€“&nbsp; I knew exactly how to get started. I also had some Test Cloud time to burn which comes with being a&nbsp;[Xamarin University][2]&nbsp;student â€“ 25 hours to be exact.\n\nI created a new&nbsp;[Xamarin UITest][3]&nbsp;project, and used&nbsp;[REPL][4]&nbsp;to assist with scripting out the steps. I copied the script into my UITest, and I included commands to take screenshots at specific points in the script. This process took about 10 minutes to complete and test on a local device.\n\nOnce I was confident that the UITest I had scripted performed as I expected, I decided to run it in Xamarin Test Cloud, simply by right-clicking on the UITest project in Visual Studio and selecting \"Run in Test Cloudâ€¦\" from the context menu. I was able to quickly and easily select 25 devices to run the test on, all of varying screen sizes, resolutions and OS versions. I was even able to select from devices that were most popular on the market, thanks to a filter provided within Xamarin Test Cloud.\n\n![Xamarin Test Cloud Device Selection][5]\n\nWithin minutes, my test was deployed to the cloud, and 10 minutes later the test had been completed on all 25 devices. This consumed about 35 minutes of Xamarin Test Cloud device time usage (approx. 1.4 minutes per device). Based on the results, I had to tweak the layout and re-run the test two more times until the change looked good across the board, but this process was simple, fast and efficient. Now that I&nbsp;knew how fast it was to test on 25 devices, I decided to push it a little further. The next run was on 114&nbsp;devices. Twenty minutes later my changes were ready to review. Again, the total device time used averaged out to 1.4 minutes per device.\n\nReviewing the UI change across all&nbsp;devices was easy, because they were laid out on a single page in grid format. It was easy to scroll down the page and quickly point out any nuances which needed to be addressed. I could even filter and sort the view to quickly see the devices of interest.  \n\n![XamarinTestCloudReviewTests][6]\n\n_Test run results of the Xamarin Store Demo&nbsp;app_\n\nWhat could have been 1 â€“ 2 days of effort for a simple UI change, was completed in less than 2 hours, and I had covered more ground in those 2 hours than I ever would have in manual testing.\n\nIn addition to that, Xamarin Test Cloud provided metrics on the application's CPU and memory usage, as well as device and test logs. The history of all tests run are maintained in my account so I can go back to review them at any time.\n\n### Lessons Learned\n\nThat was the scenario for the effort needed to test a simple UI change. Now compound that effort with a greenfield application, which requires the team to fully test each screen and feature on as many devices as possible. Expecting that a mobile development team and QA will be able to cover ground through manual device testing is pretty far-fetched. Not to mention the costs the company would have to absorb to continue to purchase new devices as they are released in order to be able to stay on top of a changing market.\n\nMany companies balk at the&nbsp;[price][7]&nbsp;of Xamarin Test Cloud without considering how much money is being wasted on the current processes in place.\n\nAs the company's team continues to build out the organization's flagship mobile app and releases multiple versions, the QA Team becomes swamped and unable to test fast enough to cover ground on all those devices in that trusty cabinet. Developers barely have enough time in the schedule to fully develop the features needed, let alone allot the actual time needed to fully test across devices.\n\nEventually, the company considers hiring a new QA member to ease the work load. Do you think it is possible to hire someone into that role at $5/hour? No? Well, that's the cost of Xamarin Test Cloud â€“ $5 per device hour when you break it down. Considering I was able to run a single UI test across 114&nbsp;devices using about 2.5 hours of device time, that would work out to be the best $12.50 ever spent. Not to mention, that's also 1 full day of developer time that is reclaimed to focus on the actual development of the product, rather than fiddling around with device after device to run manual UI tests. Last but not least, your QA team will be more focused on testing the application's functionality rather than being bogged down with logging UI related issues.\n\nIf you're still skeptical at how Xamarin Test Cloud can help accelerate your mobile app testing time, make use of the&nbsp;[free 60 minutes of Xamarin Test Cloud time][8]&nbsp;you're given each month with your&nbsp;Xamarin Platform Subscription.\n\n### Final Thoughts\n\nSpend a little, save a lot, and keep your mobile product team happy in the process.\n\n\n[1]: https://developer.xamarin.com/guides/testcloud/introduction-to-test-cloud/\n[2]: https://xamarin.com/university\n[3]: https://developer.xamarin.com/guides/testcloud/uitest/\n[4]: https://developer.xamarin.com/guides/testcloud/uitest/working-with/repl/\n[5]: http://solola.ca/wp-content/uploads/2015/11/XamarinTestCloudDeviceSelection.png\n[6]: http://solola.ca/wp-content/uploads/2015/11/XamarinTestCloudReviewTests.png\n[7]: https://xamarin.com/test-cloud#pricing\n[8]: https://www.xamarin.com/test-cloud  \n","categories":[],"tags":[]},{"title":"My Favourite Online Dev Tools","authorId":"david_wesst","slug":"my-favourite-online-dev-tools","date":"2015-10-28 12:44:09+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/my-favourite-online-dev-tools/","link":"","permalink":"https://westerndevs.com/_/my-favourite-online-dev-tools/","excerpt":"I'm talking about online or web-based tools, not toolboxes. So entire development suites like Cloud9 or Visual Studio Online are off the table because they are full tool suites. These tools are simple, but continually help me out in a pinch.","raw":"---\nlayout: post\ntitle: My Favourite Online Dev Tools\ndate: 2015-10-28 08:44:09\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/\n---\n\nI'm talking about online or web-based tools, not toolboxes. So entire development suites like Cloud9 or Visual Studio Online are off the table because they are full tool suites. These tools are simple, but continually help me out in a pinch.\n\n<!--more-->\n  \nMake sure to share your favourite tools in the comments below so I can learn and grow from your knowledge. \n\n## [JSONLint](http://jsonlint.com)\n\n![http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/jsonlint.png](http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/jsonlint.png)\n\nFor the past few weeks, I've been writing and debugging a REST API client written in Java, where my POJO is being serialized into JSON and sent off in a request. I hit a few bugs where the JSON that was being generated wasn't 100% valid. This made validating it easy, and I keep it open whenever I'm dealing with any JSON that isn't continually being edited in VS Code.\n\nAlso, I missed that there was [pro.jsonlint.com](http://pro.jsonlint.com/) which provides a diff function. This would have saved me a ton of time, had I read the description provided on the main page. \n\n## [Regexr](http://www.regexr.com/)\n\n![http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/regexr.png](http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/regexr.png)\n\nBeing a JavaScript person, you'd think I'd be a pro with regular expressions. Thanks to [Regexr.com](http://regexr.com) I can at least appear to be. Between the reference guide on the left side, real-time match highlighting, I really like this tool and it has made learning and using regexes in my day-to-day very easy\n\nPlus, the slick design doesn't hurt either.\n\n## [Colorrrs](http://hex.colorrrs.com/)\n\n![http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/colorrrs.png](http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/colorrrs.png)\n\nThere are other HEX to RGB converters out there, I know. I just really like this one because it gives me a full screen preview of the colour I'm look at, and it's a pretty website.\n\nI like pretty websites.\n\n## [SmartView](http://smartviewapp.com/)\n\n![http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/smartviewapp.png](http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/smartviewapp.png)\n\nNow we're getting into the protein part of our meal.\n\nI've used Trello in the past, and generally end up ditching the project boards because I'm not a Kanban or Agile professional. Lucky for me, my fellow Western Dev [Amir Barylko](http://www.westerndevs.com/bios/amir_barylko/) is a professional and has used is expertise to guide me along the path and provides me with some metrics on my throughput.\n\nThey start you off with a free account, which is where I began for a personal kanban for coordinating my side project efforts for myself. I suggest you check it out and get productive.\n\n_**Full Disclosure**:  As mentioned above, Amir is a colleague of mine and I do support his work. That being said, I wouldn't have added it to my list if I didn't use it as much as I do, or if I found it as useful as it really is._ \n\n## [GitHub Explore](https://github.com/explore) and [GitHub Gists](https://gist.github.com/)\n\n![http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/github-gist.png](http://blog.davidwesst.com/2015/10/My-Favourite-Online-Dev-Tools/github-gist.png)\n\nLast, but not least, I use [GitHub Explore](https://github.com/explore) and [GitHub Gists](https://gist.github.com/) to explore and find code that can help guide me through solving problems, or by finding open source software and/or libraries that can help solve a problem for me.\n\nWhy re-invent the wheel when someone else has lead a team to and refined a solution to your problem already? At the very least, you can learn from their code and figure out how it applies to you're trying to do.\n\n---\nThanks for playing. ~ DW","categories":[],"tags":[]},{"title":"Running a .NET app against a Postgres database in Docker","authorId":"kyle_baley","slug":"running-a-net-app-against-postgres-database-in-docker","date":"2015-10-25 21:24:14+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/running-a-net-app-against-postgres-database-in-docker/","link":"","permalink":"https://westerndevs.com/_/running-a-net-app-against-postgres-database-in-docker/","excerpt":"Some days/weeks/time ago, I did a presentation at MeasureUP called &quot;Docker For People Who Think Docker Is This Weird Linux Thing That Doesn't Impact Me&quot;. The slides for that presentation can be found here and the sample application here.","raw":"---\nlayout: post\ntitle:  \"Running a .NET app against a Postgres database in Docker\"\ndate: 2015-10-25T13:24:14-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\nSome days/weeks/time ago, I did a presentation at MeasureUP called \"Docker For People Who Think Docker Is This Weird Linux Thing That Doesn't Impact Me\". The slides for that presentation can be found [here](http://www.slideshare.net/KyleBaley/docker-for-people-who-have-heard-of-docker-but-think-its-just-this-weird-linux-thing-that-doesnt-impact-me) and the sample application [here](https://github.com/stimms/AzureCodeCamp).\n\n<!--more-->\n  \n### Using the sample app with PostgreSQL\n\nThe sample application is just a plain ol' .NET application. It is meant to showcase different ways of doing things. One of those things is data access. You can configure the app to access the data from SQL storage, Azure table storage, or in-memory. By default, it uses the in-memory option so you can clone the app and launch it immediately just to see how it works.\n\n![PancakeProwler](http://i.imgur.com/xeKON0u.png) \n\nQuick summary: Calgary, Alberta hosts an annual event called the [Calgary Stampede](http://www.calgarystampede.com/). One of the highlights of the 10-ish day event is the pancake breakfast, whereby dozens/hundreds of businesses offer up pancakes to people who want to eat like the pioneers did, assuming the pioneers had pancake grills the size of an Olympic swimming pool.\n\nThe sample app gives you a way to enter these pancake breakfast events and each day, will show that day's breakfasts on a map. There's also a recipe section to share pancake recipes but we won't be using that here.\n\nTo work with Docker we need to set the app up to use a data access mechanism that will work on Docker. The sample app supports Postgres so that will be our database of choice. Our first step is to get the app up and running locally with Postgres *without* Docker. So, assuming you have Postgres installed, find the `ContainerBuilder.cs` file in the `PancakeProwler.Web` project. In this file, comment out the following near the top of the file:\n\n{% codeblock lang:csharp %}\n// Uncomment for InMemory Storage\nbuilder.RegisterAssemblyTypes(typeof(Data.InMemory.Repositories.RecipeRepository).Assembly)\n       .AsImplementedInterfaces()\n       .SingleInstance();\n{% endcodeblock %}\n\nAnd uncomment the following later on:\n\n{% codeblock lang:csharp %}\n// Uncomment for PostgreSQL storage\nbuilder.RegisterAssemblyTypes(typeof(PancakeProwler.Data.Postgres.IPostgresRepository).Assembly)\n    .AsImplementedInterfaces().InstancePerRequest().PropertiesAutowired();\n{% endcodeblock %}\n\nThis configures the application to use Postgres. You'll also need to do a couple of more tasks:\n\n* Create a user in Postgres\n* Create a Pancakes database in Postgres\n* Update the `Postgres` connection string in the web project's `web.config` to match the username and database you created\n\nThe first two steps can be accomplished with the following script in Postgres:\n\n{% codeblock lang:sql %}\nCREATE DATABASE \"Pancakes\";\n\nCREATE USER \"Matt\" WITH PASSWORD 'moo';\n\nGRANT ALL PRIVILEGES ON DATABASE \"Pancakes\" TO \"Matt\";\n{% endcodeblock %}\n\nSave this to a file. Change the username/password if you like but be aware that the sample app has these values hard-wired into the connection string. Then execute the following from the command line:\n\n    psql -U postgres -a -f \"C:\\path\\to\\sqlfile.sql\"\n\nAt this point, you can launch the application and create events that will show up on the map. If you changed the username and/or password, you'll need to update the Postgres connection string first.\n\nYou might have noticed that you didn't create any tables yet but the app still works. The sample is helpful in this regard because all you need is a database. If the tables aren't there yet, they will be created the first time you launch the app.\n\n> Note: recipes rely on having a search provider configured. We won't cover that here but I hope to come back to it in the future.\n\nNext, we'll switch things up so you can run this against Postgres running in a Docker container.\n\n### Switching to Docker\n\nI'm going to give away the ending here and say that there is no magic. Literally, all we're doing in this section is installing Postgres on another \"machine\" and connecting to it. The commands to execute this are just a little less click-y and more type-y.\n\nThe first step, of course, is installing Docker. At the time of writing, this means installing [Docker Machine](http://docs.docker.com/windows/started/). \n\nWith Docker Machine installed, launch the Docker Quickstart Terminal and wait until you see an ASCII whale:\n\n![Docker Machine](http://i.imgur.com/UOgoWfK.png)\n\nIf this is your first time running Docker, just know that a lightweight Linux virtual machine has been launched in VirtualBox on your machine. Check your Start screen and you'll see VirtualBox if you want to investigate it but the `docker-machine` command will let you interact with it for many things. For example:\n\n    docker-machine ip default\n\nThis will give you the IP address of the default virtual machine, which is the one created when you first launched the Docker terminal. Make a note of this IP address and update the Postgres connection string in your web.config to point to it. You can leave the username and password the same:\n\n{% codeblock lang:xml %}\n<add name=\"Postgres\" connectionString=\"server=192.168.99.100;user id=Matt;password=moo;database=Pancakes\" providerName=\"Npgsql\" />\n{% endcodeblock %}\n\nNow we're ready to launch the container:\n\n    docker run --name my-postgres -e POSTGRES_PASSWORD=moo -p 5432:5432 -d postgres`\n\nBreaking this down:\n\n<style>\n    .docker-table+table td {\n        padding: 8px;   \n        border: 1px solid #ccc;\n    }\n    .docker-table+table td:nth-child(1) {\n        width:220px;\n    }\n    .docker-table+table tr:nth-child(even) td {\n        background-color: #eee;\n    }\n</style>\n\n<div class=\"docker-table\"></div>\n\n|   |   |\n|---|---|\n| `docker run` | Runs a docker container from an image |\n| `--name my-postgres` | The name we give the container to make it easier for us to work with. If you leave this off, Docker will assign a relatively easy-to-remember name like \"floral-academy\" or \"crazy-einstein\". You also get a less easy-to-remember identifier which works just as well but is...less...easy-to-remember |\n| `-e POSTGRES_PASSWORD=moo` | The `-e` flag passes an environment variable to the container. In this case, we're setting the password of the default postgres user |\n| `-p 5432:5432` | Publishes a port from the container to the host. Postgres runs on port 5432 by default so we publish this port so we can interact with Postgres directly from the host|\n| `-d` | Run the container in the background. Without this, the command will sit there waiting for you to kill it manually |\n| `postgres` | The name of the image you are creating the container from. We're using the [official postgres image](https://hub.docker.com/_/postgres/) from Docker Hub. |\n\nIf this is the first time you've launched Postgres in Docker, it will take a few seconds at least, possibly even a few minutes. It's downloading the Postgres image from Docker Hub and storing it locally. This happens only the first time for a particular image. Every subsequent postgres container you create will use this local image.\n\nNow we have a Postgres container running. Just like with the local version, we need to create a user and a database. We can use the same script as above and a similar command:\n\n    psql -h 192.168.99.100 -U postgres -a -f \"C:\\path\\to\\sqlfile.sql\"\n\nThe only difference is the addition of `-h 192.168.99.100`. You should use whatever IP address you got above from the `docker-machine ip default` command here. For me, the IP address was 192.168.99.100.\n\nWith the database and user created, and your web.config updated, we'll need to stop the application in Visual Studio and re-run it. The reason for this is that the application won't recognize that we've changed database so we need to \"reboot\" it to trigger the process for creating the initial table structure.\n\nOnce the application has been restarted, you can now create pancake breakfast events and they will be stored in your Docker container rather than locally. You can even launch pgAdmin (the Postgres admin tool) and connect to the database in your Docker container and work with it like you would any other remote database.\n\n### Next steps\n\nFrom here, where you go is up to you. The sample application can be configured to use [Elastic Search](https://www.elastic.co/) for the recipes. You could start an Elastic Search container and configure the app to search against that container. The principle is the same as with Postgres. Make sure you open both ports 9200 and 9300 and update the `ElasticSearchBaseUri` entry in `web.config`. The command I used in the presentation was:\n\n    docker run --name elastic -p 9200:9200 -p 9300:9300 -d elasticsearch\n\nI also highly recommend Nigel Poulton's [Docker Deep Dive](http://www.pluralsight.com/courses/docker-deep-dive) course on Pluralsight. You'll need access to Linux either natively or in a VM but it's a great course.\n\nThere are also a number of posts right here on Western Devs, including an [intro to Docker for OSX](http://www.westerndevs.com/docker/yet-another-docker-intro/), tips on [running Docker on Windows 10](http://www.westerndevs.com/getting-docker-running-on-windows-10/), and a summary or two on a discussion [we had on it internally](http://www.westerndevs.com/westerndevs-learn-about-docker-part-2/).\n\nOther than that, Docker is great for experimentation. Postgres and Elastic Search are both available pre-configured in Docker on Azure. If you have access to Azure, you could spin up a Linux VM with either of them and try to use that with your application. Or look into Docker Compose and try to create a container with both.\n\nFor my part, I'm hoping to convert the sample application to ASP.NET 5 and see if I can get it running in a Windows Server Container. I've been saying that for a couple of months but I'm putting it on the internet in an effort to make it true.","categories":[],"tags":[]},{"title":"Using Java Build Script Tasks in Visual Studio Code","authorId":"david_wesst","slug":"using-java-build-script-tasks-in-visual-studio-code","date":"2015-10-21 12:18:19+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/using-java-build-script-tasks-in-visual-studio-code/","link":"","permalink":"https://westerndevs.com/_/using-java-build-script-tasks-in-visual-studio-code/","excerpt":"I previously shared how I setup a custom problem matching in Visual Studio Code for compiling Java and displaying the errors inline with a custom problem matcher.","raw":"---\nlayout: post\ntitle: Using Java Build Script Tasks in Visual Studio Code\ndate: 2015-10-21 08:18:19\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/\n---\n\nI [previously shared](http://www.westerndevs.com/custom-tasks-for-java-in-visual-studio-code/) how I setup a custom problem matching in Visual Studio Code for compiling Java  and displaying the errors inline with a custom problem matcher.\n\n<!--more-->\n  \nThe shortcoming with [Tasks](https://code.visualstudio.com/docs/editor/tasks) was that you could only define one, which is (in my humble opinion) by design to help developers by forcing them to create a build script rather than setting up tasks, as a build script is not coupled to the IDE and can be used elsewhere.\n\nCode has support for build systems like Grunt, Gulp, and Jake, but what if we want to handle something totally different, like use Gradle on a Java project.\n\nHow do you do that? Good question. Let me show you.\n\n### Define a Build Script\nI've setup a standard Gradle project by using the ```gradle init``` and created a class with a main method named ```HelloLink```.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/hellolink-file.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/hellolink-file.png)\n\nNext up, I want to create a custom build task. We'll call our task ```getSword```.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-project.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-project.png)\n\nNow, we need to do the fun part: hook it into Code. \n\n### Add a Task to VS Code\nIf we take a page out of [my previous post](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/), we need so to setup our ```tasks.json``` file. Do that by pressing ```Ctrl + Shift + P``` and searching for \"task\" and select for _Configure Task Runner_.\n\nLet's add gradle as our task, which should leave our tasks file looking like:\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-task.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-task.png)\n\nWhen we run it, by pressing ```Ctrl + P``` and entering ```task gradle``` we should get the output window displaying our gradle command.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-task-output.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-task-output.png)\n\nAnd now we're back to where we started, but gradle (like any build system) has way more tasks than just my custom one. How do I gain access to those?\n\nSimple: you add tasks to your gradle task.\n\n### Adding a Task to your Task\nIf you haven't already seen it, you have intellisense while you're editing your ```tasks.json```. If you browse through the multitude of JSON properties, you'll find one called _tasks_ which is where we can define our gradle specific tasks.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/tasks-intellisense.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/tasks-intellisense.png)\n\nWe define, what I call a subtask, just like we do a regular task, except the JSON schema is a little different. After adding a task for ```getSword``` and , my tasks file now looks like:\n\n{% codeblock lang:javascript %}\n{\n\t\"version\": \"0.1.0\",\n\t\"command\": \"gradle\",\n\t\"isShellCommand\": true,\n\t\"args\": [],\t// no args, but we could add some\n\t\"tasks\": [\n\t\t{\n\t\t\t\"taskName\": \"getSword\",\n\t\t\t\"showOutput\": \"always\",\n\t\t\t\"echoCommand\": true\n\t\t}\n\t]\t\n}\n{% endcodeblock %}\n\nGreat, but so far we haven't done anything actually useful. Let's apply what we've learned here and from [my previous post](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/) and get a useful command setup in Code.\n\n### Doing Something Real\nSo, not only do we want a task to call custom tasks, but we'll probably want one to compile and run our Java too. The [gradle application](https://docs.gradle.org/current/userguide/application_plugin.html) provides us with a ```run``` command that we'll call as our default build command, which will give us the _F5_ experience we know from our big, bloated, buddy, Visual Studio proper.\n\nJust like we did with the ```javac``` command, we need to add a problem matcher to our task. \n\nOur tasks file now looks like:\n\n{% codeblock lang:javascript %}\n{\n\t\"version\": \"0.1.0\",\n\t\"command\": \"gradle\",\n\t\"isShellCommand\": true,\n\t\"args\": [\"\"], \n\t\"tasks\": [\n\t\t{\n\t\t\t\"taskName\": \"getSword\",\n\t\t\t\"showOutput\": \"always\",\n\t\t\t\"echoCommand\": true\n\t\t},\n\t\t{\n\t\t\t\"taskName\": \"run\",\n\t\t\t\"showOutput\": \"silent\",\n\t\t\t\"isBuildCommand\": true,\n\t\t\t\"problemMatcher\": {\n\t\t\t\t\"owner\": \"external\",\n\t\t\t\t\"fileLocation\": \"absolute\",\n\t\t\t\t\"pattern\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"regexp\": \"^(.+\\\\.java):(\\\\d):(?:\\\\s+(error)):(?:\\\\s+(.*))$\",\n\t\t\t\t\t\t\"file\": 1,\n\t\t\t\t\t\t\"location\": 2,\n\t\t\t\t\t\t\"severity\": 3,\n\t\t\t\t\t\t\"message\": 4,\n\t\t\t\t\t\t\"loop\": true\t// add this to loop through multiple lines\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t}\n\t]\t\n}\n{% endcodeblock %}\n\nAnd if you run it, you should get an error in our source code. Press ```Ctrl + Shift + M``` to see the warning about our static method, and click it to go to the error.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-error.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/gradle-error.png)\n\nAs you probably noticed, we set our ```run``` task, we've set it to be the _Build Command_ so that when we hit ```Ctrl + Shift + B``` it will execute our command. So, once we go and fix our application we can run it with a keyboard shortcut.\n\nYou can open up your output window by hitting ```Ctrl + Shift + U``` and see our application in action.\n\n![http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/complete.png](http://blog.davidwesst.com/2015/10/Using-Java-Build-Script-Tasks-in-Visual-Studio-Code/complete.png)\n\n## The Point\nThis is just one simple example using Gradle, but you could use Maven or Ant, or whatever custom build script tool you want, assuming it has a command line response. \n\nThere are plenty of other things you can configure with tasks, including the OS-specific commands that need to be executed. Take a look at the [JSON Schema](https://code.visualstudio.com/docs/editor/tasks_appendix) for more details.\n\n---\nThanks for playing. ~ DW ","categories":[],"tags":[]},{"title":"Life Outside .NET","slug":"podcast-life-outside-net","date":"2015-10-19 23:48:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-life-outside-net/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-life-outside-net/","excerpt":"The Western Devs discuss working outside your comfort zone","raw":"---\nlayout: podcast\ntitle: \"Life Outside .NET\"\ndate: 2015-10-19T15:48:10-04:00\nrecorded: 2015-10-16\ncategories: podcasts\nexcerpt: \"The Western Devs discuss working outside your comfort zone\"\ncomments: true\npodcast:\n    filename: \"LifeOutsideNET.mp3\"\n    length: \"41:07\"\n    filesize: 39477170\n    libsynId: 3982713\n    anchorFmId: Life-Outside--NET-evqdif\nparticipants:\n    - kyle_baley\n    - lori_lalonde\n    - dave_white\n    - amir_barylko\n    - donald_belcham\n    - dave_paquette\n    - tom_opgenorth\nlinks:\n    - Prairie Dev Con|http://www.prairiedevcon.com/\n    - VI|http://www.vim.org/\n    - Xamarin|https://xamarin.com/\n    - Chocolatey|https://chocolatey.org/\n    - PowerTab|https://powertab.codeplex.com/\n    - ZSH|http://www.zsh.org/\n    - ConEmu|https://github.com/Maximus5/ConEmu\n    - Cygwin|https://www.cygwin.com/\n    - Msysgit|https://git-for-windows.github.io/\n    - Bash|https://en.wikipedia.org/wiki/Bash_(Unix_shell)\n    - Nuget|https://www.nuget.org/\n    - Fake|http://fsharp.github.io/FAKE/\n    - Cake|https://github.com/cake-build/cake\n    - Bake|http://planete.inria.fr/software/bake/index.html\n    - Scala|http://www.scala-lang.org/\n    - Clojure|http://clojure.org/\n    - Erlang|http://www.erlang.org/\n    - IntelliJ|https://www.jetbrains.com/idea/\n    - TeamCity|https://www.jetbrains.com/teamcity/\n    - RubyMine|https://www.jetbrains.com/ruby/\n\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* What if .NET disappeared tomorrow?\n* Programming vs. architecting vs. project management\n* Other technologies in ASP.NET vNext\n* Working outside Windows?\n* Development and infrastructure issues on Windows vs. non-Windows\n* The importance of patience when diving into a new language/OS/technology/tool\n* Becoming familiar with the command line/PowerShell\n* The ease of getting set up with a new environment using a VM\n* Bash-like terminals in Windows\n* Using deployment as a means of learning new tools\n* Working with ASP.NET vNext without Visual Studio\n* Experimenting with your build script and test projects\n* Conferences/user groups/code retreats as a tool for learning\n* Microsoft's new direction: chasing a market or listening to the community?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Refactor vs. Rewrite","slug":"podcast-brownfield-applications","date":"2015-10-15 04:54:55+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-brownfield-applications/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-brownfield-applications/","excerpt":"The Western Devs tackle the eternal debate: do we refactor or rewrite?","raw":"---\nlayout: podcast\ntitle: \"Refactor vs. Rewrite\"\ndate: 2015-10-14T20:54:55-04:00\nrecorded: 2015-10-09\ncategories: podcasts\nexcerpt: \"The Western Devs tackle the eternal debate: do we refactor or rewrite?\"\ncomments: true\npodcast:\n    filename: \"RefactorRewrite.mp3\"\n    length: \"42:40\"\n    filesize: 40960093\n    libsynId: 5292128\n    anchorFmId: Refactor-vs--Rewrite-evqdij\nparticipants:\n    - dylan_smith\n    - kyle_baley\n    - simon_timms\n    - amir_barylko\n    - donald_belcham\n    - david_wesst\nlinks:\n    - That Brownfield Book|https://leanpub.com/thatbrownfieldbook\n    - Evolving ASP.NET Web Applications|https://leanpub.com/evolvinglegacyaspnetapplications\n    - Working Effectively With Legacy Code|http://www.amazon.com/Working-Effectively-Legacy-Michael-Feathers/dp/0131177052\n    - Strangler Application|http://www.martinfowler.com/bliki/StranglerApplication.html\n    - The Boy Scout Rule|http://programmer.97things.oreilly.com/wiki/index.php/The_Boy_Scout_Rule\n    - Fixing a light bulb|http://www.dailymotion.com/video/x2gp98t\nmusic:\n    - title: Doctor Man\n      artist: Johnnie Christie and the Boats\n      url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* NetConfUY\n* Dropping into a brownfield application\n* Rewrite vs. refactor\n* Political factors\n* Setting up a safety net\n* What is correct: What the app does or what it should do?\n* The challenge of limiting scope during refactoring\n* The legacy catch-22: You need tests to refactor but you need to refactor before you can write tests\n* Selling a refactoring to your client\n* Identifying risk to your client\n* The Boy Scout Rule: Leave the code better than you found it\n* The refactoring rabbit hole\n* Using metrics to determine where to focus\n* Finding quick wins\n* Strangler pattern to carve off pieces\n* Ensuring the dev team is on board\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Custom Tasks for Java in Visual Studio Code","authorId":"david_wesst","slug":"custom-tasks-for-java-in-visual-studio-code","date":"2015-10-14 12:25:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/custom-tasks-for-java-in-visual-studio-code/","link":"","permalink":"https://westerndevs.com/_/custom-tasks-for-java-in-visual-studio-code/","excerpt":"Now that I'm a Java Developer, I no longer worry about the bloating feeling I get when I need to open up the original Visual Studio. Now I worry about opening another instance of Eclipse. Don't get me wrong, Visual Studio and Eclipse are both great tools, but there are plenty of times where I don't need to bring a forklift just to move a single box.","raw":"---\nlayout: post\ntitle:  Custom Tasks for Java in Visual Studio Code\ndate: 2015-10-14 08:25:50\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/\n---\n\nNow that I'm a Java Developer, I no longer worry about the bloating feeling I get when I need to open up the original Visual Studio. Now I worry about opening another instance of Eclipse. Don't get me wrong, Visual Studio and Eclipse are both great tools, but there are plenty of times where I don't need to bring a forklift just to move a single box.\n\n<!--more-->\n  \nThis is why I love Visual Studio Code.\n\nThat being said, we don't have native rich support for Java in Code. We _do_ have syntax and bracket matching for Java, I wanted to see if I could compile and possibly display any errors directly in code.\n\nWe do that using [Tasks](https://code.visualstudio.com/docs/editor/tasks), and here's what I did:\n\n### Configure a Task Runner\nTo start you need a task runner for your project, which is where you will...you guessed it...configure tasks.\n\n1. Hit ``Ctrl + Shift + P`` to bring up the command palette. Search for \"task\" and you'll get the _Configure Task Runner_ command. \n2. Select it and press ``Enter`` to generate a tasks.json file.\n\n![http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/configure-task-runner.png](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/configure-task-runner.png)\n\nThe default _tasks.json_ file has some samples along with some reference variables at the top of the file.\n\nNext up, we create our task.\n\n### Configure a Task\nTo see if we have any errors, we want to run our Java compiler on our code to get any error. In the example below, I'm using a simple project with a ```src``` and a ```target``` directory where the ```src``` directory contains all of our Java files.\n\n1. First, clear out the other tasks in the file. We only want the one we're going to use which is our javac command.\n2. Add the task to run the Java compiler\n\n{% codeblock lang:javascript %}\n{\n\t\"version\": \"0.1.0\",\n\t\"command\": \"javac\",\n\t\"showOutput\": \"always\",\n\t\"isShellCommand\": true,\n\t\"args\": [\"-d\",\"${workspaceRoot}\\\\build\\\\classes\",\"${workspaceRoot}\\\\src\\\\main\\\\java\\\\*.java\"]\n}\n{% endcodeblock %}\n\n3. Now, we can run our command by hitting ```Ctrl + Shift + P``` and typing _Run Task_ should show us our new Java task.\n\n![http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/new-javac-task.png](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/new-javac-task.png)\n\n4. Select it to run it, and assuming you have an error, it should display something in the output window.\n\n![http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/javac-output.png](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/javac-output.png)\n\nWe're making progress, but it would be nice to see those errors inline in our code files, wouldn't it? That's where Problem Matchers come in.\n\n### Add a Problem Matcher to your Task\nNow that we have a task running, we need to tell Code how to process the output. We do that using configuring a problem matcher that uses a regular expression to parse the output.\n\nHere's what my task looks like now:\n\n{% codeblock lang:javascript %}\n{\n\t\"version\": \"0.1.0\",\n\t\"command\": \"javac\",\n\t\"showOutput\": \"silent\",\t// changed so that the output window doesn't pop up constantly\n\t\"isShellCommand\": true,\n\t\"args\": [\"-d\",\"${workspaceRoot}\\\\target\",\"${workspaceRoot}\\\\src\\\\*.java\"],\n\t\"problemMatcher\": {\n\t\t\"owner\": \"external\",\n\t\t\"fileLocation\": [\"absolute\"],\n\t\t\"pattern\": [\n\t\t\t{\n\t\t\t\t\"regexp\": \"^(.+\\\\.java):(\\\\d):(?:\\\\s+(error)):(?:\\\\s+(.*))$\",\n\t\t\t\t\"file\": 1,\n\t\t\t\t\"location\": 2,\n\t\t\t\t\"severity\": 3,\n\t\t\t\t\"message\": 4\n\t\t\t}\n\t\t]\n\t}\n}\n{% endcodeblock %}\n\nIf you'd like the full scoop on how this works, you can check out the explanation on the Code documentation on [writing problem matchers](https://code.visualstudio.com/Docs/editor/tasks#_defining-a-problem-matcher) but basically each of the variables listed beneath the ```regexp``` represent a part of the VS code message that should display.\n\nNow that I've updated my task. If we run it again by hitting ```Ctrl + P``` and typing _task javac_ we should get the following:\n\n![http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/error-inline.png](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/error-inline.png)\n\n...and if we bring up our warnings by hitting ```Ctrl + Shift + M``` we should see:\n\n![http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/error-warning.png](http://blog.davidwesst.com/2015/10/Custom-Tasks-for-Java-in-Visual-Studio-Code/error-warning.png)\n\nClick on the warning, should bring you to the file with the error in it.\n\n## Hey, Why Can't I Add More Tasks to tasks.json?\nGreat question! Although I don't have an official answer, here's my opinion: **It's a bad idea**.\n\nThe ```tasks.json``` file is an IDE specific configuration file. All it's really doing to making shortcuts through your IDE to execute different commands from the command line. So rather than setup multiple tasks for each individual command, you should do yourself and your project a favour and setup a build script and use that in your tasks.json file.\n\nA build script will have all the commands and scripts you need to run, build, test, or do whatever to your project. Plus, it's usable _outside_ of Code, so it's much more useful.\n\nAlthough totally customizable (which I'll cover in [an upcoming post](http://westerndevs.com/using-java-build-script-tasks-in-visual-studio-code/)), you can see this action now if you're using [Gulp, Grunt, or Jake](https://code.visualstudio.com/Docs/editor/tasks#_mapping-gulp-grunt-and-jake-output-to-problem-matchers) in your existing projects.\n\n---\nThanks for playing. ~ DW\n\n##### References\n1. [Tasks in Visual Studio Code](https://code.visualstudio.com/Docs/editor/tasks)\n2. [tasks.json Schema reference](https://code.visualstudio.com/docs/editor/tasks_appendix)\n3. [Mapping Gulp, Grunt and Jake Output to Problem Matchers](https://code.visualstudio.com/Docs/editor/tasks#_mapping-gulp-grunt-and-jake-output-to-problem-matchers)","categories":[],"tags":[]},{"title":"Getting Started With ELK using Docker","authorId":"simon_timms","slug":"getting-started-with-elk","date":"2015-10-13 07:56:53+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/getting-started-with-elk/","link":"","permalink":"https://westerndevs.com/_/getting-started-with-elk/","excerpt":"Being able to trace calls through services an be difficult. We need to find some way to gather and aggregate these disparate logs. This is exactly what the ELK stack does.","raw":"---\nlayout: post\ntitle:  Getting Started With ELK using Docker\ndate: 2015-10-12T21:56:53-06:00\ncategories:\nexcerpt: Being able to trace calls through services an be difficult. We need to find some way to gather and aggregate these disparate logs. This is exactly what the ELK stack does.\ncomments: true\nauthorId: simon_timms\noriginalurl:\n---\n\nIn my last post I talked about how I was setting up docker to experiment with [Elastic Search, Logstash and Kibana](https://deviantony.wordpress.com/2014/05/19/centralized-logging-with-an-elk-stack-elasticsearch-logback-kibana/). One of the challenges with microservices is that we have a lot of different processes all over the network. It would not be uncommon to have 30 or 40 services on many different machines. The services could be deployed within virtual machines or even docker containers. The same isolation of services that makes developing and evolving microservices easy makes logging very difficult. Being able to trace calls through services can be difficult and logs on VMs or in containers may disappear as these abstractions are recycled. We need to find some way to gather and aggregate these disparate logs.\n\nThis is exactly what the ELK stack does. Let's look at all the components involved one at a time and then we'll see how to use is.\n\n**Logstash** is the first component: it is a service which can be used to gather data from multiple sources and the perform post processing on it. For instance you might have a log file that contains the user id of a user who made a request against the server. Log stash could have a rule in place to look up the associated user name which is much friendlier for end users. Another example might be resolving an IP address to a geographic location. Logstash can also normalize data from many different sources so it is easier to correlate. You can think of logstash as really being a sewage plant which takes in various sources of garbage(your logs), cleans it and then dumps the consistent clean water into holding tanks. The holding tank can be anything but in our case it will be Elasticsearch.\n\nThat brings us to **Elasticsearch**.  You may have heard of Elasticsearch as a general purpose search engine. It is a service which indexes large number of documents and provides an interface for searching over them. It can be used for almost anything and many people use it as a replacement for a database driving their entire application off search. In this case it is used to hold logs piped in from Logstash. This is an innovative idea. Normally log files become an inaccessible dumping ground for all sorts of data.  You only open up the logs when there is a problem discovered through other means. It is hard to find the one entry that you really need or to extract trends. Keeping the log data in a search engine really opens up the data for investigation.\n\nBuild on top of the Elasticsearch database is **Kibana**. Kibana is a data visualizaiton tool combined with an interface for Elasticsearch's indexes. You can use Kibana to search for logs, to visualize the data or to do investigations of issues.\n\nThe one component I haven't mentioned here is a way to get your log data from various different machines into your centralized logstash. You applications can write to logstash directly but there is always the possibility that logstash will be down and then you need to write a bunch of retry logic yourself. No fun! Instead it is better to simply write you log files locally and then ship the log files off to logstash. You can use logstash itself for that but logstash is built on top of the JVM which is kind of inefficient and means you have to install the JVM onto all you machines. Yuck. Instead you can use one of a number of standalone tools for shipping the logs. I like one which was called Lumberjack and is now called [logstash-forwarder](https://github.com/elastic/logstash-forwarder). It is written in Go and jolly quick.\n\nOkay, okay enough introduction let's get going.\n\n# Staring the ELK\n\nThere are a couple of pre-built docker containers out there with ELK pre-configured. I found that sebp/elk was the best of them. It is well documented [here](http://elk-docker.readthedocs.io/). We run the container with  \n\n```\ndocker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk sebp/elk\n```\n\nYou can see that we're exposing 3 ports\n\n - 5601 Kibana web interface\n - 9200 Elasticsearch's web interface\n - 5000 Logstash (this is where we would send logs from logstash-forwarder)\n\n If you're running with docker-machine don't forget to add at least ports 5601 and 5000 to the virtual machine.\n\n With that kibana should be up and running. You can test it out by going to http://localhost:5000. Of course there isn't much to visualize because we haven't put any data in there yet. Next up we have to push some data into it. To do so I pulled down logstash-forwarder and ran it locally.\n\n It didn't work because of SSL.\n\n```\n2015/10/12 19:13:13.452439 Connecting to [127.0.0.1]:5000 (elk)\n2015/10/12 19:13:13.470699 Failed to tls handshake with 127.0.0.1 x509: certificate is valid for , not elk\n```\n\n The problem here is that the certificate for logstash-forwarder locally is not the same as the one on the image. You can grab the certificate from the container by running\n\n```\ndocker cp 61c2e23ad51a:/etc/pki/tls/private/logstash-forwarder.key lumberjack.key\ndocker cp 61c2e23ad51a:/etc/pki/tls/certs/logstash-forwarder.crt lumberjack.crt\n```\n\n Even with that I couldn't get it to work because the host name in the certificate is not the same as the one I used in logstash-forwarder's config file. I initially tried using 127.0.0.1 but I guess using IP addresses in certificates is really crummy and the [logstash-forwarder readme](https://github.com/elastic/logstash-forwarder) recommends against it.\n\n Thus I logged into the docker image and generated a new certificate. Logging required launching the container with a shell specified\n\n```\n docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk sebp/elk /bin/bash\n```\n\n Once logged into the container I ran\n\n```\n openssl req -x509  -batch -nodes -newkey rsa:2048 -keyout /etc/pki/tls/private/logstash-forwarder.key -out /etc/pki/tls/certs/logstash-forwarder.crt -subj /CN=elk\n```\n\n You'll note that I included ```/CN=elk``` in the command. This is the name of the domain to use in the certificate. I added a line pointing elk to 127.0.0.1 in my hosts file\n\n```\n 127.0.0.1\telk\n```\n\n Finally I got logstash-forwarder running and sending messages.  The config file I used looked like\n\n {% codeblock lang:json %}\n {\n  \"network\": {\n    \"servers\": [ \"elk:5000\" ],\n    \"ssl certificate\": \"./lumberjack.crt\",\n    \"ssl key\": \"./lumberjack.key\",\n    \"ssl ca\": \"./lumberjack.crt\",\n    \"ssl strict verify\": false,\n    \"timeout\": 15\n  },\n\n  # The list of files configurations\n  \"files\": [\n    {\n      \"paths\": [\n        \"/tmp/testlog.log\"\n      ],\n      \"fields\": { \"type\": \"syslog\" }\n    }, {\n      # A path of \"-\" means stdin.\n      \"paths\": [ \"-\" ],\n      \"fields\": { \"type\": \"stdin\" }\n    }\n  ]\n}\n{% endcodeblock %}\n\nI used stdin so I could just type thing into the console and have them show up in kibana. I also put in a log file to which I could append lines. The output from logstash-forwarder tells me the log entries are being forwarded\n\n```\n2015/10/12 21:12:19.365424 Launching harvester on new file: /tmp/testlog.log\n2015/10/12 21:12:19.365512 harvest: \"/tmp/testlog.log\" (offset snapshot:0)\n2015/10/12 21:12:20.109398 Registrar: processing 1 events\n2015/10/12 21:12:30.109288 Registrar: processing 1 events\n2015/10/12 21:13:15.111247 Registrar: processing 1 events\n2015/10/12 21:13:25.109295 Registrar: processing 1 events\n```\n\nIndeed they do show up in Kibana\n\n![http://i.imgur.com/FuS46Hf.jpg](http://i.imgur.com/FuS46Hf.jpg)\n\nThere are quite a few moving pieces here but the docker image takes care of most of them. In a future post we'll look at how to add some more information to our logs and how to get logs from ASP.net and C# in general into kibana.\n","categories":[],"tags":[]},{"title":"Layer Already Being Pulled by Another Client. Waiting.","authorId":"simon_timms","slug":"layer-already-being-pulled-by-another-client-waiting","date":"2015-10-13 07:23:33+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/layer-already-being-pulled-by-another-client-waiting/","link":"","permalink":"https://westerndevs.com/_/layer-already-being-pulled-by-another-client-waiting/","excerpt":"I've been seeing a lot of this frustrating error when working with docker today. It turns out that pressing ^C when docker is downloading layers is not a good thing. In my case I changed hotspots which broke the download so I hit ^C. There are a couple of issues on github, here and here but basically nobody cares that the docker experience in this scenario is crummy. If you encounter this error it seems the only way to solve it is to restart the machine on which docker is running. If you're running docker against a VM then restarting the machine seems to fix it.","raw":"---\nlayout: post\ntitle:  Layer Already Being Pulled by Another Client. Waiting.\ndate: 2015-10-12T21:23:33-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\noriginalurl:\n---\n\nI've been seeing a lot of this frustrating error when working with docker today. It turns out that pressing ^C when docker is downloading layers is not a good thing. In my case I changed hotspots which broke the download so I hit ^C. There are a couple of issues on github, [here](https://github.com/docker/docker/issues/15603) and [here](https://github.com/docker/docker/issues/3115) but basically nobody cares that the docker experience in this scenario is crummy. If you encounter this error it seems the only way to solve it is to restart the machine on which docker is running. If you're running docker against a VM then restarting the machine seems to fix it. \n\n<!--more-->\n  \n```\ndocker-machine ls\ndocker-machine restart default\n```\n\nIf you're running against actual hardware then I guess you're going to have to press the power button. \n\nThe cause appears to be that docker continues to download the package in the background but there is no way to surface that information to the client. I hope this is something that is fixed soon. ","categories":[],"tags":[]},{"title":"Microservice Sizing","authorId":"donald_belcham","slug":"microservices-sizing","date":"2015-10-12 20:01:34+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microservices-sizing/","link":"","permalink":"https://westerndevs.com/_/microservices-sizing/","excerpt":"As I mentioned in my last blog post (Microservices and Boundaries), I regularly see the question &quot;How big should my microservice be?&quot; The fast answer, albeit not the easy one, is that they should be the 'right' size. In that last blog post I talked about getting the right functionality into the right places (Antel for phone related functionality, Abitab for payment related functionality). There are a lot of people giving a lot of advise about scoping microservices, and I disagree with the majority of it. Here are some of the suggestions I've seen.","raw":"---\nlayout: post\ntitle: \"Microservice Sizing\"\ndate: 2015-10-12 10:01:34 -0600\ncomments: true\nauthorId: donald_belcham\n---\n\nAs I mentioned in my last blog post ([Microservices and Boundaries](http://www.westerndevs.com/microservices-and-boundaries/)), I regularly see the question \"How big should my microservice be?\" The fast answer, albeit not the easy one, is that they should be the 'right' size. In that last blog post I talked about getting the right functionality into the right places (Antel for phone related functionality, Abitab for payment related functionality). There are a lot of people giving a lot of advise about scoping microservices, and I disagree with the majority of it. Here are some of the suggestions I've seen.\n\n<!--more-->\n  \n## One Week's Work\nThis scoping idea follows the premise that any microservice that you make should only take one week to write. There's no discussion about the difficulty of the task at hand, the proper encapsulation, what the bounded context is or what the velocity of your development team is. Without fail, every one of your microservices should fit into a work week.\n\nMy question is what if the work isn't done at the end of one week? I was once told that at that point you should throw it away and start over with a smaller scope. I'm not sure about you but to me that is about as wasteful as you could be. Run into a problem with a third party API that you can't get resolved in a week? Scrap that work and start again.\n\nDon't think that what I'm saying is that we should allow our teams to take months to build microservices. We should be able to iterate on them quickly. This is one of the biggest benefits of taking the microservices approach. I think that the approach to writing your microservices should be one of Minimum Viable Product (MVP). If you write the bare minimum to have a functioning product, you will minimize the time spent on it. If the problem space is well bounded you will likely be working on microservices for very short periods of time. You shouldn't, however, set an arbitrary timebox as a means to scope the work.\n\n## X Number of Lines\nI can't count the number of times that I've heard/read \"your microservice should be no more than X lines of code.\" Usually the value of X is absurdly small, even for the most terse programming languages. Common numbers I've seen thrown around are 250, 100 and 10. Yes, 10. Can you imagine trying to write a 10 line microservice that does everything that I explained that Antel was doing in my previous post? Of course not. You couldn't even write the business logic for that process let alone all the infrastructure code that would be required (transactioning, logging, diagnostics) to create a healthy and sustainable microservice (more on this in a future post).\n\nLong ago we decided that LoC was a horrible metric for measuring anything to do with software development. Why would we introduce it again? The number of Lines of Code required for a microservice will only be known once all the functionality has been written. Sure, you might be able to shrink that count with some judicious refactoring, but you're not going to change 1500 lines of code into 100. If you can I'd suggest that you have a bigger cultural, or staffing, issue at hand.\n\n## Two Pizza Team\nI _hate_ this metric. I've seen it pop up in so many different discussions for so many different things. First, why pizza? Are we just a bunch of frat kids that haven't let our culinary preferences evolve? Second, have you seen the size of pizzas in some countries (looking at you United States of Gluttony)? You could feed a batallion with two large pizzas. Third, what if you can't decide on only two combinations of toppings? Sure you could get half-and-half done, but there's only so combinations you can request before the pizza joint hangs up on you.\n\nMore importantly I'd like to ask what does this metric prove? Focus on technical and team structure concerns when you think about that. The only thing that I can think of is that it could cap the size of the team. What does that mean for code? Lower overall velocity? Maybe, but that is probably more influenced by the quality of the individuals on the team.\n\n## Summary\nI'm a firm believer in working on microservices based on bounded contexts and minimum viable products. I don't need some catchy phrase telling me how I need to staff up my team or plan my work. Instead I know that I need to invest time into understanding my problem and setting expectations on the work.\nJust like microservices aren't a panacea for software development, none of these are for sizing your microservices.","categories":[],"tags":[]},{"title":"Self generating data","authorId":"amir_barylko","slug":"property-testing","date":"2015-10-12 08:39:34+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/property-testing/","link":"","permalink":"https://westerndevs.com/_/property-testing/","excerpt":"Every week I meet for Code and Coffee with other devs to chat about all kind of topics (often related to software) and lately we have been doing Katas from CodeWars under the WpgDotNet clan. This time around I was working with @QuinnWilson and @AdamKrieger doing the Highest and lowest Kata using Ruby and RSpec. Is a simple Kata but I wanted to put focus on TDD, self data generation and property testing...","raw":"---\nlayout: post\ntitle: \"Self generating data\"\ndate: 2015-10-11 23:39:34 -0500\ncomments: true\nauthorId: amir_barylko\n---\nEvery week I meet for [Code and Coffee](http://www.meetup.com/wpgcoffeecode) with other devs to chat about all kind of topics (often related to software) and lately we have been doing Katas from [CodeWars](http://codewars.com) under the _WpgDotNet_ clan.\n\nThis time around I was working with [@QuinnWilson](https://twitter.com/QuinnWilson) and [@AdamKrieger](https://twitter.com/AdamKrieger) doing the [Highest and lowest](http://www.codewars.com/kata/highest-and-lowest) Kata using Ruby and [RSpec](http://rspec.info).\n\nIs a simple Kata but I wanted to put focus on TDD, self data generation and property testing...\n\n<!--more-->\n\n## The scenarios\n\nI won't go into the TDD steps because I want to fast forward to data generation.\n\nWe created three tests, from the domain of the problem first we selected strings that contain only one number then the result should be that number as max and min.\n\n{% codeblock lang:ruby %}\ncontext 'When the string has only one number' do                       \n  it 'Returns the same number twice' do                                \n    str = \"1\"                                                          \n    expect(highest_lowest str).to eq \"1 1\"                             \n  end                                                                  \nend   \n{% endcodeblock %}\n\nFor our second scenario we thought that it would be easy if the sequence of numbers where already sorted, so the min and max are the `first` and `last`.                                                                 \n{% codeblock lang:ruby %}\ncontext 'When the numbers are sorted' do                               \n  it 'returns the last and the first' do                               \n    str = '-5 -2 8 12 32 150'                                          \n    expect(highest_lowest str).to eq \"150 -5\"                          \n  end                                                                  \nend                                                                    \n{% endcodeblock %}\n\nThe last scenario includes all the strings with sequence of integers.\n\n{% codeblock lang:ruby %}\ncontext 'For any string with one or more numbers separated by space' do\n  it 'returns the max and the min' do                                  \n    str = '22 -5 28 -294 33 1794 10000 2'                              \n    expect(highest_lowest str).to eq \"10000 -294\"                      \n  end                                                                  \nend                                                                    \n{% endcodeblock %}\n\nRuby has a `minmax` method on `Array` but we wanted to do it using a `fold`. So here is our implementation:\n\n{% codeblock lang:ruby %}\ndef highest_lowest(str)\n  str.split.map(&:to_i).inject([MIN, MAX]) do |mm, i|\n    [[mm.first, i].max, [mm.last, i].min]\n  end.join ' '\nend                                                \n{% endcodeblock %}\n\n## Self generating data\n\nWe did write only one example on each scenario to start but now we wanted to cover more cases.\n\nNot only I want to generate random data but I want to have a reasonable distribution of values and\ndescribe it as a property that the function has to satisfy.\n\nLibraries like [QuickCheck](https://hackage.haskell.org/package/QuickCheck) (for _Haskell_, main inspiration to others), [FsCheck](https://fscheck.github.io/FsCheck/) (for _F#_) and [ScalaCheck](https://www.scalacheck.org) (for _Scala_) do exactly that. For Ruby we used [Rantly](https://github.com/hayeah/rantly).\n\n### Single integer\nThe first scenario requires a string with one _integer_.\n\nTo do that with _Rantly_ we can use the _integer_ generator `integer`. After converting it to a `string` we have the input for the test.\n\nImagine a property that looks like:\n\n    Given a string with a single integer N\n    The result is a string like \"{N} {N}\"\n\n{% codeblock lang:ruby %}\nit 'Returns the same number twice' do\n  property_of {\n    n = integer\n    n.to_s\n  }.check do |str, n|\n    expect(highest_lowest str).to eq \"#{n} #{n}\"\n  end\nend\n{% endcodeblock %}\n\n_Rantly_ is going to generate 100 cases by default and _check_ to make sure the equality holds for all of them.\n\n### Sorted integers\n\nHaving a sorted sequence of numbers is easy to specify where the min and max should be.\n\nSo the property could be something like (in pseudo formal but not so much english):\n\n    Given an ordered string of integers\n    Then the first is the MIN and the last is the MAX\n    And the result is a string like \"{MAX} {MIN}\"    \n\n{% codeblock lang:ruby %}\ncontext 'When the numbers are sorted' do\n  it 'returns the last and the first' do\n    property_of {\n      arr = array { integer }.sort\n      [arr.join(' '), arr.last, arr.first]\n    }.check { |str, max, min|\n      expect(highest_lowest str).to eq \"#{max} #{min}\"\n    }\n  end\nend\n{% endcodeblock %}\n\n### The general case\n\nThinking of the actual _postcondition_ of the problem, the property could be something like:\n\n    Given any non empty collection of integers\n    And exist MIN and MAX that belong to the collection\n    Where MAX is the maximum and MIN is the minimum\n    Then the result is a string like \"{MAX} {MIN}\"\n\nConsidering for a moment the domain (all the possible instances) of _any non empty collection of integers_. That would include sets of a single element and also sorted elements, thus we cover the other two scenarios.\n\nI want to generate all the integers but at the same time have the max and min so I don't write the test duplicating the implementation to find maximum and minimum.\n\nTo do that, I will generate first the min and max and then add integers in between.\n\n{% codeblock lang:ruby %}\nit 'returns the max and the min' do\n  property_of {\n    min, max = [integer, integer].minmax\n    arr = array { range min, max } + [max, min]\n    [arr.shuffle.join(' '), max, min]\n  }.check(200) do |str, max, min|\n    expect(highest_lowest str).to eq \"#{max} #{min}\"\n  end\nend\n{% endcodeblock %}\n\nI changed the number of cases to _200_ just to illustrate how to override the default.\n\n## Using properties\n\nProperties can be very useful to help identify test cases plus data generation makes much easier to describe a scenario and find a domain for the test.\n\nHaving a combination of both _property testing_ and _example testing_ can be a good balance to have tests that are accurate and also descriptive.\n","categories":[],"tags":[]},{"title":"Ethics","slug":"podcast-ethics","date":"2015-10-10 23:32:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-ethics/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-ethics/","excerpt":"The Western Devs welcome Lori Lalonde and D'Arcy Lussier to the podcast and discuss ethics in software","raw":"---\nlayout: podcast\ntitle:  \"Ethics\"\ndate: 2015-10-10T15:32:22-04:00\nrecorded: 2015-10-02\ncategories: podcasts\nexcerpt: \"The Western Devs welcome Lori Lalonde and D'Arcy Lussier to the podcast and discuss ethics in software\"\ncomments: true\npodcast:\n    filename: \"Ethics.mp3\"\n    length: \"45:10\"\n    filesize: 43357074\n    libsynId: 5311399\n    anchorFmId: Ethics-evqdi7\nparticipants:\n    - dylan_smith\n    - kyle_baley\n    - darcy_lussier\n    - simon_timms\n    - lori_lalonde\n    - james_chambers\n    - dave_white\n    - amir_barylko\nlinks:\n    - The Volkswagen scandal|http://www.vox.com/2015/9/21/9365667/volkswagen-clean-diesel-recall-passenger-cars\n    - Ashley Madison fake profiles|http://gizmodo.com/almost-none-of-the-women-in-the-ashley-madison-database-1725558944\n    - Ashley Madison's 'deleted' data|http://www.zdnet.com/article/ashley-madison-hack-how-much-user-data-did-paid-delete-function-obliterate/\n    - Moonraker|http://www.imdb.com/title/tt0079574/\n    - Programming ethics|https://en.wikipedia.org/wiki/Programming_ethics\n    - Association for Computing Machinery code of ethics|http://www.acm.org/about/code-of-ethics\n    - Software Engineering code of ethics|http://www.acm.org/about/se-code\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n\n---\n\n### Synopsis\n\n* Roundabouts and traffic circles\n* ASP.NET vNext\n* The Volkswagen scandal\n* Personal ethics vs. corporate ethics\n* Moonraker and death rays\n* What is the developer's responsibility?\n* Marketing tactics vs. outright deception\n* Ashley Madison: Bots and deleting data\n* Which Western Devs to hire for a dating site\n* The developer's recourse\n* Is it unethical to release software without automated tests?\n* Should there be a software code of ethics?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Capturing HTTPS Traffic in Java with Eclipse and Fiddler","authorId":"david_wesst","slug":"capture-https-traffic-in-java-with-eclipse-and-fiddler","date":"2015-10-07 13:41:35+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/capture-https-traffic-in-java-with-eclipse-and-fiddler/","link":"","permalink":"https://westerndevs.com/_/capture-https-traffic-in-java-with-eclipse-and-fiddler/","excerpt":"I've been struggling with a JSON parsing error where my application is using the Spring to send and receive messages from a RESTful Web Service. It's pretty straight forward: I've annotated my object properties to match up with the appropriate JSON keys, Spring takes my POJO and turns it into a JSON string sends the request along with the JSON as the body to the HTTPS endpoint, et voilÃ !","raw":"---\nlayout: post\ntitle:  Capturing HTTPS Traffic in Java with Eclipse and Fiddler\ndate: 2015-10-07 09:41:35\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/\n---\nI've been struggling with a JSON parsing error where my application is using the [Spring](https://spring.io/guides/gs/consuming-rest/) to send and receive messages from a RESTful Web Service. It's pretty straight forward: I've annotated my object properties to match up with the appropriate JSON keys, Spring takes my POJO and turns it into a JSON string sends the request along with the JSON as the body to the HTTPS endpoint, et voilÃ !  \n\n<!--more-->\n  \n## The Problem\n\nThe problem comes in when something goes wrong with the request/response. Because the Spring obfuscates the actual request/response content, debugging it means you need to take a look at the traffic being sent over the wire. Since we're using a good RESTful service, the connection is done through HTTPS, meaning it's encrypted with a certificate that we don't have.\n\nOn top of that, it appears that Fiddler doesn't automatically capture Java HTTP traffic automatically, so that's a thing too.\n\nAfter some internet sleuthing, I put together a solution that I wanted to share with you all, and so that I don't forget how to do it myself.\n\n### Setup\n\n1. Downlaod and Install [Fiddler](http://www.telerik.com/fiddler). I used Fiddler4, because I'm awesome.\n2. Run it and make sure it's capturing HTTP traffic\n3. Open *Tools --> Fiddler Options --> Connections Tab* and take note of the \"Fiddler listens on port\" value. It's likely 8888, but best to be sure.\n4. In the same window select *HTTPS Tab* and make sure sure that the following options **are checked**:\n\t+ Capture HTTPS CONNECTS\n\t+ Decrypt HTTPS traffic (...from all processes)\n\n![http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/certificate-warning.png](http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/certificate-warning.png)\n\t\t\n6. Read, and if you're alright with it, install the certificate.\n5. On the HTTPS tab, click the *Export Root Certificate to Desktop* and click OK.\n\n![http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/fiddler-options.png](http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/fiddler-options.png)\n\n### Generating a Keystore\n\n1. Open a command line terminal as an administrator\n2. Run the keytool for the JDK your application is using:\n\n{% codeblock lang:powershell %}\n\n<JDK_Home>\\bin\\keytool.exe -import -file C:\\Users\\<Username>\\Desktop\\FiddlerRoot.cer^\n -keystore FiddlerKeystore -alias Fiddler\n\n{% endcodeblock %}\n\n3. Enter a password and remember it\n4. Your keystore is created as a file named \"FiddlerKeystore*. Take note of where it is located on your machine.\n\n### Configuring Eclipse\n\n**NOTE:** You are not required to use Eclipse for this, but it seems to be the popular way of writing Java code.\n\n1. Open your project and go to *Run --> Run Configurations*\n2. Select the Run Configuration you want to use where you'll capture the HTTPS traffic.\n3. Select the *Arguments* tab\n4. Add the following to the *VMargs* textbox:\n\n{% codeblock lang:java %}\n\n-DproxySet=true\n-DproxyHost=127.0.0.1\n-DproxyPort=8888\n-Djavax.net.ssl.trustStore=\"path\\to\\keystore\\FiddlerKeystore\"\n-Djavax.net.ssl.trustStorePassword=yourpassword\n\n{% endcodeblock %}\n\n5. Click the *Apply* button\n6. Click the *Run* button to try it out\n\n![http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/eclipse-settings.png](http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/eclipse-settings.png)\n\nTada! You're done, and you should now be able to run your code and see the HTTP request and response, completely.\n\n![http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/fiddler-success.png](http://blog.davidwesst.com/2015/10/Capturing-HTTPS-Traffic-in-Java-with-Eclipse-and-Fiddler/fiddler-success.png)\n\t\n### Alternative Solution --- Configuring Your Code\n\nAdd the following lines to the application that you want to capture the HTTPS traffic.\n\n{% codeblock lang:java %}\n\n// for capturing HTTP traffic\nSystem.setProperty(\"http.proxyHost\", \"127.0.0.1\");\nSystem.setProperty(\"http.proxyPort\", \"8888\");\n// for capturing HTTPS traffic\nSystem.setProperty(\"https.proxyHost\", \"127.0.0.1\");\nSystem.setProperty(\"https.proxyPort\", \"8888\");\n\n{% endcodeblock %}\n\n--\nThanks for playing. ~ DW\n\n###### References\n\n1. [Stack Overflow - How to Capture HTTPS with Fiddler in Java](http://stackoverflow.com/questions/8549749/how-to-capture-https-with-fiddler-in-java)\n2. [How to Use Eclipse with Fiddler](http://codeketchup.blogspot.ca/2014/03/how-to-use-eclipse-with-fiddler-step-by.html)\n","categories":[],"tags":[]},{"title":"Yet Another Docker Intro on OSX","authorId":"simon_timms","slug":"yet-another-docker-intro","date":"2015-10-05 09:32:11+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"docker/yet-another-docker-intro/","link":"","permalink":"https://westerndevs.com/docker/yet-another-docker-intro/","excerpt":"You would think that there were enough introductions to Docker out there already to convince me that the topic is well covered and unnecessary. Unfortunately the sickening mix of hubris and stubbornness that endears me so to rodents also makes me believe I can contribute.","raw":"---\nlayout: post\ntitle:  Yet Another Docker Intro on OSX\ndate: 2015-10-04T23:32:11-06:00\ncategories: docker\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/10/04/yet-another-docker-intro/\n---\n\nYou would think that there were enough introductions to Docker out there already to convince me that the topic is well covered and unnecessary. Unfortunately the sickening mix of hubris and stubbornness that endears me so to rodents also makes me believe I can contribute. \n\n<!--more-->\n  \nIn my case I want to play a bit with the ELK stack: that's Elasticsearch, Logstash and Kibana. I could install these all directly on the macbook that is my primary machine but I actually already have a copy of Elasticsearch installed and I don't want to polute my existing environment. Thus the very 2015 solution that is docker. If you've missed hearing the noise about docker over the last year then you're in for a treat. \n\nThe story of docker is the story of isolating your software so that one piece of software doesn't break another. This isn't a new concept and one could argue that really that's what kernel controlled processes do. Each process has its own memory space and, as far as the process is concerned, the memory space is the same as the computer's memory space. However the kernel is lying to the process and is really remapping the memory addresses the program is using into the real memory space. If you consider the speed of processors today and the ubiquity of systems capable of running more than one process at a time then, as a civilization, we are lying at a rate several orders of magnitude greater than any other point in human history. \n\nAny way, docker extends the process isolation model such that the isolation is stronger. Docker is a series of tools built on top of the linux kernel. The entire file system is now abstracted away, networking is virtualized, other processes are hidden and, in theory, it is impossible to break out of a container and damage other processes on the same machine. In practice everybody is very open about how it might be possible to break out of machine or, at the very least, gather information from the system running the container. Containers are a weaker form of isolation than virtual machines.\n\n![http://imgur.com/ntGolVE.png](http://i.imgur.com/ntGolVE.png)\n\nOn the flip side processes are more performant than containers which are, in turn more performant than virtual machines. The reason is simple: with more isolation more things need to run in each context bogging the machine down. Choosing an isolation level is an exercise in deciding how much trust you have in the processes you run to no interference with other things. In the scenario where you've written all the services then you can have a very high level of trust in them and run them with minimal isolation in a process. If it is SAP then you probably want the highest level of isolation possible:  put the computer in a box and fire it to the moon. \n\nAnother nice feature of docker is that the containers can be shipped as a whole. They tend not to be prohibitively large as you might see with a virtual machine. This vastly improves the ease of deploy. In a world of micro-services it is easy to bundle up your services and ship them off as images. You can even have the result of your build process be a docker image. \n\nThe degree to which docker will change the world of software development and deployment remains an open one. While I feel like docker is a fairly disruptive technology the impact is still a couple of years out. I'd like to think that it is going to put a bunch of system administrators out of a job but in reality it is just going to change their job. Everybody needs a little shakeup now and then to keep them on their toes. \n\nAnyway back to docker on OSX:\n\nIf you read carefully to this point you might have noticed that I said that docker runs on top of the Linux kernel. Of course OSX doesn't have a linux kernel on which you can run docker. To solve this we actually run docker on top of a small virtual machine. To manage this we used to use a tool called boot2docker but this has, recently, been replace with docker-machine. \n\nI had an older install of docker on my machine but I thought I might like to work a bit with docker compose as I was running a number of services.  Docker compose allows for coordinating a number of containers to setup a whole environment. In order to keep the theme of isolating services it is desirable to run each service in its own container. So if you imagine a typical web application we would run teh web server in one container and the database in another one. These containers can be on the same machine.  \n\nThus I grabbed the installation package from the docker website then followed the installation instructions at [http://docs.docker.com/mac/step_one/](http://docs.docker.com/mac/step_one/). With docker installed I was able to let docker-machine create a new virtual machine in virtual box. \n\n![http://i.imgur.com/5uQjfq8.jpg](http://i.imgur.com/5uQjfq8.jpg)\n\nAll looks pretty nifty. I then kicked off the ubiqutious hello-world image\n\n<pre>~/Projects/western-devs-website/_posts$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n\n535020c3e8ad: Pull complete \naf340544ed62: Pull complete \nDigest: sha256:a68868bfe696c00866942e8f5ca39e3e31b79c1e50feaee4ce5e28df2f051d5c\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker Hub account:\n https://hub.docker.com\n\nFor more examples and ideas, visit:\n https://docs.docker.com/userguide/\n </pre>\n\nIt is shocking how poorly implemented this image is, notice that at no point does it actually just print \"Hello World\". Don't worry, though, not everything in docker land is so poorly implemented. \n\nThis hello world demo is kind of boring so let's see if we can find a more exciting one. I'd like to serve a web page from the container. To do this I'd like to use nginx. There is already an nginx container so I can create a new Dockerfile for it. A Dockerfile gives docker some instructions about how to build a container out of a number of images. The Dockerfile here contains \n\n<pre>\nFROM nginx\nCOPY *.html /usr/share/nginx/html/\n</pre>\n\nThe first line set the base image on which we want to base our container. The second line copies the local files with the .html extension to the web server directory on the nginx server container. To use this file we'll have to build a docker image\n\n<pre>\n/tmp/nginx$ docker build -t nginx_test .\nSending build context to Docker daemon 3.072 kB\nStep 0 : FROM nginx\nlatest: Pulling from library/nginx\n843e2bded498: Pull complete \n8c00acfb0175: Pull complete \n426ac73b867e: Pull complete \nd6c6bbd63f57: Pull complete \n4ac684e3f295: Pull complete \n91391bd3c4d3: Pull complete \nb4587525ed53: Pull complete \n0240288f5187: Pull complete \n28c109ec1572: Pull complete \n063d51552dac: Pull complete \nd8a70839d961: Pull complete \nceab60537ad2: Pull complete \nDigest: sha256:9d0768452fe8f43c23292d24ec0fbd0ce06c98f776a084623d62ee12c4b7d58c\nStatus: Downloaded newer image for nginx:latest\n ---> ceab60537ad2\nStep 1 : COPY *.html /usr/share/nginx/html/\n ---> ce25a968717f\nRemoving intermediate container c45b9eb73bc7\nSuccessfully built ce25a968717f\n</pre>\n\nThe docker build command starts by pulling down the already build nginx container. Then it copies our files over and reports a hash for the container which makes it easily identifiable. To run this container we need to do\n\n<pre>\n/tmp/nginx$ docker run --name simple_html -d -p 3001:80 -p 3002:443 nginx_test\n</pre>\n\nThis instructs docker to run the container nginx_test and call it simple_html. The -d tells docker to run the container in the background and finally the -p give the ports to forward, in this case we would like our local machine's port 3001 to be mapped to the port inside the docker image 80 - the normal web server port. So now we should be able to connect to the web server. If we open up chrome and go to localhost:3001 we get \n\n![http://i.imgur.com/8Hdq9hN.jpg](http://i.imgur.com/8Hdq9hN.jpg)\n\nWell that doesn't look right! The problem is that docker doesnâ€™t realize that it is being run in a virtual machine so we need to forward the port from the vm to our local machine\n\n  Docker container:80 -> vm host:3001 -> OSX:3001\n\nThis is easily done from the virtual machine manager \n\n![http://i.imgur.com/cGXHwRZ.jpg](http://i.imgur.com/cGXHwRZ.jpg)\n\nNow we get \n\n![http://i.imgur.com/h8UJTSN.jpg](http://i.imgur.com/h8UJTSN.jpg)\n\nThis is the content of the html file I put into the container. Perfect! I'm now ready to start playing with more complex containers. \n\n**Tip**\n\nOne thing I have found is that running docker in virtual box at the same time as running parallels causes the whole system to hang. I suspect that running two different virtual machine tools is too much for something and a conflict results. I believe there is an effort underway to bring parallels support to docker-machine for the 0.5 release. Until then you can read [http://kb.parallels.com/en/123356](http://kb.parallels.com/en/123356) and look at the docker-machine fork at [https://github.com/Parallels/docker-machine](https://github.com/Parallels/docker-machine).\n","categories":[{"name":"docker","slug":"docker","permalink":"https://westerndevs.com/categories/docker/"}],"tags":[]},{"title":"Welcome Lori Lalonde","slug":"welcome-lori-lalonde","date":"2015-10-04 21:49:53+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/welcome-lori-lalonde/","link":"","permalink":"https://westerndevs.com/_/welcome-lori-lalonde/","excerpt":"We're thrilled to introduce our newest member, Lori Lalonde. Lori is well-known to most us through her presentations and on Twitter. She dove right into Slack with us and you'll hear her dulcet tones shortly on our next podcast.","raw":"---\nlayout: post\ntitle:  Welcome Lori Lalonde\ndate: 2015-10-04T13:49:53-04:00\ncategories:\ncomments: true\n---\n\nWe're thrilled to introduce our newest member, [Lori Lalonde](http://www.westerndevs.com/bios/lori_lalonde). Lori is well-known to most us through her [presentations](http://solola.ca/speaking/) and on [Twitter](https://twitter.com/loriblalonde). She dove right into Slack with us and you'll hear her dulcet tones shortly on our next [podcast](http://www.westerndevs.com/podcasts/).\n\n<!--more-->\n  \nLori has written [two books](http://www.westerndevs.com/whatwevedone) and is a regular speaker on Xamarin and Windows Phone development. She is actively involved in the development community and is the user group leader for [Canada's Technology Triangle .NET User Group](http://meetup.com/cttdnug).\n\nA big Western Devs Welcome&copy; to you, Lori!","categories":[],"tags":[]},{"title":"Winner Selector","authorId":"simon_timms","slug":"winner-selector","date":"2015-10-03 20:53:29+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/winner-selector/","link":"","permalink":"https://westerndevs.com/_/winner-selector/","excerpt":"A quick tool for picking winners from a list","raw":"---\nlayout: post\ntitle:  Winner Selector\ndate: 2015-10-03T10:53:29-06:00\ncategories:\nexcerpt: A quick tool for picking winners from a list\ncomments: true\nauthorId: simon_timms\n---\n\nWe constantly run into the problem of drawing winners for events. It is silly because it is such an easy problem and yet every time I'm left stumbling for a solution. Here is a really simple one:\n\nSimply paste in a list of people, one per line and hit pick a winner. \n<textarea id=\"sourceNames\" rows=\"20\"></textarea>\n<span id=\"foundPeople\"></span>\n<button id=\"winnerPicker\">Pick a winner from the list</button>\n\nWe welcome code audits of this page: simply check out the source to see how it works. [Source on github](https://raw.githubusercontent.com/westerndevs/western-devs-website/source/_posts/2015-10-03-winner-selector.markdown)\n\n<script>\n\nwindow.onload = function(){\n\tdocument.querySelector(\"#winnerPicker\").addEventListener(\"click\", showWinner);\n\tdocument.querySelector(\"#sourceNames\").addEventListener(\"change\", countPeople);\n\t}\n\t\nfunction countPeople(){\n\tvar names = document.querySelector(\"#sourceNames\").value.split(\"\\n\");\n\tdocument.querySelector(\"#foundPeople\").innerHTML = \"Found \" + names.length+ \" people in the list\";\n}\t\nfunction showWinner(){\n\tvar names = document.querySelector(\"#sourceNames\").value.split(\"\\n\");\n\tvar rand = Math.floor(Math.random() * 10000)%names.length;\n\talert(\"The winner is: \" + names[rand]);\n}\n\n</script>","categories":[],"tags":[]},{"title":"Going Independent","slug":"podcast-going-independent","date":"2015-10-01 03:07:37+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-going-independent/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-going-independent/","excerpt":"In honour of Botswanan Independence Day, the Western Devs discuss going independent with special guest, Wendy Closson","raw":"---\nlayout: podcast\ntitle:  \"Going Independent\"\ndate: 2015-09-30T19:07:37-04:00\nrecorded: 2015-09-18\ncategories: podcasts\nexcerpt: \"In honour of Botswanan Independence Day, the Western Devs discuss going independent with special guest, Wendy Closson\"\ncomments: true\npodcast:\n    filename: \"GoingIndependent.mp3\"\n    length: \"48:05\"\n    filesize: 46166206\n    libsynId: 5316374\n    anchorFmId: Going-Independent-evqdil\nparticipants:\n    - dylan_smith\n    - kyle_baley\n    - david_wesst\n    - simon_timms\n    - donald_belcham\n    - james_chambers\n    - dave_white\nlinks:\n    - Wendy Closson (webpage)|http://www.justaddwendy.com/\n    - Wendy Closson (Twitter)|https://twitter.com/wendypclosson\n    - Going Indie|http://www.goingindie.com/article_details.php?id=3\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* Who's independent around here?\n* Working for a consulting company vs. going through a headhunter vs. doing it yourself\n* Reasons for going independent\n* Pros and cons of independence vs. working for a consultant\n* Managing the marketing and branding\n* How to tell when it's not working\n* The financial realities of independence\n* The challenges of running your own business\n* How to make the leap\n* Travelling for contracts\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Microservices and Boundaries","authorId":"donald_belcham","slug":"microservices-and-boundaries","date":"2015-09-29 23:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microservices-and-boundaries/","link":"","permalink":"https://westerndevs.com/_/microservices-and-boundaries/","excerpt":"One of the most common questions Iâ€™ve been getting asked about microservices is â€œHow big?â€ I was recently down in Montevideo Uruguay speaking at the .NetConf UY Meetup speaking about microservices. As part of my vacation in Uruguay I wanted to get a local SIM card for my phone so that I would have data without relying on free WiFi. At the time I was getting the SIM and making things work it seemed like one of the most frustrating experiences Iâ€™ve ever had. Lots of running around, lots of wasted time. Once I got onto the plane home I started thinking about it in a different light.","raw":"---\nlayout: post\ntitle:  Microservices and Boundaries\ndate: 2015-09-29T13:30:00-06:00\ncomments: true\nauthorId: donald_belcham\n---\nOne of the most common questions Iâ€™ve been getting asked about microservices is â€œHow big?â€ I was recently down in Montevideo Uruguay speaking at the [.NetConf UY](http://www.netconf.uy) Meetup speaking about microservices. As part of my vacation in Uruguay I wanted to get a local SIM card for my phone so that I would have data without relying on free WiFi. At the time I was getting the SIM and making things work it seemed like one of the most frustrating experiences Iâ€™ve ever had. Lots of running around, lots of wasted time. Once I got onto the plane home I started thinking about it in a different light.\n\n<!--more-->\n  \nHereâ€™s the process I went through to get data on a SIM.\n\n![Antel](http://farm6.staticflickr.com/5737/21313922360_6daa6624d8_m.jpg)\n\nFirst I went to an [ANTEL](http://www.antel.com.uy/antel/) store. ANTEL is the state owned telecom. Itâ€™s cheap, but like any good state company it has bureaucracy. I went into the store and the first stop is a concierge/secretary desk. Once I explained, through bad spanish and worse charades, what I wanted I was told that they would need my passport. Of course I didnâ€™t have it so it was back to the hotel to get a passport. Back at the ANTEL store there was, of course, a new person manning the front desk so it was more bad spanish and Italian inspired charades. My request was understood and I was given a number and told to wait (Uruguay *loves* its number dispensers). About 5 minutes later I was called to a desk and the process of signing up begins. \n\nAfter some furious keyboard smashing, and more horrible spanish on my part, I was told to go to the cashier to pay. I went across the lobby (really itâ€™s all just one big room) and waited in line for the guy working the cashier wicket to finish with his mobile phone. I walk up and brutalize his native tongue. He looks at me, spins his chair around and pulls some paper off the printer. More furious keyboard smashing, a test of my rudimentary understanding of numbers in spanish and my payment was made. The hands me some paper and points me to another line and another wicket.\n\nI line up once again and was called to the counter. I handed over the paperwork, the lady disappeared into the back room and re-appeared with the SIM card. A brief explanation of the phone number and security code and I was told I was done there. So I headed back over to the original gentleman that was helping me. At that point I was told that I was finished.\n\nThe SIM card went into the phone and I was connected to the ANTEL networkâ€¦except I had no service. I couldnâ€™t text, call or use data. I went back to the hotel where I had WiFi figuring that I needed to simply tweak some settings for the APN. An hour later I had changed the APN and still had nothing. At this point I gave up and called my friends at [Kaizen Softworks](http://www.kzsoftworks.com/ \"\"). Within a minute I was told that I needed to go visit a place called Abitab. Once there we could charge our phone number with money. So off I go to the local Abitab (which I luckily knew about since I changed money at it earlier in the day).\n\n![Abitab](http://farm6.staticflickr.com/5691/21490886762_0d42a67d7b_m.jpg)\n\nI get to Abitab, go to a wicket, show them the paperwork with my phone number on it, tell them I want to put 400 pesos on the account and 3 minutes later Iâ€™m done. So now I have a SIM card, it has funds, but I still donâ€™t have service. I needed to request a â€œplanâ€ to use. Luckily when I was sitting in the hotel trying to figure out how to get service I found the magic code to do that. So I send an SMS to a predetermined number and boomâ€¦.internet access.\n\nThere were a lot of steps in that process. More than what was needed some might say. If you think about it, there are good separation of concerns and lessons for microservices here. Letâ€™s take a look at the process a different way.\n\nANTEL provides telecom services. I was a consumer (UI if you will) in need of those services so I reached out to their endpoints. From 40,000 feet this is a pretty standard microservice interaction. The first thing that the microservice required is that I authenticate myself (provide a passport). Once Iâ€™ve done that they start the process of creating a client record for me and putting in the purchase order for the SIM card. This is a pretty straight forward synchronous operation. In essence this is the code that runs in the controller of your REST WebAPI endpoint.\n\n![Image1](http://farm1.staticflickr.com/584/21817801705_5f577c2843_z.jpg)\n\nWhen the purchase order is successfully created anyone that cares is notified. In this case the cashier is notified by having a print out appear on his printer. Iâ€™m also notified that I need to go to a new location to do more â€˜stuffâ€™. In technical terms an event was published onto some kind of service bus. The cashier subscribed to that event type and printed a purchase order. The original service that I contacted provided a URI as part of its result to me that I needed to navigate to. That URI pointed me to the cashier service.\n\nOnce I made payment to the cashier service it published a successful payment event onto a service bus. The cashier service also provided me with a URI to visit to get the SIM card, which was the new line-up. The fulfillment service listened to that event and when I made my request to them they provided me with the SIM card and a bunch of meta data to go with it.\n\nLike a good little consumer (UI) I made a call final call to an endpoint (the original person I dealt with) to check to see if my transaction was complete.\n\nAfter dealing with ANTEL I moved on to Abitab, or the payment services. I called on one of their endpoints to make a payment. Again, I need to provide â€œproofâ€ of who I am. In this case the level of proof is lower; I only need to provide the account number I want to deposit money to. The fact is, I can deposit money to any account I want and the service isnâ€™t going to complain. Itâ€™s not hurting anyone but me if I do it wrong. So to make this payment I provided the amount of the payment, the means of payment and the account the payment should be applied to. I got a receipt. That concludes my interaction with the payment service.\n\nBehind the scenes the Abitab service sent a message to some ANTEL service that notified ANTEL that the account needed to have credit applied to it and how much that credit should be. This is a service-to-service communication. Abitab knows nothing about the functionality of Antel. All Abitab does is publish a message that an event (payment) occurred. Abitab doesnâ€™t need to know if, when, or how Antel applies the credit to the account so there is no need for a confirmation message so it can use a messaging pattern effectively here.\n\n![Image2](http://farm1.staticflickr.com/641/21817803695_c8444f341e_z.jpg)\n\nThe separations of concerns in this case was very clear. One microservice (company) was responsible for all things phone related (SIM card, activation, etc) and another microservice was responsible for payments. If youâ€™ve dealt with a system like this (Antel and Abitab) youâ€™ll know that there are some significant benefits from this separation. First, and foremost, you can pay anything imaginable at Abitab. Need to pay your water bill? Go to Abitab. You TV bill? Go to Abitab. All Abitab does is take the payment and send a message to the company concerned tell them that a payment has been received. \n\nWhen youâ€™re writing microservices you need to look at the responsibilities and concerns of the services.  They need to be well encapsulated but also single-y responsible. If it feels like one microservice needs the functionality of another, look at ways to communicate between them. Ideally this communication should be unidirectional and in the form of commands. By taking this approach you will create microservices that do not effect each other in deployment, down(up)-time or changes.\n \n","categories":[],"tags":[]},{"title":"Don't Let Crud, Corruption, and Communism Kill Your Smart Watch","authorId":"tom_opgenorth","slug":"dont-let-crud-corruption-and-communism-kill-your-smart-watch","date":"2015-09-26 00:56:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/dont-let-crud-corruption-and-communism-kill-your-smart-watch/","link":"","permalink":"https://westerndevs.com/_/dont-let-crud-corruption-and-communism-kill-your-smart-watch/","excerpt":"I have a Samsung Gear Live. One day, all of sudden, it wouldn't turn on after I had it charging in it's little charging dock. I thought the problem was with the watch, and Google the symptoms led me to one of two conclusions:","raw":"---\nlayout: post\ntitle:  Don't Let Crud, Corruption, and Communism Kill Your Smart Watch\ndate: 2015-09-25T14:56:05-06:00\ncomments: true\nauthorId: tom_opgenorth\noriginalurl: http://www.opgenorth.net/blog/2015/09/25/dont-let-crud-corruption-and-communism-kill-your-smartwatch/\n---\n\nI have a [Samsung Gear Live](http://www.samsung.com/global/microsite/gear/gearlive_design.html). One day, all of sudden, it wouldn't turn on after I had it charging in it's little charging dock. I thought the problem was with the watch, and Google the symptoms led me to one of two conclusions:\n\n<!--more-->\n  \n1. The watch was defective, and I should send it back under warranty.\n2. I could fix the problem by taking the watch apart, disconnecting the battery for a minute, and then reconnecting the battery. Or something like that.\n\nI was a bit dismayed that for a device not even a year old that these were my options. I was mulling over the options, and I flipped the watch over to see what tools I would need to take the watch apart (who doesn't like disassembling electronics), when I notice that the contact points on the watch were filthy (as one the NCOs on my infantry course used to say when inspecting rifles &ndash; \"crud, corruption, and communism\"). You can see the contacts for the charging dock right in the picture below:\n\n![](/images/back-of-samsung-gear-live.jpg)\n\nI figured cleaning these contact points was easier than taking the watch apart and quicker than sending the watch back, so I grabbed an eraser and gave the contacts a quick scrub.  I plugged the watch back into the charging cradle, and all of a sudden it started charging again.\n\nI've had to do this a couple of times now. I recently found out that some erasers may leave a thin film of residue that may attract dirt. There are a couple of options available:\n\n1. **Rubbing alcohol and a Q-Tip**. One fellow I work with will clean the back of his watch every week with this.\n2. **CAIG Deoxit**. Another co-worker with an electronics background suggested [Deoxit](http://www.parts-express.com/caig-deoxit-d5s-6-spray-5-oz--341-200?utm_source=google&utm_medium=cpc&utm_campaign=pla) by Caig. Put a bit on a Q-Tip, and then clean away. This product will not only clean the contacts and remove oxidation but will leave behind a thin layer to help prevent oxidation.\n\nAnybody else experienced this problem with their watch? What solutions did you use?\n","categories":[],"tags":[]},{"title":"Downgrading Windows Phone 10 Technical Preview - The Lowlights","authorId":"david_wesst","slug":"downgrading-windows-phone-10-technical-preview-the-lowlights","date":"2015-09-22 18:56:02+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/downgrading-windows-phone-10-technical-preview-the-lowlights/","link":"","permalink":"https://westerndevs.com/_/downgrading-windows-phone-10-technical-preview-the-lowlights/","excerpt":"I have to say that I really did like the Windows Phone 10 technical preview. I think that the new phone OS is heading the right direction, and that if you have a secondary device or want to be &quot;hardcore&quot; , you should install it and check it out.","raw":"---\nlayout: post\ntitle:  \"Downgrading Windows Phone 10 Technical Preview - The Lowlights\"\ndate: 2015-09-22T10:56:02-04:00\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/09/Downgrading-Windows-Phone-10-Technical-Preview-The-Lowlights/\n---\n\n{% img pull-left \"http://blog.davidwesst.com/2015/09/Downgrading-Windows-Phone-10-Technical-Preview-The-Lowlights/windows-10-device-screen.jpg\" %}\n\nI have to say that I really did like the Windows Phone 10 technical preview. I think that the new phone OS is heading the right direction, and that if you have a secondary device or want to be _\"hardcore\"_ , you should install it and check it out.\n\n<!--more-->\n  \nI'll talk about that in a future post, for now I want to share the steps I took to downgrade my Lumia 920 from the Windows 10 Technical Preview back to Windows Phone 8.1.\n\nNot being a phone expert, the idea of downgrading incorrectly was pretty scary proposition as I didn't want to ruin it nor lose any of my data. But, this weekend I managed to find the guidance from [windowscentral.com][2] I needed and it wasn't nearly as scary as I thought it would be.\n\nFollow the guide from windowscentral.com above. The tool is easy and self explanitory, and was quite happy that it just did everything it needed to do to get my phone back onto a officially released Windows Phone OS.\n\nNevertheless I hit a few bumps in the roads that may or may not have been caused by me, but I would be remiss if I didn't share my experience on the web, as hopefully it prevents someone else from hitting the same issues.\n\n### Mobile Carrier Settings Lost\n\nThis one wasn't such a big deal to me.\n\nBeing on the Windows Phone 10 preview, whenever a new upgrade was installed, I would lose my settings. Not sure why that would happen, but it did. The downgrade wasn't any different. Here's the fun part though: you won't realize you don't have a data connection until you need it.\n\nThe SIM card worked just fine as phone calls and text messages would come in just fine. Plus, my apps were downloaded and re-installed as I was on my wireless network, so I didn't consider my data connection to be be broken.\n\nUntil I needed it out in the wild.\n\nSo, it's not a big deal as you just need to take note of what your carrier settings are, or do a quick web search for them prior to make sure you have them on hand after the downgrade completes.\n\n### Backup Your OneDrive\n\nAfter the downgrade was complete, all of my photos and files backed up on OneDrive were scortched. Not just on the phone, but in the cloud too.\n\nThis step burned me pretty badly, considering I had a bunch of photos of my new daughter on there and they are all gone now. Given, it may have been my phone settings or something else that I didn't configure properly. Either way, I should have been smart and backed up my files onto a medium other than OneDrive itself.\n\nFor a free service it's great, but man does it burn when something goes wrong.\n\n## The Point\n\n{% img pull-right \"http://blog.davidwesst.com/2015/09/Downgrading-Windows-Phone-10-Technical-Preview-The-Lowlights/onedrive-scortched.png\" %}\n\nOutside of the lowlights listed above, I liked the downgrading experience. It was simple, intuitive, and will encourage me to take part in future technical preview programs from Microsoft.\n\nJust make sure you back up everything before you do.\n\n<div class=\"clearfix\"> </div>\nâ€“ \nThanks for Playing. ~ DW\n\n[2]: http://www.windowscentral.com/roll-back-windows-phone-81-windows-10-preview\n  ","categories":[],"tags":[]},{"title":"Adventures in Windows IoT Core for Raspberry Pi 2","authorId":"darcy_lussier","slug":"adventures-in-windows-iot-core-for-raspberry-pi-2","date":"2015-09-22 18:50:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/adventures-in-windows-iot-core-for-raspberry-pi-2/","link":"","permalink":"https://westerndevs.com/_/adventures-in-windows-iot-core-for-raspberry-pi-2/","excerpt":"Ever since I won a Raspberry Pi 2 at Microsoft Ignite I've been trying to figure out what to do with it. This week I decided to look at the Windows 10 IoT Core for Raspberry Pi 2 and see what I could do to get something up and running.","raw":"---\nlayout: post\ntitle:  \"Adventures in Windows IoT Core for Raspberry Pi 2\"\ndate: 2015-09-22T10:50:22-04:00\ncategories:\ncomments: true\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/09/20/166942.aspx\n---\n\nEver since I won a Raspberry Pi 2 at Microsoft Ignite I've been trying to figure out what to do with it. This week I decided to look at the Windows 10 IoT Core for Raspberry Pi 2 and see what I could do to get something up and running.\n\n<!--more-->\n\nI'm not going to re-hash how to set up the device or how to configure your development environment; there's already some great articles that cover this which I link to further below. But I will share some first impressions which may prep you for working with the platform.\n\nWindows IoT Core (WIoTC) is, as their own website states, a work in progress; a very early work in progress. Compared to Raspian, which has a desktop-like GUI complete with windows, WIoTC seems very primitive. You're very limited in what information you can get (I couldn't get the MAC address of the Raspberry Pi 2 from the WIoTC interface) and the suggested way to manage your device seems to be through PowerShell via another PC on the same network.\n\nDeployed apps can either be a Universal Windows App or Headless (background process). Only one UWA will be displayed at a time. There's no built in menu or anything, so don't think of this as a Windows Phone type of experienceâ€¦its less than that. Users won't automatically have an \"Installed Programs\" list to refer to. Also, while you can deploy a Universal Windows App to the Pi, [there's a documented list of unavailable APIs that aren't part of WIoTC][1]. So you can't just assume what you write for Win 10 desktop or even the phone platform will transfer over easily.\n\nWIoTC doesn't take full advantage of the hardware yet either. [Mike Dailly][2] posted [a Vine showing that GameMaker Studio games can run][3] on the platform, but very slowly due to the GPU not being used at all (graphics are drawn using software emulation).\n\nThis may sound very negative, but considering how new WIoTC is and that they likely targeting the base/required functionality for an initial release I think its pretty cool that using Visual Studio I can deploy an app to this mini-computer on my desk. There's great opportunity with WIoTC and I'm actually happy they didn't try to force a bloated platform out for v1; better to involve the community and develop things as needed instead of making assumptions.\n\nNow that I've done my first Hello World app, its time to start looking at the next step of playing with my Raspberry Pi. Stay tuned!\n\n#### Links\nMicrosoft Dev Center â€“ IoT   \n<https://dev.windows.com/en-us/iot>\nThis site is the best starting point for setting up your device, dev environment, doing your first project, and getting up to date information.\n\nMichael Crump's Guided Tour of Windows 10 IoT Core   \n<http://developer.telerik.com/featured/a-guided-tour-of-windows-10-iot-core/>\nGreat walkthrough by [Michael Crump][5] which also shows how to use one of the Telerik controls and links to related topics.\n\n[1]: http://ms-iot.github.io/content/en-US/win10/UnavailableApis.htm\n[2]: https://twitter.com/mdf200\n[3]: https://vine.co/v/eDu3FF5Prdr\n[5]: https://twitter.com/mbcrump\n","categories":[],"tags":[]},{"title":"Custom MVC6 Tag Helper Samples","authorId":"dave_paquette","slug":"custom-mvc-6-tag-helper-samples","date":"2015-09-21 01:20:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/custom-mvc-6-tag-helper-samples/","link":"","permalink":"https://westerndevs.com/_/custom-mvc-6-tag-helper-samples/","excerpt":"A group of us who have been exploring MVC 6 Tag Helpers have created a repository of Tag Helper Samples. The repository contains a set of real world samples that can help you understand how to build your own custom tag helpers.","raw":"---\nlayout: post\ntitle:  \"Custom MVC6 Tag Helper Samples\"\ndate: 2015-09-20T17:20:00-04:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/09/20/custom-mvc-6-tag-helper-samples.aspx\n---\nA group of us who have been exploring MVC 6 Tag Helpers have created a repository of [Tag Helper Samples](https://github.com/dpaquette/TagHelperSamples). The repository contains a set of real world samples that can help you understand how to build your own custom tag helpers.\n\n<!--more-->\n\nSo far, we have been focusing on Tag Helpers that make it easier to use various Bootstrap components. We chose Bootstrap because Bootstrap components are often verbose and it can be easy to miss a particular class or a specific attribute. I find that this is especially when you consider all the accessibility _aria-*_ attributes. So far, we have implemented tag helpers for Bootstrap [Alerts](http://getbootstrap.com/components/#alerts), [Progress Bars](http://getbootstrap.com/components/#progress) and most recently [Modals](http://getbootstrap.com/javascript/#modals).\n\n## Alert\n\nThe [alert tag helper](https://github.com/dpaquette/TagHelperSamples/blob/master/TagHelperSamples/src/TagHelperSamples.Bootstrap/AlertTagHelper.cs), contributed by Rick Strahl, makes it easy to display Bootstrap alerts containing Font-Awesome icons.\n\n```\n<alert message=\"Payment has been processed.\" icon=\"success\">\n</alert>\n```\n\nWill output the following HTML:\n\n```\n<div class=\"alert alert-success\" role=\"alert\">\n  <i class=\"fa fa-check\"></i> Payment has been processed.\n</div>\n```\n\n## Progress Bar\n\nDisplaying a progress bar in Bootstrap is a rather verbose set of elements and attributes:\n\n```\n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" aria-valuenow=\"60\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: 60%;\">\n    <span class=\"sr-only\">60% Complete</span>\n  </div>\n</div>\n```\nThe [progress bar tag helper](https://github.com/dpaquette/TagHelperSamples/blob/master/TagHelperSamples/src/TagHelperSamples.Bootstrap/ProgressBarTagHelper.cs) provides a much cleaner syntax:\n\n```\n<div bs-progress-value=\"66\">\n</div>\n```\n\n## Modal\n\nBootstrap modals are also rather convoluted items. The simplest possible modal consists of too many nested divs and in my opinion is hard to read:\n\n```\n<div class=\"modal fade\">\n  <div class=\"modal-dialog\">\n    <div class=\"modal-content\">\n      <div class=\"modal-header\">\n        <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button>\n        <h4 class=\"modal-title\">Modal title</h4>\n      </div>\n      <div class=\"modal-body\">\n        <p>One fine body&hellip;</p>\n      </div>\n      <div class=\"modal-footer\">\n        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n        <button type=\"button\" class=\"btn btn-primary\">Save changes</button>\n      </div>\n    </div><!-- /.modal-content -->\n  </div><!-- /.modal-dialog -->\n</div><!-- /.modal -->\n```\n\nThe same modal using the [modal tag helper](https://github.com/dpaquette/TagHelperSamples/blob/master/TagHelperSamples/src/TagHelperSamples.Bootstrap/ModalTagHelper.cs) is much easier to read and will produce the same output:\n\n```\n<modal id=\"simpleModal\" title=\"Modal Title\" >\n    <modal-body>\n        <p>One fine body&hellip;</p>\n    </modal-body>\n    <modal-footer>\n        <button type=\"button\" class=\"btn btn-primary\">Save changes</button>\n    </modal-footer>\n</modal>\n```\n\n## Wrapping it up\n\nFeel free to browse the [sample code](https://github.com/dpaquette/TagHelperSamples/tree/master/TagHelperSamples/src) or [view them in action on Azure](http://taghelpersamples.azurewebsites.net/). If you have ideas for other Tag Helpers, feel free to [log an issue](https://github.com/dpaquette/TagHelperSamples/issues) in the repo. Better yet, you could also submit a pull request.\n\nA big thank you to [Rick Anderson](https://twitter.com/RickAndMSFT) for suggesting this and getting us started and to [Rick Strahl](http://weblog.west-wind.com/) for contributing.\n","categories":[],"tags":[]},{"title":"Kanban","slug":"podcast-kanban","date":"2015-09-19 23:46:54+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-kanban/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-kanban/","excerpt":"The WesternDevs try their hand at pronouncing, then describing Kanban","raw":"---\nlayout: podcast\ntitle:  \"Kanban\"\ndate: 2015-09-19T15:46:54-04:00\nrecorded: 2015-09-11\ncategories: podcasts\nexcerpt: \"The WesternDevs try their hand at pronouncing, then describing Kanban\"\ncomments: true\npodcast:\n    filename: \"Kanban.mp3\"\n    length: \"50:38\"\n    filesize: 48603716\n    libsynId: 5316367\n    anchorFmId: Kanban-evqdj7\nparticipants:\n    - dylan_smith\n    - kyle_baley\n    - dave_paquette\n    - amir_barylko\n    - david_wesst\n    - simon_timms\nlinks:\n    - Kanban (Lean Manufacturing)|https://en.wikipedia.org/wiki/Kanban\n    - Kanban (Process Management)|https://en.wikipedia.org/wiki/Kanban_(development)\n    - LeanKanban University|http://edu.leankanban.com/\n    - An introduction into Kanban|https://www.atlassian.com/agile/kanban\n    - SmartView|http://smartviewapp.com/\n    - Jira|https://www.atlassian.com/software/jira\n    - AgileZen|http://www.agilezen.com/\n    - LeanKit|http://leankit.com/\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n\n---\n\n### Synopsis\n\n* Definition(s) of Kanban\n* Layering Kanban onto an existing process\n* Model first, then improve\n* Using Kanban with Scrum\n* The importance of visualization\n* Work-in-process limits\n* Metrics for WIP limits\n* Relationship among Lean and Agile and Kanban\n* Identifying and eliminating waste\n* Empowering your team\n* Where Kanban doesn't work\n* Kanban as a mindset rather than a process\n* Determining success of Kanban\n* Lead time\n* Kanban in distributed teams\n* [Amir's](http://www.westerndevs.com/bios/amir_barylko/) motivation to build SmartView\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Task could not find sgen.exe using SdkToolPath","authorId":"donald_belcham","slug":"sgen-not-found","date":"2015-09-18 02:20:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/sgen-not-found/","link":"","permalink":"https://westerndevs.com/_/sgen-not-found/","excerpt":"I spent the better part of this afternoon fighting with this error (and arguing Canadian voting rights with the Western Devs). I was trying to run our projectâ€™s build script which uses nAnt and MSBuild to work all the compilation magic we need. There are a lot of pieces of information on how to solve this on the web. Most solutions revolve around &quot;Install Visual Studio 2010&quot;, &quot;Install the Windows Software Development Kit for Windows X&quot;, or &quot;Turn off the generation of serialization assemblies in your projects/solution&quot;. Some of these are just downright scary solutionsâ€¦others wonâ€™t work in my situation.","raw":"---\nlayout: post\ntitle:  Task could not find sgen.exe using SdkToolPath\ndate: 2015-09-17T16:20:00-06:00\ncategories:\ncomments: true\nauthorId: donald_belcham\n---\n\nI spent the better part of this afternoon fighting with this error (and arguing Canadian voting rights with the [Western Devs](http://www.westerndevs.com)). I was trying to run our projectâ€™s build script which uses nAnt and MSBuild to work all the compilation magic we need. There are a lot of pieces of information on how to solve this on the web. Most solutions revolve around _\"Install Visual Studio 2010\"_, _\"Install the Windows Software Development Kit for Windows X\"_, or _\"Turn off the generation of serialization assemblies in your projects/solution\"_. Some of these are just downright scary solutionsâ€¦others wonâ€™t work in my situation.\n\n<!--more-->\n  \nI do almost all of my development work in Azure VMs these days. I can spin up a new one with Visual Studio already installed in minutes. This allows me to easily and quickly keep all of my different clients and projects in isolated buckets. So for one of my current projects I created a Windows 2012 R2 + Visual Studio 2015 VM. This is the VM that started throwing the above error. So when I asked the all knowing Google (and its smarter friend StackOverflow) I was perplexed by those three common solutions that I was finding.\n\n* **Install Visual Studio 2010** â€“ ummmmâ€¦.no. Why should I need to do that? \n* **Install the Windows SDK** â€“ there really isnâ€™t one for Windows 2012. \n* **Turn of generation of serialization assemblies** â€“ I have no idea if this project needs them or not so Iâ€™m not willing to make that change\n\nThis left me to piece together a solution on my ownâ€¦which is likely why it took all afternoon. After looking through all the information that was buried in comments on answers for issues on StackOverflow, I managed to come up with this:\n\nOpen up regedit on your computer. Youâ€™re not going to change anything so just calm down. Navigate to **Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\MSBuild\\ToolsVersions\\4.0**\n\n![Step 1](http://farm6.staticflickr.com/5618/21309498739_f91817e2d6_z.jpg)\n\nNote the **SDK40ToolsPath** entry and the value for it. This value will point to another registry key which you need to go find next. Itâ€™s likely going to point at something that starts with **Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Microsoft SDKs\\Windows\\**\n\n![Step 2](http://farm6.staticflickr.com/5727/21309498889_112153e5f6_z.jpg)\n\nAs you can see here thereâ€™s an InstallationFolder key that has a file path for you to go find in Windows Explorer.\n\n![Step 3](http://farm1.staticflickr.com/757/21308591438_fbc2579c84_z.jpg)\n\nWhen you get to that folder youâ€™ll probably see what I saw aboveâ€¦no sgen.exe file. This is what the problem is. MSBuild is looking for sgen.exe in this location and canâ€™t find it. How do you fix that? You could change registry settings, but there is risk in thatâ€¦and I told you earlier that you wouldnâ€™t have to. Another option is to find a copy of sgen.exe and put it in that folder. From what I can tell sgen.exe is pretty stand alone and any version you find should work. I went to a slightly newer SDK version on my machine and copied the file from there.\n\n![Step 4](http://farm1.staticflickr.com/700/21470168646_90a88cf5b3_z.jpg)\n\nWith sgen.exe now in the right folder, my build works just fineâ€¦so fine that removing the file keeps the build working and I wasnâ€™t able to get a screenshot of the error for this post.\n","categories":[],"tags":[]},{"title":"An Intro to Android Data Binding","authorId":"tom_opgenorth","slug":"an-intro-to-android-data-binding","date":"2015-09-16 06:32:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/an-intro-to-android-data-binding/","link":"","permalink":"https://westerndevs.com/_/an-intro-to-android-data-binding/","excerpt":"In May, 2015 at Google announced a data binding library for Android. It's long overdue â€“ developers no longer have to come up with their own schemes for displaying or retrieving data from their views. With two-way data binding, it's possible to remove a lot of redundant boilerplate code from the activities and fragments that make up an application.","raw":"---\nlayout: post\ntitle:  An Intro to Android Data Binding\ndate: 2015-09-15T20:32:12-06:00\ncategories:\ncomments: true\nauthorId: tom_opgenorth\noriginalurl: http://www.opgenorth.net/blog/2015/06/21/android-data-binding-intro/\n---\n\nIn May, 2015 at Google announced [a data binding library for Android](https://developer.android.com/tools/data-binding/guide.html). It's long overdue &ndash; developers no longer have to come up with their own schemes for displaying or retrieving data from their views. With two-way data binding, it's possible to remove a lot of redundant boilerplate code from the activities and fragments that make up an application.\n\n<!--more-->\n  \n_Just a warning - Android data binding is still a beta product, so as such things may or may not work when they should, as they should, and the documentation may or may not be accurate._\n\nThere were several steps/phases that I went through while I was learning this. Here's what I did:\n\n1. **Add data binding to Android Studio** &ndash; This is a one time thing, a couple of lines in some Gradle files.\n2. **Create a POJO for the binding** &ndash; You don't necessarily want to bind to a domain object. Arguable it's a cleaner design to have another class with responsiblity of data binding (and maybe some validation too). Model-View-ViewModel is an excellent pattern in this regard.\n3. **Update the layout file** &ndash; We help the data binding library out by adding some meta-data/markup to our layout files.\n4. **Update the activity to declare the data binding** &ndash; This will tell the data binding library how to connect the views to the POJO.\n\nThe [source code for this sample](https://github.com/topgenorth/drunken-bear) is up on Github.\n\n# Adding Data Binding to Your Project\n\nFirst off, make sure you're running Android Studio 1.3 or higher. As long as you're keeping current with the Android Studio\n\nNext I had to edit the project's **build.gradle** file, my  `dependencies` section looks like this:\n\n\tdependencies {\n\t    classpath 'com.android.tools.build:gradle:1.3.1'\n\t    // TODO: when the final verison of dataBinder is release, change this to use a version number.\n\t    classpath 'com.android.databinding:dataBinder:1.+'\n\t}\n\nAfter that, I updated the **build.gradle** for the app module. The first two line in the file are:\n\n\tapply plugin: 'com.android.application'\n\tapply plugin: 'com.android.databinding'\n\nThat's pretty much about it. Now that our project is aware of data binding, let's see about the code and UI changes I had to make.\n\n# Using Data Binding\n\nFrom here, you might be best off reading [Google's docs on data binding](https://developer.android.com/tools/data-binding/guide.html), just to get a feel for how things work. If you're familiar with data binding in XAML (say WPF or Xamarin.Forms), you might notice some simularities.\n\n(*Allow me digress a bit and offer this piece of advice again: think twice about binding directly to your data model. This is a perfect opportunity to bring some Model-View-ViewModel goodness into your Android application. I'm not going to talk to much about MVVM though.*)\n\n## Updating the Source Code\n\nTo keep my UI as code free as possible, I abstracted much of the data binding logic into the following class (his isn't all the code, just the parts relevant for this example):\n\n{% codeblock lang:java %}\npublic class PhonewordViewModel extends BaseObservable {\n    private boolean mIsTranslated = false;\n    private String mPhoneNumber = \"\";\n    private String mPhoneWord = \"\";\n    private String mCallButtonText = \"Call\";\n\n    @Bindable\n    public String getPhoneNumber() {\n        return mPhoneNumber;\n    }\n\n    @Bindable\n    public String getCallButtonText() {\n        return mCallButtonText;\n    }\n\n    @Bindable\n    public boolean getIsTranslated() {\n        return mIsTranslated;\n    }\n\n    @Bindable\n    public String getPhoneWord() {\n        return mPhoneWord;\n    }\n\n\n    public void setPhoneWord(String phoneWord) {\n        mPhoneWord = phoneWord;\n        onTranslate(null);\n\n    }\n\n    public void onTranslate(View v) {\n        mPhoneNumber = toNumber(mPhoneWord);\n\n        if (TextUtils.isEmpty(mPhoneNumber)) {\n            mCallButtonText = \"Call\";\n            mIsTranslated = false;\n        } else {\n            mIsTranslated = true;\n            mCallButtonText = \"Call \" + mPhoneNumber + \"?\";\n        }\n        notifyPropertyChanged(net.opgenorth.phoneword.BR.phoneNumber);\n        notifyPropertyChanged(net.opgenorth.phoneword.BR.isTranslated);\n        notifyPropertyChanged(net.opgenorth.phoneword.BR.callButtonText);\n    }\n}\n{% endcodeblock %}\n\nHere I've encapsulated logic into a view class that subclasses `BaseObservable`. Subclassing isn't mandatory &ndash; a naked POJO will work too. However, by making a custom view `BaseObservable` provides the infrastructure for setting up the data binding; and this custom view class can notify registered listeners as values change. As well, POJO's should be kept as dumb as possible without any intricate knowledge of views. By sticking the data binding logic in a view class like this I, honour the whole \"separation of concerns\" concept.\n\nNotice that the getters are adorned with the `@Bindable` annotation - this identifies how the listeners should retrieve values from the properties.\n\nIt's the responsibility of the bound class to notify clients when a property has changed. You can see this happening with the use of `notifyPropertyChanged`. This causes a signal to be raised to listeners; this is how they find out the name has changed.\n\nThe `BR` class is generated by the data binding library. It is to data binding what the `R` class is to layout files. Each POJO field or method adorned with `@Bindable` will have a constant declared in the `BR` class at compile time corresponding to the name. So, `getPhoneNumber()` becomes `BR.phoneNumber`.\n\nWith the code out of the way, it's time to update the layout.\n\n## Update the XML Layout\n\nThere were a couple of changes that I needed to make to my existing layout for things to work:\n\n1. Declare some variables in my layout.\n2. Identify properties on the various widgets that will be bound to the variable declared above.\n3. Establish the data binding in the Activity.\n\nAndroid's data binding requires that `<layout>` be the root element of the layout. My old layout started with a `<LinearLayout>`. It's also necessary to add a `<data/>` section that will declare variables and the classes that will be bound to.\n\n### Declare A Variable\n\nWe need to declare a variable that the data binding framework can... bind too. I had to add a `<data>` element with a child `<variable>` element that names the variable and identifies the type Android should use for the binding:\n\n\n{% codeblock lang:xml %}\n<layout xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:app=\"http://schemas.android.com/apk/res-auto\">\n\t<data>\n\t\t<variable\n\t\t\tname=\"phonewordVM\"\n\t\t\ttype=\"net.opgenorth.phoneword.PhonewordViewModel\" />\n\t</data>\n\n\t<!-- my old layout is here, but omitted for clarity -->\n\n</layout>\n{% endcodeblock %}\n\nThis declares a variable `phonewordVM` that I can use inside my layout file.\n\nNotice that the `xmlns:app=\"http://schemas.android.com/apk/res-auto\"` will automatically drag local namespaces into your XML. This helps you out a bit because you don't have to explicitly declare all the namespaces in layout file.\n\n### Declare the Bindings in the Layout\n\nNext, I need to set up the binding. In this example, all I want to do is to bind `setPhoneWord()`/`getPhoneWord()` in my custom view class to an `EditText`. This little XML snippet shows the binding in action:\n\n{% codeblock lang:xml %}\n<EditText\n\tandroid:id=\"@+id/phoneword_text\"\n\tandroid:layout_width=\"fill_parent\"\n\tandroid:layout_height=\"wrap_content\"\n\tandroid:layout_marginLeft=\"10dp\"\n\tandroid:layout_marginRight=\"10dp\"\n\tandroid:hint=\"@string/phoneword_label_text\"\n\tandroid:text=\"@{phonewordVM.phoneWord}\"\n\ttools:ignore=\"TextFields\" />\n{% endcodeblock %}\n\nNotice the syntax to declare the binding: `@{phonewordVM.phoneWord}` &ndash; this is how I setup the binding in the layout file. With this in place, the last thing to do is to setup the data binding in the activity.\n\n# Establish the Data Binding\n\nFinally, setting up the data binding. This is a very minimal amount of code. We no longer have to first get a reference to a view, access properties on the view, and then manually transfer the value of that view to some domain object or variable in our application. Android Data Binding takes are of all that for me.\n\nBelow is a snippet from the fragment:\n\n{% codeblock lang:java %}\n\npublic class MainActivityFragment extends Fragment {\n\n    private PhonewordViewModel mPhonewordViewModel;\n    private FragmentMainBinding mBinding;\n\n    public MainActivityFragment() {\n    }\n\n    @Override\n    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {\n        mPhonewordViewModel = new PhonewordViewModel();\n\n        mBinding = DataBindingUtil.inflate(inflater, R.layout.fragment_main, container, false);\n        mBinding.setPhonewordVM(mPhonewordViewModel);\n        View v = mBinding.getRoot();\n\n        mBinding.callButton.setOnClickListener(\n                new View.OnClickListener() {\n                    @Override\n                    public void onClick(View v) {\n                        final Intent callIntent = new Intent(Intent.ACTION_CALL);\n                        AlertDialog.Builder alertDialogBuilder = new AlertDialog.Builder(getActivity());\n                        alertDialogBuilder\n                                .setMessage(mBinding.callButton.getText())\n                                .setNeutralButton(R.string.call_button_text, new DialogInterface.OnClickListener() {\n                                    @Override\n                                    public void onClick(DialogInterface dialog, int which) {\n                                        callIntent.setData(Uri.parse(\"tel:\" + mPhonewordViewModel.getPhoneNumber()));\n                                        PhonewordUtils.savePhoneword(getActivity(), mPhonewordViewModel.getPhoneWord());\n                                        startActivity(callIntent);\n                                    }\n                                })\n                                .setNegativeButton(R.string.cancel_text, new DialogInterface.OnClickListener() {\n                                    @Override\n                                    public void onClick(DialogInterface dialog, int which) {\n                                        // Nothing to do here.\n                                    }\n                                })\n                                .show();\n                    }\n                }\n        );\n\n\n        mBinding.translateButton.setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                mPhonewordViewModel.setPhoneWord(mBinding.phonewordText.getText().toString());\n                mPhonewordViewModel.translatePhoneWord();\n            }\n        });\n\n        return v;\n    }\n}\n\n{% endcodeblock %}\n\nThere are a couple of key things to notice here. First, observe that the fragment inflates a view called `fragment_main.xml`. The data binding library generates the code of a class called `FragmentMainBinding`. The name of the binding class is derived from the name of the layout file, with the work `Binding` appended to it.\n\nOnce the binding is instantiated, I tell it what object to bind to. The data binding library created a setter called `setPhonewordVM` &ndash; this is because we declared the variable `phonewordVM` in our layout file above.\n\nAnother interesting thing is that the code for the fragment does not use `findViewById` or hold a reference to any of the views layout. That is because the `FragmentMainBinding` has those references. So, for example, if I want to get the value of an `EditText` with the id `+@id/phonewordText`, then `mBinding.phonewordText.getText()` will do the trick.\n\nI set the `OnClickListener` for the buttons in a very traditional way. In theory, the data binding library should allow to bind event listeners to methods on a view model. However, I have't been able to get that to work yet. Hopefully I'll have more luck next version of the data binding library (and/or an update to the docs for the data binding library)\n\n# Sie Sind Fertig\n\nWith all this, data binding has been accomplished. It may seem like a lot of code, and perhaps it is for such a trivial example. Where the true power of this comes into play is when you want to write tests for your code. Two way data binding lays the framework for the Model-View-View Model pattern, which in turn helps you create a loosely coupled app that is easier to test.\n","categories":[],"tags":[]},{"title":"CrashPlan Tip - Move the cache directory","authorId":"simon_timms","slug":"crashplan-tip","date":"2015-09-15 20:19:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/crashplan-tip/","link":"","permalink":"https://westerndevs.com/_/crashplan-tip/","excerpt":"I use CrashPlan to back up my collection of computers. It is a great tool and has saved me on a number of occasions. Most memorably was the time that I forgot the password for my comically well-encrypted drive. Restoring from crash plan got me back all the important things.","raw":"---\nlayout: post\ntitle:  CrashPlan Tip - Move the cache directory\ndate: 2015-09-15T10:19:12-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\n---\n\nI use CrashPlan to back up my collection of computers. It is a great tool and has saved me on a number of occasions. Most memorably was the time that I forgot the password for my comically well-encrypted drive. Restoring from crash plan got me back all the important things. \n\n<!--more-->\n  \nOn my main machine I'm running out of disk space at an alarming rate. I have a 256GiB SSD in there as well as a 2TB spinning rust. I have tons of room on the rust drive but my SSD, which once seemed very large, is getting full. I cleaned up all I could but was still out of luck. A trip to [NCIX](http://ncix.com) and [MemoryExpress](http://memoryexpress.com) told me I was looking at between $250 and $350 for a 500GiB Samsung 850 EVO or Pro. I'd write it off as a business expense but I'd still rather have the money. I downloaded and installed a disk space tool called [Space Sniffer](http://www.uderzo.it/main_products/space_sniffer/index.html). \n\n![Image from the How-To-Geek](http://i.imgur.com/SZXsBSC.png)\n\nRunning this tool suggested that something like 15GiB of my space was being used by the CrashPlan cache. I guess this is used to speed up restores and backups in CrashPlan. Considering that my entire CrashPlan backup on this machine is only something like 25GiB that seemed excessive. Googling about I found an [unsupported How-To](http://support.code42.com/CrashPlan/Latest/Troubleshooting/Reassigning_Cache_Folder_To_A_Different_Directory) from CrashPlan that talked of how to move the cache. Following these steps and moving my cache directory off of fast disk worked perfectly. I am back up to a bunch of free space and I can defer the cost of a new drive for another year. ","categories":[],"tags":[]},{"title":"A discussion on knockout","authorId":"simon_timms","slug":"a-discussion-on-knockout","date":"2015-09-11 08:26:34+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/a-discussion-on-knockout/","link":"","permalink":"https://westerndevs.com/_/a-discussion-on-knockout/","excerpt":"It is rare that a day passes on the Western Devs' slack channel that we don't have some lively discussion. Today was my day to rant about knockout.js.","raw":"---\nlayout: post\ntitle:  A discussion on knockout\ndate: 2015-09-10T22:26:34-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\n---\n\nIt is rare that a day passes on the Western Devs' slack channel that we don't have some lively discussion. Today was my day to rant about knockout.js. \n\n<!--more-->\n  \nIf you haven't used knockout.js it is a JavaScript library used for bi-directional model binding. It was the first library of its type that I encountered, being deeply involved in the Microsoft web mentality at that point. \n\nMy current project uses knockout.js and although we've been successful with it there has been a great deal of pain. In fact it has been so irritating that I no longer recommend using knockout to anybody working on a serious project. This may seem crazy because there are a lot of sites out there, big sites, that make use of knockout. The new Azure portal is, perhaps, the most famous of these sites.\n\nWhat are my issues? Allow me to tell you: \n\n<ol>\n\t<li> The ko.observable model creates confusion between what is an observable and what isn't. There is not a day that passes that I don't mess up a binding or a piece of logic because I expect a variable to be a value and it is a function. This is because binding requires using ko.observables to actually function properly. I don't know if it is helpful that binding in either way works for basic properties.\n\t</li>\n</ol>\n{% codeblock lang:html %}\n<span data-bind=\"someproperty\"/>\nâ€‹\n<span data-bind=\"someproperty()\"/>\n{% endcodeblock %}\n\n<div style=\"margin-left: 40px\">\nOnce you start making more complicated bindings then you have to remeber to make the function call\n</div>\n{% codeblock lang:html %}\n<span data-bind=\"someproperty() + 1\"/>\nâ€‹{% endcodeblock %}\n\n<ol start=\"2\">\n<li>Observable array is missing a bunch of functions that exist on Array. I'm not talking just the new array prototype functions in ES6 but older functions like filter which have been around since 2011. This is super confusing. Why aren't the rest of the functions implemented? I'm not sure. I find the lack of filter especially irritating.</li> \n\n<li>The syntax around loops is messy. If you're in a foreach loop then it is really weird to get stuff from the outside</li>\n</ol>\n\n{% codeblock lang:html %}\n<tbody data-bind=\"foreach: somecollection\">\n\t<tr>\n\t\t<td data-bind=\"text: rowvalue\"/>\n\t\t<td data-bind=\"text: $parents[0].valueFromOuterModel\"/>\n\t</tr>\n</tbody>\t\nâ€‹{% endcodeblock %}\n\n\n<ol start=\"4\">\n<li>Binding using data- attributes encourages you to move your logic into the html code where it is all but untestable \n</li>\n</ol>\n{% codeblock lang:html %}\n<select class=\"form-control\" data-bind=\"value: item.State, \n\t\tattr:{'name': 'StateDropdown' + rowSuffix }, \n\t\tforeach: $parents[1].states().filter(function(item){ return item.Value() == $parents[0].State() || $parents[1].rows().map(function(row){return row.State();}).indexOf(item.Value()) < 0;}) \">\n{% endcodeblock %}\n\n<div style=\"margin-left: 40px\">\n<a href=\"http://www.westerndevs.com/bios/dave_white/\">Dave White</a> called me to the carpet for this complaint. \"Why aren't you doing your filtering in the model?\" he asked. Quite right, I should be doing it in the model and I, indeed, refactored the code later to do that. The point was that because you're doing the bindings in the html it is easy to fall into the lazy trap of just leaving it. Maybe knockout shouldn't support expressions in the data-bindings. \n</div>\n<ol start=\"5\">\n<li>\n\t<p>Knockout supports creating components but these are rendered at a non-deterministic time. This means that if you want to do something after the component has rendered you're pretty much left guessing. You can't do it right after you call the model bind because that is defered and as there is no componentHasMounted equivilent to hook into you can't do it there. If you wanted to, say, add an autocomplete to a dynamically added text box then when would you call that code? </p>\n\t<p>\n\nI honestly don't know. I've tried just setting a timeout and I've also tried hooking into mutation observers. The first is hacky and the second has limited browser support. \n</p>\n<p>\n<b>Update:</b> Dave and I poked about a bit and there might be a solution in using synchronous components, I'll experiment and update some more. \n</p>\n</li>\n</ol>\n<a href=\"http://www.westerndevs.com/bios/amir_barylko/\">Amir</a> asked what I would use instead. <a href=\"http://facebook.github.io/react/\">React</a>. Maybe <a href=\"http://aurelia.io/\">Aurelia</a>, although I haven't explored it enough yet. ","categories":[],"tags":[]},{"title":"Supporting Options and Arguments in Your dnx Commands","authorId":"james_chambers","slug":"supporting-options-and-arguments-in-your-dnx-commands","date":"2015-09-10 18:00:53+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/supporting-options-and-arguments-in-your-dnx-commands/","link":"","permalink":"https://westerndevs.com/_/supporting-options-and-arguments-in-your-dnx-commands/","excerpt":"Grab yourself your copy of Visual Studio 2015 and buckle up! Today we're going to create our own dnx command with support for options and arguments.","raw":"---\nlayout: post\ntitle:  \"Supporting Options and Arguments in Your dnx Commands\"\ndate: 2015-09-10T09:00:53-05:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/09/supporting-options-and-arguments-in-your-dnx-commands/\n---\n\nGrab yourself your copy of [Visual Studio 2015][1] and buckle up! Today we're going to create our own dnx command with support for options and arguments.\n\n<!--more-->\n\nIn my [previous post][2] on dnx commands I showed how you could create your own command as part of your project that could be invoked via the .Net Execution Environment, a.k.a., dnx. While this works fine in simple scenarios, chances are you might need to have more than one \"command\" embedded in your tooling. Right away you have concerns for parsing the arguments and options that are passed in, which will quickly lead to a more complex application than you were originally intending.\n\n> **Important Note**&nbsp; I am building the samples here in this post on Beta 6, knowing that there are two changes coming in, the first is that they are dropping the project path argument to dnx (the period, or \"current directory\"), and the second being the high likelihood that there will continue to be refinements in the namespaces of these libraries. I'll update these when I complete my upgrade to Beta 7.\n\n## A Real-world Example\n\nConsider Entity Framework, where you can access a number of different commands. It provides tooling to your application by making a number of commands related to your project, your entities, your database and your context available from the command line. This is great, because it also means that you can use it in automation tasks.\n\n![image][3]\n\nHere's the command as executed from the command line, followed by a call to get the help on a specific command, migration:\n\n{% codeblock lang:bash %}\ndnx . ef\ndnx . ef migration -h\n{% endcodeblock %}\n\nSo, think about those switches for a second, and the mistakes and string manipulation you'd need to do to pull that all together. What about supporting help and organizing your commands? Being able to accept different options and arguments can grow to be an exhausting exercise in bloatâ€¦\n\n## Unless!\n\nâ€¦unless, of course, you had an abstraction over those parsing bits to work with.&nbsp; Quite wonderfully, Microsoft has made available the bits you need to take away those pains, and it all starts with the following package (and a bit of secret sauce):\n\n```\nMicrosoft.Framework.CommandLineUtils.Sources\n```\n\nAnd here's the secret sauceâ€¦instead of using something like \"1.0.0-\\*\" for your version, use this instead: { \"version\": \"1.0.0-\\*\", \"type\": \"build\" }. This notation bakes the abstractions into your application so that you don't have to bundle and distribute multiple DLLs/dependencies when you author and share commands.\n\n> The full version of the final, working project in this post is [available on GitHub][4]. Feel free to pull down a copy and try this out for yourself!\n\nLet's get started.\n\n## Creating a Project\n\nAs [_previously covered_][2], creating an ASP.NET 5 command line app is all that is required to get started with creating your commands. We have to add that package as a dependency as well, which should look like this in it's entirety in your project.json:\n\n{% codeblock lang:json %}\n    \"dependencies\": {\n      \"Microsoft.Framework.CommandLineUtils.Sources\": { \"version\": \"1.0.0-*\", \"type\": \"build\" }\n    }\n {% endcodeblock %}\n\nNext, we need to make sure that our command is available and named as we'd like it to be called, which is also done in the project.json. Mine looks like this:\n\n{% codeblock lang:json %}\n\"commands\": {\n  \"sample-fu\": \"DnxCommandArguments\"\n},\n {% endcodeblock %}\n\nYou can imagine, of course, that it will be invoked much like Entity Framework, but with \"sample-fu\" instead of \"ef\". Feel free to name yours as you wish. With that out of the way, we can start to do the heavy lifting in getting our commands exposed to external tooling.\n\n## Working with the CommandLineUtils Objects\n\nHere is a bare-bones application that just displays it's own help message:\n\n{% codeblock lang:csharp %}\npublic int Main(string[] args)\n{\n    var app = new CommandLineApplication\n    {\n        Name = \"sample-fu\",\n        Description = \"Runs different methods as dnx commands\",\n        FullName = \"Sample-Fu - Your Do-nothing dnx Commandifier\"\n    };\n\n    // show the help for the application\n    app.OnExecute(() =>\n    {\n        app.ShowHelp();\n        return 2;\n    });\n\n    return app.Execute(args);\n}\n{% endcodeblock %}\n\nYou can see that our Main method is basically creating an instance of the CommandLineApplication class, initializing some properties and finally wiring up a Func to be executed at some point in the future.&nbsp; Main returns the result of app.Execute, which in turn handles the processing of anything passed in and itself returns the appropriate value (0 for success, anything else for non-success).&nbsp; Here it is in action (the completed version), simply by typing dnx . sample-fu at the commandline:\n\n![image][5]\n\nA quick note here as wellâ€¦the OnExecute() is called if no other command turns out to be appropriate to run, as determined by the internal handling in CommandLineApplication. In effect, we're saying, \"If the user passes nothing in, show the help.\" Help is derived from the configuration of commands, so to illustrate that, we need to add one.\n\n## Wiring Up a Command\n\nNow we get into the fun stuff. Let's write a command that takes a string as an argument and echos it right back out, and add an option to reverse the string.\n\n{% codeblock lang:csharp %}\napp.Command(\"display\", c =>\n{\n    c.Description = \"Displays a message of your choosing to console.\";\n\n    var reverseOption = c.Option(\"-r|--reverse\", \"Display the message in reverse\", CommandOptionType.NoValue);\n    var messageArg = c.Argument(\"[message]\", \"The message you wish to display\");\n    c.HelpOption(\"-?|-h|--help\");\n\n    c.OnExecute(() =>\n    {\n        var message = messageArg.Value;\n        if (reverseOption.HasValue())\n        {\n            message = new string(message.ToCharArray().Reverse().ToArray());\n        }\n        Console.WriteLine(message);\n        return 0;\n    });\n});\n{% endcodeblock %}\n\nCommand takes a name and an action in which we can add our options and arguments and process the input as required.&nbsp; We write a Func for OnExecute here as well, which will be called if the user types the command \"display\".&nbsp; The option is implemented as a \"NoValue\" option type, so the parser is not expecting any valueâ€¦it's either on the command line or it isn't.\n\nThe order of args is important, using the pattern:\n\n&nbsp;&nbsp;&nbsp; COMMAND OPTIONS ARGUMENTS\n\nYou'll get some errors if you don't follow that order (and there are some open GitHub issues to help make better parsing and error messages available).\n\n## A More Complicated Example\n\nNext up, let's implement a command that can do one of two operations based on the option specified, and takes two values for an argument. Here a basic implementation of a calc method, supporting addition and multiplication:\n\n{% codeblock lang:csharp %}\n    //  the \"calc\" command\n    app.Command(\"calc\", c =>\n    {\n        c.Description = \"Evaluates arguments with the operation specified.\";\n\n        var operationOption = c.Option(\"-o|--operation <operation>\", \"You can add or multiply the terms specified using 'add' or 'mul'.\", CommandOptionType.SingleValue);\n        var termsArg = c.Argument(\"[terms]\", \"The numbers to use as a term\", true);\n        c.HelpOption(\"-?|-h|--help\");\n\n        c.OnExecute(() =>\n        {\n            // check to see if we got what we were expecting\n            if (!operationOption.HasValue())\n            {\n                Console.WriteLine(\"No operation specified.\");\n                return 1;\n            }\n            if (termsArg.Values.Count != 2)\n            {\n                Console.WriteLine(\"You must specify exactly 2 terms.\");\n                return 1;\n            }\n\n            // perform the operation\n            var operation = operationOption.Value();\n            var term1 = int.Parse(termsArg.Values[0]);\n            var term2 = int.Parse(termsArg.Values[1]);\n            if (operation.ToLower() == \"mul\")\n            {\n                var result = term1 * term2;\n                Console.WriteLine($\" {term1} x {term2} = {result}\");\n            }\n            else\n            {\n                var result = term1 + term2;\n                Console.WriteLine($\" {term1} + {term2} = {result}\");\n            }\n            return 0;\n        });\n    });\n{% endcodeblock %}\n\nOf note are the differences between the options and the arguments versus the first command. The option accepts one of two values, and the argument can accept exactly two values. We have to do a bit of validation on our own here, but these are the basic mechanics of getting commands working.\n\nTaking it to the next level, you may wish to encapsulate your code in a class, or leverage the fact that DNX (and thus, your commands) are aware of the project context that you are running inâ€¦remember that if you are running in a project directory, you have the ability to read from the project.json.\n\n## Next Steps\n\nBe sure to grab [Visual Studio 2015][1] and then start experimenting with commands. You can have a look at some of the other repos/projects that leverage CommandLineUtils, or check out the [completed project from this post on GitHub][4].\n\nHappy Coding! ![Smile][6]\n\n[1]: https://www.visualstudio.com/?Wt.mc_id=DX_MVP4038205\n[2]: http://jameschambers.com/2015/08/writing-custom-commands-for-dnx-with-asp-net-5-0/\n[3]: http://jameschambers.com/wp-content/uploads/2015/09/image2.png \"image\"\n[4]: https://github.com/MisterJames/DnxCommandsWithOptsArgs/\n[5]: http://jameschambers.com/wp-content/uploads/2015/09/image3.png \"image\"\n[6]: http://jameschambers.com/wp-content/uploads/2015/09/wlEmoticon-smile1.png\n","categories":[],"tags":[]},{"title":"Work/Life Balance","slug":"podcast-work-life-balance","date":"2015-09-09 09:03:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-work-life-balance/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-work-life-balance/","excerpt":"Wherein the Western Devs try to balance work and not work","raw":"---\nlayout: podcast\ntitle:  \"Work/Life Balance\"\ndate: 2015-09-09T00:03:30-05:00\nrecorded: 2015-09-04\ncategories: podcasts\nexcerpt: \"Wherein the Western Devs try to balance work and not work\"\ncomments: true\npodcast:\n    filename: \"WorkLifeBalance.mp3\"\n    length: \"43:16\"\n    filesize: 41543561\n    libsynId: 5292087\n    anchorFmId: WorkLife-Balance-evqdic\nparticipants:\n    - dylan_smith\n    - kyle_baley\n    - james_chambers\n    - donald_belcham\nlinks:\n    - Is Daddy on a call?|http://www.hanselman.com/blog/IsDaddyOnACallABusyLightPresenceIndicatorForLyncForMyHomeOffice.aspx\n    - Wake up and get shit done|http://jameschambers.com/2015/03/wake-up-and-get-st-done-a-practice-of-awesome/\n    - Working from home and walking to work|http://jameschambers.com/2015/03/working-from-home-and-walking-to-work-surviving-remote-work/\n    - The Sabbath Manifesto|http://www.sabbathmanifesto.org/\n    - Work-life balance on Wikipedia|https://en.wikipedia.org/wiki/Work%E2%80%93life_balance\n    - Must know strategies for achieving work-life balance|https://www.cornerstone.edu/blogs/lifelong-learning-matters/post/must-know-strategies-for-achieving-work-life-balance\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* Why are we still talking about work/life balance in 2015?\n* Your balance != someone else's balance\n* The four pillars of balance: family, self, work, community\n* Recognizing when lines blur between pillars\n* Commitment and obligation\n* The digital sabbath: disconnecting\n* Stories of burnout\n* Maintaining balance while working from home\n* \"I'm not available right now\"\n* Isolation and location of your office\n* \"Going\" to work when you don't need to *go* to work\n* Disengaging at the end of the day\n* Technology that helps work/life balance\n* What do you do in your downtime?\n* Know when to make adjustments\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"WesternDevs learn about Docker - Part 2","authorId":"dave_white","slug":"westerndevs-learn-about-docker-part-2","date":"2015-09-08 05:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/westerndevs-learn-about-docker-part-2/","link":"","permalink":"https://westerndevs.com/_/westerndevs-learn-about-docker-part-2/","excerpt":"Dislaimer This blog post serves two purposes: Act as a historical record of a conversation with a bunch of interesting links in context, and to share a bit of an insider look at how conversations happen in the WesternDevs slack channels.","raw":"---\nlayout: post\ntitle: \"WesternDevs learn about Docker - Part 2\"\ndate: 2015-09-08 1:30:00\nauthorId: dave_white\noriginalurl: \ncomments: true\n---\n\n##### Dislaimer\nThis blog post serves two purposes: Act as a historical record of a conversation with a bunch of interesting links in context, and to share a bit of an insider look at how conversations happen in the WesternDevs slack channels.\n\n<!--more-->\n  \n### Introduction\nAs Tom mentioned in his post about [Docker containers for Novices][1] which I'm considering Part 1 of this post, the WesternDevs had a conversation about how Docker containers work. A good portion of our group has a lot of experience in the Windows world and not a lot of *nix experience which is where containers seem to have been born from. \n\nOne of the things that I love about WesternDevs is the fairly voracious appetite that all of us have to understand the various technologies that we use and talk about while also ensuring that the whole group understands. Leave no man behind while we learn I guess you would say.  \n\n### The seed of the conversation\n[\"Can AD, DHCP, DNS run in a container?\" seems like an innocuous question][13], but it was the question that started the conversation.\n\nSeveral of us started to posit that there are some things (in a Windows world) that would be fundamentally required for a container to run and that maybe AD (identity) was one of those things. \n\nAs often happens on Slack, someone says something that gets the rest of us dissenting. It is usually [@dylansmith][2], which was the case this time. His statement (paraphrased) was that \"Containers are like VMs so of course AD, DHCP, DNS, etc can run in containers.\" which several of us (myself included) disagreed with.  And thus began a very productive WD conversation.\n\nSo [@dylansmith][2] opened with \"Containers are like VMs\" and I ([@davewhite][3]) responded with no, they are not, they are more like AppDomain than VMs. This set the stage for the two points of view in the conversation.\n\n### What is a Container\nThe WesternDevs all agreed that containers are a tool for isolation into consistent environments. [Tom's post][1] quite nicely summed up our thoughts. I don't think there was any disagreement amongst us on that point. How containers achieved that was strongly debated and led to a deeper understanding of what a \"shared kernel\" is. \n\n>Containers running on a single machine all share the same operating system kernel so they start instantly and make more efficient use of RAM.\n\nA VM does not share a kernel with anything else, it shares physical resources with other VMs via the HyperVisor. \n\nWe agreed that VMs give extra confidence in the level of isolation that is afforded the running applications. VMs do not afford the same level of performance as a container at start-up, and will consume more of the physical resources of the hardware that a comparable solution using containers.\n\n### Windows vs. Linux\nOne of the things that underlied this conversation was the fundamental difference between the Windows kernel and Linux kernel. As far as we understand, the Windows kernel is big. Really big. And pulling all of the \"user\" stuff out of the \"system\" stuff will be very difficult for Microsoft and thus make containers on Windows ([which is coming whether you like it or not][4]) require a two-pronged approach, VMs that think they are containers and just pure containers. \n\nThe Linux kernel is much smaller, providing basic resources to \"user\" modules. [@stimms][6] provided an awesome link to [ChimeraCoder][5] presentation about achieving Docker containers without Docker as some information on how the linux kernel works. The difference between [systemd][7] and a distro was nicely described in this presentation, and helped move the conversation a long.\n\nThis also helped us understand we could run different distros of Linux in containers, as long as they all shared the same systemd version. You'd just install the distro of your choice in the container! Distros that didn't share the same kernel wouldn't be able to live side-by-side in containers on that host. Here is a great [SuperUser][8] post about this that helped as well as a [Docker article][9] about the underlying technology that Docker uses.\n\nAnd Tom also shared that...\n>But itâ€™s the reason why Docker requires a VM to run on Windows or OS X. Neither of those have the proper kernel extensions for Docker to work properly.\n\n### Throttling/Managing Container Resource Usage\nAfter that, the conversation turned to managing container resources. If a container is using a hosts kernel, wouldn't that mean that a container could consume all of the resources of the machine/host without knowledge of any other containers? \n\nAs it turns out, Linux already provides for this in the kernel. Linux has something called \"cgroups\" that can be used to group processes and [manage all kinds of resources][10]. This is how Docker (and probably all [systemd][7] based containers) works.\n\n### What services does a host (Linux kernel) need to provide for a Container\nWith our core understanding of how Docker works rounding into consensus, we started to get back to the original question of what could run in a Windows container? [@topgenorth][11] to the rescue once again providing the group with a link to an introduction blog post on the [extensions that allow linux to create and manage containers][12].\n\nWe started with this list as a basis for discussing what might need to be shared by a Windows host to it's containers so that they could run. I don't think we have an answer on what all of the comparable Windows services are, but we have started to explore that question. \n\n### Wrapping up the Conversation\nSo in the end, we decided that a container was either an \"AppDomain on steroids\" or a \"Dumbed down VM\" and that we didn't know for sure what containers on a Windows Server would look like in the end. A VM clearly provides isolation over and above what is intended by containers and doesn't provide the performance benefits of using a shared kernel. But an AppDomain in itself doesn't provide the level of isolation, control, or repeatable environments that real containers and VMs can provide. \n\n### Final Thoughts\nIn the end as is very often the case, we decided that we were all right, we were all wrong, and that we had learned a lot during the conversation and we hope that you can share in our learning and new-found common understanding of how containers work and how they might work in a Windows world. \n\n[1]: http://www.westerndevs.com/docker-containers-explained-for-the-novice/\n[2]: http://www.westerndevs.com/bios/dylan_smith/\n[3]: http://www.westerndevs.com/bios/dave_white/\n[4]: http://www.westerndevs.com/windows-server-containers-are-coming-whether-you-like-it-or-not/\n[5]: http://chimeracoder.github.io/docker-without-docker/#1\n[6]: http://www.westerndevs.com/bios/simon_timms/\n[7]: http://chimeracoder.github.io/docker-without-docker/#9\n[8]: http://superuser.com/questions/889472/docker-containers-have-their-own-kernel-or-not\n[9]: https://docs.docker.com/introduction/understanding-docker/#the-underlying-technology\n[10]: https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/\n[11]: http://www.westerndevs.com/bios/tom_opgenorth/\n[12]: https://linuxcontainers.org/lxc/introduction/\n[13]: http://codebetter.com/kylebaley/2015/09/01/windows-server-containers-are-coming-whether-you-like-it-or-not/#comment-2232115672\n\n","categories":[],"tags":[]},{"title":"Western Devs: Three Months On","slug":"western-devs-three-months-on","date":"2015-09-04 21:32:55+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/western-devs-three-months-on/","link":"","permalink":"https://westerndevs.com/_/western-devs-three-months-on/","excerpt":"Yesterday was the three month anniversary of our first post and while our official first post was on June 17, we're celebrating anyway.","raw":"---\nlayout: post\ntitle: \"Western Devs: Three Months On\"\ndate: 2015-09-04T13:32:55-04:00\ncomments: true\n---\n\nYesterday was the three month anniversary of our [first post][1] and while our *official* first post was on June 17, we're celebrating anyway. \n\n<!--more-->\n  \nSince that time, we have generated [60 posts](http://www.westerndevs.com/posts/) by 10 different people, as well as [6 podcasts](http://www.westerndevs.com/podcasts/)<sup>*</sup>. We've spoken at three events and are scheduled at [eight more][4] in the next few months. A sampling of some of the topics we've covered:\n\n* [Running Docker on Windows 10](http://www.westerndevs.com/getting-docker-running-on-windows-10/)\n* [Microservices](http://www.westerndevs.com/microservices-a-gentle-introduction/)\n* [ASP.NET 5](http://www.westerndevs.com/writing-custom-commands-for-dnx-with-asp-net-5-0/)\n* [MVC 6](http://www.westerndevs.com/mvc-6-image-tag-helper/)\n* [PowerShell](http://www.westerndevs.com/setting-up-an-iis-site-using-powershell/)\n* [Eclipse](http://www.westerndevs.com/running-tomcat-apps-on-docker-through-eclipse/)\n* [Azure ARM](http://www.westerndevs.com/using-azure-arm-to-deploy-a-docker-container/)\n* [BDD and TDD](http://www.westerndevs.com/bdd-vs-tdd/)\n* [Running a code camp](http://www.westerndevs.com/running-your-first-code-camp/)\n* [The #NoEstimates movement](http://www.westerndevs.com/podcasts/podcast-no-estimates/)\n* [UI testing](http://www.westerndevs.com/on-ui-testing/)\n* [DevOps](http://www.westerndevs.com/podcasts/podcast-devops/)\n\nUnderlying all of this is an extremely active Slack channel populated by a group of very lucky and grateful developers. Part support group/part continuous psychotherapy, it is there where many of the seeds for our posts and podcasts start and where the line is blurred between work and life balance. Outside of our technical prowess, it is where we discuss smoked meat and veggies, tangos, the accuracy of Big Ben, [redaction](https://twitter.com/Dave_Paquette/status/626524178041024512), incorporating [DQ Blizzard deliveries](https://twitter.com/WesternDevs/status/618493359942860800) into our contracts, and the best way to get a first class ticket on Emirates Airlines (hint: through [Alaska Airlines](http://onemileatatime.boardingarea.com/2015/08/03/alaska-emirates-first-class/)).\n\nWill the momentum continue? It's hard to say. Although the online presence started only recently, this has been boiling under the surface for [many years now](http://www.westerndevs.com/whoweare/) so chances are good. We're still finding our feet in some areas but for now, we're still talking.\n\n<p class=\"footnote\">\n*For reference, in that same time, <a href=\"http://lostechies.com\">Los Techies</a> has had 26 posts and <a href=\"http://codebetter.com\">CodeBetter</a> nine. Just sayin' <span class=\"fa fa-smile-o\"></span>\n</p>\n\n[1]: http://www.westerndevs.com/mvc-6-cache-tag-helper/\n[2]: http://lostechies.com\n[3]: http://codebetter.com\n[4]: http://www.westerndevs.com/wherewellbe/","categories":[],"tags":[]},{"title":"Docker Containers Explained for the Novice","authorId":"tom_opgenorth","slug":"docker-containers-explained-for-the-novice","date":"2015-09-04 18:19:31+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/docker-containers-explained-for-the-novice/","link":"","permalink":"https://westerndevs.com/_/docker-containers-explained-for-the-novice/","excerpt":"Over at the WesternDev &quot;consortium&quot; a random discussion broke out about containers: what are they, how are they different from virtual machines, and how do they work. While no means a &quot;container expert&quot;, I have dabbled a bit and sought to add some clarity to the discussion. It seems that I made enough sense and so thought I would summarize the dicussion here.","raw":"---\nlayout: post\ntitle:  Docker Containers Explained for the Novice\ndate: 2015-09-04T08:19:31-06:00\ncomments: true\nauthorId: tom_opgenorth\noriginalurl: http://www.opgenorth.net/blog/2015/09/02/docker-containers-explained-for-the-novice/\n---\nOver at the [WesternDev](http://www.westerndevs.com/) \"consortium\" a random discussion broke out about _containers_: what are they, how are they different from virtual machines, and how do they work. While no means a \"container expert\", I have dabbled a bit and sought to add some clarity to the discussion. It seems that I made enough sense and so thought I would summarize the dicussion here.\n\n<!--more-->\n\nThe whole idea behind containers is to isolate an application in a known environment. This helps prevent strange interactions with other software or libraries installed as well. I think [Docker](http://www.docker.com) has the best, concise description of what containers are:\n\n> Containers running on a single machine all share the same operating system kernel so they start instantly and make more efficient use of RAM.\n\nand\n\n> ... containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries â€“ anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in.\n\nSo, while both containers and virtual machines provide isolation, they differ in how they do it. VM's will emulate the hardware; each VM thinks it's a computer with it's own CPU's, RAM, hard disk, kernel, etc. This isolation is provided by the virtualization host which runs on the hardware.\n\nContainers, on the other hand, have a \"host\" that uses some kernel extensions to isolate software, but otherwise everything is running on the hardware. Containers share the host computer's RAM, CPUs, and even the kernel, however each container is secluded from the others and the host operating system. Because of this, containers can startup much faster and appear to be more responsive &ndash; they don't have to talk to a middle man to get access to the hardware.\n\nMost modern Linux distros ship with somee [extensions](https://linuxcontainers.org) to support containers out of the box, so, in theory, you can just dive right and start creating containers without having to do anything extra on Linux.\n\nIn practice it's easier to use something like Docker to create and manage your containers. Docker also provides a way to share containers via [DockerHub](https://hub.docker.com/). You search DockerHub for something you need, like say [Mono](https://hub.docker.com/_/mono/), and then you grab the [Dockerfile](https://docs.docker.com/reference/builder/) (a recipe that tells Docker how to build the container), and away you go. Alternately, you can create your own custom Docker images based on an existing Dockerfile. It's kind of like subclassing a container, if you will.\n","categories":[],"tags":[]},{"title":"Adding Prefixes to Tag Helpers in MVC 6","authorId":"dave_paquette","slug":"adding-prefixes-to-tag-helpers-in-mvc-6","date":"2015-09-04 01:24:56+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/adding-prefixes-to-tag-helpers-in-mvc-6/","link":"","permalink":"https://westerndevs.com/_/adding-prefixes-to-tag-helpers-in-mvc-6/","excerpt":"Some people have said that they would prefer all Tag Helper elements in MVC 6 to be prefixed. I honestly don't see myself doing this but it is easy to turn on if you prefer tag helper elements to be prefixed with some special text.","raw":"---\nlayout: post\ntitle:  \"Adding Prefixes to Tag Helpers in MVC 6\"\ndate: 2015-09-03T17:24:56-04:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/09/03/adding-prefixes-to-tag-helpers-in-mvc-6.aspx\n---\n\nSome people have said that they would prefer all Tag Helper elements in MVC 6 to be prefixed. I honestly don't see myself doing this but it is easy to turn on if you prefer tag helper elements to be prefixed with some special text.\n\n<!--more-->\n\nSimply add the @tagHelperPrefix directive to the _ViewImports.cshtml file in your project:\n\n    @tagHelperPrefix \"th:\"\n\nNow, Razor will only recognize elements as Tag Helpers if the elements are prefixed with \"th:\".\n\n![image][1]\n\nYou can choose whatever prefix you want for your project. As I said, I probably won't be using this myself but at least there is an easy way to turn on tag helper prefixes for those who want to be very explicit about tag helpers.\n\nOne nice thing with prefixes is that it enables is a quick way to identify what tag helpers exist in a project. When you type in the prefix, IntelliSense will show you a list of elements that can be processed by tag helpers:\n\n![image][2]\n\nWhat do you think? Prefix or not prefix?\n\n[1]: http://www.davepaquette.com/wp-content/uploads/2015/09/image_thumb.png \"image\"\n[2]: http://www.davepaquette.com/wp-content/uploads/2015/09/image_thumb1.png \"image\"\n  ","categories":[],"tags":[]},{"title":"A Couple of Things About Powershell Remoting","authorId":"simon_timms","slug":"a-couple-of-things-about-powershell-remoting","date":"2015-09-03 18:18:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/a-couple-of-things-about-powershell-remoting/","link":"","permalink":"https://westerndevs.com/_/a-couple-of-things-about-powershell-remoting/","excerpt":"I couldn't find the answers to these questions readily anywhere on the internet so I thought I would write them down here for the good of mankind. When using remoting as a different user does the target account or my account need to be an admin?","raw":"---\nlayout: post\ntitle:  A Couple of Things About Powershell Remoting\ndate: 2015-09-03T08:18:46-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\n---\n\nI couldn't find the answers to these questions readily anywhere on the internet so I thought I would write them down here for the good of mankind.\n\n**When using remoting as a different user does the target account or my account need to be an admin?**\n\n<!--more-->\n\nThe target account is the key one here. If you're logging in with a different user account you don't need to have any sort of admin rights yourself but the account specifed in the credentials you pass in will.\n\n**Do you really need to be an administrator to use remoting?**\n\nNo. You either need to be an admin or in the \"Remote Management Users\" group. \n\n**Is remoting pretty much just SSH?**\n\nMore or less. The usual way of using remoting is to use the ```Invoke-Command``` commandlet which just executes a single command. However there is another one called ```Enter-PSSession``` and this one lets you enter an interactive mode.\n\n{% codeblock lang:powershell %}\n    Invoke-command remotemachinename -Credential $cred -scriptblock { \n\tsomecommand.exe --some-parameters \n}\n{% endcodeblock %}\n\nvs\n\n{% codeblock lang:powershell %}\n    Enter-PSSession -computername remotemachinename -authentication credssp -Credential $cred \n{% endcodeblock %}","categories":[],"tags":[]},{"title":"Configuring Features for Many TeamProjects in TFS 2015","authorId":"dave_white","slug":"configuring-features-for-many-teamprojects-in-tfs-2015","date":"2015-09-03 05:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/configuring-features-for-many-teamprojects-in-tfs-2015/","link":"","permalink":"https://westerndevs.com/_/configuring-features-for-many-teamprojects-in-tfs-2015/","excerpt":"One of the problems that comes with having multiple Team Project Collections and multiple Team Projects (in TFS) is the administrative burden required to upgrade or manage all of these projects. Security permissions, WIT modifications, configuration are all a 0..n problem so the more Team Projects you have, the more work it is, out of the box, to manage your TFS implementation.","raw":"---\nlayout: post\ntitle: \"Configuring Features for Many TeamProjects in TFS 2015\"\ndate: 2015-09-03 1:30:00\nauthorId: dave_white\noriginalurl: \ncomments: true\n---\nOne of the problems that comes with having multiple Team Project Collections and multiple Team Projects (in TFS) is the administrative burden required to upgrade or manage all of these projects. \nSecurity permissions, WIT modifications, configuration are all a 0..n problem so the more Team Projects you have, the more work it is, out of the box, to manage your TFS implementation.\n\n<!--more-->\n\nThere are numerous people and projects who have stepped up to help reduce this burden with applications, PowerShell scripts, and techniques for getting more work done with less effort.\n\nOne of those projects is [Features4tfs][1], a command line application project that builds on a couple blog posts to make feature configuration easier when dealing with multiple TeamProjects.\n\nUnfortunately, I've discovered that something happened between TFS 2015 RC and TFS 2015 RTM and this project no longer works. I've updated the code to use the latest RTM Object model binaries\nbut I've just been unable to get it working. A few other people have run into this problem as well, and we've been unable to get any help or answers about this problem.\n\nRegardless of getting help or not, I need to keep my client's migration/upgrade project moving forward and to that end, [PowerShell, IE Automation][2] and [my recent work with the TFS 2015 Object Model in PowerShell][3] to the rescue!\n\n### Implementing the Automate-IEConfigureFeatures Script\n\nIn order to understand this script, you'll need to make sure you understand what I'm doing with [IE Automation][2] and using a [TFS PowerShell Module][3] that I've discussed previously on this [(and my)][4] blog. I'll be using techniques from both those posts.\n\nFirst, we need to enhance my TFS PowerShell module to add a cmdlet that it didn't have from the previous post.\n#### Implement Get-TfsTeamProjects CmdLet\n{% codeblock lang:powershell %}\nfunction Get-TfsTeamProjects() {\n<# \n    .SYNOPSIS\n    Get a collection of Team Projects from a Team Project Collection\n    .DESCRIPTION\n    Get a collection of Team Projects from a Team Project Collection (TPC) using the Id (guid) from the TPC object\n    .EXAMPLE\n    Get-TfsTeamProjects $configServer \"000000-0000-000000-000000000\" <--- GUID\n    .EXAMPLE\n    Get-TfsTeamProjects $cs <tpcID Here>\n    .PARAMETER configServer\n    The TfsConfigurationServer object that represents a connection to TFS server that you'd like to access\n    .PARAMETER teamProjectCollectionId\n    The id (guid) of the TeamProjectCollection that you'd like to get a list of TeamProjects from\n#>\n\n    [CmdLetBinding()]\n    param(\n        [parameter(Mandatory = $true, ValueFromPipeline = $true)]\n        [Microsoft.TeamFoundation.Client.TfsConfigurationServer]$configServer, \n        [parameter(Mandatory = $true)]\n        [guid]$teamProjectCollectionId\n\n    )\n    begin{}\n    process{\n         $tpc = $configServer.GetTeamProjectCollection($teamProjectCollectionId)\n         #Get WorkItemStore\n         $wiService = $tpc.GetService([Microsoft.TeamFoundation.WorkItemTracking.Client.WorkItemStore])\n         #Get a list of TeamProjects\n         $wiService.Projects\n    }\n    end{}\n} #end function Get-TfsTeamProjects \n{% endcodeblock %}\n\nIn this CmdLet, we build on our understanding of the TFS Object Model and, using the WorkItemStore, get a list of all TeamProject in a TPC and return that list from the cmdlet.\n\n#### Composing our IE Automation Script\n\nLuckily, the Feature Configuration page is simple, easily addressable, and behaves consistently so it is actually very easy to automate.\n\nNow we're going to Import-Module (ipmo alias in PowerShell) my TFS PowerShell module. We'll use that functionality for connecting to TFS and getting the lists of TeamProjectCollections and TeamProjects. \nThis Script is not going to be a cmdlet, so it isn't going to be as pretty (or well documented, or perhaps efficient) as the TFS module we've been using.\n\nThere is a function in this Script to help with quickly finding buttons that we're expecting on the TFS Web Access Admin page we're working on.\n\n{% codeblock lang:powershell %}\ncls\n\nipmo <TfsPowerShellModuleNameHere>\n\nfunction Find-Button($ieDoc, $btnText){\n    $btns = $ieDoc.getElementsByTagName(\"button\")\n    foreach($innerBtn in $btns) \n    {\n        if($innerBtn.parentElement.className -ne \"ui-dialog-buttonset\") {continue}\n        $innerSpans = $innerBtn.getElementsByTagName(\"span\")\n        foreach($span in $innerSpans)\n        {\n            if (($span.InnerText) -and ($span.InnerText.Contains($btnText))) {\n                #find the button that has a span that has the text btnText\n                $span.parentElement\n                break;\n            }\n        }\n    }\n}\n\n$ie = new-object -ComObject \"InternetExplorer.Application\"\n\n$cs = Get-TfsConfigServer <TFS AppTier URL here>\n$tpcIds = Get-TfsTeamProjectCollectionIds $cs\n\nforeach ($tpcId in $tpcIds){\n    \n    $tpc = Get-TfsTeamProjectCollection $cs -teamProjectCollectionId $tpcId\n    [string]$tpcUri =  $tpc.Uri.AbsoluteUri\n\n    $projects = Get-TfsTeamProjects -configServer $cs -teamProjectCollectionId $tpcId\n    foreach ($proj in $projects){\n        [string]$projectName = $proj.Name\n        $requestUri = [string]::Format(\"{0}/{1}/_admin#_a=enableFeatures\", $tpcUri, $projectName.Replace(\" \", \"%20\"))\n        $verifyButtonText = \"Verify\"\n        $configureButtonText = \"Configure\"\n        $closeButtonText = \"Close\"\n\n        $ie.visible = $true\n        $ie.silent = $true\n        $ie.navigate($requestUri)\n        while($ie.Busy) { Start-Sleep -Milliseconds 100 }\n\n        $doc = $ie.Document\n        \n        #discover Verification button \n        $btn = Find-Button $doc $verifyButtonText\n\n        if ($btn -eq $null) { continue }\n    \n        #start Verification\n        $btn.click()\n\n        Start-Sleep -Milliseconds 1000\n\n        $buttonNotFound = $true\n        #wait for verification to complete\n        while ($buttonNotFound) {\n            $closeBtn = $null;$configBtn = $null;\n            $closeBtn = Find-Button $doc $closeButtonText\n            $configBtn = Find-Button $doc $configureButtonText\n            if (($closeBtn -ne $null) -or ($configBtn -ne $null)){\n                $buttonNotFound = $false;\n            }else {\n                Start-Sleep -Milliseconds 1000\n            }\n        }\n\n        if ($closeBtn -ne $null) {\n            Write-Host \"Cannot configure features for TeamProject \" -NoNewline\n            Write-host \"($($proj.Name)). \" -NoNewLine -ForegroundColor Yellow\n            Write-Host \"It needs to be upgraded first.\"\n            $warningText = $doc.getElementById(\"issues-textarea-id\").InnerText\n            Write-Host $warningText -ForegroundColor Red | fl -Force\n            $closeBtn.click()\n        }\n        elseif ($configBtn -ne $null) {\n            #start Configuration\n            $configBtn.click()\n\n            #wait for configuration to complete\n            Start-Sleep -Milliseconds 500\n\n            #close Configuration\n            $buttonNotFound = $true\n            while ($buttonNotFound) {\n                $closeBtn = $null;\n                $closeBtn = Find-Button $doc $closeButtonText\n                if ($closeBtn -ne $null){\n                    $buttonNotFound = $false;\n                }else {\n                    Start-Sleep -Milliseconds 500\n                }\n            }\n\n            $closeBtn.click()\n            \n        }\n        else{\n            Write-Host \"Failed to find a button\"\n        }\n    }\n}\n\n{% endcodeblock %}\n\n### Final Thoughts\nSo that is it. I hope that the script is self-explanatory enough for you. I hope that you take away from this blog that there are usually many ways to solve \na problem and sometimes we just have to roll up our sleeves and get our hands dirty and do our work in a functional and non-elegant manner. \nDon't let minor technical glitches get in the way of getting your work done.\n\nThere are the side benefits to this that you don't need to understand how the Feature Configuration works at a code level. You just need to be able to get \nyour automation to click buttons.  \n\n\n[1]: https://features4tfs.codeplex.com\n[2]: http://www.westerndevs.com/simple-powershell-automation-browser-based-tasks/\n[3]: http://www.westerndevs.com/tfs-module-in-powershell-using-nuget/\n[4]: http://www.agileramblings.com\n","categories":[],"tags":[]},{"title":"Why Gulp?","authorId":"dave_paquette","slug":"why-gulp","date":"2015-09-03 02:41:57+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/why-gulp/","link":"","permalink":"https://westerndevs.com/_/why-gulp/","excerpt":"I recently made some updates to my blog post on How to Use Gulp in Visual Studio. I don't usually go back and update old blog posts, but this one receives a fair amount of daily traffic. There was a minor mistake in the way I had setup my gulp watch and I wanted to fix that to avoid confusion. I also get a lot of questions about why using a task runner like Gulp is a 'better approach' than the way things are done in ASP.NET 4.x. I have addressed some of those questions in the original post but I will go into more detail here.","raw":"---\nlayout: post\ntitle:  \"Why Gulp?\"\ndate: 2015-09-02T18:41:57-04:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/09/01/why-gulp.aspx\n---\n\nI recently made some updates to my blog post on [How to Use Gulp in Visual Studio][1]. I don't usually go back and update old blog posts, but this one receives a fair amount of daily traffic. There was a minor mistake in the way I had setup my gulp watch and I wanted to fix that to avoid confusion. I also get a lot of questions about why using a task runner like Gulp is a 'better approach' than the way things are done in ASP.NET 4.x. I have addressed some of those questions in the original post but I will go into more detail here.\n\n<!--more-->\n\nLet's start with a quick example using the 2 approaches.\n\n### System.Web.Optimization\n\nIn previous versions of ASP.NET, optimizations such as bundling and minification are done using the System.Web.Optimization package. In this approach, we configure our bundles in C#:\n\n{% codeblock lang:csharp %}\npublic class BundleConfig\n{\n    // For more information on bundling, visit http://go.microsoft.com/fwlink/?LinkId=301862\n    public static void RegisterBundles(BundleCollection bundles)\n    {\n \n        bundles.Add(new ScriptBundle(\"~/bundles/js\").Include(\n                    \"~/app/Script1.js\",\n                    \"~/app/Script2.js\"));\n \n    }\n}\n{% endcodeblock %}\n\nThose bundles are referenced in our Razor views as follows:\n\n    @Scripts.Render(\"~/bundles/js\")\n\nWhen running in Release mode, the server combines the files in a bundle into a single minified file and renders a single &lt;link&gt; or &lt;script&gt; tag for the bundle. When running in Debug mode, the server renders individual &lt;link&gt; or &lt;script&gt; tags for each file in the bundle. The file optimization step is done at runtime. A version hash is added to the bundle URL to support aggressively caching the asset on the client side.\n\n#### Task Runners\n\nWhen using a Task Runner like Gulp (or Grunt), optimizations like bundling and minification are done at build/compile time. The bundles and any step related to bundling are configured in a JavaScript file that is executed by the task runner. Here is a simple example of a gulp file that does the same optimizations as the example above:\n\n{% codeblock lang:javascript %}\n// include plug-ins\nvar gulp = require('gulp');\nvar concat = require('gulp-concat');\nvar uglify = require('gulp-uglify');\n \nvar config = {\n    //Include all js files but exclude any min.js files\n    src: ['app/**/*.js', '!app/**/*.min.js']\n}\n \ngulp.task('scripts', function () {\n \n    return gulp.src(config.src)\n      .pipe(uglify())\n      .pipe(concat('all.min.js'))\n      .pipe(gulp.dest('app/'));\n});\n \n//Set a default tasks\ngulp.task('default', ['scripts'], function () { });\n{% endcodeblock %}\n\n_Note that this is a simplified example. For a more complete example see my [original post][6]._\n\nBy running the scripts task, all the JS files in my app folder are combined and minified into a single all.min.js file. In ASP.NET 5, we can decided based on our current environment if we should include references to the individual files or the single combined and minified file.\n\n{% codeblock lang:xml %}\n<environment names=\"Development\">\n    <script asp-src-include=\"~/app/**/*.js\" asp-src-exclude=\"~/app/**/*.min.js\"></script>\n</environment>\n<environment names=\"Staging,Production\">\n    <script src=\"~/app/all.min.js\" asp-append-version=\"true\"></script>\n</environment>\n{% endcodeblock %}\n\nIn this case, the files are combined and minified at build/compile time. The minified version of the file is published to the server. At runtime, Razor tag helpers are responsible for [deciding which script tags][2] to include. The tag helpers also [append the file version hash][3] to support aggressively caching the files on the client side. As was covered in my original post, we can use the Task Runner Explorer to link the Scripts task to the build event in Visual Studio. Using a watch, I can automatically run the Scripts task anytime a JS file changes.\n\n## Why I prefer the Task Runner approach\n\nNow let's get into the details of why I prefer using a task runner like Gulp over the runtime optimization approach taken by System.Web.Optimization.\n\n### Runtime vs. Compile-Time Optimizations\n\nSystem.Web.Optimization takes the approach of bundling/minifying your assets at runtime. The first time a request comes in for a bundle, it will combine and minify all the files in that bundle and cache the results for the next request. While the cost of this is minimal, it has always seemed to me that it is a strange to use server resources to do this task. At the time of publishing our application to the server, we already know what the code is. To me it makes more sense to do this step on the build server or on the developer machine BEFORE publishing the application. Task runners like Gulp take the approach of doing these asset optimization steps at compile/build time.\n\nThis becomes a bigger advantage when we start doing more than just bundling and minification. My typical _scripts_ task takes all theTypeScript files from my app, compiles them to JavaScript, combines the output of that to a single minified JS file and writes out source maps. Gulp allows me to easily automate all of this with a single task. Compiling TypeScript and generating source maps is just not possible with System.Web.Optimization and I don't think anyone would argue that doing all those steps on the web server at runtime would make sense anyway. Yes, some of these steps could be handled using Visual Studio pluginsâ€¦more on that later.\n\nFor the vast majority of applications, I think the task runner approach is more logical. You are shipping known, pre-optimized assets to your production server. Don't make your server do more than it needs to.\n\n_Note that there are some specific use cases such as CMS tools that require runtime optimizations because the assets might not be known at compile time._\n\n### Extensibility and Consistency\n\nThere is no question that the runtime bundling in MVC 5 provides a better 'out-of-the-box' experience. When you create a new project, bundling and minification is setup and working. It is easy to add new files. People generally understand the concepts and don't need to spend a lot of time fiddling with the bundle configuration. As I have eluded to in the TypeScript example, System.Web.Optimization starts to fall apart for me is when you want to take things 1 step further.\n\nLet's consider another example. What if I want to start using a CSS pre-processor like LESS or SASS? There is no way built-in way to tie CSS pre-processors into System.Web.Optimization. Now you need to start looking for VS plugins to do this task. If we're lucky, these will work well. In my experience they have some problems, are often out-of-date or are just not available. One big problem with using VS plugins is that I can't make use of those on the build server which means I now need to check my generated CSS files in source control. I much prefer to only check in my LESS or SASS source files and have the build server generate the CSS files. (Checking in generated files pollutes the commit logs and makes code reviews a lot less effective).\n\nAnother problem is trying to make sure that everyone on the team has the right plugins installed. There are ways to enforce this, but it is not very easy.\n\nWith Gulp, all we need to do is include a gulp plugin (eg, gulp-less) and add the less compilation step to my stylesheet task. It is a 1 or 2 line change to my gulp file. The node package manager is able to ensure that everyone on the team has the right gulp plugins installed. Since everything is command line based, it is also very easy to [call the same tasks from the build server][4].\n\nSo the big advantages that I see are extensibility and consistency. System.Web.Optimization is very good at doing a couple things, but it is also limited to doing those couple of things. When we want to take things a little further, we start to run into some pain points with ensuring a consistent development environment. Gulp on the other hand is extremely flexible and extensible in a way that makes it easy to provide consistency environment and consistent builds across your entire team.\n\n## Wrapping it up\n\nIn small and simple MVC 5 projects, I still use System.Web.Optimization for it's simplicity. For more complex projects where I want to use some newer web dev tooling, I use Gulp. Gulp gives me a lot more options and the opportunity to design a better workflow for my team.\n\nThe File-New Project experience in current beta version of MVC 6 uses Gulp. I'm excited about this, but the default gulp file is in need of some work. It is difficult to extend and contains some errors that will cause problems for those who are new to Gulp. Of course, this is a beta version and the team is still working on this. I am hopeful that the experience will improve before the official release of MVC 6. In the meantime, don't be afraid to learn about Gulp and all the amazing things it can do. I find the [Gulp Recipes][5] to be a very valuable learning tool.\n\n[1]: http://www.davepaquette.com/archive/2014/10/08/how-to-use-gulp-in-visual-studio.aspx\n[2]: http://www.davepaquette.com/archive/2015/05/05/web-optimization-development-and-production-in-asp-net-mvc6.aspx\n[3]: http://www.davepaquette.com/archive/2015/05/06/link-and-script-tag-helpers-in-mvc6.aspx\n[4]: http://www.davepaquette.com/archive/2015/04/08/integrating-gulp-and-bower-with-visual-studio-online-hosted-builds.aspx\n[5]: https://github.com/gulpjs/gulp/tree/master/docs/recipes\n[6]: http://www.davepaquette.com/archive/2014/10/08/how-to-use-gulp-in-visual-studio.aspx\n\n","categories":[],"tags":[]},{"title":"Upgrading NPM in Visual Studio 2015","authorId":"james_chambers","slug":"upgrading-npm-in-visual-studio-2015","date":"2015-09-03 02:31:14+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/upgrading-npm-in-visual-studio-2015/","link":"","permalink":"https://westerndevs.com/_/upgrading-npm-in-visual-studio-2015/","excerpt":"Visual Studio 2015 (download here) ships with it's own version of several external tools, such as grunt, node and npm. If you are wanting to take advantage of newer versions of these tools, you have three options:","raw":"---\nlayout: post\ntitle:  \"Upgrading NPM in Visual Studio 2015\"\ndate: 2015-09-02T18:31:14-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/09/upgrading-npm-in-visual-studio-2015/\n---\n\nVisual Studio 2015 ([_download here_][1]) ships with it's own version of several external tools, such as grunt, node and npm.&nbsp; If you are wanting to take advantage of newer versions of these tools, you have three options:\n\n<!--more-->\n\n1. Wait for VS 2015 to upgrade the tooling and ship an update.\n2. Hack the tooling proxies used by Visual Studio.\n3. Use the built-in external tool path editor to slip your updated versions in.\n\nWaiting for updates is no fun. Let's hack a little.\n\n## Wait a minute! Why are we doing this?\n\n{% img pull-right \"http://jameschambers.com/wp-content/uploads/2015/09/image_thumb.png\" %}\n\nFor me the primary motivator was the path length limitations in Windows. Nested node_modules folders buried 19 levels deep is no fun when you hit the max path length. For me, I was trying to share the files on OneDrive and hit 255 characters pretty quickly.\n\nOlder versions of npm resolved package dependencies by pulling in a package, creating a node_modules folder inside of it, then putting all the packages in there. Except, of course, if one of those packages contained more dependencies, then we were into the recursive bits of package resolution and very deep paths, ultimately toppling a lot of Windows tooling.\n\nThe latest major version of npm â€“ version 3.0.x and above â€“ creates a flat store of packages (very similar to what we know in NuGet) and only pulls one copy of each required version of each required package. Much nicer. So, back to the dicing!\n\n## Hacking up the VS Tooling Proxies\n\nThese are pretty straightforward, once you find them. For me, they were located in the following directory:\n\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\Extensions\\Microsoft\\Web Tools\\External\n\nFor example, here the entire contents of npm.cmd:\n\n    @\"%~dp0\\node\\node\" \"%~dp0\\npm\\node_modules\\npm\\bin\\npm-cli.js\" %*\n\nThe %~dp0 is the old command line way of bringing the current drive letter (the d in the command), the path (the letter p here) and the current directory of the executing script (represented by 0) into context. So, basically, \"start from where you're running\". It's a very hard-to-read version of \".\" in most other notations. So, the command is running node (which is an exe), passing in the VS version of npm, and pushing into it the rest of the parameters that were passed along. So, when VS issues an \"npm install\", this command kicks in, runs npm via node and passes \"install\" as the command to npm.\n\nWith that knowledge, we can simply update the call that is proxied through to our current version. I installed node (which includes npm), then updated npm to the latest version (thanks to [_this module_][3]) and updated my npm.cmd to the following:\n\n    @\"C:\\Program Files (x86)\\nodejs\\node.exe\" \"C:\\Program Files (x86)\\nodejs\\node_modules\\npm\\bin\\npm-cli.js\" %*\n\nOf course, here be dragons: I have no idea how stable this will be with updates to VS, and/or how badly you may be crippling features if you mess this up. So, make sure you take a backup of your scripts before modifying them.&nbsp; This will be super-handy if you have some other requirement â€“ like the order of params on tooling changes â€“ but otherwise likely isn't needed. Thankfully, there is a UI-way of doing this, too.\n\n## Not Hacking Your Visual Studio Tooling\n\nProbably a more pleasing solution for your boss.\n\nThis one is pretty straightforward as well, and can be done by right-clicking on the \"Dependencies\" node in Solution Explorer, or by typing \"external web tools\" in the QuickLaunch bar.\n\nFrom here, just add a new entry and move it to the top. For me, npm is located in the nodejs install directory, and this is good enough to get VS to see it first.\n\n![image][4]\n\nNote, I did seem to have some issues with caching and/or gremlins here, so you may need to restart Visual Studio for the tooling paths to be picked up.\n\n## What I Don't Like\n\nCouple of things here that I don't care for:\n\n1. **Not consistent between team members**: there seems to be no way to put in your solution/project a hint at the version of tooling you wish to use. In my case, a developer with npm 2 trying to run an install off of OneDrive would fail.\n2. **Visual Studio external tools are internal**: yeah, you read that right. I'm not a fan of the way these projects are packed in, in such a way that the path to update them or pick versions is non-obvious.\n\n## Next Steps\n\nNot too much to do, but if you run into long paths, nested node_modules kicking your butt or other out-of-date tooling, this should get you on your way.\n\nMake sure you grab your [_copy of VS 2015_][1] and start diving into the next phase of our careers!\n\nHappy coding! ![Smile][5]\n\n[1]: https://www.visualstudio.com/?Wt.mc_id=DX_MVP4038205\n[3]: https://www.npmjs.com/package/npm-windows-upgrade\n[4]: http://jameschambers.com/wp-content/uploads/2015/09/image_thumb1.png \"image\"\n[5]: http://jameschambers.com/wp-content/uploads/2015/09/wlEmoticon-smile.png\n  ","categories":[],"tags":[]},{"title":"No Estimates","slug":"podcast-no-estimates","date":"2015-09-02 03:42:12+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-no-estimates/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-no-estimates/","excerpt":"In this episode, special guest Steve Rogalsky helps the Western Devs understand the #NoEstimates movement","raw":"---\nlayout: podcast\ntitle: \"No Estimates\"\ndate: 2015-09-01T19:42:12-04:00\nrecorded: 2015-08-27\ncategories: podcasts\nexcerpt: \"In this episode, special guest Steve Rogalsky helps the Western Devs understand the #NoEstimates movement\"\ncomments: true\npodcast:\n    filename: \"NoEstimates.mp3\"\n    length: \"47:50\"\n    filesize: 57405212\n    libsynId: 5292016\n    anchorFmId: No-Estimates-evqdk0\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - simon_timms\n    - dylan_smith\n    - kyle_baley\n    - james_chambers\n    - dave_white\n    - tom_opgenorth\nlinks:\n    - Journeying toward NoEstimates|http://winnipegagilist.blogspot.com/2015/05/journeying-towards-noestimates.html\n    - Steve Rogalsky on Twitter|https://twitter.com/srogalsky\n    - Troy Magennis on Twitter|https://twitter.com/t_magennis\n    - How Many Licks|http://www.amazon.ca/How-Many-Licks-Estimate-Anything/dp/0762435607\n    - How to Measure Anything|http://www.amazon.ca/How-Measure-Anything-Intangibles-Business/dp/1118539273/ref=sr_1_1?ie=UTF8&qid=1440783530&sr=8-1&keywords=how+to+measure+anything\n    - Forecasting and Simulating Software Development Projects|http://www.amazon.com/Forecasting-Simulating-Software-Development-Projects/dp/1466454830/ref=asap_bc?ie=UTF8\n    - NoEstimates on Twitter|https://twitter.com/search?q=%23noestimates\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n\n### Synopsis\n\n* Why do we estimate?\n* Why do our estimates suck?\n* \"Wah, wah, wah, estimating is hard!\"\n* The #NoEstimates movement\n* Estimates vs commitments\n* Measuring your past to forecast your future\n* Building in chunks to create forecast data\n* #NoEstimates == Informated estimating?\n* Right-sizing user stories/Law of averages\n* Alternatives to story points for estimating\n* Probabilistic forecasting\n* Planning poker: Creating a common understanding\n* Acceptance criteria\n* Setting expectations with clients for \"No Estimates\"\n* Gaining trust with a client as an alternative to providing estimates\n* Understanding *why* estimates are needed to determine *if* they're needed\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Windows Server Containers Are Coming Whether You Like It or Not","authorId":"kyle_baley","slug":"windows-server-containers-are-coming-whether-you-like-it-or-not","date":"2015-08-31 19:01:20+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/windows-server-containers-are-coming-whether-you-like-it-or-not/","link":"","permalink":"https://westerndevs.com/_/windows-server-containers-are-coming-whether-you-like-it-or-not/","excerpt":"UPDATE: April 27, 2017 Much of the information in this post is out-of-date and the links have been removed since they no longer exist. For the latest on the state of containers on Windows, check out the documentation.","raw":"---\nlayout: post\ntitle:  \"Windows Server Containers Are Coming Whether You Like It or Not\"\ndate: 2015-08-31T11:01:20-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\n**UPDATE: April 27, 2017** Much of the information in this post is out-of-date and the links have been removed since they no longer exist.  For the latest on the state of containers on Windows, check out the [documentation](https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/).\n\n<!--more-->\n\nAfter posting giddily on [Docker in the Windows world](http://www.westerndevs.com/docker-is-coming-whether-you-like-it-or-not/) recently, Microsoft released Windows Server 2016 Technical Preview 3 with container support. I've had a chance to play with it a little so let's see where this goes...\n\n### It's a preview\n\nLike movie previews, this is equal parts exciting and frustrating. Exciting because you get a teaser of things to come. Frustrating because you just want it to work *now*. And extra frustration points for various technical issues I've run into that, I hope, are due to the \"technical preview\" label.\n\nFor example, installing container support into an existing VM is mind-numbingly slow. Kudos to the team for making it easy to install but at the point where you run ContainerSetup.ps1, be prepared to wait for, by my watch, at least 45 minutes without any visual indication that something is happening. The only reason I knew something *was* happening is because I saw the size of the VM go up (slowly) on my host hard drive. This is on a 70Mbps internet connection so I don't think this can be attributed to \"island problems\" either.\n\nI've heard tell of issues setting up container support in a Hyper-V VM as well. That's second-hand info as I'm using Fusion on a Mac rather than Hyper-V. If you run into problems setting it up on Hyper-V, consider switching to the <span style=\"text-decoration: line-through;\">instructions for setting up containers on non-Hyper-V VMs instead</span> (no longer available).\n\nThere's also the Azure option. Microsoft was gracious enough to provide an Azure image for Windows Server 2016 pre-configured with container support. This works well if you're on Azure and I was able to run the nginx tutorial on it with no issues. I had less success with the IIS 10 tutorial even locally. I could get it running but was not able to create a new image based on the container I had.\n\n### It's also a start\n\nTechnical issues aside, I haven't been this excited about technology in Windows since...ASP.NET MVC, I guess, if my tag cloud is to be believed. And since this is a technical preview designed to garner feedback, here's what I want to see in the Windows container world\n\n#### Docker client *and* PowerShell support\n\nI love that I can use the Docker client to work with Windows containers. I can leverage what I've already learned with Docker in Linux. But I also love that I can spin up containers with PowerShell so I don't need to mix technologies in a continuous integration/continuous deployment environment if I already have PowerShell scripts set up for other aspects of my process.\n\n#### Support for legacy .NET applications\n\nI can't take credit for this. I've been talking with [Gabriel Schenker](https://lostechies.com/gabrielschenker/) about containers a lot lately and it was he who suggested they need to have support for .NET 4, .NET 3.5, and even .NET 2.0. It makes sense though. There are a lot of .NET apps out there and it would be a shame if they couldn't take advantage of containers.\n\n#### Smooth local development\n\nDocker Machine is great for getting up and running fast on a local Windows VM. To fully take advantage of containers, devs need to be able to work with them locally with no friction, whether that means a Windows Container version of Docker Machine or the ability to work with containers natively in Windows 10.\n\n#### ARM support\n\nAt Western Devs, we have a [PowerShell script](http://www.westerndevs.com/using-azure-arm-to-deploy-a-docker-container/) that will spin up a new Azure Linux virtual machine, install docker, create a container, and run our website on it. It goes without saying (even though I'm saying it) that I'd like to do the same with Windows containers.\n\n#### Lots of images out of the gate\n\nI'd like to wean myself off VMs a little. I picture a world where I have one base VM and I use various containers for the different pieces of the app I'm working on. E.g. A SQL Server container, an IIS container, an ElasticSearch container, possibly even a Visual Studio container. I pick and choose which containers I need to build up my dev environment and use just one (or a small handful) of VMs.\n\n---\nIn the meantime, I'm excited enough about Windows containers that I hope to incorporate a small demo with them in my talk at [MeasureUP](http://measureup.io) in a few scant weeks so if you're in the Austin area, come on by to see it.\n\nIt is a glorious world ahead in this space and it puts a smile on this hillbilly's face to see it unfold.\n\nKyle the Barely Contained\n","categories":[],"tags":[]},{"title":"Ooops, Repointing Git Head","authorId":"simon_timms","slug":"ooops-repointing-git-head","date":"2015-08-31 17:01:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/ooops-repointing-git-head/","link":"","permalink":"https://westerndevs.com/_/ooops-repointing-git-head/","excerpt":"I screwed up. I force pushed a branch but I forgot to tell git which branch to push so it clobbered another branch.","raw":"---\nlayout: post\ntitle:  \"Ooops, Repointing Git Head\"\ndate: 2015-08-31T09:01:05-04:00\ncategories:\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/08/28/ooops-repointing-git-head/\n---\n\nI screwed up. I force pushed a branch but I forgot to tell git which branch to push so it clobbered another branch.\n\n<!--more-->\n\n    C:\\code\\project [feature/feature27]> git push -f\n    Password for 'http://simon@remote.server.com:7990':\n    Counting objects: 63, done.\n    Delta compression using up to 8 threads.\n    Compressing objects: 100% (61/61), done.\n    Writing objects: 100% (63/63), 9.25 KiB | 0 bytes/s, done.\n    Total 63 (delta 50), reused 0 (delta 0)\n    To http://simon@remote.server.com:7990/scm/ev/everest.git\n     + 0baa5b8...e9a1c19 develop -> develop (forced update)  <--oops!\n     + dbe6fce...5557ae7 feature/feature27 -> feature/feature27 (forced update)\n\nDrat, since I hadn't updated develop in a few hours there were a bunch of changes in it that I just killed. Fortunately I know that git is really just a glorified linked list and that nothing is ever deleted. I just needed to update where the head pointer was pointing. I grabbed the SHA of the latest develop commit from the build server knowing that it was late at night and nobody else was likely to have snuck a commit into develop that the server missed.\n\nThen I just force updated my local develop and pushed it back up\n\n    git branch -f develop bbff5b810a19383fb11950a5d1e36676dd3ca85d  <-- sha from build server\n    git push\n\nAll was good again.  ","categories":[],"tags":[]},{"title":"Running Process as a Different User on Windows","authorId":"simon_timms","slug":"running-process-as-a-different-user-on-windows","date":"2015-08-28 00:26:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/running-process-as-a-different-user-on-windows/","link":"","permalink":"https://westerndevs.com/_/running-process-as-a-different-user-on-windows/","excerpt":"Running commands as another user on Windows can be a bit tricky, but this is a method that worked for me.","raw":"---\nlayout: post\ntitle:  Running Process as a Different User on Windows\ndate: 2015-08-27T14:26:10-06:00\ncategories:\nexcerpt: Running commands as another user on Windows can be a bit tricky, but this is a method that worked for me.\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/09/02/running-process-as-a-different-user-on-windows/\n---\n\nAs part of a build pipeline I'm working on the octopus deploy process needs to talk to the database using roundhouse as a different user from the one running the deployment. This is done because the database uses integrated AD authentication, which I quite like. If this build were running on Linux then it would be as simple as editing the sudoers file and calling the command using sudo. Unfortunately this is Windows and the command line has long been a secondary concern.\n\nI started by asking on the [western devs](http://westerndevs.com) slack channel to see if anybody else had done this and how. [Dave Paquette](http://www.westerndevs.com/bios/dave_paquette/) suggested using [psexec](https://technet.microsoft.com/en-us/sysinternals/bb897553.aspx). This is a tool designed for running commands on a remote computer but if you leave the computer name off it will run on the local machine. This sounded perfect.\n\nHowever I had a great deal of trouble getting psexec to work in the way I wanted. The command I wanted to run seemed to fail all the time giving an confusing error code [-1073741502](https://social.technet.microsoft.com/forums/en-US/db8ce5e5-988c-4f1c-93f4-5ff1f2fa29e8/psexec-on-remote-server-giving-program-exit-code1073741502). The fix provided didn't seem to work for me so after an afternoon of bashing my head against psexec I went looking for another solution. Running remote processes gave me an idea: what about powershell remoting?\n\nSome investigation suggested that the command I wanted to run would look like\n\n{% codeblock lang:powershell %}\n\tInvoke-command localhost -scriptblock { rh.exe --some-parameters }\n{% endcodeblock %}\n\nThis would remote to localhost and run the roundhouse command as the current user. To get it to work using a different user then the command needed credentials passed into it. I had the credentials stored as [sensitive variables](http://docs.octopusdeploy.com/display/OD/Sensitive+variables) in Octopus which set them up as variables in powershell. To turn these into credentials you need to do\n\n{% codeblock lang:powershell %}    \n\t$pwd = ConvertTo-SecureString $deployPassword -asplaintext -force\n$cred =new-object -TypeName System.Management.Automation.PSCredential -argumentlist $deployUser,$pwd\n{% endcodeblock %}\nNow these can be passed into invoke command as\n\n{% codeblock lang:powershell %}\n    Invoke-command localhost -authentication credssp -Credential $cred -scriptblock {\n\trh.exe --some-parameters\n}\n{% endcodeblock %}\n\nYou might notice that authentication flag, this tells powershell the sort of authentication and cor credssp you also need to enable [Credential Security Service Provider](http://blogs.technet.com/b/heyscriptingguy/archive/2012/11/14/enable-powershell-quot-second-hop-quot-functionality-with-credssp.aspx). To do this we run\n\n{% codeblock lang:powershell %}\n    Enable-WSManCredSSP -Role server\nEnable-WSManCredSSP -Role client -DelegateComputer \"*\"\n{% endcodeblock %}\nFrom an admin powershell session on the machine. Normally you would run these on different machines but we're remoting to local host so it is both the client and the server. I've enabled client for all machines but you might want to lock this down a bit more.\n\nFinally I needed to pass some parameters to roundhouse proper. This can be done by passing them into the script block and then receiving them as parameters inside the block\n\n{% codeblock lang:powershell %}\n    Invoke-command localhost -authentication credssp -Credential $cred -scriptblock {\n\tparam($roundHouseExe,$databaseServer,$targetDb,$databaseEnvironment,$fName)\n\t& \"$roundHouseExe\" --s=$databaseServer --d=\"${targetDb}\" --f=$fName /ni\n} -argumentlist $roundHouseExe,$databaseServer,$targetDb,$databaseEnvironment,$fName\n{% endcodeblock %}\n\nThe whole process is pretty convoluted but that's Windows command line for you. What would this look like in bash on Linux?\n\n{% codeblock lang:bash %}\n    sudo $roundHouseExe --s=$databaseServer --d=\"$targetDb\" --f=$fName /ni\n{% endcodeblock %}   \nSo yeah.\n","categories":[],"tags":[]},{"title":"Using Azure ARM to Deploy a Docker Container","authorId":"dylan_smith","slug":"using-azure-arm-to-deploy-a-docker-container","date":"2015-08-24 23:13:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/using-azure-arm-to-deploy-a-docker-container/","link":"","permalink":"https://westerndevs.com/_/using-azure-arm-to-deploy-a-docker-container/","excerpt":"If youâ€™ve been following the WesternDevs blog youâ€™ll have seen a few posts lately about our adventures with infrastructure for our blog with Jekyll/Docker.","raw":"---\nlayout: post\ntitle:  Using Azure ARM to Deploy a Docker Container\ndate: 2015-08-24T11:13:00-08:00\ncategories:\ncomments: true\nauthorId: dylan_smith\n---\n\nIf youâ€™ve been following the WesternDevs blog youâ€™ll have seen a few posts lately about our adventures with infrastructure for our blog with Jekyll/Docker.\n\n<!--more-->\n\n  * [Docker on Windows 10 Problems](http://www.westerndevs.com/docker-on-windows-10-problems/)\n  * [Getting Docker Running on Windows 10](http://www.westerndevs.com/getting-docker-running-on-windows-10/)\n  * [Docker and Western Devs](http://www.westerndevs.com/docker-and-western-devs/)\n\nWe decided to use Jekyll to host our blog, which most of us had never used before.  All of us need a way to fire up a Jekyll instance to test our changes (even simple things like how a new post will render).  Jekyll is really made for Linux, and most of us run Windows.  Although [Jekyll can run on windows in theory](http://jekyll-windows.juthilo.com/), we have struggled to get it to work.  Amir came to the rescue, and created a [Docker image that includes Jekyll](https://hub.docker.com/r/abarylko/western-devs/) configured according to our needs.\n\nNow we have a new problem â€“ most of us havenâ€™t used Docker before.  We had some struggles, just getting Docker up and running and configured on Windows took a little bit of work for those of us that hadnâ€™t used it before.  Those of us using Windows 10 discovered there were [additional challenges getting Docker running](Docker on Windows 10 Problems).  And for me personally, I do all my work in VMâ€™s (either local VMâ€™s, or Azure VMâ€™s) and I didnâ€™t want to install Docker/VirtualBox on my host OS, and I discovered that you canâ€™t install Docker/VBox inside a Hyper-V Windows VM.\n\nI decided I was going to get something running in Azure, and I had an additional goal of making what I did repeatable and automated so that my fellow Western Devs could easily do what I did.  Iâ€™ve been doing *a lot* of work with Azure ARM Templates lately, so that was the approach I took.  I noticed there is a pre-existing image with Ubuntu available, and there is a Docker VM Extension that you can apply during provisioning that will install/configure Docker, and it can use Docker Compose to spin up one or more containers too.\n\nI created the JSON ARM Template, and a simple PowerShell script to deploy it.  Now any of my fellow WesternDevs can simply run a PS1 script, get prompted for a few pieces of info (azure credentials, azure subscription, github branch name, resource group name), and ~10 mins later they will have a new VM in Azure, with Docker installed, our WesternDevs image deployed, and our Jekyll site up and running with the code from their branch.  Then they can bring it up in a web browser and test out their changes before merging with Master.  When theyâ€™re done they can delete the Azure resource group if they wish, or keep it around for future testing.\n\n\n## The Gory Details\n\nIf youâ€™ve never used Azure ARM Templates before, itâ€™s a JSON file that describes a set of Azure Resources and their configurations.  You can use a PowerShell cmdlet to give the JSON to Azure, and it will spin up a new Resource Group and a bunch of new resources based on what is described in the JSON.  For the WesternDevs template the JSON describes the following resources:\n\n  * Storage Account\n  * Public IP Address\n  * Virtual Network\n  * Network Interface\n  * Network Security Group\n  * Virtual Machine\n  * VM Extension â€“ DockerExtension\n\nThe full JSON file is included at the end of this post.  It can also be [found on GitHub](https://github.com/westerndevs/western-devs-website/tree/source/_azure).  Some of the configuration that is described in the JSON template includes:\n\n  * The Network Security Group exposes port 22 for SSH, and port 4000 for HTTP (this is what our Jekyll/Docker is configured to use)\n  * The VM is created from an image in the Azure Gallery provided by Canonical that has Ubuntu 15.04 on it.\n\nVM Extensions are additional components that can be applied to your VM as part of the provisioning process.  There is an extension available called DockerExtension that will install and configure Docker for you as part of the provisioning process.  Here is the relevant part of the template:\n\n{% codeblock lang:json %}\n{\n  \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n  \"name\": \"[concat(variables('vmName'),'/', variables('extensionName'))]\",\n  \"apiVersion\": \"2015-05-01-preview\",\n  \"location\": \"[variables('location')]\",\n  \"dependsOn\": [\n    \"[concat('Microsoft.Compute/virtualMachines/', variables('vmName'))]\"\n  ],\n  \"properties\": {\n    \"publisher\": \"Microsoft.Azure.Extensions\",\n    \"type\": \"DockerExtension\",\n    \"typeHandlerVersion\": \"1.0\",\n    \"autoUpgradeMinorVersion\": true,\n    \"settings\": {\n      \"compose\": {\n        \"wddocker\": {\n          \"image\": \"abarylko/western-devs:v1\",\n          \"ports\": [\n            \"4000:4000\"\n          ],\n          \"stdin_open\": true,\n          \"command\": \"[concat('bash -c \\\"git clone https://github.com/westerndevs/western-devs-website.git && cd western-devs-website && git checkout ', parameters('branchName'), ' && sed -i s/www.westerndevs.com/', variables('dnsNameForPublicIP'), '.westus.cloudapp.azure.com:4000/g _config.yml && bundle install && jekyll serve --host 0.0.0.0 --force_polling\\\"')]\"\n        }\n      }\n    }\n  }\n}\n{% endcodeblock %}\n\nThis tells it to apply the DockerExtension to the VM previously created.  Additionally it uses Docker Compose to allow you to specify one or more Docker containers that it will pull down from DockerHub, deploy into Docker, allow you to specify configuration such as ports to map to the host, and allow you to run command(s) on the docker image.\n\nIn the template above we tell it to grab the Docker image abarylko/western-devs:v1 which was created by my friend [Amir Barylko](http://www.westerndevs.com/bios/amir_barylko/) and already has Jekyll installed.  Then we tell it to map port 4000 from the docker container to port 4000 on the host Linux VM.  Lastly we give it a few bash commands to run on the docker container when it starts up:\n\n  * git clone https://github.com/westerndevs/western-devs-website.git\n  * cd western-devs-website\n  * git checkout [branchName]\n  * sed â€“i s/www.westerndevs.com/[VmDns].westus.cloudapp.azure.com:4000/g _config.yml\n  * bundle install\n  * jekyll serve â€“host 0.0.0.0 â€“force_polling\n\nThis will grab the github repo in to the docker container, checkout our branch that we want to test (the branch name is passed as a parameter into the ARM template as weâ€™ll see below in the Powershell), update the _config.yml file (which is a config file Jekyll uses) to replace the public url with the URL for our Azure VM (so when we test the site, the links all point to the same testing site URL), use bundle to install all our gems, then fire up Jekyll to run our site.\n\nNow that we have a JSON file that describes our Azure Resources, we need a way to deploy this.  This is a simple bit of PowerShell.  My goal here was to make this as simple as possible for somebody to use, even if they arenâ€™t comfortable with PowerShell/Azure/Docker/Linux/Jekyll/etc.  Itâ€™s as simple as running the PS1, being prompted for 4 things (new resource group name, github branch name, azure login, azure subscription), then waiting ~10 mins for Azure to do itâ€™s thing.\n\n{% codeblock lang:powershell %}\n$EnvName = Read-Host \"Name for Azure Resource Group (must be globally unique)?\"\n$BranchName = Read-Host \"Name of branch in git?\"\n\nAdd-AzureAccount | Out-Null\n$AllSubscriptions = Get-AzureSubscription\n\nif ($AllSubscriptions.Count -eq 1)\n{\n    Select-AzureSubscription $AllSubscriptions[0].SubscriptionName\n    Write-Host (\"Only 1 Azure Subscription found. Using \" + $AllSubscriptions[0].SubscriptionName)\n}\nelse\n{\n    $AzureSub = $AllSubscriptions | Out-GridView -Title \"Select Azure Subscription\" -PassThru\n    Select-AzureSubscription $AzureSub.SubscriptionName\n}\n\nSwitch-AzureMode AzureResourceManager\n\n$TemplateFile = Join-Path $PSScriptRoot \"wddocker.json\"\n$params = @{branchName=\"$BranchName\"}\nNew-AzureResourceGroup -Location \"West US\" -Name $EnvName -TemplateFile $TemplateFile -TemplateParameterObject $params -verbose\n\nWrite-Host \"Your site will be available at this URL: http://$EnvName.westus.cloudapp.azure.com:4000\"\nRead-Host \"Press enter to launch a browser to your new site - if it gives an error wait a minute then refresh\"\nStart-Process \"http://$EnvName.westus.cloudapp.azure.com:4000\"\n\nWrite-Host \"When you're finished testing you should delete the Azure Resource Group\"\nWrite-Host \"You can do it yourself in portal.azure.com\"\nWrite-Host \"Or in PS using Remove-AzureResourceGroup -Name $EnvName\"\nWrite-Host \"Or this script can do it for you automatically (leave this open until done testing)\"\nWrite-Host \"\"\n$DeleteRG = Read-Host \"Do you want to delete the Azure Resource Group $EnvName now (y/n)?\"\n\nif ($DeleteRG.ToUpper() -eq \"Y\")\n{\n    Remove-AzureResourceGroup -Name $EnvName -Force -Verbose\n}\n\nRead-Host \"Press enter to close this window\"\n{% endcodeblock %}\n\nThe interesting line here is the one that does New-AzureResourceGroup.  That passes the JSON template to azure and tells it to create a new resource group and provision the resources described in the template.  We also tell it the azure datacenter location where everything should be created, and pass in a parameter that contains the branch name.\n\nThe rest of the script is just collecting some values from the user, and at the end it will launch your browser to the newly created site, and give you the option to delete all the azure resources just created if you wish.\n\n## Try it for Yourself\nYou can easily give this a try yourself.\n\n 1. [Download wddocker.json and deploy.ps1 from github](https://github.com/westerndevs/western-devs-website/tree/source/_azure)\n 2. If you havenâ€™t already you need to install azure powershell. You can get the installer in the [github folder](https://github.com/westerndevs/western-devs-website/tree/source/_azure), or [from Microsoft](https://azure.microsoft.com/en-us/documentation/articles/powershell-install-configure/)\n 3. Run deploy.ps1\n 4. When prompted for branch name use: source\n 5. ???\n 6. Profit!!!\n\n![ARM Deployment](http://i.imgur.com/xfiW4Ta.png)\n\n## Complete ARM Template JSON\n\n{% codeblock lang:json %}\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"branchName\": {\n      \"type\": \"string\",\n\t  \"defaultValue\": \"source\"\n    }\n  },\n  \"variables\": {\n\t\"newStorageAccountName\": \"[toLower(resourceGroup().name)]\",\n\t\"location\": \"[resourceGroup().location]\",\n\t\"adminUsername\": \"WesternDevs\",\n\t\"adminPassword\": \"P2ssw0rd\",\n\t\"dnsNameForPublicIP\": \"[toLower(resourceGroup().name)]\",\n    \"imagePublisher\": \"Canonical\",\n    \"imageOffer\": \"UbuntuServer\",\n    \"ubuntuOSVersion\": \"15.04\",\n    \"OSDiskName\": \"[resourceGroup().name]\",\n    \"nsgName\": \"myNSG\",\n    \"nicName\": \"myVMNic\",\n    \"extensionName\": \"DockerExtension\",\n    \"addressPrefix\": \"10.0.0.0/16\",\n    \"subnetName\": \"Subnet\",\n    \"subnetPrefix\": \"10.0.0.0/24\",\n    \"storageAccountType\": \"Standard_LRS\",\n    \"publicIPAddressName\": \"myPublicIP\",\n    \"publicIPAddressType\": \"Dynamic\",\n    \"vmStorageAccountContainerName\": \"vhds\",\n    \"vmName\": \"[resourceGroup().name]\",\n    \"vmSize\": \"Basic_A1\",\n    \"virtualNetworkName\": \"MyVNET\",\n    \"vnetID\": \"[resourceId('Microsoft.Network/virtualNetworks',variables('virtualNetworkName'))]\",\n    \"nsgID\": \"[resourceId('Microsoft.Network/networkSecurityGroups',variables('nsgName'))]\",\n    \"subnetRef\": \"[concat(variables('vnetID'),'/subnets/',variables('subnetName'))]\"\n  },\n  \"resources\": [\n    {\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"name\": \"[variables('newStorageAccountName')]\",\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"location\": \"[variables('location')]\",\n      \"properties\": {\n        \"accountType\": \"[variables('storageAccountType')]\"\n      }\n    },\n    {\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"type\": \"Microsoft.Network/publicIPAddresses\",\n      \"name\": \"[variables('publicIPAddressName')]\",\n      \"location\": \"[variables('location')]\",\n      \"properties\": {\n        \"publicIPAllocationMethod\": \"[variables('publicIPAddressType')]\",\n        \"dnsSettings\": {\n          \"domainNameLabel\": \"[variables('dnsNameForPublicIP')]\"\n        }\n      }\n    },\n    {\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"type\": \"Microsoft.Network/virtualNetworks\",\n      \"name\": \"[variables('virtualNetworkName')]\",\n      \"location\": \"[variables('location')]\",\n      \"dependsOn\": [\n        \"[variables('nsgID')]\"\n      ],\n      \"properties\": {\n        \"addressSpace\": {\n          \"addressPrefixes\": [\n            \"[variables('addressPrefix')]\"\n          ]\n        },\n        \"subnets\": [\n          {\n            \"name\": \"[variables('subnetName')]\",\n            \"properties\": {\n              \"addressPrefix\": \"[variables('subnetPrefix')]\",\n              \"networkSecurityGroup\": {\n                \"id\": \"[variables('nsgID')]\"\n              }\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"type\": \"Microsoft.Network/networkInterfaces\",\n      \"name\": \"[variables('nicName')]\",\n      \"location\": \"[variables('location')]\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Network/publicIPAddresses/', variables('publicIPAddressName'))]\",\n        \"[concat('Microsoft.Network/virtualNetworks/', variables('virtualNetworkName'))]\"\n      ],\n      \"properties\": {\n        \"ipConfigurations\": [\n          {\n            \"name\": \"ipconfig1\",\n            \"properties\": {\n              \"privateIPAllocationMethod\": \"Dynamic\",\n              \"publicIPAddress\": {\n                \"id\": \"[resourceId('Microsoft.Network/publicIPAddresses',variables('publicIPAddressName'))]\"\n              },\n              \"subnet\": {\n                \"id\": \"[variables('subnetRef')]\"\n              }\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"type\": \"Microsoft.Network/networkSecurityGroups\",\n      \"name\": \"[variables('nsgName')]\",\n      \"location\": \"[variables('location')]\",\n      \"properties\": {\n        \"securityRules\": [\n          {\n            \"name\": \"http\",\n            \"properties\": {\n              \"description\": \"Allow HTTP\",\n              \"protocol\": \"Tcp\",\n              \"sourcePortRange\": \"*\",\n              \"destinationPortRange\": \"4000\",\n              \"sourceAddressPrefix\": \"Internet\",\n              \"destinationAddressPrefix\": \"*\",\n              \"access\": \"Allow\",\n              \"priority\": 100,\n              \"direction\": \"Inbound\"\n            }\n          },\n          {\n            \"name\": \"ssh\",\n            \"properties\": {\n              \"description\": \"Allow SSH\",\n              \"protocol\": \"Tcp\",\n              \"sourcePortRange\": \"*\",\n              \"destinationPortRange\": \"22\",\n              \"sourceAddressPrefix\": \"Internet\",\n              \"destinationAddressPrefix\": \"*\",\n              \"access\": \"Allow\",\n              \"priority\": 110,\n              \"direction\": \"Inbound\"\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"type\": \"Microsoft.Compute/virtualMachines\",\n      \"name\": \"[variables('vmName')]\",\n      \"location\": \"[variables('location')]\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('newStorageAccountName'))]\",\n        \"[concat('Microsoft.Network/networkInterfaces/', variables('nicName'))]\"\n      ],\n      \"properties\": {\n        \"hardwareProfile\": {\n          \"vmSize\": \"[variables('vmSize')]\"\n        },\n        \"osProfile\": {\n          \"computername\": \"[variables('vmName')]\",\n          \"adminUsername\": \"[variables('adminUsername')]\",\n          \"adminPassword\": \"[variables('adminPassword')]\"\n        },\n        \"storageProfile\": {\n          \"imageReference\": {\n            \"publisher\": \"[variables('imagePublisher')]\",\n            \"offer\": \"[variables('imageOffer')]\",\n            \"sku\": \"[variables('ubuntuOSVersion')]\",\n            \"version\": \"latest\"\n          },\n          \"osDisk\": {\n            \"name\": \"osdisk1\",\n            \"vhd\": {\n              \"uri\": \"[concat('http://',variables('newStorageAccountName'),'.blob.core.windows.net/',variables('vmStorageAccountContainerName'),'/',variables('OSDiskName'),'.vhd')]\"\n            },\n            \"caching\": \"ReadWrite\",\n            \"createOption\": \"FromImage\"\n          }\n        },\n        \"networkProfile\": {\n          \"networkInterfaces\": [\n            {\n              \"id\": \"[resourceId('Microsoft.Network/networkInterfaces',variables('nicName'))]\"\n            }\n          ]\n        }\n      }\n    },\n    {\n      \"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n      \"name\": \"[concat(variables('vmName'),'/', variables('extensionName'))]\",\n      \"apiVersion\": \"2015-05-01-preview\",\n      \"location\": \"[variables('location')]\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Compute/virtualMachines/', variables('vmName'))]\"\n      ],\n      \"properties\": {\n        \"publisher\": \"Microsoft.Azure.Extensions\",\n        \"type\": \"DockerExtension\",\n        \"typeHandlerVersion\": \"1.0\",\n        \"autoUpgradeMinorVersion\": true,\n\t\t\"settings\": {\n          \"compose\": {\n            \"wddocker\": {\n              \"image\": \"abarylko/western-devs:v1\",\n              \"ports\": [\n                \"4000:4000\"\n              ],\n\t\t\t  \"stdin_open\": true,\n\t\t\t  \"command\": \"[concat('bash -c \\\"git clone https://github.com/westerndevs/western-devs-website.git && cd western-devs-website && git checkout ', parameters('branchName'), ' && sed -i s/www.westerndevs.com/', variables('dnsNameForPublicIP'), '.westus.cloudapp.azure.com:4000/g _config.yml && bundle install && jekyll serve --host 0.0.0.0 --force_polling\\\"')]\"\n            }\n          }\n\t\t}\n      }\n    }\n  ]\n}\n{% endcodeblock %}\n","categories":[],"tags":[]},{"title":"Internal Open Source","slug":"podcast-internal-open-source","date":"2015-08-24 02:00:37+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-internal-open-source/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-internal-open-source/","excerpt":"The Western Devs sound off on why your company should share and collaborate on its internal code","raw":"---\nlayout: podcast\ntitle:  \"Internal Open Source\"\ndate: 2015-08-23T18:00:37-04:00\nrecorded: 2015-08-07\ncategories: podcasts\nexcerpt: \"The Western Devs sound off on why your company should share and collaborate on its internal code\"\ncomments: true\npodcast:\n    filename: \"InternalOpenSource.mp3\"\n    length: \"43:17\"\n    filesize: 51937269\n    libsynId: 5316378\n    anchorFmId: Internal-Open-Source-evqdis\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - simon_timms\n    - dylan_smith\n    - kyle_baley\n    - david_wesst\n    - james_chambers\nlinks:\n    - .NET Core API Review|https://channel9.msdn.com/Series/NET-Framework/NET-Core-API-Review-2015-01-14\n    - Using open source methods in a private company|http://blog.smartbear.com/open-source/using-open-source-methods-in-a-private-company/\n    - Open source as a project model for internal work|http://blog.kevinlamping.com/open-source-as-a-project-model-for-internal-work/\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n### Synopsis\n\n* What is \"internal open source\" or \"internal shared source\"?\n* Sharing libraries (via a shared package server) vs. shared source\n* The importance of a manager for an internally shared project\n* Open source vs. traditional corporate: switching to a sharing culture\n* Who's responsible for this feature?\n* The need for open communication when sharing code: Can I add this feature to your code?\n* How can you sell shared source to your boss?\n* Cross collaboration and moving to shared engineering standards\n* Exposing your code to the company: scary or beneficial\n* Risks of sharing your code\n* Proper management of forks and pull requests\n* The .NET Core's PR review meetings\n* Staged approach to implementing internal shared source: open up your own project first\n* Legal concerns about sharing your code outside your immediate team\n* Is your code really useful to anyone else?\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Docker on Western Devs","authorId":"kyle_baley","slug":"docker-and-western-devs","date":"2015-08-24 00:38:59+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/docker-and-western-devs/","link":"","permalink":"https://westerndevs.com/_/docker-and-western-devs/","excerpt":"In a month, I'll be attempting to hound my share of glory at MeasureUP with a talk on using Docker for people who may not think it impacts them. In it, I'll demonstrate some uses of Docker today in a .NET application. As I prepare for this talk, there's one thing we Western Devs have forgotten to talk about. Namely, some of us are already using Docker regularly just to post on the site.","raw":"---\nlayout: post\ntitle:  \"Docker on Western Devs\"\ndate: 2015-08-23T16:38:59-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\nIn a month, I'll be attempting to hound my share of glory at [MeasureUP](http://measureup.io) with a talk on using Docker for people who may not think it impacts them. In it, I'll demonstrate some uses of Docker today in a .NET application. As I prepare for this talk, there's one thing we [Western Devs](http://www.westerndevs.com) have forgotten to talk about. Namely, some of us are already using Docker regularly just to post on the site.\n\n<!--more-->\n\nWestern Devs uses Jekyll. Someone suggested it, I tried it, it worked well, decision was done. Except that it doesn't work well on Windows. It's not officially supported on the platform and while there's a [good guide](http://jekyll-windows.juthilo.com/) on getting it running, we haven't been able to do so ourselves. Some issue with a gem we're using and Nokogiri and lib2xml and some such nonsense.\n\nSo in an effort to streamline things, [Amir Barylko](http://www.westerndevs.com/bios/amir_barylko/) create a [Docker image](https://github.com/westerndevs/western-devs-website/blob/source/Dockerfile). It's based on the Ruby base image (version 2.2). After grabbing the base image, it will:\n\n* Install some packages for building Ruby\n* Install the bundler gem\n* Clone the source code into the /root/jekyll folder\n* Run `bundle install`\n* Expose port 4000, the default port for running Jekyll\n\nWith this in place, Windows users can run the website locally without having to install Ruby, Python, or Jekyll. The command to launch the container is:\n\n`docker run -t -p 4000:4000 -v //c/path/to/code:/root/jekyll abarylko/western-devs:v1 sh -c 'bundle install && rake serve'`\n\nThis will:\n\n* create a container based on the `abarylko/western-devs:v1` image\n* export port 4000 to the host VM\n* map the path to the source code on your machine to /root/jekyll in the container\n* run `bundle install && rake serve` to update gems and launch Jekyll in the container\n\nTo make this work 100%, you also need to expose port 4000 in VirtualBox so that it's visible from the VM to the host. Also, I've had trouble getting a container working with my local source located anywhere except C:\\Users\\\\*mysuername*. There's a permission issue somewhere in there where the container appears to successfully map the drive but can't actually see the contents of the folder. This manifests itself in an error message that says `Gemfile not found`.\n\nNow, Windows users can navigate to localhost:4000 and see the site running locally. Furthermore, they can add and make changes to their posts, save them, and the changes will get reflected in the browser. Eventually, that is. I've noticed a 10-15 second delay between the time you press Save to the time when the changes actually get reflected. Haven't determined a root cause for this yet. Maybe we just need to soup up the VM.\n\nSo far, this has been working reasonably well for us. To the point, where fellow Western Dev, [Dylan Smith](http://www.westerndevs.com/bios/dylan_smith/) has automated the deployment of the image to Azure via [a Powershell script](https://github.com/westerndevs/western-devs-website/tree/source/_azure). That will be the subject of a separate post. Which will give me time to figure out how the thing works.","categories":[],"tags":[]},{"title":"PSA: Setting Up Containers in a VM in Windows Server 2016 Tech Preview 3","authorId":"kyle_baley","slug":"psa-setting-up-containers-in-a-vm-in-windows-server-2016-tech-preview-3","date":"2015-08-20 00:38:59+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/psa-setting-up-containers-in-a-vm-in-windows-server-2016-tech-preview-3/","link":"","permalink":"https://westerndevs.com/_/psa-setting-up-containers-in-a-vm-in-windows-server-2016-tech-preview-3/","excerpt":"Windows Server 2016 Tech Preview 3 has just been released and it has container support! There's documentation on it already to do basic stuff and it's easy to follow. So I'm going to repeat it verbatim quickly mention the one and only major issue I ran into.","raw":"---\nlayout: post\ntitle:  \"PSA: Setting Up Containers in a VM in Windows Server 2016 Tech Preview 3\"\ndate: 2015-08-19T16:38:59-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\n[Windows Server 2016 Tech Preview 3](https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-technical-preview) has just been released and it has container support! There's documentation on it already to do basic stuff and it's easy to follow. So I'm going to <s>repeat it verbatim</s> quickly mention the one and only major issue I ran into.\n\n<!--more-->\n\nI created a VM in Fusion for the server which went pretty smoothly. When presented with a list of operating systems, I selected Windows Server 2012 and it installed fine from the ISO file. After that, I started on the documentation and at the step where you run the ContainerSetup.ps1 powershell script, I got hit with an error:\n\n    New-NetNat : No matching interface was found for prefix (null).\n    At C:\\ContainerSetup.ps1:247 char:5\n    New-NetNat -Name ContainerNAT -InternalIPInterfaceAddressPrefix $ ...\n    CategoryInfo : NotSpecified (MSFT_NetNat:root/StandardCimv2/MSFT_NetNat) [New-NetNat], CimException\n    FullyQualifiedErrorId : Windows System Error 1169,New-NetNat\n\n![New-NetNat error](http://i.imgur.com/W6BxfIE.png)\n\nThis tripped me up for a while for a few reasons:\n\n1) I'm not crazy familiar with PowerShell\n2) Nothing came up for the error message in the Bingoogle\n3) InternalIPInterfaceAddressPrefix doesn't even appear in the documentation for [New-NetNat](https://technet.microsoft.com/en-us/library/dn283361(v=wps.630).aspx)\n\nThe answer, which came to me in a [rubber ducking](https://en.wikipedia.org/wiki/Rubber_duck_debugging) episode as I typed on a question on the forums, was in the VM's network configuration in VMWare Fusion:\n\n![Network settings](http://i.imgur.com/nzfzr4X.png = 200x)\n\nBy default, VMs get created with the \"Share with my Mac\" setting. Changing this to Autodetect allowed the script to continue.\n\nBy that time, I had discovered that an Azure VM image already exists for Server 2016 Tech Preview 3 and even with the Containers Feature already enabled. So that's as far as I ventured with the VM.\n","categories":[],"tags":[]},{"title":"Change Management - the Missing Podcast","authorId":"simon_timms","slug":"change-management-the-missing-podcast","date":"2015-08-15 20:29:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/change-management-the-missing-podcast/","link":"","permalink":"https://westerndevs.com/_/change-management-the-missing-podcast/","excerpt":"Some people are really good at computers. I am, apparently, not one of those people. This last Friday we had a fantastic podcast with","raw":"---\nlayout: post\ntitle:  Change Management - the Missing Podcast\ndate: 2015-08-15T10:29:50-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\n---\n\nSome people are really good at computers. I am, apparently, not one of those people. This last Friday we had a fantastic podcast with\n\n<!--more-->\n\n- [Dylan Smith](http://www.westerndevs.com/bios/dylan_smith/)\n- [Dave White](http://www.westerndevs.com/bios/dave_white/)\n- [Simon Timms](http://www.westerndevs.com/bios/simon_timms/)\n- [David Wesst](http://www.westerndevs.com/bios/david_wesst/)\n- [Amir Barylko](http://www.westerndevs.com/bios/amir_barylko/)\n\nHowever I managed to not record one second of it. I pressed record but apparently that is not sufficient to actually record it. As a form of penance I decided I would write up what I could remember from the talk. Thus:\n\nWe started by talking about Simon's post on [change management for the evolving world](http://www.westerndevs.com/change-management-for-the-evolving-world/). We talked about how we saw numerous advantages in moving quickly inside an organization. Dylan suggested that lag time between a feature being complete and not deployed is money wasted. If a need has been identified and addressed then not shipping it is costing the company. Dylan also introduced us to the idea of flow efficiency which is an idea taken from Kanban. It is a measurement of how much time a work items spends waiting for resources as opposed to actively being worked upon. In most organizations this efficiency is around 4% while in highly efficient kanban teams that efficiency is closer to 40% or even 50%. There is some inefficiency that is simply not possible to eliminate. \n\nDavid Wesst talked about how his large public sector company was facing challenges that could not easily be addressed by programmatically building infrastructure to test all changes. Many of the changes with which he was involved were related to ancient equipment that didn't have the capability of being virtualized or hardware changes. Change management is much larger that how we migrate changes to configuration or software. A change to a network switch or to a DNS entry cannot easily be modeled. As such having a change management review process was important because no one person could keep enough of the infrastructure in their head to understand all the impacts of the changes. In order to help with that the company with which Dave Wesst works had built a database which identified all of the interdependencies between applications and services. This system is to be kept fully up to date so that developers and admins have an easy place to check for the potential impacts of their changes. \n\nWe identified that the change management meeting process was a bottleneck. The fact that the meeting happens only once a week and contains a great deal of very expensive talent was a problem. In order to be more efficient we struck upon the idea of taking the approval process out of a meeting and putting it into some sort of a workflow tool like SharePoint(yuck). In this way individuals could approve changes whenever they had time. This would not only provide a really good audit trail but also give an easy way to highlight where approval bottlenecks were. Dylan suggested that we all read the book [The Phoenix Project](http://www.amazon.com/The-Phoenix-Project-Helping-Business-ebook/dp/B00AZRBLHO). As a matter of fact Dylan suggested we read it no fewer than 10 times, so probably read it. \n\nNot all changes need to go through the full heavy weight process. There are, at least, two classes of changes: simple common place changes and complicated abnormal changes. The first category might be things like adding a new DNS entry while the second category could be things like upgrading a database. Clearly a big process would be detrimental to the first class of changes. Applying a single process blindly is not a good way to operate - instead apply some intelligence to how processes are created and adhered to. \n\nWe also talked about how we can effectively audit software and configuration changes. An idea was that logging into a production system could require entering a ticket number. The session could then be recorded either through screen recording software on Windows or script(1) on Unix-like systems. Dylan also told us about a company where he worked that required that a low cost employee shadow any changes just to make sure that the person making the changes didn't deviate from what they were trying to do. \n\nIn the end we decided that we had pretty much solved change management and that we all deserved huge raises and possibly knighthoods.\n","categories":[],"tags":[]},{"title":"Getting Docker running on Windows 10","authorId":"dave_white","slug":"getting-docker-running-on-windows-10","date":"2015-08-14 05:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/getting-docker-running-on-windows-10/","link":"","permalink":"https://westerndevs.com/_/getting-docker-running-on-windows-10/","excerpt":"Just a quick post about a couple things I've learned yesterday and today. Docker is now available to run on Windows 10. I'm not going to go into the details as they are better covered in other posts, but I'll share the steps I followed to get Docker running on my Windows 10 laptop.","raw":"---\nlayout: post\ntitle: \"Getting Docker running on Windows 10\"\ndate: 2015-08-14 1:30:00\nauthorId: dave_white\noriginalurl: http://agileramblings.com/2015/08/14/getting-docker-running-on-windows-10/\ncomments: true\n---\nJust a quick post about a couple things I've learned yesterday and today.\n\nDocker is now available to run on Windows 10. I'm not going to go into the details as they are better covered in other posts, but I'll share the steps I followed to get Docker running on my Windows 10 laptop.\n\n<!--more-->\n\n![Docker-Windows-10][1]\n\n### Visit David Wesst's Blog post (cross-posted to Western Devs)\n\nDave was the first of the Western Dev guys to talk about trying to get Docker working on Windows 10. He blogged about his adventure and that is where I started.\n\n[http://blog.davidwesst.com/2015/08/docker-on-windows-10-problems](http://blog.davidwesst.com/2015/08/docker-on-windows-10-problems)\n\n[http://www.westerndevs.com/docker-on-windows-10-problems](http://www.westerndevs.com/docker-on-windows-10-problems)\n\n### Visit the Docker GitHub site\n\nDave's post directed me towards a couple other places. Namely, the Docker issues GitHub site and the Docker windows installation page.\n\n[https://docs.docker.com/installation/windows](https://docs.docker.com/installation/windows)\n\nThis was pretty important because it was where the conversation about getting Docker (more accurately, VirtualBox) running on Windows 10. There is an issue in VirtualBox (current stable build) that does not allow it to work on Windows 10. This issue has been resolved in a Test build. The link to the test build is here.\n\n<span style=\"text-decoration: line-through;\">https://www.virtualbox.org/wiki/testbuilds</span> (no longer active)\n\nI didn't actually need to go to the VirtualBox website to get the build because the latest test version of the Docker for Windows installer has the test version of VirtualBox already inside of it. You can find the link to the current test installer here.\n\n[https://github.com/docker/toolbox/issues/77](https://github.com/docker/toolbox/issues/77)\n\n### Follow the start up direction\n\nThe next thing I did was follow all of the start-up directions from the docker windows install documents. VirtualBox was installed, all of the Docker Toolbox items where installed, and so I fired it all up. And it didn't work. What was going on? The VM very quickly informed me that it couldn't find a 64bit cpu/os which is required to run docker.\n\n>**This kernel requires an x86-64 CPU, but only detected an i686 CPU. Unable to boot â€“ please use a kernel appropriate for your&nbsp;CPU**\n\nWell, that was weird. I have an modern laptop (Dell XPS 15) running 64 bit Windows 10 Enterprise. What could be the problem? Google Foo to the rescue!\n\nFirst I found posts suggesting that the CPU Intel Virtualization Technologies were not enabled. I didn't think that was true because I had already been running some HyperV machines on my laptop, but I did re-boot into my BIOS and ensure that Intel VT-x/AMD-V where enabled. They were.\n\nSo google a bit more, and I find that Virtual Box might need me to change the \"type\" of the VM to \"Other\" and the OS to \"Other/64bit\" or something like that. But interestingly enough, those were not options that I had in the VM.\n\n![VirtualBox-OS-Options][2]\n\nThis screenshot was taken after the fix (which I'm getting to) but originally, none of the 64 bit versions of the OSes were available as a choice.\n\n![VirtualBox-OS-bit-options][3]\n\nOne last thing I found was to remove the HyperV feature from Windows 10, but that wasn't a viable option for me. I have some HyperV virtual machines that I run (and need to run) so I didn't even explore that option.\n\nAt this point, I worked around for a bit and then gave up for the evening. Better to sleep on it and see if I could start fresh in the morning.\n\n### Scott Hanselman to the Rescue\n\nI'm not sure what search I did in the morning that got me to Scott Hanselman's post. I should really just always go there first because I find so much good information about Windows development (native and cross-platform) there. But specifically, it was this post that finally solved my problem.\n\n[http://www.hanselman.com/blog/switcheasilybetweenvirtualboxandhypervwithabcdeditbootentryinwindows81.aspx](http://www.hanselman.com/blog/switcheasilybetweenvirtualboxandhypervwithabcdeditbootentryinwindows81.aspx)\n\nI didn't know this until today, but as it turns out, HyperV and VirtualBox will not run together side-by-side in 64 bit modes. And Scott's blog post about rebooting to a **hypervisorlaunchtype off** mode of Windows 8.1 worked flawlessly for Windows 10. So I didn't have to un-install the HyperV feature, but as it turns out, I did have to disable HyperV. I'm sure glad I don't have to add/remove it daily though!\n\n### Final Thoughts\n\nSo that was it! Thanks to [David Wesst][4], [WesternDevs][5], [Docker ][6]and [Scott Hanselman][7], I now have Docker running on my Windows 10 laptop. Just not at the same time as my HyperV virtual machines. :D\n\n[1]: https://agileramblings.files.wordpress.com/2015/08/docker-windows-10.png?w=600&amp;h=308\n[2]: https://agileramblings.files.wordpress.com/2015/08/virtualbox-os-options.png?w=450&amp;h=330\n[3]: https://agileramblings.files.wordpress.com/2015/08/virtualbox-os-bit-options.png?w=450&amp;h=330\n[4]: http://blog.davidwesst.com/\n[5]: http://www.westerndevs.com/\n[6]: https://www.docker.com/\n[7]: http://www.hanselman.com/\n","categories":[],"tags":[]},{"title":"Azure SQL Point in Time Restore Is Near Useless","authorId":"simon_timms","slug":"azure-sql-point-in-time-restore-is-near-useless","date":"2015-08-14 05:11:45+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/azure-sql-point-in-time-restore-is-near-useless/","link":"","permalink":"https://westerndevs.com/_/azure-sql-point-in-time-restore-is-near-useless/","excerpt":"Azure SQL Point In Time restores are so slow as to be near useless.","raw":"---\nlayout: post\ntitle:  Azure SQL Point in Time Restore Is Near Useless\ndate: 2015-08-13T19:11:45-06:00\ncategories:\nexcerpt: Azure SQL Point In Time restores are so slow as to be near useless.\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/08/14/azure-point-in-time-restore-is-useless/\n---\n\nAbout a year ago Microsoft rolled out Azure [point in time restore](http://azure.microsoft.com/blog/2014/10/01/azure-sql-database-point-in-time-restore/) on their SQL databases. The idea is that you can restore your database to any point in time from the last little while (how long ago you can restore from is a function of the database scale). This means that if something weird happened to your data 8 hours ago you can restore back to that point. It even support restoring databases that have recently been deleted.\n\nMy reading of the marketing material around this feature is that it is meant to replace full database backups in a number of scenarios. In fact if you go to do a database export you're warned about the performance implications and that point in time restore is much preferred. The problem is that it is slow.\n\nCripplingly. Shockingly. Amazingly. Slow.\n\nThe database I'm working with is about 140MiB as a backup file and just shy of 700MiB when deployed on a database server. Downloading and restoring the database on my laptop, a 3 year old macbook pro running an ancient version of Parallels takes between 6 and 10 minutes. Not a huge deal.\n\nOn azure I have some great statistics because restoring the database is part of our QA process. Since I switched from restoring nightly backups to using point in time restores I've done 45 builds. Of these 6 of them have failed to complete the restore before I gave up which usually takes a day. The rest are distributed like this in minutes\n\n![Scatter!](http://i.imgur.com/LvegUyg.jpg)\n\nAs you can see 23 of restores, or 59% took more than 50 minutes. There are a few there that are creeping up on 5 hours. That is insane. This is a very small database when you consider that these S1 databases scale to 250gig.  Even if we take our fastest restore at 7 minutes and plot it out then this is a 29 hour restore process. What sort of a business can survive a 29 hour outage? If we take the longest then it is 47 days. By that time the company's assets have been sold at auction and the shareholders have collected 10 cents on the dollar.\n\nWhen I first set this process up it was on a web scale database and used a backup file. The restore consistently took 15 minutes. Then standard databases were released and the restore time increased to a consistent 40 minutes. Now I'm unable to tell the QA team to within 4 hours when the build will be up.\n\nFortunately I have a contact on the Azure SQL team who I pinged about the issue. Apparently this is a known issue and a fix is being rolled out in the next few weeks. I really hope that is the case because in the current configuration point in time restores are so slow and inconsistent that they're in effect useless for disaster recovery scenarios as even for testing scenarios.\n","categories":[],"tags":[]},{"title":"Writing Custom Commands for DNX With ASP.NET 5.0","authorId":"james_chambers","slug":"writing-custom-commands-for-dnx-with-asp-net-5-0","date":"2015-08-14 01:50:41+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/writing-custom-commands-for-dnx-with-asp-net-5-0/","link":"","permalink":"https://westerndevs.com/_/writing-custom-commands-for-dnx-with-asp-net-5-0/","excerpt":"If you are a developer on the .NET stack, you've now got access to a great new extension to your development environment. DNX, or the .NET Execution Environment, is a powerful new extensibility point that you can leverage to build project extensions, cross-platform utilities, build-time extensions and support for automation. In this article I'll walk you through the process of building your own custom DNX command on top of ASP.NET 5.0.","raw":"---\nlayout: post\ntitle:  \"Writing Custom Commands for DNX With ASP.NET 5.0\"\ndate: 2015-08-13T23:50:41+02:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/08/writing-custom-commands-for-dnx-with-asp-net-5-0/\n---\n\nIf you are a developer on the .NET stack, you've now got access to a great new extension to your development environment. DNX, or the .NET Execution Environment, is a powerful new extensibility point that you can leverage to build project extensions, cross-platform utilities, build-time extensions and support for automation. In this article I'll walk you through the process of building your own custom DNX command on top of ASP.NET 5.0.\n\n<!--more-->\n\n## Where You've Seen It\n\nDNX has the ability to scan a project.json and look for commands that you install as packages or that you create yourself. If you've started following the examples of the MVC Framework or perhaps with Entity Framework, you may have seen things like this in your project.json:\n\n{% codeblock lang:javascript %}\n\"commands\": {\n  \"web\": \"Microsoft.AspNet.Hosting --config hosting.ini\",\n  \"ef\": \"EntityFramework.Commands\"\n}\n{% endcodeblock %}\n\n{% img pull-right \"http://jameschambers.com/wp-content/uploads/2015/08/image_thumb2.png\" %}\n\nThese entries are here so that DNX understands the alias you assign (such as \"web\" or \"ef\") and how it maps to an assembly that you've created or taken on as a dependency.&nbsp; The EF reference is quite straightforward above, simply saying that any call to \"ef\" via DNX will go into the entry point in EntityFramework.Commands.&nbsp; You would invoke that as follows from the directory of your _project_:\n\n    dnx . ef\n\nAll parameters that are passed in are available to you as well, so if you were to instead use:\n\n    dnx . ef help migration\n\nThen EF would be getting the params \"help migrations\" to parse and process. As can be clearly seen in the \"web\" alias, you can also specify defaults that get passed into the command when it is executed, thus, the call to web in the above project.json passes in the path and filename of the configuration file to be used when starting IIS express.\n\nThere is no special meaning to \"ef\" or \"web\". These are just names that you assign so that the correct mapping can be made. If you changed \"ef\" to \"right-said-fred\" you would be able to run migrations from the command line like so:\n\n    dnx . right-said-fred migration add too-sexy\n\nGreat! So you can create commands, pass in parameters and share these commands through the project.json file. But now what?\n\n## Now What?\n\nI'm so glad you asked!\n\nSo far things really aren't too different from any other console app you might create. I mean, you can parse args and do whatever you like in those apps as well.\n\nBut here's the winner-winner-chicken-dinner bits: did you notice the \".\" that is passed into DNX? That is actually the path to the project.json file, and this is important.\n\n> **Important Note**: From beta 7 onward (or already if you're on the nightly builds) DNX will implicitly run with an appbase of the current directory, removing the need for the \".\" in the command. I'll try to remember to come back to this post to correct that when beta 7 is out in the wild. Read more about the change on the [ASP.NET Announcement repo][2] on GitHub.\n\nDNX doesn't actually do a lot on its own, not other than providing an execution context under which you can run your commands. But this is a good thing! By passing in the path to a project.json, you feed DNX the command mappings that you want to use, and in turn, DNX provides you with all of the benefits of running inside of the ASP.NET 5.0 bits. Your console app just got access to Dependency Injection as a first-class citizen in your project, with access to information about whichever app it was that contained that project.json file.&nbsp;\n\nConsider the EF command mapping again for migrations for a second: what is going on when you tell it to add a migration?&nbsp; It goes something like this:\n\n1. DNX looks for the project.json at the path you provide\n2. It parses the project.json file and finds the command mapping associated with your statement\n3. I **_creates an instance_** of the class that contains your command, injecting environment and project information as is available\n4. It checks the rest of what you've passed in, and invokes the command, passing in any parameters that you've supplied\n\n## How to Build Your Own\n\nThis is actually super easy!&nbsp; Here's what you need to do:\n\n1. Create a new ASP.NET 5 Console Application in Visual Studio 2015\n2. Add any services interfaces you need as parameters to the constructor of the Program class â€“ but this is optional in the \"hello dnx\" realm of requirements\n3. Add your logic to your Main method â€“ start with something as simple as a Console.WriteLine statement\n\nFrom there, you can drop to a command line and run your command. That's it!\n\n> **Pro Tip** You can easily get a command line in your project folder by right-clicking on the project in Solution Explorer and selecting \"Open Folder in File Explorer\". When File Explorer opens, simply type in \"cmd\" or \"powershell\" in the location bar and you'll get your shell.\n\nThe secret as to why it works from the console can be found in your project.json: when you create a console app from the project templates, the command alias mapping for your project is automatically added to your project.&nbsp; In this same way, along with referencing your new command project, _other projects_ can now consume your command.\n\n## Beyond Hello World\n\nIt is far more likely that you're going to need to do something in the context of the project which uses your command. Minimally, you're likely going to need some configuration drawn in as a default or as a parameter in your command. Let's look at how you would take that hello world app you created in three steps and do something a little more meaningful with it.\n\nFirst, let's add some dependencies to your project.json:\n\n{% codeblock lang:javascript %}\n\"dependencies\": {\n    \"Microsoft.Framework.Runtime.Abstractions\": \"1.0.0-beta6\",\n    \"Microsoft.Framework.Configuration.Abstractions\": \"1.0.0-beta6\",\n    \"Microsoft.Framework.Configuration.Json\": \"1.0.0-beta6\",\n    \"Microsoft.Framework.Configuration.UserSecrets\": \"1.0.0-beta6\",\n    \"Microsoft.Framework.Configuration.CommandLine\": \"1.0.0-beta6\"\n}\n{% endcodeblock %}\n\nNow let's add a new JSON file to our project called config.json with the following contents:\n\n{% codeblock lang:javascript %}\n{\n  \"command-text\": \"Say hello to my little DNX\"\n}\n{% endcodeblock %}\n\nGetting there. Next, let's bulk up the constructor of the Program class, add a private member and a Configuration property:\n\n{% codeblock lang:csharp %}\nprivate readonly IApplicationEnvironment _appEnv;\n\npublic Program(IApplicationEnvironment appEnv)\n{\n    _appEnv = appEnv;\n}\n\npublic IConfiguration Configuration { get; set; }\n{% endcodeblock %}\n\nWe also need to add a method to Program that handles loading the config, taking in what it can from the config file, but loading on top of that any arguments passed in from the console:\n\n{% codeblock lang:csharp %}\n    private void BuildConfiguration(string[] args)\n    {\n        var builder = new ConfigurationBuilder(_appEnv.ApplicationBasePath)\n            .AddJsonFile(\"config.json\")\n            .AddCommandLine(args);\n\n        Configuration = builder.Build();\n    }\n{% endcodeblock %}\n\nFinally, we'll add a little more meat to our our Main method:\n\n{% codeblock lang:csharp %}\n    public void Main(string[] args)\n    {\n        BuildConfiguration(args);\n\n        Console.WriteLine(Configuration.Get(\"command-text\"));\n        Console.ReadLine();\n    }\n{% endcodeblock %}\n\nThe above sample can now be executed as a command. I've got the following command mapping in my project.json file (yes, the same project you use to create the command can also expose the command):\n\n{% codeblock lang:javascript %}\n\"commands\": {\n    \"DnxCommands\": \"DnxCommands\"\n}\n{% endcodeblock %}\n\nThis means that from the console in the dir of my project I can just type in the following:\n\n    dnx . DnxCommands\n\nI can also now reference this project from any other project (or push my bits to NuGet and share them to any project) and use the command from there. Other projects can add the \"command-text\" key to their config.json files and specify their own value, or they can feed in the parameter as an arg to the command:\n\n    dnx . DnxCommands command-text=\"'Pop!' goes the weasel\"\n\nIn my [sample solution on GitHub][3], I also have a second project which renames the alias and has it's own config file that is read in by the command.\n\n## Next Steps\n\nAll of this opens the doors for some pretty powerful scenarios. Think about what you can do in your build pipeline without having to write, expose and consume custom msbuild targets. You can create commands that are used to build up local databases for new environments or automate the seeding of tables for integration tests. You could add scaffolders and image optimizers and deployment tools and send text messages to your Grandma.\n\nWhat you should do next is to look at the kinds of things you do when you're working on your solution â€“ not in it â€“ and think about how you might be able to simplify those tasks. If there are complex parts of your build scripts that you encounter from one project to the next, perhaps you can abstract some of those bits away into a command and then shift to using simplified build scripts that invoke your commands via DNX.\n\nTo get some inspiration, check out my [sample project on GitHub][3], the DNX commands for other libraries (such as\n<span style=\"text-decoration: line-through;\">EF</span> or <span style=\"text-decoration: line-through;\">xUnit</span>) and try writing a few of your own.\n\nHappy coding! ![Smile][6]\n\n[2]: https://github.com/aspnet/Announcements/issues/52\n[3]: https://github.com/MisterJames/DnxCommands/\n[6]: https://jcblogimages.blob.core.windows.net/img/2015/08/wlEmoticon-smile1.png  \n","categories":[],"tags":[]},{"title":"Converting .NET 4.6 Projects to the VS 2015 Project System","authorId":"james_chambers","slug":"converting-net-4-6-projects-to-the-vs-2015-project-system","date":"2015-08-14 01:22:51+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/converting-net-4-6-projects-to-the-vs-2015-project-system/","link":"","permalink":"https://westerndevs.com/_/converting-net-4-6-projects-to-the-vs-2015-project-system/","excerpt":"To take advantage of multi-targeted outputs from our project â€“ allowing our assemblies to be used from multiple frameworks across the organization â€“ we want to upgrade our projects to use the new project system in Visual Studio 2015. Previously, we would have needed a base project and then a separate project for each framework target (PCL, 4.5, 3.5, 4.5.2, etc), but in today's solutions we can have a single project output all of the assets we wish to support.","raw":"---\nlayout: post\ntitle:  \"Converting .NET 4.6 Projects to the VS 2015 Project System\"\ndate: 2015-08-13T23:22:51+02:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/08/converting-net-4-6-projects-to-the-vs-2015-project-system/\n---\n\nTo take advantage of multi-targeted outputs from our project â€“ allowing our assemblies to be used from multiple frameworks across the organization â€“ we want to upgrade our projects to use the new project system in Visual Studio 2015. Previously, we would have needed a base project and then a separate project for each framework target (PCL, 4.5, 3.5, 4.5.2, etc), but in today's solutions we can have a single project output all of the assets we wish to support.\n\n<!--more-->\n\n>_In this series we're working through the conversion of [Clear Measure's][1] [Bootcamp MVC][2] 5-based application and migrating it to MVC 6. You can track the entire series of posts from the [intro page][3]._\n\n## Recreating the Solution and Projects\n\nAs of right now, there are no tools in place that would support an in-place migration from the old project system to the new one. Because we wanted to preserve project naming and namespaces, I copied everything out into a new directory â€“ the solution and the projects â€“ and rebuilt the solution from scratch.\n\nI would anticipate a project conversion process at some point, even one that was able to provide the basics (like moving package dependencies to project.json) and guidance on the remaining pieces (like why part of the project wasn't able to convert, and how you might approach it). This post will walk through those steps of the conversion, but it will be done manually.\n\nI wanted to maintain all the same names of the assemblies, namespaces and outputs, and the only way to currently do this is to clear out the src folder and start over. Don't worry, our code is still good, we just have to wrangle it into new containers.\n\n## Step 0: Folder Reorganization\n\nOne of the first changes that I made was a reorganization of the tooling that is used to support the build. Some of the build script relied on packages existing on disk (NUnit's console runner, AliaSql) but this is an order-of-operations problem. When you grab the solution from the repo, you're not actually able to build it until you restore the packages. Further, these assets are **solution-level **concerns, not project-level concerns, so which project do you install them into? NuGet does not have the concept of solution-level packages that apply to the solution itself, so while it works perfectly well for projects, NuGet is inherently not ideal for incorporating solution dependencies.\n\nTo remedy this, I have moved these types of assets into a tools folder and updated the build scripts accordingly. This approach is likely a matter of opinion more than anything, but the reality is that we want the directory structure to reflect which concerns **_are in_** the solution versus which concerns **_work on_** the solution.\n\nI would like to note that there are still improvements to be made here â€“ for instance, I know many teams actually have build scripts that are capable of not only restoring packages, but have the ability to go and grab NuGet itself â€“ so expect some more changes as we continue to move through this migration. Automation is _awesome_.\n\n## Step 1 â€“ Core\n\n{% img pull-right \"http://jameschambers.com/wp-content/uploads/2015/08/image_thumb.png\" %}\n\nOur Core project was a breeze to port because it's at the heart of the system in an [Onion Architecture][5] and takes on very few dependencies. I started the conversion by going through the motions of creating a new Core project, using the DLL project from the \"Web Templates\" part of the dialog.&nbsp; The first project also creates the solution, and the convention for the way the solutions are laid out on disk has changed.\n\nSoâ€¦the build broke.\n\nThankfully, this was easy to resolve with just a couple of quick fixes, but you'll likely have to take similar steps on your project:\n\n* First, update your paths to point at the correct location on disk\n* Second, comment out all the build steps that have to come later, like running unit or integration tests\n\n{% img pull-right \"http://jameschambers.com/wp-content/uploads/2015/08/image_thumb1.png\" %}\n\nWe can't run unit tests quite yet (we need to convert those projects as well), but we can make sure that the project is building correctly.\n\nWe're not modifying code at this point, so provided we can get the solution building we can have a good level of confidence â€“ but not a guarantee â€“ that our code is still in good shape. We want those tests back online before we merge this branch back to develop.\n\nWith the build running, I was able to jump back into Visual Studio and start adding back the code. In my case, nearly everything worked just by copying in the files from my backup location and pasting them into the project. It's a bit tedious, but it's by no means difficult or complicated.\n\nThe only package that I had to add at this point was a legacy dependency from NHibernate, namely the Iesi.Collections package. This is done by opening up the project.json for Core and updating the \"dependencies\" part of the project file. As soon as you save the file out, Visual Studio goes off and runs a background install of the packages that it finds you've put in there, along with any dependencies of those packages.\n\n![image][7]\n\nFinding the right package and most recent version is quite easy in the project.json world. As you start typing a package name, in-line search kicks in and starts suggesting matches. Available versions of the packages are displayed, and VS indicates if those packages are available locally in a cache or found on a remote NuGet repository, indicated by the icon you see. All packages sources are queried for package information, so you can get packages and their version information from private repositories as well.\n\nOnce the packages were restored the solution built fine in Visual Studio 2015 and I was able to return to my console to run the build script.\n\n![image][8]\n\n## Step 2: Data Access\n\nOther than the fact that Data Access has a few more dependencies, it was really more of the same to get the Data Access project online and building through our script. I added another DLL to the solution, added the source files and installed the dependencies via project.json.\n\nWhen I compiled the project at this point, some of the changes of the .NET Framework and the strategy of the team started to surface. For instance, typically you might find a reference to System.Data from your GAC in a project, however, in the new cross-platform project system and under the assumption that you may not have a GAC at all, the .NET folks have taken the mantra of \"NuGet all the things.\" To get access to the System.Data namespace and the IDataReader interface that was used in the DataAccess project, I had to add a reference to System.Data version 4.0.0 from NuGet (via project.json).\n\nOther projects will have similar hits on moved packages. It is likely safe to use the GAC in situations where you know what the build environment looks like and are sure that build agents and other developers will have access to the required dependencies. But it is a more stable approach â€“ and a better chance to successful compile our application â€“ to instead reference those binaries from a package repository.\n\nThe other notable piece was in how we reference other projects in our own solution; today they look a lot like referencing other packages. Whether you go through the Add Reference dialog or if you prefer to edit the project file by hand, you're going to also need to introduce a dependency on Core, which is done simply by adding the following line to the dependencies:\n\n<div class=\"notice\">\n\n    \"Core\": \"1.0.0-*\"\n\n</div>\n\nExcellent! Almost ready to build!\n\n## Step 3: Clean Up\n\nJust a couple of other notes that I took and a couple of tips I've learned as I created these projects:\n\n* You'll have to set the default namespaces so that new classes that are introduced adhere to your conventions\n* You need to enable the \"Produce outputs on build\" in order for your project to build a NuGet package (this is in the build options)\n\nYou're also in charge of wiring up any dependencies your modules need where they aren't satisfied with a single package for all output types. For instance, when I tried a small gamut of output targets I ran into this problem:\n\n![image][9]\n\nThe new .NET Platform (the base for Windows, web, mobile and x-plat) was not supported given the dependencies I have listed in my project, namely it is the IESI Collections that is the problem here. Ideally, you want to be able to support as many runtimes as possible, so you want to target the lowest common denominator. That is likely going to be \"dotnet\" going forward (which could in turn be used to build up applications for web, Windows or phone) but more realistically things like \"net46\", which is just the 4.6 version of .NET, or \"dnx46\", which is the new bits (think MVC Framework) running on top of .NET 4.6. In the cases where you don't have a package that matches the target you need, you have a couple of choices, listed in order of easiest to most difficult:\n\n* Contact the package authors to see if there is a new version coming\n* If it's open source, contribute and get an output built for dotnet\n* Add runtime-specific dependencies to get the project building, then use compiler switches to implement different blocks of code based on the target framework\n* Switch off of that version of the package, or switch to an alternate package to get the same functionality and then update your code as required\n\nSadly, that last one is likely the way we're going to need to go, especially if we want to target x-plat development. This is not an easy task, but getting to this point in the migration is and only takes a couple of hours. If you haven't done this sanity check in your project to identify packages that may cause issues during migrations, I would suggest that your assessment is not complete.\n\nFor the time being, we are concerned about supporting .NET 4.6 and DNX running on 4.6 for our project, so that is where I have left things. This is a reasonable compromise allowing continued development in web and Windows.\n\n## Moving On\n\nThe main tenets of our application are now alive and kicking in our Visual Studio 2015 solution with the new project system in place. In the next post in this series we'll have a look at getting the tests online and updating the build script to execute our tests.\n\nIf you'd like to follow along with the progression as we get this fully converted you can check out <span style=\"text-decoration: line-through;\">the branch on GitHub</span>.\n\nHappy coding! ![Smile][11]\n\n[1]: http://clear-measure.com/\n[2]: https://github.com/ClearMeasureLabs/ClearMeasureBootcamp/\n[3]: http://jameschambers.com/2015/07/upgrading-a-real-world-mvc-5-application-to-mvc-6/\n[4]: http://jameschambers.com/wp-content/uploads/2015/08/image_thumb.png \"image\"\n[5]: http://jeffreypalermo.com/blog/the-onion-architecture-part-1/\n[6]: http://jameschambers.com/wp-content/uploads/2015/08/image_thumb1.png \"image\"\n[7]: http://jameschambers.com/wp-content/uploads/2015/08/image2.png \"image\"\n[8]: http://jameschambers.com/wp-content/uploads/2015/08/image3.png \"image\"\n[9]: http://jameschambers.com/wp-content/uploads/2015/08/image4.png \"image\"\n[11]: https://jcblogimages.blob.core.windows.net/img/2015/08/wlEmoticon-smile.png\n","categories":[],"tags":[]},{"title":"Setting Up an IIS Site Using PowerShell","authorId":"simon_timms","slug":"setting-up-an-iis-site-using-powershell","date":"2015-08-12 21:29:25+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/setting-up-an-iis-site-using-powershell/","link":"","permalink":"https://westerndevs.com/_/setting-up-an-iis-site-using-powershell/","excerpt":"The cloud has been such an omnipresent force in my development life that I'd kind of forgotten that IIS even existed. There are, however, some companies that either aren't ready for the cloud or have legitimate legal limitations that make using the cloud difficult.","raw":"---\nlayout: post\ntitle:  Setting Up an IIS Site Using PowerShell\ndate: 2015-08-12T11:29:25-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/08/12/setting-up-an-iis-site-using-powershell/\n---\n\nThe cloud has been such an omnipresent force in my development life that I'd kind of forgotten that IIS even existed. There are, however, some companies that either aren't ready for the cloud or have legitimate legal limitations that make using the cloud difficult.\n\n<!--more-->\n\nThis doesn't mean that we should abandon some of the niceties of deploying to the cloud such as being able to promote easily between environments. As part of being able to deploy automatically to new environments I wanted to be able to move to a machine that had nothing but IIS installed and run a script to do the deployment.\n\nI was originally thinking about looking into PowerShell Desired State Configuration but noted brain-box [Dylan Smith](http://www.westerndevs.com/bios/dylan_smith/) told me not to bother. He feeling was that it was a great idea whose time had come but the technology wasn't there yet. Instead he suggested just using PowerShell proper.\n\nWell okay. I had no idea how to do that.\n\nSo I started digging. I found that PowerShell is really pretty good at setting up IIS. It isn't super well documented, however. The PowerShell documentation is crummy in comparison with stuff in the .net framework. I did hear on an episode of Dot Net Rocks that the UI for IIS calls out to PowerShell for everything now. So it must be possible.\n\nThe first step is to load in the powershell module for IIS\n\n{% codeblock lang:powershell %}\nImport-Module WebAdministration\n{% endcodeblock %}\n\nThat gives us access to all sorts of cool IIS stuff. You can get information on the current configuration by cding into the IIS namespace.\n\n{% codeblock lang:powershell %}\nC:\\WINDOWS\\system32> cd IIS:\nIIS:\\> ls\n\nName\n----\nAppPools\nSites\nSslBindings\n{% endcodeblock %}\n\nWell that's pretty darn cool. From here you can poke about and look at the AppPools and sites. I was told that by fellow Western Dev [Don Belcham](http://www.westerndevs.com/bios/donald_belcham/) that I should have one AppPool for each application so the first step is to create a new AppPool. I want to be able to deploy over my existing deploys so I have to turff it first.\n\n{% codeblock lang:powershell %}\nif(Test-Path IIS:\\AppPools\\CoolWebSite)\n{\n\techo \"App pool exists - removing\"\n\tRemove-WebAppPool CoolWebSite\n\tgci IIS:\\AppPools\n}\n$pool = New-Item IIS:\\AppPools\\CoolWebSite\n{% endcodeblock %}\n\nThis particular site needs to run as a particular user instead of the AppPoolUser or LocalSystem or anything like that. These will be passed in as a variable. We need to set the identity type to the confusing value of 3. This maps to using a specific user. The documentation on this is [near impossible to find](https://msdn.microsoft.com/en-us/library/ms689446(v=vs.90).aspx).\n\n{% codeblock lang:powershell %}\n$pool.processModel.identityType = 3\n$pool.processModel.userName = $deployUserName\n$pool.processModel.password = $deployUserPassword\n$pool | set-item\n{% endcodeblock %}\n\nOpa! We have an app pool. Next up a website. We'll follow the same model of deleting and adding. Really this delete block should be executed before adding the AppPool.\n\n{% codeblock lang:powershell %}\nif(Test-Path IIS:\\Sites\\CoolWebSite)\n{\necho \"Website exists - removing\"\n\nRemove-WebSite CoolWebSite\ngci IIS:\\Sites\n}\necho \"Creating new website\"\nNew-Website -name \"CoolWebSite\" -PhysicalPath $deploy_dir -ApplicationPool \"CoolWebSite\" -HostHeader $deployUrl\n{% endcodeblock %}\n\nThe final step for this site is to change the authentication to turn off anonymous and turn on windows authentication. This requires using a setter to set individual properties.\n\n{% codeblock lang:powershell %}\nSet-WebConfigurationProperty -filter /system.webServer/security/authentication/windowsAuthentication -name enabled -value true -PSPath IIS:\\Sites\\CoolWebSite\nSet-WebConfigurationProperty -filter /system.webServer/security/authentication/anonymousAuthentication -name enabled -value false -PSPath IIS:\\Sites\\CoolWebSite\n{% endcodeblock %}\n\nI'm not completely sure but I would bet that most other properties can also be set via these properties.\n\nWell that's all pretty cool. I think will still investigate PowerShell DSC because I really like the idea of specifying the state I want IIS to be in and have something else figure out how to get there. This is especially true for finicky things like setting authentication.\n","categories":[],"tags":[]},{"title":"Western Devs (and MVP) Found My Spirit Animal","authorId":"david_wesst","slug":"western-devs-found-my-spirit-animal","date":"2015-08-12 15:27:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/western-devs-found-my-spirit-animal/","link":"","permalink":"https://westerndevs.com/_/western-devs-found-my-spirit-animal/","excerpt":"What you're looking at is a lobster, in a ditch, drinking scotch, whose mouth is on fire. That is my spirit animal that was created on the Twitter-verse through the power of the Western Devs, which I would not be a part of had I not become an Microsoft Edge MVP with Microsoft.","raw":"---\nlayout: post\ntitle: \"Western Devs (and MVP) Found My Spirit Animal\"\ndate: 2015-08-12 11:27:05\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/08/Western-Devs-Found-My-Spirit-Animal/\ncomments: true\n---\n\nWhat you're looking at is a lobster, in a ditch, drinking scotch, whose mouth is on fire. That is my spirit animal that was created on the [Twitter-verse](https://twitter.com/CyrenaFriesen/status/627209542816829440) through the power of the [Western Devs](http://www.westerndevs.com), which I would not be a part of had I not become an [Microsoft Edge MVP](https://mvp.microsoft.com/en-us/PublicProfile/4032619?fullName=David%20%20Wesst) with Microsoft.\n\n<!--more-->\n\n![](http://blog.davidwesst.com/2015/08/Western-Devs-Found-My-Spirit-Animal/lobster-fire-mouth.png)\n\nThis amazing drawing represents a large number of \"hilarious\" stories that come from many conferences and adventures that have come through the MVP Program. Although I'm a bit late with the post, after being renewed this past April, I have had the chance to reflect on my five years and appreciate being a part of these communities for so long now.\n\n## Beyond the Career\nIt wasn't until last year where I realized how important these groups of people really were. I had experienced a family tragedy that really impacted me and my family, and it took me months to sort it out in my mind.\n\nThe first thing I was able to really grab onto to pull me out of the funk was my community within the MVP Program and my collegues from the Western Devs.\n\nI know it sounds sentimental, but it's true. I almost quit all of my community work and thought wiping the slate clean and going nuclear with my career was the only way to start bringing back some semblance of normal to my life. All of that changed with the 2014 MVP Conference and was able to talk face-to-face with people and start finding my passion for technology again. I really focused on that, and have thrived on it since.\n\n## The Point\nMy point is that I hope someone else reads this and realizes how important their profesional network is. Not just for the professional stuff, but as a reminder on why you \"keep on keepin' on\" as some might say.\n\nI don't know where things are going, but that's okay. What I do know is that I love technology and with communities like the Western Devs and the MVP Program, I will continue to grow and expand my horizons with technology.\n\nAnd _that_ is the whole point.\n\n### Special Thanks To...\nA special shout out to [D'Arcy Lussier](https://twitter.com/Darcy_Lussier) for pulling the discussion out of the Western Devs fog and turning it into reality, as he did the lobster. [Cyrena Friesen](https://twitter.com/CyrenaFriesen) for making it a reality on Twitter. And [Donald Belcham](https://twitter.com/dbelcham) for lighting the lobster on fire with scotch and reminding it that it'll forever be in a ditch.\n\n![](http://blog.davidwesst.com/2015/08/Western-Devs-Found-My-Spirit-Animal/twitter-source.png)\n","categories":[],"tags":[]},{"title":"Microservices and Isolation","authorId":"donald_belcham","slug":"Microservices-and-isolation","date":"2015-08-12 07:09:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/Microservices-and-isolation/","link":"","permalink":"https://westerndevs.com/_/Microservices-and-isolation/","excerpt":"In my first post I made reference to the idea of microservice isolation a number of times. I figured that this is as good of a topic as any to start with. The concept of isolation and boundaries is core to how you build your microservices. Let's leave boundaries for another post because it's a complicated and deep concept by itself.","raw":"---\nlayout: post\ntitle: \"Microservices and Isolation\"\ndate: 2015-08-11 21:09:40 -0600\ncomments: true\nauthorId: donald_belcham\noriginalurl: http://www.igloocoder.com/2881/microservices-and-isolation\n---\n\nIn my [first post][8] I made reference to the idea of microservice isolation a number of times. I figured that this is as good of a topic as any to start with. The concept of isolation and boundaries is core to how you build your microservices. Let's leave boundaries for another post because it's a complicated and deep concept by itself.\n\n<!--more-->\n\nIt doesn't matter who you listen to or read, microservice isolation is going to be in the content. Why are they consistently bringing it up? Isolation is at the heart of microservices. A microservice is meant to be architected, created, deployed, maintained and retired without affecting any other microservice. You can't do any, let alone all, of that without good isolation.\n\n##  Databases\n\nProbably the most common point made when talking about isolation is database sharing, or better stated, the idea that you should avoid it. Traditional monolithic application development usually sees that one large codebase working with one large database. Any area of the monolith can access any area of the database. Not only does the monolith's codebase usually end up looking like a plate of spaghetti, so does the monolithic database. I can't tell you the number of times that I've worked on brownfield codebases that have issues with data access, deadlocks being the most common. No matter how well factored a monolithic codebase is, the fact remains that the single database is an integration point for all the different moving pieces in that codebase.\n\nTo be able to release a microservice without affecting any other microservice we need to eliminate any integration that occurs at the database level. If you isolate the database so that only one microservice has access to it you've just said that the only thing that will be affected if the database changes is that one microservice. The testing surface area just shrunk for any of those changes. Another benefit is that have fewer pieces of code interacting with the database so you can, in theory, better control how and when that code does its thing. This makes it easier to write code that avoids deadlocks, row locks, and other performance killing or error inducing situations.\n\n![](http://farm1.staticflickr.com/311/20501646971_8ba8beb442_c.jpg)\n\nIf you listen to enough people talk about microservices for a long enough time you'll hear a common theme; 1 database per microservice. I'm going to disagree with the masses here and tell you something slightly different. You should have a **_minimum_ of 1 data _store_** for each of your microservices. The difference is subtle but it's important in my mind. There are times when you might want to store data in multiple different ways within one microservice. As an example you may be writing a Marketing  Campaign microservice. A RDBMS or noSQL database makes a lot of sense for storing the information about campaigns, content, targets, etc. But if you need to do statistical analysis of the campaign feedback (i.e. email bounces, unsubscribes, click-throughs, etc.) RDBMS and noSQL might not make the most sense. You might be better served with a data cube or some other type of data storage.\n\nIs it going to be normal to have multiple data stores for one microservice? Noâ€¦but you shouldn't be worried if it does happen as long as you stay true to one thing: the microservice owns the data stores and no other microservices can access them.\n\n##  Deployment Isolation\n\nOne of the primary goals of moving to a microservices architecture is that you're able to deploy changes to one microservice without affecting any others. Additionally you want to ensure that if a microservice starts to fail it's not going to bring down the other microservices around it. As such, this means that you're going to need to have each microservice deployed in complete isolation. If you're on a .NET stack you can't share AppPools between them. If they do changes to the permissions or availability of that AppPool could (or likely will) affect other microservices. My experience with Apache is quite limited, but I'm sure there are similar concerns there.\n\nOne of the big current talking points around microservices is [Docker][2]. Building compartmentalized and deployable contiguous packages seems to address this goal. The only current issue is that Docker builds a smaller fence around the technologies that you can choose when solving your problems.  Docker, currently, doesn't support Windows based applications. You can build your [.NET apps and run them in a Linux Docker][3] container, but that's as close as you getâ€¦which might not be close enough for some \"E\"nterprise-y places.\n\nAnother piece of the deployment puzzle is what is commonly referred to as 'lock-step' deployments. A lock-step deployment is one where deploying one microservice requires a mandatory deployment of a different one. Usually this happens because the two components (or microservices in this case) are tightly coupled. Usually that coupling is related to api signature changes. I'm going to do a whole blog post on this later in the series, but its suffice to say for now that if you are doing lock-step deployments you need to stop and solve that problem before anything else. If you aren't doing lock-step deployments you need to be vigilant to the signs of them and fight them off as they pop up.\n\nSomething that makes noticing lock-step deployments more difficult to notice, but is going to be mandatory in your deployment situation, is automation. Everything about your deployment process will need to be automated. If you're coming from a mentality, or reality, of deploying single, monolithic applications you're in for a big shock. You're no longer deploying one application. You're deploying many different microservices. There are a lot more deployable parts, and they're all individually deployable. My project only had 4 microservices and we found that manual deployment was worse than onerous. Everything from the installation of the microservice to the provisioning of the environments that the microservice will run in has to be automated. Ideally you're going to have automated verification tests as part of your deployment. The automation process gives you the ability to easily create frictionless lock-step deployments thoughâ€¦so you're going to have to be vigilant with your automation.\n\nI know that some of you are probably think \"That's all fine and good but I will always have to change APIs at some point which means that I need to deploy both the API and the consumers of the API together\". Well, you don't have toâ€¦which kind of leads toâ€¦\n\n##  Microservice <-> Microservice communication\n\nAt times there will be no way to avoid communication between two microservices. Sometimes this need to communicate is a sign that you have your bounded context wrong (I'll be going over bounded contexts in a future post). Sometimes the communication is warranted. Let's assume that the bounded contexts are correct for this discussion.\n\nTo keep microservices isolated we need to pay attention to how they communicate with each other. I'm going to do an entire blog post on this but because there can be so many things that come into play. It's safe to say that you need to pay attention to a couple of different key pieces. First, take the approach of having very strict standards for communication between microservices, and loose standards for the technology implementations within each microservice. If you're going to use REST and JSON (which seems to be the winning standard) for communication. Be strict about how those endpoints are created and exist. Also, don't be afraid to use messaging and pub/sub patterns to notify subscribing microservices about events that happen in publishing microservices.\n\n![](http://farm1.staticflickr.com/261/20307629680_1731d3ea66_c.jpg)\n\nThe second thing that you need to do, which is related to the first, is spend some time up front in deciding what your API versioning story is going to be. API versioning is going to play a big part in maintaining deployment isolation. Your solution will probably require more than simply 'adding a version header' to the communication. You're probably going to need infrastructure to route communications to different deployed versions of the APIs. Remember that each microservice is an isolated deployable so there is no reason that you couldn't have two or more instances (say v1 and v2) up and running at any one time. Consuming microservices can continue to make uninterrupted use of the v1 endpoints and as they have time/need they can migrate to v2.\n\n##  Source code\n\nNow that we've talked about the architecture of your microservices let's take a look at the source code. So far we've been talking about isolating the microservice data stores, the deployment strategy and cross microservice communication. In all of those areas we didn't come straight out and say it but each microservice is a separate application. How do you currently organize separate applications in your source control tool? Probably as separate repositories. Keep doing this. Every microservice gets its own repository. Full Stop. The result is that you're going to see an increase in the number of repositories that you have to manage. Instead of one (as you'd probably have with a monolith) repository you're going to have 1+N where N is the growing number of microservices that you're going to have.\n\nHow you organize the pieces of the puzzle within that application is going to depend on many things. It's going to depend on the components the microservice needs. The more \"things\" (service buses, web services, background processing, etc) then the more complicated the application's source code structure is likely to be. You might have four or five assemblies, a couple or executables and other deliverables in a single microservice. As long as you have the minimum required to deliver the required functionality then I think you'll be okay. More moving pieces can mean that you're doing too much in the microservice though. It could be creeping towards monolith territory. So carefully watch how each microservice evolves into it's final deliverable.\n\nAnother thing to consider is how you perform development isolation in your VCS. I'm not going to get into a branches vs feature-toggles discussion here, but you have to do something like that in your development practices. Things are going to move a lot faster when you're developing, enhancing and maintaining microservices. Being able to rapidly turn around bug fixes, feature additions or changes becomes a competitive advantage. You need to work within your VCS in a manner that supports this advantage.\n\n##  Continuous Integration\n\nFollowing in the steps of the source code is the continuous integration process. Because every microservices is an independent application you're going to need to have CI processes to support each and every microservice. This was one of the things that caught my project off guard. We didn't see it coming but we sure noticed it when it happened. As we created new microservices we needed to create all of the supporting CI infrastructure too. In our case we needed to create a build project for every deployment environment that we had to support. We didn't have this automated and we felt the pain. [TeamCity][5] helped us a lot but it still took time to setup everything. This was the first hint to us that we needed to automate everything.\n\n##  Teams\n\nThere is a lot of talk about sizing microservices (which, again, I'll cover in a future post). One of the things that continually seems to come up in that discussion is the size of teams. What is often lost is that teams developing microservices should be both independent and isolatedâ€¦just like the applications that they're building. [Conway's law][6] usually makes this a difficult prospect in a 'E'nterprise development environment. The change to developing isolated and compartmentalized microservices is going to require team reorganization to better align the teams and the products being produced.\n\nTeams need to be fully responsible for the entirety of the microservice that they're developing. They can't rely on \"the database guys to make those changes\" or \"infrastructure to create that VM for us\". All of those capabilities have to be enabled for and entrusted to the team doing the development. Of course these independent teams will need to communicate with each other, especially if there one of the teams is consuming the other's microservice. I think I'll have a future post talking about [Consumer Driven Contracts][7] and how that enhances the communication between the teams.\n\n##  Summary\n\nWhen talking about isolation and microservices many conversations tend to stop at the \"one microservice, one database\" level. There are so many other isolation concerns that will appear during the process of building, deploying and maintaining those microservices. The more I've researched and worked on microservices the more I've become of the opinion that there are a bunch of things related to microservices that we used to get away not doing on monolithic projects but that we absolutely can't ignore anymore. You can't put off figuring out an API versioning scheme. You're going to need it sooner than you think. You can't \"figure out your branching strategy when the time comes\" because you're going to be working on v2 much sooner than you think.\n\nIsolation is going to save you a lot of headaches. In the case microservices I'd probably consider leaning towards what feels like 'too much' isolation when making decisions rather than taking what likely will be the easier way out of the problem at hand.\n\n[1]: https://farm1.staticflickr.com/311/20501646971_8ba8beb442_c.jpg\n[2]: https://www.docker.com\n[3]: http://www.hanselman.com/blog/PublishingAnASPNET5AppToDockerOnLinuxWithVisualStudio.aspx\n[4]: https://farm1.staticflickr.com/261/20307629680_1731d3ea66_c.jpg\n[5]: http://www.jetbrains.com/teamcity/\n[6]: http://www.thoughtworks.com/insights/blog/demystifying-conways-law\n[7]: http://martinfowler.com/articles/consumerDrivenContracts.html\n[8]: http://www.westerndevs.com/microservices-a-gentle-introduction\n","categories":[],"tags":[]},{"title":"BDD vs TDD","authorId":"amir_barylko","slug":"bdd-vs-tdd","date":"2015-08-11 11:09:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/bdd-vs-tdd/","link":"","permalink":"https://westerndevs.com/_/bdd-vs-tdd/","excerpt":"Testing is a very important part of software development, but should we do black box testing or test every line of code? How can we find balance between writing the right thing (BDD) and writing things right (TDD)?","raw":"---\nlayout: post\ntitle: \"BDD vs TDD\"\ndate: 2015-08-11 02:09:40 -0500\ncomments: true\nauthorId: amir_barylko\noriginalurl: http://orthocoders.com/blog/2015/07/12/bdd-vs-tdd/\n---\n\nTesting is a very important part of software development, but should we do black box testing or test every line of code?\n\nHow can we find balance between writing the right thing (BDD) and writing things right (TDD)?\n\n<!--more-->\n\n## A perfect world\n\n_Test Driven Development_ (TDD) makes you think. Itâ€™s not only about the _test first approach_ but also about following some [rules](http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd).\n\nTDD, however, is not enough to ensure that you are building the features that address the clientâ€™s expectations.\n\n_Behaviour Driven Development_ (BDD) makes you think about the whole feature. Owners/champions participation helps you write the acceptance criteria and drive the implementation until the feature is implemented.\n\nWhen you combine _BDD_ and _TDD_ you get the best of both worlds: you are able to match your clientsâ€™ expectations with high quality code and write the minimum amount of code possible.\n\n![](http://orthocoders.com/images/bdd_cycle.jpg)\n\n\nClearly both BDD and TDD are useful, however, is not always easy to implement both.\n\nIdeally, to find balance you need to make sure you are well versed in both BDD and TDD, so you can be objective and, in the words of a great wizard, choose [between what is easy and what is right](http://www.goodreads.com/quotes/701025-dark-times-lie-ahead-of-us-and-there-will-be).\nThat means we need to overcome two major drawbacks (to start): the __learning curve__ and __poor tooling__.\n\n### Learning curve\n\nAs with any new methodology both TDD and BDD share a big drawback: _the learning curve_. Not only you need to learn how to implement each of them but also how to combine them.\n\nThis part is __crucial__ to find balance between these methodologies. If your team doesn't feel comfortable doing either and see it as a _drag_ they will resist using them or will implement only the bit they can do quickly and find an excuse not to use the other methodology.\n\nOnline you can find multiple examples of how to use both BDD and TDD, but they are often simplified and don't deal with more complex and frequent problems such as populating databases, generating testing data, making sure the acceptance cases run on multiple browsers, brittle tests, etc.\n\nDon't get discouraged because you don't get results as quickly as you thought you would. Implementing BDD and TDD is going to take time, but it is very, very, very important (can't stress it enough) to make sure that writing tests is as natural for your team as _using the turn signal when you drive_. You just do it, don't think if it is worth it or not, it is automatic.\n\nMany of these problems in your path, find you willâ€¦ mitigate them you shall...\n\nWhatâ€™s the secret?  Training, practice, exercise and...then more practice.  There are great [books](https://pragprog.com/book/hwcuc/the-cucumber-book) to help you follow the process and _tooling_ is essential.\n\n### The right tool for the job\n\nIn my experience, one of the main reasons developers get discouraged to write tests is _poor tooling_.\n\nI think we are past finding testing frameworks and runners, still we have areas that are not always covered.\n\n#### Testing hooks & metadata\n\nMost frameworks you may choose to work with will have some kind of _hooks_. Before tests, after tests, before all tests, etc.\n\nBeing able to _tag_ tests and identify which ones are using the database, is another great feature to have.\n\n[RSpec](http://rspec.info/) is a great tool to take a peek at what kind of features could be useful for your current need. Probably there are similar libraries with the language you work with that implement the same or similar ideas.\n\nAnd if not, why not collaborate with the community and build some?\n\n#### Fake domain data\n\nIt is very common to need dummy data to support your scenarios. They can be useful for acceptance testing and for unit testing as well.\n\nHaving an easy way to generate fake data is really important. It will simplify how you write your tests and also give you the feeling that you are talking in domain terms, making much easier to understand what the test is about and implement the code.\n\nLibraries like [Factory Girl](https://github.com/thoughtbot/factory_girl),  [FsCheck](https://fscheck.github.io/FsCheck/), [AutoFixture](https://github.com/AutoFixture), [GenFu](https://github.com/MisterJames/GenFu) are great examples to know what to look for.\n\nNot only you can generate good customers, bad customers, etc., but you can also generate multiple cases to test and, even further, why not look into some automatic generation.\n\n#### Databases\n\nWorking with databases should be simple and straightforward: loading data before the tests, cleaning after, restoring the schema (if you have one) to a certain point, and populating.\n\nCombining database manipulation with hooks and data generation will give you lots of power to set up scenarios in an easy, painless way.\n\n## The importance of *NOT* being *brittle*\n\nLetâ€™s imagine we have mastered the _tools_ and _the learning curve_. Now we have a new foe that threatens our Ninja skills and may bring our confidence to the floor: __Brittle tests__.\n\nTests have to be maintained and kept running _green_ all the time, otherwise defeats the purpose.\n\nYou want your tests to be (somehow) _resilient_ to changes in the code. This is a more advanced problem and here are a few tips to check before we can talk about balance.\n\n### Isolation\n\nAvoid assuming a particular state that you don't control. Tests may run now but may not run in the future.\n\nAny state needed to make the test pass has to be set up before the test runs and torn down after the test finishes running.\n\n### Implementation coupling\n\nTests should be about _inputs_ and _outputs_, not about how they are implemented. Testing a method that queries the database by mocking the way the query is implemented is __dangerous__.  Similarly, testing with a very small set of data may lead you to the wrong conclusion.\n\nYou will be better off focusing on [_Properties Testing_](http://fsharpforfunandprofit.com/posts/property-based-testing). Libraries like [FsCheck](https://fscheck.github.io/FsCheck/) and [Rantly](https://github.com/abargnesi/rantly) can help you enforce pre-conditions and post-conditions and inspire you to look for other ways to make your testing robust.\n\n\n### Mocking the mock\n\nTests should work for you, not the other way around. If you find yourself creating more code in order to make your current code _testable_ that deserves some attention.\n\nWhenever you postpone testing complexity and replace it with a mock remember that the same complexity will have to be tested later.\n\nCreating interfaces to improve reusability is a great idea, however, make sure you need it.\nFor example, abstracting the [repository](/repository-nightmares/) pattern may not be always a good idea.\n\n### Tools galore\n\nThough there's a plethora of tools to choose from, they will not always come in the exact shape and color you need them.\n\nDon't get discouraged! Learn from it and build your own.\n\nFor example, writing domain specific languages to help you test will ease the pain of repeating common scenarios again and again. The investment will pay off.\n\n## Finding balance\n\nNow our team has overcome the hardships of testing and is quite confident implementing both _TDD_ and _BDD_. They can do them with their eyes closed while dancing the Macarena.\n\nSo what's the right balance? For me it is __confidence__.\n\n    Do I need to write tests for every domain model, every single scenario, every api call?\n\nWell...it is up to you.\n\nI lean on the __acceptance__ side often to ensure everything works as expected.\n\nIf you have a senior team then perhaps you can relax a bit more and choose which unit tests should be implemented to find a balance between BDD and TDD that you feel comfortable with.\n\nIf you don't see crystal clear that writing one test will imply that the other part is working, then don't skip anything.\n\nEven for acceptance test if you find features that are really similar to others and strongly believe that implementing test scenarios wonâ€™t be necessary because your unit/integration test already covers that, go ahead and skip them.\n\nIt is a fine line, but the key is __confidence__.\n","categories":[],"tags":[]},{"title":"Docker on Windows 10 Problems","authorId":"david_wesst","slug":"docker-on-windows-10-problems","date":"2015-08-10 13:20:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/docker-on-windows-10-problems/","link":"","permalink":"https://westerndevs.com/_/docker-on-windows-10-problems/","excerpt":"UPDATE -- Solution Found Another Western Dev that goes by the name of Dave White found a solution and I confirmed that it works. The solution entails using a test build of the new Docker tool suite, so use at your own risk, but it does work! You can find the solution from Dave White here, and learn more about the man himself here.","raw":"---\nlayout: post\ntitle: \"Docker on Windows 10 Problems\"\ndate: 2015-08-10 09:20:40\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/08/Docker-on-Windows-10-Problems/\n---\n\n## UPDATE -- Solution Found\nAnother Western Dev that goes by the name of Dave White found a solution and I confirmed that it works. The solution entails using a test build of the new Docker tool suite, so use at your own risk, but it does work!\n\nYou can find the solution from Dave White [here](http://www.westerndevs.com/getting-docker-running-on-windows-10/), and learn more about the man himself [here](http://www.westerndevs.com/bios/dave_white/).\n\n<!--more-->\n\n---\n![](http://blog.davidwesst.com/2015/08/Docker-on-Windows-10-Problems/docker-logo.png)\n\nI've hit some issues getting docker running on Windows 10. Turns out it's an issue with VirtualBox and it's being worked on, but I figured I'd share the details here just in case you're having the same issue.\n\nYou get the full scoop [here on GitHub](https://github.com/boot2docker/boot2docker/issues/1015) and [here on the Virtualbox ticket site](https://www.virtualbox.org/ticket/14040).\n\nFor those too busy to read the links, here's the executive summary.\n\n### The Symptoms\nYou install [boot2docker](http://boot2docker.io) on a Windows 10 machine, and you can't seem to run the intro command `docker run hello-world` as instructed by the installer.\n\nWhen you run `boot2docker init -v` you get an error `Failed to create host-only network interface`.\n\nYou have VirtualBox 5.0.x installed and Docker gives you problems like those described above.\n\n### The Solution\nSo far there isn't an official fix, but there is [test build](https://github.com/boot2docker/boot2docker/issues/1015#issuecomment-127313908) as described on the boot2docker issue linked above. Personally, I haven't given it a shot, but it looks like things are on their way to be mended.\n\nHere's hoping that it gets fixed so I can finally upgrade my desktop to Windows 10, and get docker running on my laptop.\n\n*Thanks for Playing. ~ DW*\n","categories":[],"tags":[]},{"title":"Running Tomcat Apps on Docker through Eclipse","authorId":"david_wesst","slug":"running-tomcat-apps-on-docker-through-eclipse","date":"2015-08-07 16:55:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/running-tomcat-apps-on-docker-through-eclipse/","link":"","permalink":"https://westerndevs.com/_/running-tomcat-apps-on-docker-through-eclipse/","excerpt":"If you didn't already know, Docker is pretty cool. Not sure what it is? My fellow Western Dev Kyle Baley explains it really well and provides some great cases about why Docker is fantastic. This post is an example of Docker being fantastic and should be considered a warm-up to Docker.","raw":"---\nlayout: post\ntitle: \"Running Tomcat Apps on Docker through Eclipse\"\ndate: 2015-08-07 12:55:40\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/08/Running-Tomcat-Apps-on-Docker-through-Eclipse/\n---\nIf you didn't already know, [Docker](https://www.docker.com) is pretty cool. Not sure what it is? My fellow Western Dev [Kyle Baley](http://www.westerndevs.com/docker-is-coming-whether-you-like-it-or-not/) explains it really well and provides some great cases about why Docker is fantastic.\n\nThis post is an example of Docker being fantastic and should be considered a warm-up to Docker.\n\n<!--more-->\n\nAlthough I'm talking Tomcat and Eclipse, this idea can be used with pretty much any combination of technologies available through Docker and your development environment of choice.\n\n## Context\nWhen I dive into the code, I don't want to have to worry about running some version of Tomcat on my development machine.\n\nDocker can solve that.\n\nI also don't like obscuring part of my solution through _IDE Magic_, such that when I run my Tomcat application I want to know how it's running. In fact, not only do I want to know _how_ it runs, but I want the steps to run the application to be automated so I don't have to think about it. I know Eclipse can do this natively, but because I'm not looking to run Tomcat on my machine, it adds some challenges.\n\nHere's what I did.\n\n## Solution\nThe solution is a simple one, but it works consistently. Plus, we'll even integrate it with Eclipse so people that don't like leaving their IDE can continue to code in in their comfort zone.\n\nI am assuming that you...\n+ ...are running Windows\n+ ...have a Maven web app project\n+ ...have setup Boot2Docker\n+ ...have a linux-flavoured shell (like Git Bash)\n\nMaven gives us a standard structure for our project. More specifically, it packages everything up into the ```/target``` directory so that we can map the volume to our docker image.\n\nNow, let's make this run every time.\n\n### The Start Script\nThe first script I created just packages things up, makes sure boot2docker is running, and runs docker.\n\nBeing that I run the script from the project root, it looks like this:\n\n{% codeblock lang:bash %}\n#!/bin/bash\n\n# package the WAR file\nmvn package\n\n# Make sure docker instance is running\nboot2docker up\n\n# Run the application\ndocker run --rm -p 8080:8080 -v //./target/mywebapp:/usr/local/tomcat/webapps/mywebapp tomcat:6.0\n{% endcodeblock %}\n\n### The Stop Script\nThanks to the power of Bingoogle, I was able to find [this](https://coderwall.com/p/ewk0mq/stop-remove-all-docker-containers).\n\n{% codeblock lang:bash %}\ndocker stop $(docker ps -a -q)\n{% endcodeblock %}\n\nAnd there are my scripts to start and stop my application. The main reason for creating these is to make sure that anyone pulling down the solution can be productive. Even those that like staying inside of Eclipse all the time.\n\n### Integrating with Eclipse\nBecause we have these scripts, we can setup a couple of \"External Tools\" to start and stop at the push of a button.\n\n![](http://blog.davidwesst.com/2015/08/Running-Tomcat-Apps-on-Docker-through-Eclipse/1-externaltoolsbutton.png)\n\nTo set it up, click the button or the drop down and select \"External Tools Configuration\", then setup a couple of programs where you call sh.exe (provided with Git Bash) and our new scripts as the arguments.\n\n![](http://blog.davidwesst.com/2015/08/Running-Tomcat-Apps-on-Docker-through-Eclipse/2-configwindow.png)\n\nNow everybody can start and stop at their leisure. Plus, if we need to add more complexity to starting and stopping, we can just extend the scripts.\n\n### Alternatively...Make a Dockerfile\nThat being said, we could also customize our docker image to include files right on the image rather than mapping volumes. Although this doesn't add any value to this example, it might later on if we want to start customizing our Tomcat server to include multiple files or other libraries that our application will need.\n\n**FUN FACT:** Copying the files over to the image means that you can't do any debugging on them in your environment right away, as opposed to if you were to map the files locally. Not a big deal in some cases, but I like to keep all the doors open for debugging.\n\nHere's an example, but you can get the full scoop on dockerfiles [here](https://docs.docker.com/articles/dockerfile_best-practices/):\n\n{% codeblock lang:bash %}\nFROM tomcat:6.0\nMAINTAINER David Wesst <questions@davidwesst.com>\n\n# add our WAR files\nADD target/\\*.war /usr/local/tomcat/webapps/\n{% endcodeblock %}\n\nThen we need to update our ```start-docker.sh``` file from above to build the new image so we can run it.\n\n{% codeblock lang:bash %}\n#!/bin/bash\n\n# package the WAR file\nmvn package\n\n# Make sure docker instance is running\nboot2docker up\n\n# Build a new docker image\ndocker build -t davidwesst/tomcatsample:dev .\n\n# Run the application\ndocker run --rm -p 8080:8080 umanitoba/tomcatsample:dev\n{% endcodeblock %}\n\n### The Point\nThis is a pretty straightforward example on how to use docker, and how to tie it into any processes or IDEs that you may have. I know that [Eclipse Mars has functionality for docker](http://www.eclipse.org/community/eclipse_newsletter/2015/june/article3.php), but sometimes we don't get to use the shiny new toy and need to stick with the old realiable ones. This solution works with Eclipse Luna, and below from what I can gather, so people that are stuck using older IDEs can still enjoy Docker.\n\n...but you should really get comfortable with the command line and [Learn to be IDE Free (Shameless Self Promotion)](http://blog.davidwesst.com/talks/).\n\n--\nThanks for Playing\n~ DW\n","categories":[],"tags":[]},{"title":"Change Management for the Evolving World","authorId":"simon_timms","slug":"change-management-for-the-evolving-world","date":"2015-08-07 04:51:52+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/change-management-for-the-evolving-world/","link":"","permalink":"https://westerndevs.com/_/change-management-for-the-evolving-world/","excerpt":"I've had this blog post percolating for a while. When I started it I was working for a large company that has some internal projects I was involved with deploying. I came to the project with a background in evolving projects rapidly. It has been my experience that people are not upset that software doesn't work so much as they are upset that when they discover a bug that it isn't fixed promptly.","raw":"---\nlayout: post\ntitle:  Change Management for the Evolving World\ndate: 2015-08-06T18:51:52-06:00\ncategories:\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/08/07/change_mangement_evolving/\n---\n\nI've had this blog post percolating for a while. When I started it I was working for a large company that has some internal projects I was involved with deploying. I came to the project with a background in evolving projects rapidly. It has been my experience that people are not upset that software doesn't work so much as they are upset that when they discover a bug that it isn't fixed promptly.\n\n<!--more-->\n\n```\nVelocity is the antidote to toxic bugs\n```\n\nUnfortunately the company had not kept up with the evolution of thinking in software deployment. Any change that needed to go in had to pass through the dreaded change management board. This slowed down deployments like crazy. Let's say that somebody discovered a bug on a Tuesday morning. I might have the fix figured out by noon. Well that's a problem because noon is the cut off for the change management meeting which is held at 5pm local time. So we've missed the change management for this week, but we're on the agenda for next week.\n\nDay 7.\n\nThe change management meeting comes around again and a concern is raised that the change might have a knock on effect on another system. Unfortunately the team responsible for that system isn't on this call so this change is shelved until that other team can be contacted. We'll put this change back on the agenda for next week.\n\nDay 14.\n\nChange management meeting number 2. The people responsible for the other system are present and confirm that their system doesn't depend on the altered functionality. We can go ahead with the change! Changes have to go in on Fridays after noon, giving the weekend to solve any problems that arise. This particular change can only be done by Liz and Liz has to be at the dentist on Friday. So we'll miss that window and have to deploy during the next window.\n\nDay 24.\n\nDeployment day has arrived! Liz runs the deployment and our changes are live. The minor problem has been solved in only 24 days. Of course during that time the user has been hounding the team on a daily basis, getting angrier and angrier. Everybody is pissed off and the business has suffered.\n\n## Change management is a difficult problem.\n\nThere is a great schism between development and operations. The cause of this is that the teams have seemingly contradictory goals. Development is about changing existing applications to address a bug or a changing business need. For the development team to be successful they must show that they are improving the product. Everything about development is geared towards this. Think of the metrics we might use around development: KLoCs, issues resolved, time to resolve an issue, and so forth. All of these are about improving the rate of change. Developers thrive on rapid change.\n\nOperations, on the other hand, their goal is to keep everything running properly. Mail server need to keep sending mail, web server need to keep serving web pages and domain controllers need to keep doing whatever it is that they do, control domains one would assume. Every time there is a change to this system then there is a good chance that something will break. This is why, if you wander into a server room, you'll likely see a few machines that look like they were hand built by Grace Hopper herself. Most operations people see any change as a potential disturbance to the carefully crafted system they have built up. This is one of the reasons that change management boards and change management meetings have been created. They are perceived as gatekeepers around the system.\n\nPersonally I've never seen a change management board or meeting that really added any value to the process. Usually it slowed down deploying changes without really improving the testing around whether the changes would have a deleterious effect.\n\nThe truth of the matter is that figuring out what a change will do is very difficult. Complex systems are near impossible to model and predict. There is a whole bunch of research on the concept but it is usally easier to just link to\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/n-mpifTiPV4\" frameborder=\"0\" allowfullscreen></iframe>\n\nLet's dig a bit deeper into the two sides of this issue.\n\n## Why do we even want rapid change?\n\nThere are a number of really good reasons we'd like to be able to change our applications quickly\n\n1. Every minute spent with undesirable behaviour is costing the business money\n2. If security holes are uncovered then our chances of being attacked increase the longer it takes us to get a fix deployed\n3. Making smaller changes mean that when something does go wrong the list of potential culprits is quite short\n\nOn the other hand we have pushing back\n\n1. We don't know the knock on effect of this change\n2. The problem is costing the business money but is it costing more money that the business being shut down totally due to a big bug?\n\nSecretly we also have pushing back the fact that the ops team are really busy keeping things going. If a deployment takes a bunch of their time then they will be very likely to try to avoid doing it. I sure can't blame them, often \"I'm too busy\" is not an acceptable excuse in corporate culture so it is replaced with bogus technical restrictions or even readings of the corporate policies that preclude rapid deployments.\n\nIf we look at the push back there is a clear theme: deployments are not well automated and we don't have good trust that things won't break during a deployment.\n\n## How can we remove the fear?\n\nThe fear that ops people have of moving quickly is well founded. It is these brave souls who are up at oh-my-goodness O'clock fixing issues in production. So the fear of deploying needs to be removed from the process. I'm sure there are all sorts of solutions based in hypnosis but to me the real solution is\n\n```\nIf something hurts do it more often\n```\nInstead of deploying once a month or once every two weeks let's deploy every single day, perhaps even more than once a day. After every deploy everybody should sit down and identify one part of the process that was painful. Take that one painful part and fix it for the next deploy. Repeat this process, involving everybody, after each deploy. Eventually you'll pay off the difficult parts and all of a sudden you can deploy more easily and more often. It doesn't take many successes before everybody becomes a believer.\n\n## What do the devs need to do?\nAs a developer I find myself falling into the trap of believing that it is the ops people who need to change. This is only half the story. Developers need to become much more involved in the running of the system. This can take many forms:\n\n- adding better instrumentation and providing understanding of what this instrumentation does\n- being available and involved during deploys\n- assisting with developing tooling\n- understanding the sorts of problems that are faced in operations\n\nPerhaps the most important thing for developers to do is to be patient. Change on this sort of a scale takes time and there is no magic way to just make everything perfect right away.\n\nI firmly believe that sort of change management we talked about at the start of the article is more theatre than practical. Sometimes it is desirable to show management that proper care and attention is being paid when making changes. Having really good test environments and automated tests is a whole lot better than the normal theatre, though.\n\nIt is time to remove the drama from deployments and close the Globe Theatre of deployments.\n","categories":[],"tags":[]},{"title":"Microservices: A Gentle Introduction","authorId":"donald_belcham","slug":"microservices-a-gentle-introduction","date":"2015-08-05 18:40:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microservices-a-gentle-introduction/","link":"","permalink":"https://westerndevs.com/_/microservices-a-gentle-introduction/","excerpt":"This past winter I started working on a project that was being architected with a mind towards using microservices. Prior to this I'd only seen the term 'microservices' floating around in the ether and really hadn't paid much attention to it. I wanted to share what we did and what I learned through the process and my subsequent research. That experience and research has led me to one belief: the microservice topic is massive. This post is going to be a kick-off to a series that will cover that material. With that, let's dig in.","raw":"---\nlayout: post\ntitle:  \"Microservices: A Gentle Introduction\"\ndate: 2015-08-05T16:40:50+02:00\ncategories:\ncomments: true\nauthorId: donald_belcham\noriginalurl: http://www.igloocoder.com/2849/microservices-a-gentle-introduction\n---\n\nThis past winter I started working on a project that was being architected with a mind towards using microservices. Prior to this I'd only seen the term 'microservices' floating around in the ether and really hadn't paid much attention to it. I wanted to share what we did and what I learned through the process and my subsequent research. That experience and research has led me to one belief: the microservice topic is massive. This post is going to be a kick-off to a series that will cover that material. With that, let's dig in.\n\n<!--more-->\n\n## What are microservices?\n\nSometimes its easier to start by describing what something isn't. Microservices are not monolithic applications. That's to say our traditional application was one unit of code. It is:\n\n* developed together (even split as multiple assemblies, it was developed as one codebase)\n* deployed together (the vague promise of \"just deploy one dll from the application by itself\" has never been practical or practiced)\n* goes through the full application lifecycle as one contiguous unit (it is built as one, maintained as one and dies as one application)\n\nSo a microservice is none of these things. In fact, to start the definition of what a microservice is you'd be safe in saying that, at a high level, microservices are the opposite of all these things. Microservices are a bunch of small applications that represent the functionality we once considered as one application. They are:\n\n* developed in isolation (contracts between microservices are established but this is the extent of their knowledge of each other)\n* deployed in isolation (each microservices is it's own encapsulated application)\n* lives and dies no relationship to any other microservice (each microservice's application lifecycle is managed independently)\n\nThere's a pretty common comparison between microservices and SOA. If you missed the whole SOA bandwagon then 1) you're younger than me, 2) I'm envious of you, and 3) it had its merits. Some people will say \"microservices are just SOA done right\". I'm not sure that I fully agree with that statement. I don't know that I disagree with it either. As an introduction to microservices you should understand that there are many parallels between them and SOA. Applications that aren't monolithic tend to be distributed. Both SOA and microservice architectures are decidedly distributed. Probably the biggest difference between SOA and microservices is that SOA was quickly absconded by the big software manufacturing companies. According to them, doing SOA right meant using an Enterprise Service Bus (most likely an Enterprise Service Broker being marketed and pitched as a service bus)â€¦and preferably using their ESB, not a competitors. Gradually those SOA implementations became ESB implementations driven by software vendors and licensing and the architectural drive of SOA was lost. Instead of a distributed system business logic was moved out of application codebases and into centralized ESB implementations. If you've ever had to debug a system that had business logic in BizTalk then you know what the result of vendors taking over SOA was.\n\nThus far (and microservices are older as an architecture than you probably think) the microservice architecture hasn't been turned into a piece of software that companies are flogging licenses for. It's questionable whether the hype (justified or not) around Docker as a core component of a microservices implementation is just the start of microservices heading the same direction as SOA. It might be, it might not be.\n\nEach microservice is a well encapsulated, independent and isolated application. If someone pitches microservices at you and there's a shared database, or two microservices must be deployed in unison, or changes to one microservice forces changes on another then they're pitching you something that's not a true microservice. These concepts are going to lead to a bunch of the items I will discuss in future blog posts.\n\n## What does all of this mean?\n\nGood question. The short story, from what I've been able to collate, is that microservices promise a bunch of things that we all want in our applications; ease of deployment, isolation, technology freedom, strong encapsulation, extensibility and more. The thing that people don't always immediately see is the pain that they can bring. Most organizations are used to building and managing large monolithic codebases. They struggle in many different ways with these applications, but the way that they work is driven by the monolithic application. As we covered earlier, microservices are nothing like monoliths. The processes, organizational structures and even technologies used for monolithic applications are not going to work when you make the move to microservices.\n\nThink about this: how does your team/organization deal with deploying your monolithic application to a development environment? What about to Dev and Test? Dev, Test and UAT? Dev, Test, UAT and Prod? I'm sure there's some friction there. I'm sure each one of those environment deployments takes time, possibly is done manually, and requires verification before its declared \"ready\". Now thinking about all the time you spend working through those environment releases and imagine doing it for 5 applications. Now 10. What would happen if you used the same processes but had 25 applications to deploy? This is what your world will be like with microservices.\n\nPart of what I hope to convey in this series of blog posts is a sense of what pain you're going to see. There's going to be technical pain as well as organizational pain. And, as I know from experience over the last 6 months, there is going to be a learning curve. To help you with that learning curve I've created a list of resources as part of a github repository ([https://github.com/dbelcham/microservice-material] [1]) that you can go through. Its not complete. It's not finished. If you find something you think should be added please submit a pull request and I'll look at getting it added. As of the moment I'm writing this it's probably weakest in the Tooling, Techniques and Platforms section. I'm hoping to give that some love soon as it will be important to my writing.\n\n## In closing\n\nMicroservices are a topic that is gaining traction and there's a lot of information that needs to be disseminated. I don't think I'm going to post any earth shattering new concepts on the topic but I want to get one location where I can put my thoughts and, hopefully, others can come for a cohesive read on the topic. I am, by no means, an expert on the topic. Feel free to disagree with what I say. Ask questions, engage in thoughtful conversation and tell me if you think there's an area that needs to be covered in more depth.\n\n[1]: https://github.com/dbelcham/microservice-material\n","categories":[],"tags":[]},{"title":"The Repository Pattern","slug":"podcast-the-repository-pattern","date":"2015-08-04 19:55:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-the-repository-pattern/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-the-repository-pattern/","excerpt":"Do we really need all these repositories?","raw":"---\nlayout: podcast\ntitle:  \"The Repository Pattern\"\ndate: 2015-08-04T11:55:26-04:00\nrecorded: 2015-07-24\ncategories: podcasts\nexcerpt: \"Do we really need all these repositories?\"\ncomments: true\npodcast:\n    filename: \"RepositoryPattern.mp3\"\n    length: \"48:07\"\n    filesize: 57742198\n    libsynId: 5292005\n    anchorFmId: The-Repository-Pattern-evqdj5\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - simon_timms\n    - dylan_smith\n    - donald_belcham\nlinks:\n    - Repository nightmares|http://www.westerndevs.com/repository-nightmares/\n    - Repository pattern|http://martinfowler.com/eaaCatalog/repository.html\n    - Double dispatch|https://en.wikipedia.org/wiki/Double_dispatch\n    - Interface Segregation Principle|https://en.wikipedia.org/wiki/Interface_segregation_principle\n    - Domain Driven Design Aggregate|http://martinfowler.com/bliki/DDD_Aggregate.html\n    - The N+1 problem|http://stackoverflow.com/questions/97197/what-is-the-n1-selects-issue\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\n---\n### Synopsis\n\n* What is the Repository Pattern?\n* Where should the business logic go?\n* Explosion of methods on repositories and the Interface Segregation Principle\n* Testing repositories: integration vs. unit\n* Transactions: Are they a database or business concern?\n* Eager vs. lazy loading: The N+1 problem\n* Extension methods and query objects as an alternative\n* The misuse of reuse: \"Hey, this looks like it does what I need it to do\"\n* The Double Dispatch Pattern\n* Managing transactions and eventual consistency in distributed systems\n* Eventual inconsistency in the real world\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Docker Is Coming Whether You Like It or Not","authorId":"kyle_baley","slug":"docker-is-coming-whether-you-like-it-or-not","date":"2015-08-04 16:31:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/docker-is-coming-whether-you-like-it-or-not/","link":"","permalink":"https://westerndevs.com/_/docker-is-coming-whether-you-like-it-or-not/","excerpt":"I'm excited about Docker. Unnaturally excited, one might say. So much so that I'll be talking about it at MeasureUp this September. In the meantime, I have to temper my enthusiasm for the time being because Docker is still a Linux-only concern. Yes, you can run Docker containers on Windows but only Linux-based ones. So no SQL Server and no IIS.","raw":"---\nlayout: post\ntitle:  \"Docker Is Coming Whether You Like It or Not\"\ndate: 2015-08-04T08:31:10-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\n---\n\nI'm excited about Docker. Unnaturally excited, one might say. So much so that I'll be talking about it at [MeasureUp](http://measureup.io/) this September.\n\nIn the meantime, I have to temper my enthusiasm for the time being because Docker is still a Linux-only concern. Yes, you can run Docker containers on Windows but only Linux-based ones. So no SQL Server and no IIS.\n\n<!--more-->\n\nBut you can't stop a hillbilly from dreaming of a world of containers. So with a grand assumption that you know what Docker is roughly all about, here's what this coder of the earth meditates on, Docker-wise, before going to sleep.\n\n### Microservices\n\nMicroservices are a hot topic these days. We've [talked about them](http://www.westerndevs.com/podcasts/podcast-microservices/) at Western Devs already and Donald Belcham has a good and active [list of resources](https://github.com/dbelcham/microservice-material). Docker is an eerily natural fit for microservices so much so that one might think it was created specifically to facilitate the architecture. You can package your entire service into a container and deploy it as a single package to your production server.\n\nI don't think you can understate the importance of a technology like Docker when it comes to microservices. Containers are so lightweight and portable, you just naturally gravitate to the pattern through normal use of containers. I can see a time in the near future where it's almost negligent **not** to use microservices with Docker. At least in the Windows world. This might already be the case in Linux.\n\n### Works On My Machine\n\nAh, the crutch of the developer and the bane of DevOps. You set it up so nicely on your machine, with all your undocumented config entries and custom permissions and the fladnoogles and the whaztrubbets and everything else required to get everything perfectly balanced. Then you get your first bug from QA: can't log in.\n\nBut what if you could test your deployment on the _exact same image_ that you deployed to? Furthermore, what if, when a bug came in that you can't reproduce locally, you could download the _exact container_ where it was occurring? NO MORE EXCUSES, THAT'S WHAT!\n\n### Continuous Integration Build Agents\n\nOn one project, we had a suite of [UI tests](http://www.westerndevs.com/on-ui-testing/) which took nigh-on eight hours in TeamCity. We optimized as much as we could and got it down to just over hours. Parallelizing them would have been a lot of effort to set up the appropriate shared resources and configurations. Eventually, we set up multiple virtual machines so that the entire parallel test run could finish in about an hour and a half. But the total test time of all those runs sequentially is now almost ten hours and my working theory is that it's due to the overhead of the VMs on the host machine.\n\n### Offloading services\n\nWhat I mean here is kind of like microservices applied to the various components of your application. You have an application that needs a database, a queue, a search components, and a cache. You could spin up a VM and install all those pieces. Or you could run a Postgres container, a RabbitMQ container, an ElasticSearch container, and a Redis container and leave your machine solely for the code.\n\nWhen it comes right down to it, Docker containers are basically practical virtual machines. I've used VMs for many years. When I first started out, it was VMWare WorkStation on Windows. People that are smarter than me (including those that would notice that I should have said, \"smarter than *I*\") told me to use them. \"One VM per client\" they would say. To the point that their host was limited to checking email and Twitter clients.\n\nI tried that and didn't like it. I didn't like waiting for the boot process on both the host *and* each client and I didn't like not taking full advantage of my host's hardware on the specific client I happened to be working on at that moment.\n\nBut containers are lightweight. Purposefully so. _Delightfully_ so. As I speak, the overworked USB drive that houses my VMs is down to 20 GB of free space. I cringe at the idea of having to spin up another one. But the idea of a dozen containers I can pick and choose from, all under a GB? That's a development environment I can get behind.\n\n---\n\nAlas, this is mostly a future world I'm discussing. Docker is Linux only and I'm in the .NET space. So I have to wait until either: a) ASP.NET is ported over to Linux, or b) Docker supports Windows-based containers. And it's a big part of my excitement that **BOTH** of those conditions will likely be met within a year.\n\nIn the meantime, who's waiting? Earlier, I mentioned Postgres, Redis, ElasticSearch, and RabbitMQ. Those all work with Windows regardless of where they're actually running. Furthermore, Azure already has pre-built containers with all of these.\n\nMuch of this will be the basis of my talk at the upcoming [MeasureUP](http://measureup.io) conference next month. So...uhhh....don't read this until after that.","categories":[],"tags":[]},{"title":"Casting in Telerik Reports","authorId":"simon_timms","slug":"casting-in-telerik-reports","date":"2015-08-02 13:03:21+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/casting-in-telerik-reports/","link":"","permalink":"https://westerndevs.com/_/casting-in-telerik-reports/","excerpt":"Short post as I couldn't find this documented anywhere. But if you need to cast a value inside the expression editor inside a Telerik Report then you can use the conversion functions","raw":"---\nlayout: post\ntitle:  Casting in Telerik Reports\ndate: 2015-08-02T11:03:21+02:00\ncategories:\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/07/30/casting-in-telerik-reports/\n---\n\nShort post as I couldn't find this documented anywhere. But if you need to cast a value inside the expression editor inside a Telerik Report then you can use the conversion functions\n\n<!--more-->\n\n* CBool\n* CDate\n* CDbl\n* CInt\n* CStr\n\nI used it to cast the denominator here to get a percentage complete:\n\n![](http://imgur.com/LE1hUUP.png)\n\nI also used the Format function to format it as a percentage. I believe the Format string here is passed directly into .net's string format function so anything that works there will work here.","categories":[],"tags":[]},{"title":"Considerations When Migrating Your Blog","authorId":"david_wesst","slug":"considerations-when-migrating-your-blog","date":"2015-08-01 20:15:22+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/considerations-when-migrating-your-blog/","link":"","permalink":"https://westerndevs.com/_/considerations-when-migrating-your-blog/","excerpt":"I'm talking about small-scale, personal blogs or projects. The word &quot;enterprise&quot; isn't used once to describe any part of the project, yet there are plenty of things I had to consider (or decide along the way) before I completed the migration. Eventually, I ended moving my Ghost blog to a Hexo-based static blog, that is hosted on Github Pages under a new subdomain.","raw":"---\nlayout: post\ntitle: \"Considerations When Migrating Your Blog\"\ndate: 2015-08-01 16:15:22\ncategories:\ncomments: true\nauthorId: david_wesst\noriginalurl: http://blog.davidwesst.com/2015/07/Considerations-When-Migrating-Your-Blog/\n---\n\nI'm talking about small-scale, personal blogs or projects.\n\nThe word \"enterprise\" isn't used once to describe any part of the project, yet there are plenty of things I had to consider (or decide along the way) before I completed the migration. Eventually, I ended moving my [Ghost](http://ghost.org) blog to a [Hexo](http://hexo.io)-based static blog, that is hosted on [Github Pages](https://pages.github.com) under a new subdomain.\n\n<!--more-->\n\nBefore I actually did any of the work, I answered a handful of questions which helped guide me through the migration and ultimately feel much more comfortable with the result.\n\nThink of this post as a checklist with a sample reflection on answering each question. Hopefully it helps guide you as it did me.\n\nThe questions I asked were:\n\n1. <a href=\"#whatisthepoint\">What is the Point of your Blog/Site?</a>\n2. <a href=\"#whydoyouwanttomigrate\">Why do you want to Migrate?</a>\n3. <a href=\"#whatdoyouneedfromahost\">What do you need from a Host?</a>\n4. <a href=\"#existingreaders\">How will your existing readers find your migrated content?</a>\n5. <a href=\"#whatisallthecontent\">What is \"All the Content\"?</a>\n\n![](http://blog.davidwesst.com/2015/07/Considerations-When-Migrating-Your-Blog/ghost-to-hexo.png)\n\n## 1. What is the Point of your Blog/Site? <a id=\"whatisthepoint\">#</a>\nThis question is something you probably asked yourself when you first setup your blog, site, or whatever. Now that you're considering migrating it to a new technology, you should revisit why it even exists. \n\nThe great part about this is that there isn't a wrong answer, as long as there _is an answer_.\n\nWhen I'm thinking about code, tech, and the what-not, I tend to skip the whole requirements gathering part of the work. Considering most of my career has revolved around defining requirements and designing user experiences to not only capture those requirements, but to enhance a user's ability to engage the system, it's pretty lazy of me to skip it.\n\nHere's the questions I asked myself before getting into the tech:\n+ Who is the audience for my blog/site?\n+ What is the definition of \"content\"? Blog posts, or is it more than that?\n+ How do I intend on sharing content?\n+ Outside of authoring it, what else do I want to do with my content?\n+ Is there anything else outside of publishing content that I want to do with the blog/site?\n\nI also asked the following questions, which have less to do with technical requirements and more to do with resource availability (for myself):\n+ How much time do I have to author content?\n+ How much time do I have to update application and/or server itself?\n+ Where do I actually spend time authoring content?\n\n### Result\nThis was probably the hardest part for me. Because I like tinkering with new technology in my off time, I like spending time on that rather than worrying about requirements and the softer parts of my projects. Ultimately, I answered the questions, which really got me thinking about what my goals were for having a personal website and/or blog.\n\nThe purpose of the blog is to publish thoughts and content from me to my audience, and provide the ability for the audience to comment. My website _could_ be the same thing, but I've always had a vision that it would be more than a blog. Given, I'm not sure what that something is, I want to keep that door open for future ideas and thus I have two applications: a blog and a website.\n\n## 2. Why do you want to Migrate? <a id=\"whydoyouwanttomigrate\">#</a>\nIt sounds like an obvious question, but I tend to forget to ask this before any personal project. There really isn't a wrong answer considering that you're your own client, but you _should_ be able answer the question. If not, then you should keep on keepin' on with what you have, and decicate your time something more meaningful. \n\nIn my case, I had a few reasons. I wanted to...\n+ ...save my posts as markdown files to make cross posting easier\n+ ...to not have to maintain or upgrade the engine or server hosting the blog ever again\n+ ...move my blog to the _blog_ subdomain\n+ ...~~setup~~ fix the permalinks so they contain the date in the URL\n+ ...make my blogging experience more \"hackable\" to keep me practicing and experimenting with new coding tools\n\nYour reasons will likely differ. That's fine. It's important to know _why_ you're doing all the work.\n\n### Result\nAnswering this helped guide me on selecting my blogging technology along with my host provider. More specifically, to let go of [Ghost on Azure](https://github.com/felixrieseberg/Ghost-Azure) and move to [Hexo](http://hexo.io) to generate the content and host it for on [Github Pages](https://pages.github.com) to start.\n\n## 3. What do you need from a Host? <a id=\"whatdoyouneedfromahost\">#</a>\n\n![](http://blog.davidwesst.com/2015/07/Considerations-When-Migrating-Your-Blog/host-thoughts.png)\n\nIt ties into the first question, but it's more specific to the features you need from a hosting provider. The features of your host will have an impact on what you can accomplish with your site migration and depending on the technologies you want to use, the host may have some specific ways you need to implement them. For example, if you're considering using something like WordPress, the hosting provider may provide you with a \"standard install image\" rather than having to setup the application from scratch.\n\nIn my case, I only needed the ability to host static files and to use a custom domain. I wanted an easy way to deploy, ideally so I can automate the whole thing at some point, but that was a secondary consideration.\n\n### Result\nI like Windows Azure as a hosting provider as it makes me feel safe and in control of my blog. \n\nThat being said, it was overkill for the amount of traffic I'm getting and how much I'm using it. I don't need things like HTTPS or an online blog editor. I just needed a place to put the generated static files. My host selection was [Github Page](https://pages.github.com) for the blog, and continuing to use a cheap Windows Azure site to handle the redirects.\n\nIf I outgrow my Github Pages site and need something with support and more control, then I'll move it back over to Windows Azure or [Surge](http://surge.sh) which [I talked about a while ago on YouTube](https://youtu.be/DsLu3K65J5w?list=PLbTA1UhK0wKjBJyqzA3aDrxcv9NdD1V8c).\n\n## 4. How will your existing readers find your migrated content? <a id=\"existingreaders\">#</a>\nIf you've worked on production web sites or applications, this is something you've likely asked your client. Being my own client, in this case, I have skipped this step a few times. Why? Well, I tell myself that my blog traffic is pretty small, so it's too much work for such a small group of users, or something along those lines.\n\nThis time I realized that not only is that a total cop-out, but it's an ass thing to do to your audience. If people are willing to read, share, or talk about any of your blog content, appreciate them and make sure that if or when they decide to share your content again, your laziness doesn't make them lose street cred because they link they bookmarked on your blog no longer works.\n\n### Result\nMy blog was hosted at [http://davidwesst.com](http://davidwesst.com) on Windows Azure. This meant I needed to find a way to redirect people to the appropriate post on the new [blog.davidwesst.com](http://blog.davidwesst.com). After taking stock of all of my blog content, I setup new NodeJS application with a web.config file on Windows Azure that contains all the mappings.\n\nYou can find the file here on my [GitHub Project](https://github.com/davidwesst/dw-website/blob/master/web.config), and a [tutorial URL Redirects and Rewrites on IIS.net](http://www.iis.net/learn/extensions/url-rewrite-module/creating-rewrite-rules-for-the-url-rewrite-module).\n\n## 5. What is \"All the Content\"? <a id=\"whatisallthecontent\">#</a>\nAgain, sounds like an obvious question, but it can be deceptive. \n\nA web site, even a personal one, is a bunch of hosted content. Pages, media, and the links between them. If you're using a content management system (CMS) to host your blog, then it has been taking care of all the logistics of linking your pages to your media files. Not only to do you need to figure out where those media files are so you can move them to your new application, you need to figure out how to possibly update references to those images.\n\nNothing overly complicated, but depending on how much time you have to spend on migrating and verifying your content, it can get overwhelming very quickly, especially if you have years of content saved up.\n\n### Result\nI did a bunch of reseach on tooling that could help make this possible. Being that Ghost is a pretty new blog engine, the migration features are still a bit raw. Hexo, on the otherhand, has plenty of tools and have some pretty flexible [migration options](https://hexo.io/docs/migration.html). Again, since Ghost is so new, nobody has written a migrator for it yet, although I doubt it would be very difficult.\n\nEven then, because of my time limitations, I took the more manual route root and used the [rss-migrator](https://hexo.io/docs/migration.html#RSS) along with the [image-migrator](https://github.com/akfish/hexo-migrator-image) to get the content moved into Hexo. Then I updated the URLs with some regular expression magic in [Visual Studio Code](https://code.visualstudio.com).\n\nI cheaped out on coding and just manually tested each post, but it worked with my schedule of just getting random late night hours at home to ensure the content moved properly.\n\n# The Point\nWhether you call the project _small_, _personal_, or _internal_, it takes some actual thought to make sure you do it right.\n\nWith smaller projects, whether they are for yourself or for your company, it's easy to take shortcuts. The challenge is that for every shortcut you take, you end up living with it in the long run. In my case, I would constantly ignore the permalink redirection, which means I'm resetting my audience every single time.\n\nOn top of that, I wanted my blog to possibly be a portfolio piece for myself. If I take shortcuts and end up ashamed of the quality of work, I'm losing some of the value of my labour.\n\nTime is valuable, and I hope that this list reminds me and others, that if you're going to take the time to make something better, put your back into it.\n\nYou can see the result of my planning by looking at the two Github repositories, one for the [blog](https://github.com/davidwesst/dw-blog) and one for the [web site](https://github.com/davidwesst/dw-website) that doesn't do anything other than redirect people to the blog.\n\n--\nThanks for playing. ~ DW\n\n","categories":[],"tags":[]},{"title":"Launching an ASP.NET 5 Application From Visual Studio 2015","authorId":"james_chambers","slug":"launching-an-asp-net-5-application-from-visual-studio-2015","date":"2015-07-31 16:21:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/launching-an-asp-net-5-application-from-visual-studio-2015/","link":"","permalink":"https://westerndevs.com/_/launching-an-asp-net-5-application-from-visual-studio-2015/","excerpt":"If you are trying to use any DNX (DotNet Execution) runtime other than dnx451 (i.e. dnx452, dnx46) you will run into the following error when running the application from Visual Studio 2015, when used with the initial release of the Beta 6 tooling:","raw":"---\nlayout: post\ntitle:  \"Launching an ASP.NET 5 Application From Visual Studio 2015\"\ndate: 2015-07-31T14:21:46+02:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/launching-an-asp-net-5-application-from-visual-studio-2015/\n---\n\nIf you are trying to use any DNX (DotNet Execution) runtime other than dnx451 (i.e. dnx452, dnx46) you will run into the following error when running the application from Visual Studio 2015, when used with the initial release of the Beta 6 tooling:\n\n<!--more-->\n\n> **The current runtime target framework is not compatible with 'YourWebApplication'.**\n> \n> Current runtime Target Framework: 'DNX,Version=v4.5.1 (dnx451)'  \n> Type: CLR  \n> Architecture: x64  \n> Version: 1.0.0-beta6-12256\n\nIf you're instead running with a debugger attached, you won't hit a breakpoint, you'll only get a 500. It doesn't matter what framework runtimes you have installed on your machine. It doesn't matter what your global.json says or what dependencies or frameworks you take or specify in project.json.\n\nThis is because the default runtime for launching IIS Express from Visual Studio is indeed dnx451. You can get around this in one of two ways:\n\n1. Launch the website from the command line in your project directory using the command \"dnx . web\". Web is a command that is exposed in your project.json and shares the needed info (config) to launch a project-specific instance of IIS.\n2. In your project properties (right-click, properties from Solution Explorer), add the following environment variable in the Debug tab:  \n&nbsp;&nbsp;&nbsp;&nbsp; DNX_IIS_RUNTIME_FRAMEWORK = dnx46\n\n![image][1]\n\nA huge thanks goes out to [Andrew Nurse][2] for providing a resolution on [this matter][3] and responding to [my issue][4] on GitHub.\n\n[1]: http://jameschambers.com/wp-content/uploads/2015/07/image25.png \"image\"\n[2]: https://twitter.com/anurse\n[3]: http://stackoverflow.com/questions/31671851/vs-2015-setting-right-target-framework-for-asp-net-5-web-project/31687529#31687529\n[4]: https://github.com/aspnet/dnx/issues/2367\n  ","categories":[],"tags":[]},{"title":"ASP.NET Beta 6 Is in the Wild","authorId":"james_chambers","slug":"asp-net-beta-6-is-in-the-wild","date":"2015-07-29 16:17:59+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/asp-net-beta-6-is-in-the-wild/","link":"","permalink":"https://westerndevs.com/_/asp-net-beta-6-is-in-the-wild/","excerpt":"The Beta 6 release of ASP.NET 5 is now available. Run the following command to upgrade from a previous version: dnvm upgrade After that, a &quot;dnvm list&quot; command will give you the following:","raw":"---\nlayout: post\ntitle:  \"ASP.NET Beta 6 Is in the Wild\"\ndate: 2015-07-29T14:17:59+02:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/asp-net-5-beta-6-is-in-the-wild/\n---\nThe Beta 6 release of ASP.NET 5 is now available. Run the following command to upgrade from a previous version:\n\n    dnvm upgrade\n\nAfter that, a \"dnvm list\" command will give you the following:\n\n<!--more-->\n\n![image][1]\n\nYou can also upgrade dnvm itself with the following command:\n\n    dnvm update-self\n\nWhich will get you up to the beta 7 version (build 10400) of DNVM.\n\n![image][2]\n\nYou'll also need the updated VS 2015 tooling, which is available here (along with the DNVM update tools if you want them seperately): <span style=\"text-decoration: line-through;\">Microsoft Visual Studio 2015 Beta 6 Tooling Download</span> (no longer active).\n\n## Why This is Important\n\nAs part of my progression in porting an MVC 5 app to MVC 6, one scenario that I needed support for was to have libraries targeting .NET 4.6 reference-able from a DNX project. MVC 6, up to this point, only supported 4.5.1, which meant that you'd have to roll back your targeting if you were on 4.5.2 or 4.6.\n\nOf course, multi-targeting is a better option, but requires the time and capacity to either slave over the old code base and NuGet packaging nuances, or port to the new project format where you have much greater in-project support for targeting multiple frameworks.\n\n## What You Get\n\nAs previously detailed by [Damien Edwards][4], there are bug fixes, features and improvements in the following areas: [Runtime][5], [MVC][6], [Razor][7], [Identity][8]. In addition to supporting .NET 4.6 in DNX, they have also added localization and have been working on other things like distributed caching, which you can [read about here][9].\n\n## What To Watch Out For\n\nThis is still a beta, and there are many moving parts.\n\nBe sure to check out the [community standup today][10] and head over to [GitHub][11] for the announcements on [breaking changes][12].\n\nHappy coding! ![Smile][13]\n\n[1]: https://jcblogimages.blob.core.windows.net/img/2015/07/image22.png \"image\"\n[2]: https://jcblogimages.blob.core.windows.net/img/2015/07/image_thumb6.png \"image\"\n[4]: https://twitter.com/DamianEdwards\n[5]: https://github.com/issues?utf8=%E2%9C%93&q=user%3Aaspnet+is%3Aissue+label%3Aenhancement+milestone%3A1.0.0-beta6\n[6]: https://github.com/issues?utf8=%E2%9C%93&q=user%3Aaspnet+is%3Aissue+label%3Aenhancement+milestone%3A6.0.0-beta6\n[7]: https://github.com/issues?utf8=%E2%9C%93&q=user%3Aaspnet+is%3Aissue+label%3Aenhancement+milestone%3A4.0.0-beta6\n[8]: https://github.com/issues?utf8=%E2%9C%93&q=user%3Aaspnet+is%3Aissue+label%3Aenhancement+milestone%3A3.0.0-beta6\n[9]: https://github.com/aspnet/Announcements/issues/43\n[10]: https://live.asp.net/\n[11]: https://github.com/aspnet/\n[12]: https://github.com/aspnet/Announcements/issues\n[13]: https://jcblogimages.blob.core.windows.net/img/2015/07/wlEmoticon-smile4.png\n","categories":[],"tags":[]},{"title":"Upgrading Projects to .NET 4.6","authorId":"james_chambers","slug":"upgrading-projects-to-net-4-6","date":"2015-07-29 15:59:39+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/upgrading-projects-to-net-4-6/","link":"","permalink":"https://westerndevs.com/_/upgrading-projects-to-net-4-6/","excerpt":"The updates in the .NET Framework provide many improvements, including support for new language features in c#, garbage collection, enhancements in cryptography support, feature toggles, new classes in the BCL and others. The RyuJIT compiler adds significant performance gains for 64bit applications, even those not originally targeting the 4.6, improves startup times and can reduce the memory footprint of your application.","raw":"---\nlayout: post\ntitle:  \"Upgrading Projects to .NET 4.6\"\ndate: 2015-07-29T13:59:39+02:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/upgrading-projects-to-net-4-6/\n---\n\nThe updates in the .NET Framework provide many improvements, including support for new language features in c#, garbage collection, enhancements in cryptography support, feature toggles, new classes in the BCL and others. The RyuJIT compiler adds significant performance gains for 64bit applications, even those not originally targeting the 4.6, improves startup times and can reduce the memory footprint of your application.\n\n<!--more-->\n\n<div class=\"notice\">\n\n_In this series we're working through the conversion of an MVC 5-based application and migrating it to MVC 6. You can track the entire series of posts from the [intro page][1]._\n\n</div>\n\nWhile the explicit modification of your projects may not be required to gain some of the 4.6 benefits, there may be other organizational factors that lead you down that path. We'll work through the mechanics of the upgrade to 4.6 in this post.\n\n<h2><i class=\"fa fa-warning\"></i> A Word of Caution</h2>\n\nUPDATE: July 28, 2015 There is a known issue with certain 64bit applications running on .NET 4.6, under certain circumstances, with certain parameter types and sizes. You can read more [about the bug finding here][2] and the issue is being [tracked on GitHub][3], followed by Microsoft's [response and recommendation][4].\n\n<div class=\"notice\">\n    \nFor this reason **I am not recommending an upgrade to 4.6** unless you understand the implications and how to properly vet the scenarios described in your environment.\n</div>\n\n## Getting Your Projects Up-to-date\n\nEvery project we create references a specific version of the .NET Framework. This has been true throughout the history of .NET, and though the way we will do it in the future will change with the new project system, the premise remains the same.\n\nFor now, you can simply open the properties tab for your project and change the target Framework.\n\n![image][5]\n\nYou will be prompted to let you know that some changes may be required.\n\n![image][6]\n\nNote that in my case, I had 7 projects with varying types of references and dependencies, and no modifications were required to the code. Your mileage may vary, of course, but this is a simple change and one that you can test quickly. With proper source control in place, this is a zero-risk test that should take only a moment or two.\n\nNow, if you were to try to build the Bootcamp project when you're only partway through the upgrade, you'd see something similar to the following:\n\n![image][7]\n\nWith a message that reads:\n\n> The primary reference \"x\" could not be resolved because it was built against the \".NETFramework,Version=v4.6\" framework. The is a higher version than the currently targeted framework \".NETFramework,Version=4.5.1\".\n\nYou may run into this in other scenarios, as well, especially if you you have references to packages or libraries that get out of sync in your upgrade process. **A project that takes on dependencies must be at (or higher than) the target framework of the compiled dependencies**. To remedy this, we simply need to complete the upgrade process on the rest of the projects.\n\n![image][8]\n\nThis was pretty painless. ![Smile][9]\n\n## Reasons to Upgrade?\n\nMoving from 4.5.x to 4.6 is not a required step in our conversion to an MVC 6 project. In fact, MVC 6 indeed runs on a different framework altogether. To that end, any environment where you have 4.6 installed will \"pull up\" other assemblies because it is a drop-in replacement for pervious versions.\n\nPerhaps your primary motivator to move to 4.6 is the perf bump in the runtime, or it might be the new language features (which only require a framework install, not a version bump in your target). But it also ensures we're compatible with other projects in our organization, particularly when we consider the default target for new projects in VS 2015 is against 4.6. If we want to leverage these from other projects in our organization, we want to make sure that we're not the lowest common denominator in the mix.\n\n## The Next Step\n\nHowever, there are a couple of other points that we should note, namely that our compiler isn't tied to the installed framework or the runtime, it's just used to _target_ a specific set of instructions that the runtime can digest. So, if our MVC 6 will be running on DNX46, or we have other .NET 4.6 projects we're all set (though, we'd have to use DNU wrap to consume our library at this point in DNX).\n\nBut what if we have different projects across our organization, or we have external teams using our libraries? The answer lies in multi-targeting, which is what we'll address in the next post in this series.\n\nUntil then, happy coding! ![Smile][9]\n\n[1]: http://jameschambers.com/2015/07/upgrading-a-real-world-mvc-5-application-to-mvc-6/\n[2]: http://nickcraver.com/blog/2015/07/27/why-you-should-wait-on-dotnet-46/\n[3]: https://github.com/dotnet/coreclr/issues/1296\n[4]: http://blogs.msdn.com/b/dotnet/archive/2015/07/28/ryujit-bug-advisory-in-the-net-framework-4-6.aspx\n[5]: http://jameschambers.com/wp-content/uploads/2015/07/image18.png \"image\"\n[6]: http://jameschambers.com/wp-content/uploads/2015/07/image19.png \"image\"\n[7]: http://jameschambers.com/wp-content/uploads/2015/07/image20.png \"image\"\n[8]: http://jameschambers.com/wp-content/uploads/2015/07/image21.png \"image\"\n[9]: http://jameschambers.com/wp-content/uploads/2015/07/wlEmoticon-smile5.png\n  ","categories":[],"tags":[]},{"title":"SaaS and Commodities","authorId":"donald_belcham","slug":"saas-and-commodities","date":"2015-07-28 18:40:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/saas-and-commodities/","link":"","permalink":"https://westerndevs.com/_/saas-and-commodities/","excerpt":"I'm doing some work right now that requires us to send SMS messages. The organization I'm working with has never had this capability before so we are starting at ground level when it comes to looking at options. As part of our process we evaluated a number of different criteria on about four different SaaS options; twilio, plivo, nexmo and sendinblue. For reasons not relevant to this post, plivo was the initial choice of the client. We moved from analysis to writing a proof of concept.","raw":"---\nlayout: post\ntitle:  SaaS and Commodities\ndate: 2015-07-28T16:40:50+02:00\ncategories:\ncomments: true\nauthorId: donald_belcham\noriginalurl: http://www.igloocoder.com/2817/saas-and-commodities\n---\n\nI'm doing some work right now that requires us to send SMS messages. The organization I'm working with has never had this capability before so we are starting at ground level when it comes to looking at options. As part of our process we evaluated a number of different criteria on about four different SaaS options; [twilio][1], [plivo][2], [nexmo][3] and [sendinblue][4]. For reasons not relevant to this post, plivo was the initial choice of the client. We moved from analysis to writing a proof of concept.\n\n<!--more-->\n\nThe first stage of doing a proof of concept is getting a user account set up. When I tried registering a new account with plivo I got the following message:\n\n![plivo_error][5]\n\nI did, however, receive an account confirmation email. I clicked on the link in the email and received the same message. Thinking that this might just be a UI issue and that the confirmation was accepted I decided to try to login. Click the login button and whamâ€¦same error messageâ€¦before you even get to see the username and password screen. This, obviously, is starting to become an issue for our proof of concept. I decide, as a last resort, to reach out to plivo to get a conversation going. I navigate to the Contact Us page to, once again, see the same error message before I see the screen. At this point the website is obviously not healthy so I navigate to the status page and see this\n\n![plivo_status][6]\n\nSo everything is healthyâ€¦.but it's not. A quick glance at their twitter account shows that plivo attempted to do some database maintenance the day prior to this effort and they claimed it was successful. Related to the database maintenance or not, I needed to move on.\n\nThis is the part that gets interesting (for me anyways). Our choice in SMS provider was a commodity selection. We went the store, looked on the shelf, read the boxes and picked one. It very well could have been any box that we selected. But the fact that making that selection was so simple means that changing the selection was equally simple. We weren't (in this specific case which may not always be the case) heavily invested in the original provider so the cost of change was minimal (zero in our case). All we had to do was make a different selection.\n\nThis highlighted something that hadn't clicked for me before. Software as commodities makes our developer experience both more dynamic and more resilient. We are able to change our minds quickly and avoid unwanted interruptions easily. The flip side of the coin is that if you're a SaaS provider you need to have your A game on all the time. Any outage, error or friction means that your potential (or current) customer will quickly and easily move to a competitor.\n\n[1]: http://twilio.com\n[2]: http://plivo.com\n[3]: http://nexmo.com\n[4]: http://sendinblue.com\n[5]: https://farm4.staticflickr.com/3739/19464256764_33dd8b79b9_z.jpg\n[6]: https://farm1.staticflickr.com/318/19465913243_18bd4dca97_z.jpg\n  \n","categories":[],"tags":[]},{"title":"Using PowerShell to Set Your Azure SQL firewall rule","authorId":"dave_white","slug":"using-powershell-to-set-your-azure-sql-firewall-rule","date":"2015-07-27 03:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/using-powershell-to-set-your-azure-sql-firewall-rule/","link":"","permalink":"https://westerndevs.com/_/using-powershell-to-set-your-azure-sql-firewall-rule/","excerpt":"If you've read a couple of my recent blog posts, you'll see that I've been working in PowerShell a lot lately. I've also been working with Azure a lot lately as well and I'm getting opportunities to put those two things together.","raw":"---\nlayout: post\ntitle:  Using PowerShell to Set Your Azure SQL firewall rule\ndate: 2015-07-26T17:30:00-06:00\ncategories:\ncomments: true\nauthorId: dave_white\noriginalurl: http://agileramblings.com/2015/07/26/using-powershell-to-set-your-azure-sql-firewall-rule/\n---\n\nIf you've read a couple of my recent blog posts, you'll see that I've been working in PowerShell a lot lately. I've also been working with Azure a lot lately as well and I'm getting opportunities to put those two things together.\n\n<!--more-->\n\nSince my laptop is moving around a lot and occasionally my home IP address changes, I do need to update my Azure SQL Firewall rule to allow my computer at my current my IP address to talk to my Azure SQL database server. \n\n[Azure SQL Database Firewall][1]\n\nI've added 4 simple functions to my .\\profile.ps1 script that makes this job really easy.\n\n{% codeblock lang:powershell %}\nfunction Set-MyAzureFirewallRule {\n    $response = Invoke-WebRequest ifconfig.me/ip\n    $ip = $response.Content.Trim()\n\tNew-AzureSqlDatabaseServerFirewallRule -StartIPAddress $ip -EndIPAddress $ip -RuleName <Name of Rule> -ServerName <your database server name here>\n}\nfunction Update-MyAzureFirewallRule{\n    $response = Invoke-WebRequest ifconfig.me/ip\n    $ip = $response.Content.Trim()\n    Set-AzureSqlDatabaseServerFirewallRule -StartIPAddress $ip -EndIPAddress $ip -RuleName <Name of Rule> -ServerName <your database server name here>\n}\nfunction Remove-MyAzureFirewallRule{\n    Remove-AzureSqlDatabaseServerFirewallRule -RuleName <Name of Rule> -ServerName <your database server name here>\n}\nfunction Get-MyAzureFirewallRule{\n    Get-AzureSqlDatabaseServerFirewallRule -RuleName <Name of Rule> -ServerName <your database server name here>\n}\n{% endcodeblock %}\n\n### Get the Azure PowerShell Module\nThe first thing you'll need to do if you want to do any work with Azure via PowerShell is download and install the Azure PowerShell modules. \n\n[Install And Configure Azure PowerShell][2]\n\nOnce you've done this, you'll be able to run Azure CommandLets in your PowerShell session.\n\n### How to get your IP address\n\nSince many times I'm behind a router that is doing NAT translations, knowing my IP address isn't as simple as typing `Get-NetIPAddress | Format-Table` or `ipconfig` in a console. That will tell me what my computer thinks the IP address is in my local network, but that isn't what Azure will see. Azure will see the IP address of my cable modem.\n\nIn order to find out what my IP address is from an external perspective, I need the help of a little service called <a href=\"http://ifconfig.me/\" target=\"_blank\">ifconfig.me</a> to tell me what my IP address is externally. If you make the whole Url <a href=\"http://ifconfig.me/ip\" target=\"_blank\">ifconfig.me/ip</a> you will get a simple text response from them with your IP address. Just give that Url a click and try it out. If you view the page source, you'll see that only text was returned.\n\n### Putting it all together\n\n So now we have the Azure PowerShell modules and we know about ifconfig.me. All we need now is the put the two together into one of our functions. I'll use my first function as the example. You'll be able to follow the rest after I describe this one. \n \n{% codeblock lang:powershell %}\nfunction Set-MyAzureFirewallRule {\n    $response = Invoke-WebRequest ifconfig.me/ip\n    $ip = $response.Content.Trim()\n\tNew-AzureSqlDatabaseServerFirewallRule -StartIPAddress $ip -EndIPAddress $ip -RuleName <Name of Rule> -ServerName <your database server name here>\n}\n{% endcodeblock %}\n\nThe first line is the PowerShell (non-Azure) CmdLet `Invoke-WebRequest ifconfig.me/ip`. This will call ifconfig.me/ip and get a response, trapped in the `$response` variable. \n\nIn the next line, I clean up the response a little bit using some .Net string functions to move my IP address into the `$ip` variable.\n\nFinally, I call the Azure PowerShell CmdLet to create a new Firewall rule in my Azure account.\n\n> You will have to have followed the instructions in [Azure PowerShell Install and Configure][2] to set up the authentication to allow this PowerShell session to access your Azure subscription.\n\nThe other three variations of this function are for completeness. You will actually probably use the `Update-MyAzureFirewallRule` most since you'll set-up the Firewall rule once the first time and then you'll just need to update it whenever your IP address changes.\n\n### Final Thoughts\n\nI hope this post makes it easier for you to access your SQL Azure database server from your laptop, where ever it may have moved. Once you've set up the rule, you'll be able to access your database server from the tools in Visual Studio, SQL Server Management Studio, or any other tool you prefer to use to work with your Azure SQL Server. \n\nEnjoy!! \n\n[1]: https://msdn.microsoft.com/en-us/library/azure/ee621782.aspx \"https://msdn.microsoft.com/en-us/library/azure/ee621782.aspx\"\n[2]: https://azure.microsoft.com/en-us/documentation/articles/powershell-install-configure/ \"https://azure.microsoft.com/en-us/documentation/articles/powershell-install-configure/\"\n","categories":[],"tags":[]},{"title":"Using IE Automation in PowerShell to Simplify Browser Based Tasks","authorId":"dave_white","slug":"simple-powershell-automation-browser-based-tasks","date":"2015-07-25 03:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/simple-powershell-automation-browser-based-tasks/","link":"","permalink":"https://westerndevs.com/_/simple-powershell-automation-browser-based-tasks/","excerpt":"As a consultant, one of the things that I need to do regularly is log into my client's WiFi networks. Sometimes this is a once per month task, sometimes it is a daily task. It was a daily version of this task that made me look into doing this a bit quicker. Opening Internet Explorer (or any browser) and then navigating to the page, typing in all of my credentials, and then submitting the request is a fairly monotonous task, and it isn't very quick.","raw":"---\nlayout: post\ntitle:  Using IE Automation in PowerShell to Simplify Browser Based Tasks\ndate: 2015-07-24T17:30:00-06:00\ncategories:\ncomments: true\nauthorId: dave_white\noriginalurl: http://agileramblings.com/2015/07/22/using-ie-automation-in-powershell-to-simplify-tasks/\n---\n\nAs a consultant, one of the things that I need to do regularly is log into my client's WiFi networks. Sometimes this is a once per month task, sometimes it is a daily task. It was a daily version of this task that made me look into doing this a bit quicker. Opening Internet Explorer (or any browser) and then navigating to the page, typing in all of my credentials, and then submitting the request is a fairly monotonous task, and it isn't very quick.\n\n<!--more-->\n\nNow a days, I almost always have a PowerShell window open, and because of another little experiment I did with PowerShell and IE, I thought it should be easy to automate my WiFi network login. So I that is what I set out to do.\n\nThe way that I'm currently working in PowerShell is to create a .ps1 file to do my development in. That way I can version control the file, and keep it separate from other things that are working or in progress. So in this case, I made a PowerShell script file called Login-GuestWifi.ps1. In this file, I just started typing lines of PowerShell script and eventually I would move it into a CmdLet or a function somewhere else.\n\nThe first line in the PowerShell script is a call to create an Internet Explorer Application.\n{% codeblock lang:powershell %}\n$ie = new-object -ComObject \"InternetExplorer.Application\"\n{% endcodeblock %}\n\nNow that you've got IE in your PowerShell code, you need to figure out what to do with this. This is going to require a little bit of work in the browser so you're going to have to open a browser and navigate to the page you're going to be working with. In my case, this was an internal IP address that I was re-directed to when using the browser for the first time on the guest WiFi network.\n\nhttp://10.10.10.10 â€“ Example URL\n\n![image][1]\n\nOnce I've navigated there, I press F12 to get to the developer tools of my browser. (I'll use IE for my examples)\n\n![image][2]\n\nUsing the Dev Tools, I'm going to discover what the fields I need to fill in are (their id or class). In this case, I found fragments of ids that were not generated. I took those fragments and put them into my PowerShell code along with URL of the login page.\n{% codeblock lang:powershell %}\n$requestUri = http://10.10.10.10/guest/wifi-guest.php\n$userIdFragment = \"weblogin_user\";\n$passwordIdFragment = \"weblogin_password\";\n$acceptTermsInputFragment = \"weblogin_visitor_accept_terms\"\n$buttonIdFragment = \"weblogin_submit\";\n{% endcodeblock %}\n\nI now have details of where to go and ability to find the elements on the page that I'm interested in. I'm going to now invoke some methods on the IE Application instance I have to navigate to the Url.\n{% codeblock lang:powershell %}\n#$ie.visible = $true\n$ie.silent = $true\n$ie.navigate($requestUri)\nwhile($ie.Busy) { Start-Sleep -Milliseconds 100 }\n{% endcodeblock %}\n\nThe first two lines indicate how IE is supposed to behave in two ways and the first one is commented out.\n\n1. Show the instance of IE. With this line commented out, we get a \"headless\" browsing experience with no visible window or rendering.  [Visible Property - MSDN][3]\n2. Do not show any dialogs that may pop up. [Silent Property - MSDN][5]\n\nThe next instruction tells IE to navigate to the Url provided.\n\nThe 4th line of this script fragment is interesting. We need to wait for IE to actually do the navigation. If we don't add this line, the PowerShell script will happily continue executing much faster than IE will retrieve and load the page into the Document Object Model (DOM) and the rest of your script will probably fail.\n\nAfter IE has loaded up the DOM, we can now find our elements, give them values, and click the Submit button.\n\n{% codeblock lang:powershell %}\n$doc = $ie.Document\n$doc.getElementsByTagName(\"input\") | % {\n    if ($_.id -ne $null){\n        if ($_.id.Contains($buttonIdFragment)) { $btn = $_ }\n        if ($_.id.Contains($acceptTermsInputFragment)) { $at = $_ }\n        if ($_.id.Contains($passwordIdFragment)) { $pwd = $_ }\n        if ($_.id.Contains($userIdFragment)) { $user = $_ }\n    }\n}\n\n$user.value = \"<user name=\"\" here=\"\">\"\n$pwd.value = \"<password here=\"\">\"\n$at.checked = \"checked\"\n$btn.disabled = $false\n$btn.click()\nWrite-Verbose \"Login Complete\"\n{% endcodeblock %}\n\nOne interesting thing about IE automation is that any JavaScript or page behaviours that we would expect to execute don't seem to run, so we need to explicitly enable the submit button in the event that it was not enabled until all of the fields were entered and the accept terms of use checkbox was clicked.\n\nAnd that's it! I now have a PowerShell script that runs in seconds and logs me into the client's guest WiFi network.\n\n![image][4]\n\nAs a final task, I took the code in my Login-GuestWifi.ps1, converted it to a function and placed it in my ./profile.ps1 file that gets invoked any time a PowerShell session is started on my machine.\n\nIt should be noted that the UserName and Password, in my case, were not secured in any fashion other than being only physically stored on my machine in my scripts file. I never checked my credentials into source control and I had no need to put them anywhere else. If needed, I could secure them but that wasn't necessary. These are not domain credentials and are only giving people access to the guest WiFi network.\n\n### Final Thoughts\n\nMy goal with this post was to exposed you to the idea of using PowerShell to automate simple web-based tasks in Internet Explorer. I've recently been using PowerShell a lot and I've just been continuously impressed with how powerful it is. So go give it a try!\n\n[1]: http://agileramblings.files.wordpress.com/2015/07/image_thumb.png?w=244&amp;h=186 \"image\"\n[2]: http://agileramblings.files.wordpress.com/2015/07/image_thumb1.png?w=244&amp;h=219 \"image\"\n[3]: https://msdn.microsoft.com/en-us/library/aa752082%28v=vs.85%29.aspx \"https://msdn.microsoft.com/en-us/library/aa752082%28v=vs.85%29.aspx\"\n[4]: http://agileramblings.files.wordpress.com/2015/07/image_thumb2.png?w=244&amp;h=80 \"image\"\n[5]: https://msdn.microsoft.com/en-us/library/aa752074(v=vs.85).aspx \"https://msdn.microsoft.com/en-us/library/aa752074(v=vs.85).aspx\"","categories":[],"tags":[]},{"title":"Getting Your Build Server Ready for VS 2015","authorId":"james_chambers","slug":"getting-your-build-server-ready-for-vs-2015","date":"2015-07-24 18:22:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/getting-your-build-server-ready-for-vs-2015/","link":"","permalink":"https://westerndevs.com/_/getting-your-build-server-ready-for-vs-2015/","excerpt":"If you're modernizing your project, one of the things you'll surely want to do is to make sure that your build server is upgraded to support VS 2015. Regardless of what CI engine you're using, there will be at least a little bit of effort required to get your project building again.","raw":"---\nlayout: post\ntitle:  \"Getting Your Build Server Ready for VS 2015\"\ndate: 2015-07-24T10:22:30-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/getting-your-build-server-ready-for-vs-2015/\n---\n\nIf you're modernizing your project, one of the things you'll surely want to do is to make sure that your build server is upgraded to support VS 2015. Regardless of what CI engine you're using, there will be at least a little bit of effort required to get your project building again.\n\n<!--more-->\n\n>In this series we're working through the conversion of an MVC 5-based application and migrating it to MVC 6. You can track the entire series of posts from the [_intro page_][1].\n\nFor the purpose of this exercise, we're using TeamCity to run our builds based on a VSC checkin. We'll get TeamCity prepped to run our build and then update our repository so that we show our build status indicator on the readme home page.\n\n## The TL;DR Details\n\nHere's the basics of what was required to get the builds back online:\n\n* Backup and upgrade TeamCity\n* Allow the agents to upgrade, or upgrade them manually\n* Install .NET 4.6 and the VS 2015 tools\n* Ensure that build targets live on your build agents\n* Run your build\n\n## Upgrading the Server\n\nI engaged my teammate [James Allen][2] here to help with some best practices, namely getting the server backed up. You can either back up the TeamCity data from the web interface or one of the other [recommended approaches][3], or you could snapshot your server for a reset should one be required. During this process, it's a good idea to spin down your build agents so that you're not wrecking anyone's builds.\n\nNext, we needed to move to version 9.1 of TeamCity, so we ran the [upgrade process][4] via the web site. This is a painless task and takes only a fraction of the time it took to back up the data. Failing any troubles (we saw none), your build server should be back online in no time, and the build agents were notified (and complied!) to update themselves as well.\n\n<img width=\"240\" height=\"191\" title=\"image\" align=\"right\" style=\"margin: 0px 0px 0px 10px; border: 0px currentcolor; border-image: none; float: right; display: inline; background-image: none;\" alt=\"image\" src=\"http://jameschambers.com/wp-content/uploads/2015/07/image_thumb5.png\" border=\"0\" scale=\"0\">Next, I downloaded and installed the .NET 4.6 installer and the VS 2015 tooling, which can be found on the [VS 2015 download page][6]. You'll need to explore through the available downloads on the page, as you can see on this screenshot, to grab the relevant files.\n\nThese installs will need to be run on every build agent.\n\nOne thing to note was that my original attempt to get the build running failed because of missing build targets at an expected location. I ended up having to copy files from my local machine, where Visual Studio 2015 is installed, from the path: C:Program Files (x86)MSBuildMicrosoftVisualStudiov14.0 on the build server.\n\n>I don't believe this is the best approach to getting the build targets on the build server. I will update this post if I find a better solution.\n\n## Verifying the Install\n\nYou'll know the tools have been installed correctly if you return to the build configuration settings and add a new build step for msbuild (you don't have to save it). You'll see that you'll have the new options in place:\n\n![image][7]\n\nThe build server should be good to go now! For us, we're not using an MSBuild build runner, our application is build with a PowerShell script via a batch file. This allows our build to be executed locally with only a small parameter change, and the CI process is entirely encapsulated in code (and under source control).\n\nProvided your project is pointing at the repository, you'll have a good shot at running the build at this point. For our project, everything worked as expected.\n\n![image][8]\n\n## Showing Some Bling\n\nNow it's time to beef up our repo, at least a little. What I'm talking about is wearing our CI on our sleeve, letting everyone on the team (or other watchers of the repository) that our builds are healthy or, perhaps, needing some love; let's display the build status indicator on our readme, like this:\n\n![image][9]\n\nFirst, drill into the build configuration and locate the advanced options under \"General Settings\". You need to enable the status widget.\n\n![image][10]\n\nAlso, from this screen, take note of your Build Configuration ID. This is important because you'll need to include it in the server request to generate the badge.\n\nFinally, include the following markdown, which is essentially a formatted link with an image inside of it:\n\n    Current Build Status [![](http://YOUR_SERVER/app/rest/builds/buildType:(id:YOUR_BUILD_CONFIGURATION_ID)/statusIcon)](http://teamcity/viewType.html?buildTypeId=btN&guest=1)\n\nBe sure to replace the obvious placeholder tokens with your own information.\n\n## Next Steps\n\nWith our build server updated and our builds back online, it's time to start shifting our targets. In the next post, we're going to update our projects and recover from any errors/challenges we may discover along the way.\n\nHappy coding! ![Smile][11]\n\n[1]: http://jameschambers.com/2015/07/upgrading-a-real-world-mvc-5-application-to-mvc-6/\n[2]: http://www.clear-measure.com/our-team/\n[3]: https://confluence.jetbrains.com/display/TCD9/TeamCity+Data+Backup\n[4]: https://confluence.jetbrains.com/display/TCD9/Upgrade\n[5]: http://jameschambers.com/wp-content/uploads/2015/07/image_thumb5.png \"image\"\n[6]: https://www.visualstudio.com/downloads/download-visual-studio-vs\n[7]: http://jameschambers.com/wp-content/uploads/2015/07/image14.png \"image\"\n[8]: http://jameschambers.com/wp-content/uploads/2015/07/image15.png \"image\"\n[9]: http://jameschambers.com/wp-content/uploads/2015/07/image16.png \"image\"\n[10]: http://jameschambers.com/wp-content/uploads/2015/07/image17.png \"image\"\n[11]: http://jameschambers.com/wp-content/uploads/2015/07/wlEmoticon-smile3.png\n  ","categories":[],"tags":[]},{"title":"Building a TFS 2015 PowerShell Module using Nuget","authorId":"dave_white","slug":"tfs-module-in-powershell-using-nuget","date":"2015-07-24 03:30:00+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/tfs-module-in-powershell-using-nuget/","link":"","permalink":"https://westerndevs.com/_/tfs-module-in-powershell-using-nuget/","excerpt":"Update: Unwittingly, I hadnâ€™t tested my Nuget approach on a server with no Visual Studio or TFS installations on it and Iâ€™ve missed a couple assemblies that are required when loading the TFS Object model. Iâ€™ve updated the line of code in my samples, but just in case, here is the new version of the line in question.","raw":"---\nlayout: post\ntitle:  Building a TFS 2015 PowerShell Module using Nuget\ndate: 2015-07-23T17:30:00-06:00\ncategories:\ncomments: true\nauthorId: dave_white\noriginalurl: http://agileramblings.com/2015/07/23/building-a-tfs-2015-powershell-module-using-nuget/\n---\n\n\n*Update:* Unwittingly, I hadnâ€™t tested my Nuget approach on a server with no Visual Studio or TFS installations on it and Iâ€™ve missed a couple assemblies that are required when loading the TFS Object model. Iâ€™ve updated the line of code in my samples, but just in case, here is the new version of the line in question.\n\n<!--more-->\n\n{% codeblock lang:powershell %}\n$net45Dlls = $allDlls | ? {$_.PSPath.Contains(\"portable\") -ne $true } | ? {$_.PSPath.Contains(\"resources\") -ne $true } | ? { ($_.PSPath.Contains(\"net45\") -eq $true) -or ($_.PSPath.Contains(\"native\") -eq $true) -or ($_.PSPath.Contains(\"Microsoft.ServiceBus\") -eq $true) }\n{% endcodeblock %}\n\nThe update is the addition of two *-or* statements to the last inclusive where clause.\n\nIâ€™ve also slightly changed the *Import-TfsAssemblies* function to include a try/catch block for better error reporting.\n\n#### Original Start\n\nWith the release of [Visual Studio 2015][1] on July 20, 2015,&nbsp;we can talk about and explore a lot of really cool things that are happening with Visual Studio (VS) and Team Foundation Server (TFS). One of the things that has been a bit of a pain when managing a TFS&nbsp;on-premises installation has been the necessity of installing Visual Studio to get the TFS client object model on your administrative workstation. With the explosive use of PowerShell to manage all things Microsoft, this has been a bit of&nbsp;a drag on using PowerShell for TFS work. There are PowerShell modules for TFS in the TFS Power Tools, but sometimes you need the power that comes with using the TFS Object Model. Which meant that you had to install Visual Studio. I'm really glad to say that is no longer the case. With the release of TFS 2015, the TFS Object Model is now available on [Nuget][2]! With our trusty nuget.exe, we can now get the TFS object model from a trusted source, without violating any license terms, to use in our own TFS PowerShell modules.\n\nI'm not going to profess to be a PowerShell wizard so I hope I'm not breaking any community best practices too badly. I'm more than happy to adapt my implementation if I get feedback on better ways of doing things! It should also be noted that I'm using PowerShell 4. This is located in the Windows Managment Framework 4 download (http://www.microsoft.com/en-ca/download/details.aspx?id=40855), a free download from Microsoft. I don't **_think&nbsp;_**you'll have any problems upgrading from previous versions of PowerShell but I'm not going to any assurances.\n\nLet's start walking through building a TFS PowerShell module!\n\n## Create A PowerShell Module\n\nI'm not going to go into a lot of details, but the basic steps to creating your PowerShell module are:\n\n1. Navigate to %USERPROFILE%\\My Documents\\WindowsPowerShell\\Modules\n2. Create a folder called MyTfsModule\n3. In the MyTfsFolder, create a file called MyTfsModule.psm1\n\nIt is important that the name of the Module folder and the Module file are the same. Otherwise, you won't be able to load your module. This one requirement&nbsp;tripped me up for a while when I started writing PowerShell modules.\n\n## Module-Specific Variables And Helper Functions\n\nThere are a few module specific variables that we need to set when the module loads and a Helper function that I use for getting/creating folders. You can put these at the top of your MyTfsModule.psm1 file.\n{% codeblock lang:powershell %}\nWrite-Host \"Loading MyTfsModule\"\n#Module location folder\n$ModuleRoot = Split-Path -Parent -Path $MyInvocation.MyCommand.Definition\n#where to put TFS Client OM files\n$omBinFolder = $(\"$ModuleRootTFSOMbin\")\n\n# TFS Object Model Assembly Names\n$vsCommon = \"Microsoft.VisualStudio.Services.Common\"\n$commonName = \"Microsoft.TeamFoundation.Common\"\n$clientName = \"Microsoft.TeamFoundation.Client\"\n$VCClientName = \"Microsoft.TeamFoundation.VersionControl.Client\"\n$WITClientName = \"Microsoft.TeamFoundation.WorkItemTracking.Client\"\n$BuildClientName = \"Microsoft.TeamFoundation.Build.Client\"\n$BuildCommonName = \"Microsoft.TeamFoundation.Build.Common\"\n\nfunction New-Folder() {\n    <#\n    .SYNOPSIS\n    This function creates new folders\n    .DESCRIPTION\n    This function will create a new folder if required or return a reference to\n    the folder that was requested to be created if it already exists.\n    .EXAMPLE\n    New-Folder \"C:TempMyNewFolder\"\n    .PARAMETER folderPath\n    String representation of the folder path requested\n    #>\n\n    [CmdLetBinding()]\n    param(\n        [parameter(Mandatory=$true, ValueFromPipeline=$true)]\n        [string]$folderPath\n    )\n    begin {}\n    process {\n        if (!(Test-Path -Path $folderPath)){\n            New-Item -ItemType directory -Path $folderPath\n        } else {\n            Get-Item -Path $folderPath\n        }\n    }\n    end {}\n} #end Function New-Directory\n{% endcodeblock %}\n## First We Get Nuget\n\nThe first thing we need to do is get the Nuget.exe from the web. This is very easily down with the following PowerShell function\n{% codeblock lang:powershell %}\nfunction Get-Nuget(){\n    <#\n    .SYNOPSIS\n    This function gets Nuget.exe from the web\n    .DESCRIPTION\n    This function gets nuget.exe from the web and stores it somewhere relative to\n    the module folder location\n    #>\n    [CmdLetBinding()]\n    param()\n\n    begin{}\n    process\n    {\n        #where to get Nuget.exe from\n        $sourceNugetExe = \"http://nuget.org/nuget.exe\"\n\n        #where to save Nuget.exe too\n        $targetNugetFolder = New-Folder $(\"$ModuleRootNuget\")\n        $targetNugetExe = $(\"$ModuleRootNugetnuget.exe\")\n\n        try\n        {\n            # check if we have gotten nuget before\n            $nugetExe = $targetNugetFolder.GetFiles() | ? {$_.Name -eq \"nuget.exe\"}\n            if ($nugetExe -eq $null){\n                #Get Nuget from a well known location on the web\n                Invoke-WebRequest $sourceNugetExe -OutFile $targetNugetExe\n            }\n        }\n        catch [Exception]\n        {\n            echo $_.Exception | format-list -force\n        }\n\n        #set an alias so we can use nuget syntactically the way we normally would\n        Set-Alias nuget $targetNugetExe -Scope Global -Verbose\n    }\n    end{}\n}\n{% endcodeblock %}\nOk! When this function is invoked, we should now see a&nbsp;nuget.exe appear at:\n\n>%USERPROFILE%\\My Documents\\WindowsPowerShell\\Modules\\MyTfsModule\\Nuget\\Nuget.exe\n\n## Using Nuget to get TFS Client Object Model\n\nNow that we have nuget, we need to get the TFS Client Object model from nuget.\n{% codeblock lang:powershell %}\nfunction Get-TfsAssembliesFromNuget(){\n    <#\n    .SYNOPSIS\n    This function gets all of the TFS Object Model assemblies from nuget\n    .DESCRIPTION\n    This function gets all of the TFS Object Model assemblies from nuget and then\n    creates a bin folder of all of the net45 assemblies so that they can be\n    referenced easily and loaded as necessary\n    #>\n    [CmdletBinding()]\n    param()\n\n    begin{}\n    process{\n        #clear out bin folder\n        $targetOMbinFolder = New-Folder $omBinFolder\n        Remove-Item $targetOMbinFolder -Force -Recurse\n        $targetOMbinFolder = New-Folder $omBinFolder\n        $targetOMFolder = New-Folder $(\"$ModuleRootTFSOM\")\n\n        #get all of the TFS 2015 Object Model packages from nuget\n        nuget install \"Microsoft.TeamFoundationServer.Client\" -OutputDirectory $targetOMFolder -ExcludeVersion -NonInteractive\n        nuget install \"Microsoft.TeamFoundationServer.ExtendedClient\" -OutputDirectory $targetOMFolder -ExcludeVersion -NonInteractive\n        nuget install \"Microsoft.VisualStudio.Services.Client\" -OutputDirectory $targetOMFolder -ExcludeVersion -NonInteractive\n        nuget install \"Microsoft.VisualStudio.Services.InteractiveClient\" -OutputDirectory $targetOMFolder -ExcludeVersion -NonInteractive\n\n        #Copy all of the required .dlls out of the nuget folder structure \n        #to a bin folder so we can reference them easily and they are co-located\n        #so that they can find each other as necessary when loading\n        $allDlls = Get-ChildItem -Path $(\"$ModuleRoot\\TFSOM\\\") -Recurse -File -Filter \"*.dll\"\n \n        # Create list of all the required .dlls\n        #exclude portable dlls\n        $requiredDlls = $allDlls | ? {$_.PSPath.Contains(\"portable\") -ne $true } \n        #exclude resource dlls\n        $requiredDlls = $requiredDlls | ? {$_.PSPath.Contains(\"resources\") -ne $true } \n        #include net45, native, and Microsoft.ServiceBus.dll\n        $requiredDlls = $requiredDlls | ? { ($_.PSPath.Contains(\"net45\") -eq $true) -or ($_.PSPath.Contains(\"native\") -eq $true) -or ($_.PSPath.Contains(\"Microsoft.ServiceBus\") -eq $true) }\n        #copy them all to a bin folder\n        $requiredDlls | % { Copy-Item -Path $_.Fullname -Destination $targetOMBinFolder}\n    }\n    end{}\n}\n{% endcodeblock %}\nThis function does a could things. First it cleans out the existing bin folder, if it exists. Then it goes to nuget to get all of the packages that are available there. They are:\n\n1. http://www.nuget.org/packages/Microsoft.VisualStudio.Services.Client/\n2. http://www.nuget.org/packages/Microsoft.VisualStudio.Services.InteractiveClient/\n3. http://www.nuget.org/packages/Microsoft.TeamFoundationServer.Client/\n4. http://www.nuget.org/packages/Microsoft.TeamFoundationServer.ExtendedClient/\n\nI use a number of switches on my invocation of the nuget.exe.\n\n* -OutputDirectory â€“ This sets the output directory for the nuget activities\n* -ExcludeVersion â€“ This tells Nuget not to append version numbers to package folders\n* -NonInteractive â€“ Don't prompt me for anything\n\nThe next part seems a bit verbose, but I'm leaving it that way as an example of achieving my intent in case you want to achieve something else. I am intending to get all of the net45, non-portable, base language (non-resource) assemblies from the directory structure that is created by nuget when getting the packages. In order to do that I:\n\n1. Find all .dll files in the directory structure, recursively\n2. Exclude .dll files that have \"portable\" in their path\n3. Exclude .dll files that have \"resource\" in their path\n4. Include only .dll files that have \"net45â€³, \"native\", or \"Microsoft.ServiceBus\" in their path\n\nAfter I've narrowed it down to that list of .dll files, I copy them all to the TFSOMbin folder where they will be referenced from. This also allows them to satisfy their dependencies on each other as required when loaded.\n\n## Loading the TFS Object Models Assemblies\n\nNow that we've retrieved the TFS Object model, and tucked it away in a bin folder we can find, we are now ready to load these assemblies into the PowerShell session that this module is in.\n{% codeblock lang:powershell %}\nfunction Import-TFSAssemblies() {\n    <#\n     .SYNOPSIS\n     This function imports TFS Object Model assemblies into the PowerShell session\n     .DESCRIPTION\n     After the TFS 2015 Object Model has been retrieved from Nuget using\n     Get-TfsAssembliesFromNuget function,  this function will import the necessary\n     (given current functions) assemblies into the PowerShell session\n    #>\n    [CmdLetBinding()]\n    param()\n\n    begin{}\n    process\n    {\n        $omBinFolder = $(\"$ModuleRootTFSOMbin\");\n        $targetOMbinFolder = New-Folder $omBinFolder;\n\n        try {\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $vsCommon + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $commonName + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $clientName + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $VCClientName + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $WITClientName + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $BuildClientName + \".dll\")\n            Add-Type -LiteralPath $($targetOMbinFolder.PSPath + $BuildCommonName + \".dll\")\n        } \n        catch {\n            $_.Exception.LoaderExceptions | $ { $_.Message }\n        }\n     }\n     end{}\n}\n{% endcodeblock %}\n## Putting the Object Model to Use\n\nNow that we have the TFS Object Model loaded into this PowerShell session, we can use it! I'm going to show three&nbsp;functions. One that gets the TfsConfigurationServer object (basically your connection to the TFS server), one that gets the TeamProjectCollection Ids and a function that will get a list of all TFS Event Subscriptions on the server.\n\n### Get-TfsConfigServer\n{% codeblock lang:powershell %}\nfunction Get-TfsConfigServer() {\n    <#\n    .SYNOPSIS\n    Get a Team Foundation Server (TFS) Configuration Server object\n    .DESCRIPTION\n    The TFS Configuration Server is used for basic authentication and represents\n    a connection to the server that is running Team Foundation Server.\n    .EXAMPLE\n    Get-TfsConfigServer \"&lt;Url to TFS&gt;\"\n    .EXAMPLE\n    Get-TfsConfigServer \"http://localhost:8080/tfs\"\n    .EXAMPLE\n    gtfs \"http://localhost:8080/tfs\"\n    .PARAMETER url\n     The Url of the TFS server that you'd like to access\n    #>\n    [CmdletBinding()]\n    param(\n        [parameter(Mandatory = $true)]\n        [string]$url\n    )\n    begin {\n        Write-Verbose \"Loading TFS OM Assemblies for 2015 (14.83.0)\"\n        Import-TFSAssemblies\n    }\n    process {\n        $retVal = [Microsoft.TeamFoundation.Client.TfsConfigurationServerFactory]::GetConfigurationServer($url)\n        [void]$retVal.Authenticate()\n        if(!$retVal.HasAuthenticated)\n        {\n            Write-Host \"Not Authenticated\"\n            Write-Output $null;\n        } else {\n            Write-Host \"Authenticated\"\n            Write-Output $retVal;\n        }\n    }\n    end {\n        Write-Verbose \"ConfigurationServer object created.\"\n    }\n} #end Function Get-TfsConfigServer\n{% endcodeblock %}\nThis function takes a Url and returns an instance of a&nbsp;Microsoft.TeamFoundation.Client.TfsConfigurationServer. This connection object will be authenticated (via Windows Integrated Authentication). If you don't have permission within the domain to administer the TFS server, you won't be able to use the functions provided by the object model. The other functions require this connection in order to do their additional work.\n\n### Get-TfsProjectCollections\n{% codeblock lang:powershell %}\nfunction Get-TfsTeamProjectCollectionIds() {\n    <#\n    .SYNOPSIS\n    Get a collection of Team Project Collection (TPC) Id\n    .DESCRIPTION\n    Get a collection of Team Project Collection (TPC) Id from the server provided\n    .EXAMPLE\n    Get-TfsTeamProjectCollectionIds $configServer\n    .EXAMPLE\n    Get-TfsConfigServer \"http://localhost:8080/tfs\" | Get-TfsTeamProjectCollectionIds\n    .PARAMETER configServer\n    The TfsConfigurationServer object that represents a connection to TFS server that you'd\n    like to access\n    #>\n    [CmdLetBinding()]\n    param(\n        [parameter(Mandatory = $true, ValueFromPipeline = $true)]\n        [Microsoft.TeamFoundation.Client.TfsConfigurationServer]$configServer\n    )\n    begin{}\n    process{\n        # Get a list of TeamProjectCollections\n        [guid[]]$types = [guid][Microsoft.TeamFoundation.Framework.Common.CatalogResourceTypes]::ProjectCollection\n        $options = [Microsoft.TeamFoundation.Framework.Common.CatalogQueryOptions]::None\n        $configServer.CatalogNode.QueryChildren( $types, $false, $options) | % { $_.Resource.Properties[\"InstanceId\"]}\n    }\n    end{}\n} #end Function Get-TfsTeamProjectCollectionIds\n{% endcodeblock %}\n### Get-TfsEventSubscriptions\n\nWe are using a 3rd party tool that subscribes to build events and we needed to know if it was releasing those subscriptions properly and also discover where this tool was running.&nbsp;We thought that the easiest way to do this was to look at all of the subscriptions in the TFS Project Collections in our AppTier.\n{% codeblock lang:powershell %}\n#adapted from http://blogs.msdn.com/b/alming/archive/2013/05/06/finding-subscriptions-in-tfs-2012-using-powershell.aspx\nfunction Get-TFSEventSubscriptions() {\n\n    [CmdLetBinding()]\n    param(\n        [parameter(Mandatory = $true)]\n        [Microsoft.TeamFoundation.Client.TfsConfigurationServer]$configServer\n    )\n\n    begin{}\n    process{\n        $tpcIds = Get-TfsTeamProjectCollectionIds $configServer\n        foreach($tpcId in $tpcIds)\n        {\n            #Get TPC instance\n            $tpc = $configServer.GetTeamProjectCollection($tpcId)\n            #TFS Services to be used\n            $eventService = $tpc.GetService(\"Microsoft.TeamFoundation.Framework.Client.IEventService\")\n            $identityService = $tpc.GetService(\"Microsoft.TeamFoundation.Framework.Client.IIdentityManagementService\")\n\n            foreach ($sub in $eventService.GetAllEventSubscriptions())\n            {\n                #First resolve the subscriber ID\n                $tfsId = $identityService.ReadIdentity(\n                    [Microsoft.TeamFoundation.Framework.Common.IdentitySearchFactor]::Identifier,\n                    $sub.Subscriber,\n                    [Microsoft.TeamFoundation.Framework.Common.MembershipQuery]::None,\n                    [Microsoft.TeamFoundation.Framework.Common.ReadIdentityOptions]::None )\n\n                if ($tfsId.UniqueName)\n                {\n                    $subscriberId = $tfsId.UniqueName\n                }\n                else\n                {\n                    $subscriberId = $tfsId.DisplayName\n                }\n\n                #then create custom PSObject\n                $subPSObj = New-Object PSObject -Property @{\n                    AppTier = $tpc.Uri\n                    ID = $sub.ID\n                    Device = $sub.Device\n                    Condition = $sub.ConditionString\n                    EventType = $sub.EventType\n                    Address = $sub.DeliveryPreference.Address\n                    Schedule = $sub.DeliveryPreference.Schedule\n                    DeliveryType = $sub.DeliveryPreference.Type\n                    SubscriberName = $subscriberId\n                    Tag = $sub.Tag\n               }\n\n               #Send object to the pipeline. You could store it on an Arraylist, but that just\n               #consumes more memory\n               $subPSObj\n\n               #This is another variation where we just add a property to the existing Subscription object\n               #this might be desirable since it will keep the other members\n               #Add-Member -InputObject $sub -NotePropertyName SubscriberName -NotePropertyValue $subscriberId\n           }\n       }\n   }\n   end{}\n}\n{% endcodeblock %}\n\n## All Done\n\nWe are now all done creating our initial MyTfsModule implementation! We should be able to load it up now and give it a spin!\n\n![MyTfsModule_In_Action][3]\n\nI've obscured the name of my running module and TFS server, but in those spots, just use the name of your module and TFS server.\n{% codeblock lang:powershell %}\nImport-Module MyTfsModule\n$configServer = Get-TfsConfigServer http://<name of your TFS server>:8080/tfs\n$allEventsOnServer = Get-TfsEventSubscriptions $configServer\n$allEventsOnServer.Length\n{% endcodeblock %}\n## Final Thoughts\n\nThe key takeaway from this post was that it is great that we can now get the TFS Object Model from Nuget. Still a bit of a pain to sort and move the downloaded assemblies&nbsp;around, but this is because we I am using PowerShell and not building some sort of C#-based project in Visual Studio which would handle the nuget packages much more elegantly.\n\nI hope this post gives you the information you need to go off and create&nbsp;your own TFS PowerShell module without having to install Visual Studio first!\n\np.s. I do have a version of this module that loads the assemblies from the install location of Visual Studio. I'll visit that shortly in another blog post.\n\n[1]: https://www.visualstudio.com/\n[2]: http://www.nuget.org/\n[3]: https://agileramblings.files.wordpress.com/2015/07/mytfsmodule_in_action.png?w=502&amp;h=176\n","categories":[],"tags":[]},{"title":"Upgrading a Real-World MVC 5 Application to MVC 6","authorId":"james_chambers","slug":"upgrading-a-real-world-mvc-5-application-to-mvc-6","date":"2015-07-23 20:08:42+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/upgrading-a-real-world-mvc-5-application-to-mvc-6/","link":"","permalink":"https://westerndevs.com/_/upgrading-a-real-world-mvc-5-application-to-mvc-6/","excerpt":"These are exciting times for web development on the Microsoft stack, but perhaps a little confusing as well. For many years the cycle of moving from one solution and project system to the next hasn't been overly complex. Sure, there have been breaking changes, I've felt those pains myself, but provided the framework you were using continued to live on, there was a reasonable migration path.","raw":"---\nlayout: post\ntitle:  Upgrading a Real-World MVC 5 Application to MVC 6\ndate: 2015-07-23T12:08:42-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/upgrading-a-real-world-mvc-5-application-to-mvc-6/\n---\n\nThese are exciting times for web development on the Microsoft stack, but perhaps a little confusing as well. For many years the cycle of moving from one solution and project system to the next hasn't been overly complex. Sure, there have been breaking changes, I've felt those pains myself, but provided the framework you were using continued to live on, there was a reasonable migration path.\n\n<!--more-->\n\nMoving to MVC 6 is going to be a big shift for a lot of development teams, but that doesn't mean it needs to be scary, complicated or introduce instability into your project.\n\nIt does, however, mean that you're going to need an attitude of learning, that you'll pick up some new tooling, you'll have to brush up on your JavaScript and work with some new concepts.\n\n## Let's Make it Happen\n\nI'm super excited to now be part of the excellent crew at [Clear Measure][1], where this type of attitude seems to be fostered, encouraged and embodied by other members of the team and, more importantly, the management.\n\nWe're now undertaking the process of converting from MVC5 => MVC6 with our [Bootcamp workshop project][2] and I have the privilege of blogging my experience with it as I go. <img style=\"margin: 5px 0px 10px 10px; float: right; display: inline; background-image: none;\" title=\"image\" src=\"http://jameschambers.com/wp-content/uploads/2015/07/image_thumb2.png\" alt=\"image\" width=\"244\" height=\"173\" align=\"right\" border=\"0\" scale=\"0\" />We're going to keep the project building and operable as we go, such that at an point it can be shipped to production or branched for feature development.  We'll be using GitFlow, feature branches, continuous integration and continuous deployment.  Our check-ins will be code that builds cleanly with passing tests.\n\n**And,** for those of you who come join in our our MVC Masters Bootcamp sessions, you'll also get to work on this code base with all the tools, exposure to pair programming, a dedicated product owner and 3 days of intense coding.\n\n<blockquote><a href=\"http://clear-measure.com/\" onclick=\"_gaq.push(['_trackEvent', 'outbound-article', 'http://clear-measure.com/', '']);\" target=\"_blank\"><img style=\"margin: 14px 9px 7px 0px; border: 0px currentcolor; float: left; display: inline; background-image: none;\" title=\"image\" src=\"http://jameschambers.com/wp-content/uploads/2015/07/image8.png\" alt=\"image\" width=\"40\" height=\"39\" align=\"left\" border=\"0\" scale=\"0\"></a><strong>Shameless plug</strong>: If you want to level up your team of developers, please reach out to <a href=\"mailto:gina@clear-measure.com??Subject=MVC%20Masters%20Bootcamp\" target=\"_blank\">Gina Hollis</a> at Clear Measure to plan an on- or off-site event. We promise to melt your minds.</blockquote>\n\n## How We're Getting There\n\n<img style=\"margin: 5px 0px 10px 10px; float: right; display: inline; background-image: none;\" title=\"image\" src=\"http://jameschambers.com/wp-content/uploads/2015/07/image_thumb3.png\" alt=\"image\" width=\"244\" height=\"188\" align=\"right\" border=\"0\" scale=\"0\" />Well, to start it off, we're beginning with our initial commit as the MVC 5 project [Jeffrey Palermo's][7] been using in the Masters Bootcamp for some time.\n\nThe application is hosted on [GitHub][2] and you can [see the issues][8] that we're identifying and working through. We're doing the whole thing as open source in hopes that other teams can learn from what we learn in the process.\n\nAnd, as I knock items off the issue list I'll be posting about them here, covering the challenges, pitfalls and wins we encounter along the way. You can bookmark this post for updates in the project. Feel free to ask questions on the issues in the repository, or ping me on Twitter ([@CanadianJames][9]).\n\nStay tuned!\n\n[1]: http://clear-measure.com/\n[2]: https://github.com/ClearMeasureLabs/ClearMeasureBootcamp\n[3]: http://jameschambers.com/wp-content/uploads/2015/07/image_thumb2.png \"image\"\n[4]: http://jameschambers.com/wp-content/uploads/2015/07/image8.png \"image\"\n[5]: mailto:gina@clear-measure.com??Subject=MVC%20Masters%20Bootcamp\n[6]: http://jameschambers.com/wp-content/uploads/2015/07/image_thumb3.png \"image\"\n[7]: https://twitter.com/jeffreypalermo\n[8]: https://github.com/ClearMeasureLabs/ClearMeasureBootcamp/issues\n[9]: https://twitter.com/CanadianJames/\n  ","categories":[],"tags":[]},{"title":"Workaround: NuGet Packages Failing to Download in Visual Studio 2015 RTM","authorId":"james_chambers","slug":"workaround-nuget-packages-failing-to-download-in-visual-studio-2015-rtm","date":"2015-07-23 18:05:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/workaround-nuget-packages-failing-to-download-in-visual-studio-2015-rtm/","link":"","permalink":"https://westerndevs.com/_/workaround-nuget-packages-failing-to-download-in-visual-studio-2015-rtm/","excerpt":"I haven't figured out a common theme yet, but certain packages are failing to restore when you attempt to install them from the NuGet primary feed via the project.json file in Visual Studio 2015. Thanks to Brock Allen for confirming I wasn't going insane.","raw":"---\nlayout: post\ntitle:  \"Workaround: NuGet Packages Failing to Download in Visual Studio 2015 RTM\"\ndate: 2015-07-23T10:05:26-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/workaround-nuget-packages-failing-to-download-in-visual-studio-2015-rtm/\n---\n\nI haven't figured out a common theme yet, but certain packages are failing to restore when you attempt to install them from the NuGet primary feed via the project.json file in Visual Studio 2015. Thanks to [Brock Allen][1] for confirming I wasn't going insane.\n\n<!--more-->\n\nA couple of things I've discovered:\n\n* This seems to be more common for prerelease packages\n* It seems to work if the package has a previous release version (not in pre)\n\nAs a workaround, you can add the packages manually via the dialog in Visual Studio, just make sure you hit that pre-release flag:\n\n![image][2]\n\nIf that doesn't work for you â€“ sometimes I'm not seeing the package above in my feed â€“ if you have it you can add another NuGet feed to an alternate package source, like I've done here with AutoFac's nightly build feed:\n\n![image][3]\n\nThe other thing is that once you get it installed in your system cache, it will resolve it from there, which I imagine makes it harder to triage for anyone trying to figure out what's going on.\n\nI'm seeing various confirmations of this on Twitter:\n\n![image][4]\n\n![image][5]\n\nWith NuGet 3 being release (and part of VS 2015) I think some package authors are unsure if it's their problem or what the case may be. Depending on the method you come at it, it's possible that you can still get the package, but I would say it seems unpredictable right now.\n\n[1]: https://twitter.com/BrockLAllen\n[2]: http://jameschambers.com/wp-content/uploads/2015/07/image3.png \"image\"\n[3]: http://jameschambers.com/wp-content/uploads/2015/07/image_thumb1.png \"image\"\n[4]: http://jameschambers.com/wp-content/uploads/2015/07/image5.png \"image\"\n[5]: http://jameschambers.com/wp-content/uploads/2015/07/image6.png \"image\"\n  ","categories":[],"tags":[]},{"title":"Unit Conversions Done (Mostly) Right","authorId":"simon_timms","slug":"unit-conversions","date":"2015-07-23 00:30:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/unit-conversions/","link":"","permalink":"https://westerndevs.com/_/unit-conversions/","excerpt":"Thanks to a certain country which, for the purposes of this blog let's call it Backwardlandia, which uses a different unit system there is frequently a need to use two wildly different units for some value. And they have so many people and so much money that we can't ignore them.","raw":"---\nlayout: post\ntitle:  Unit Conversions Done (Mostly) Right\ndate: 2015-07-22T14:30:30-06:00\ncategories:\nexcerpt: Thanks to a certain country which, for the purposes of this blog let's call it Backwardlandia, which uses a different unit system there is frequently a need to use two wildly different units for some value. And they have so many people and so much money that we can't ignore them.\ncomments: true\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/07/22/unit_convesions/\n---\n\nThanks to a certain country which, for the purposes of this blog let's call it Backwardlandia, which uses a different unit system there is frequently a need to use two wildly different units for some value. Temperature is a classic one, it could be represented in Centigrade, Fahrenheit or Kelvin Rankine (that's the absolute temperature scale, same as Kelvin, but using Fahrenheit). Centigrade is a great, well devised unit that is based on the freezing and boiling points of water at one standard atmosphere. Fahrenheit is a temperature system based on the number of pigs heads you can fit in a copper kettle sold by some bloke on Fleet Street in 1832. Basically it is a disaster. Nonetheless Backwardlandia needs it and they have so many people and so much money that we can't ignore them. \n\n<!--more-->\n\nI cannot count the number of terrible approaches there are to doing unit conversions. Even the [real pros](http://www.cnn.com/TECH/space/9909/30/mars.metric.02/) get it wrong from time to time. I spent a pretty good amount of time working with a system that put unit conversions in between the database and the data layer in the stored procedures. The issue with that was that it wasn't easily testable and it meant that directly querying the table could yield you units in either metric or imperial. You needed to explore the stored procedures to have any idea what units were being used. It also meant that any other system that wanted to use this database had to be aware of the, possibly irregular, units used within. \n\nMoving the logic a layer away from the database puts it in the data retrieval logic. There could be a worse place for it but it does mean that all of your functions need to have the unit system in which they are currently operating passed into them. Your nice clean database retrievals become polluted with knowing about the units. \n\nIt would likely end up looking something like this:\n\n{% codeblock lang:csharp %}\npublic IEnumerable<Pipes> GetPipesForWell(int wellId, UnitSystem unitSystem)\n{\n    using(var connection = GetConnection()){\n    \tvar result = connection.Query<Pipes>(\"select id, boreDiameter from pipes where wellId=@wellId\", new { wellId});\n        return NormalizeForUnits(result, unitSystem);\n    }\n}\n{% endcodeblock %}\n\nI've abstracted away some of the complexity with a magic function that accounts for the units and it is still a complex mess. \n\n##A View Level Concern\nI believe that unit conversion should be treated as a view level concern. This means that we delay doing unit conversions until the very last second. By doing this we don't have to pass down the current unit information to some layer deep in our application. All the data is persisted in a known unit system(I recommend metric) and we never have any confusion about what the units are. This is the exact same approach I suggest for dealing with times and time zones. Everything that touches my database or any persistent store is in a common time zone, specifically UTC. \n\nIf you want to feel extra confident then stop treating your numbers as primitives and treat them as a value and a unit.  Just by having the name of the type contain the unit system you'll make future developers, including yourself, think twice about what unit system they're using.\n\n{% codeblock lang:csharp %}\npublic class TemperatureInCentigrade{\n\tprivate readonly double _value;\n\tpublic TemperatureInCentigrade(double value){\n    \t_value = value;\n    }\n    \n    public TemperatureInCentigrade Add(TemperatureInCentigrade toAdd) \n    {\n    \treturn new TemperatureInCentigrade(_value + toAdd.AsNumeric());\n    }\n}\n{% endcodeblock %}\n\nYou'll also notice in this class that I've made the value immutable. By doing so we save ourselves from a whole bunch of potential bugs. This is the same approach that functional programming languages take. \n\nHaving a complex type keep track of your units also protects you from taking illogical actions. For instance consider a unit that holds a distance in meters. The ```DistanceInMeters``` class would likely not contains a ```Multiply``` function or, if it did, the function would return ```AreaInSquareMeters```. The compiler would protect you from making a lot of mistakes and this sort of thing would likely eliminate a bunch of manual testing. \n\nThe actual act of converting units is pretty simple and there are numerous libraries out there which can do a very effective job for us. I am personally a big fan of the [js-quantities library](https://github.com/gentooboontoo/js-quantities). This lets you push your unit conversions all the way down to the browser. Of course math in JavaScript can, from time to time, be flaky. For the vast majority of non-scientific applications the level of resolution that JavaScripts native math supports is wholly sufficient. You generally don't even need to worry about it.\n\nIf you're not doing a lot of your rendering in JavaScript then there are libraries for .net which can handle unit conversions (disclaimer, I stole this list from the github page for QuantityType and haven't tried them all). \n\n- [Quantity Types](https://github.com/objorke/QuantityTypes)\n- [CSUnits](https://github.com/cureos/csunits)\n- [NGenericDimensions](https://ngenericdimensions.codeplex.com/)\n- [quantities.net](http://sourceforge.net/projects/quantitiesnet/)\n- [unitcon](http://sourceforge.net/projects/unitcon/)\n- [units](http://www.gnu.org/software/units/)\n- [Measurement Unit Conversion Library](http://www.codeproject.com/Articles/23087/Measurement-Unit-Conversion-Library)\n- [Units of Measure Library](http://www.codeproject.com/Articles/404573/Units-of-Measure-Library-for-NET)\n- [Units of Measure Validator for C#](http://www.codeproject.com/Articles/413750/Units-of-Measure-Validator-for-Csharp)\n- [Working with Units and Amounts](http://www.codeproject.com/Articles/611731/Working-with-Units-and-Amounts)\n- [Units.NET](https://github.com/InitialForce/UnitsNet)\n- [Gu.Units](https://github.com/JohanLarsson/Gu.Units)\n- [Unit Class Library](https://bitbucket.org/Clearspan/unit-class-library/wiki/Home)\n\nOtherwise this might be a fine time to try out F# which supports units of measure [natively](https://msdn.microsoft.com/en-us/library/dd233243.aspx).\n\nThe long and short of it is that we're trying to remove unit system confusion from our application and to do that we want to expose as little of the application to divergent units as possible. Catch the units as they are entered, normalize them and then pass them on to the rest of your code. You'll save yourself a lot of headaches by taking this approach, trust a person who has done it wrong many times.\n","categories":[],"tags":[]},{"title":"Response From Postsharp.net Is Not a Valid Nuget V2 Service Response","authorId":"james_chambers","slug":"response-from-postsharp-net-is-not-a-valid-nuget-v2-service-response","date":"2015-07-22 22:16:30+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/response-from-postsharp-net-is-not-a-valid-nuget-v2-service-response/","link":"","permalink":"https://westerndevs.com/_/response-from-postsharp-net-is-not-a-valid-nuget-v2-service-response/","excerpt":"After installing PostSharp.net on my machine for a project (I did the MSI install) I started getting errors during the package restore that ended up blocking my builds. They looked a lot like this: Error: FindPackagesById: EntityFramework.Core Response from https://www.postsharp.net=&quot;&quot; nuget=&quot;&quot; packages=&quot;&quot; findpackagesbyid()?id=&quot;EntityFramework.Core&quot; is not a valid NuGet v2 service response.","raw":"---\nlayout: post\ntitle:  Response From Postsharp.net Is Not a Valid Nuget V2 Service Response\ndate: 2015-07-22T14:16:30-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/response-from-postsharp-net-is-not-a-valid-nuget-v2-service-response/\n---\n\nAfter installing PostSharp.net on my machine for a project (I did the MSI install) I started getting errors during the package restore that ended up blocking my builds. They looked a lot like this:\n\n> Error: FindPackagesById: EntityFramework.Core Response from https://www.postsharp.net=\"\" nuget=\"\" packages=\"\" findpackagesbyid()?id=\"EntityFramework.Core\" is not a valid NuGet v2 service response.\n\n<!--more-->\n\n![image][1]\n\nNow, an important note here: I'm on a machine that's seen various updates and changes to VS 2015, and this was a version of PostSharp that wasn't originally built for the RTM version of Visual Studio. Soâ€¦this may be entirely circumstantial, but it's what I ran into.\n\nAnd it wasn't just on that one package (others would give the same result) and it wasn't just on one project. I tried to isolate this, but couldn't find the source. Why was PostSharp getting in the way of my package restore? Even using DNU from the command line, **_after_** I explicitly uninstalled it? I started setting compiler variables to block PostSharp on those projects, but that got frustrating quickly, so I resorted to uninstalling everything I could find of it.\n\nAfter the uninstall, I still was stumped, same errors all over again. With the help of my friend [Donald Belcham][2], I was able to find traces of PostSharp still on my machine, located in the system-wide NuGet package source feed configuration:\n\n![image][3]\n\nUnchecking that box above does the trick.\n\nMight be an edge case if you run into this, but if you do, and this helps, consider buying Don a scotch!\n\nHappy coding. ![Smile][4]\n\n[1]: http://jameschambers.com/wp-content/uploads/2015/07/image1.png \"image\"\n[2]: https://twitter.com/dbelcham\n[3]: http://jameschambers.com/wp-content/uploads/2015/07/image2.png \"image\"\n[4]: http://jameschambers.com/wp-content/uploads/2015/07/wlEmoticon-smile1.png\n","categories":[],"tags":[]},{"title":"DevOps","slug":"podcast-devops","date":"2015-07-21 19:55:26+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-devops/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-devops/","excerpt":"Continuing the theme of making grandiose claims about vaguely defined terms, the Western Devs take on the Rise of DevOps","raw":"---\nlayout: podcast\ntitle:  \"DevOps\"\ndate: 2015-07-21T11:55:26-04:00\nrecorded: 2015-07-17\ncategories: podcasts\nexcerpt: \"Continuing the theme of making grandiose claims about vaguely defined terms, the Western Devs take on the Rise of DevOps\"\ncomments: true\npodcast:\n    filename: \"DevOps.mp3\"\n    length: \"51:55\"\n    filesize: 62319361\n    libsynId: 5291994\n    anchorFmId: DevOps-evqdin\nredirect_from: \"/whatwevesaid/podcast-devops/\"\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - kyle_baley\n    - dave_white\n    - james_chambers\n    - simon_timms\n    - dylan_smith\nlinks:\n    - \"DevOps on Wikipedia|https://en.wikipedia.org/wiki/DevOps\"\n    - \"What is DevOps|http://theagileadmin.com/what-is-devops/\"\n    - \"How DevOps is killing the developer|https://jeffknupp.com/blog/2014/04/15/how-devops-is-killing-the-developer/\"\n    - \"AppInsights|https://azure.microsoft.com/en-us/documentation/articles/app-insights-get-started/\"\n    - \"NewRelic|http://newrelic.com/\"\n    - \"statsd|https://github.com/etsy/statsd\"\n    - \"LogEntries|https://logentries.com/\"\n    - \"MixPanel|https://mixpanel.com/\"\n    - \"Octopus|https://octopusdeploy.com/\"\n    - \"Docker|https://www.docker.com/\"\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\nalias: /whatwevesaid/podcast-devops/\n---\n\n### Synopsis\n\n* DevOps: Person, role, culture, methodology, or movement?\n* The importance of collaboration and communication\n* Automation, continuous delivery/continuous deployment: are they required for DevOps?\n* What happens to your code after you commit?\n* Attitude defines your culture, not the other way around\n* DevOps is *not* a developer that does operational tasks\n* Is the backlash coming? Lessons from Agile\n* An incremental approach to implementing DevOps practices in an organization\n* Security benefits of being able to release often\n* Metrics, logging, statistics, monitoring: what happens after you release?\n* Is quality dead? If so, can DevOps fix it?\n* Does DevOps change how you code? No-downtime deployments, A/B testing and feature toggles\n* Enabling smaller deployments that use external services\n* How DevOps practices make CQRS, eventing, and microservices easier\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Cancelling Long Running Queries in ASP.NET MVC and Web API","authorId":"dave_paquette","slug":"cancelling-long-running-queries-in-asp-net-mvc-and-web-api","date":"2015-07-20 22:11:50+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/cancelling-long-running-queries-in-asp-net-mvc-and-web-api/","link":"","permalink":"https://westerndevs.com/_/cancelling-long-running-queries-in-asp-net-mvc-and-web-api/","excerpt":"A lot has been written about the importance of using async controller actions and async queries in MVC and Web API when dealing with long running queries. If done properly, it can hopefully improve throughput of your ASP.NET applications. While async won't solve the problem of your database being a bottleneck, it can help to ensure that your web server is still able to process other smaller/shorter requests. It will especially ensure requests that do not require access to that database can be processed in a timely fashion.","raw":"---\nlayout: post\ntitle:  Cancelling Long Running Queries in ASP.NET MVC and Web API\ndate: 2015-07-20T14:11:50-04:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/07/19/cancelling-long-running-queries-in-asp-net-mvc-and-web-api.aspx\n---\n\n\nA lot has been written about the importance of using [async controller actions][1] and [async queries][2] in MVC and Web API when dealing with long running queries. If done properly, it can hopefully [improve throughput][3] of your ASP.NET applications. While async won't solve the problem of your database being a bottleneck, it can help to ensure that your web server is still able to process other smaller/shorter requests. It will especially ensure requests that do not require access to that database can be processed in a timely fashion.\n\n<!--more-->\n\nThere is one very important aspect that is often missed in the tutorials that talk about async and that is cancellation.\n\n> NOTE: For the purpose of this article, I am referring to long running queries in terms of read queries (those that are returning data but not modifying data). Cancelling queries that have modified data might not be a good choice for your application. Do you really want to cancel a Save because the user navigated to another page in your application? Maybe you do but probably not. Aside from the data issue, this also won't likely help performance because the database server will need to rollback that transaction which could be a costly operation.\n\n## What is cancellation and how does it work?\n\nCancellation is a way to signal to an async task that it should stop doing whatever it happens to be doing. In .NET, this is done using a CancellationToken. An instance of a cancellation token is passed to the async task and the async task monitors the token to see if a cancellation has been requested. If a cancellation is requested, it should stop doing what it is doing. Most Async methods in .NET will have an overload that accepts a cancellation token.\n\nHere is a simple console application that illustrates how cancellation tokens work. In this example, a cancellation token is created (via a CancellationTokenSource) and passed along to an async task that does some work. When the user presses the 'z' key, the Cancel method is called on the CancellationTokenSource. This sets the IsCancellationRequested property to true for the token which will cause the async task to stop doing the work.\n\n{% codeblock lang:csharp %}\nstatic void Main(string[] args)\n{\n    Console.WriteLine(\"Silly counter: Press Z to Stop\");\n    var tokenSource = new CancellationTokenSource();\n    var cancellationToken = tokenSource.Token;\n    Task.Run(() =>\n    {\n        long n = 0;                \n        while (!cancellationToken.IsCancellationRequested)\n        {\n            Console.WriteLine(n);\n            n = n + 1;\n        }\n    }, cancellationToken);\n \n    while (true)\n    {            \n        if (Console.Read() == 'z')\n        {\n            tokenSource.Cancel();                  \n        }\n    }            \n}\n{% endcodeblock %}\n\n## Why is this so important?\n\nThink about this in the context of a long-running query in a web application. Somewhere on the other side of this long running query is a frustrated user. WHY IS THIS TAKING SO LONG???  What is that user likely to do? Will they sit there diligently waiting for a query to finish running? The longer the query is running, the less likely the user will wait. It is likely the user will give up and navigate to another page or they might hit the browser refresh button in hopes that it will load faster next time.\n\nSo, what happens now you have a query running on the server and the user has moved on to another page (or another site all together). Unfortunately, both your database server and web server will continue processing the request, wasting resources executing a query that likely no one cares about anymore.\n\n## Cancellation and the Browser\n\nWhen I was first looking in to this, I assumed there was no way in my MVC controller to be notified when the user has moved to another page. I turns out I was wrong. If a user navigates to a new page or refreshes the browser, any HTTP Requests that are in progress will be cancelled.\n\nHere is an example where the user visited the MyReallySlowReport page. After waiting for nearly 5 seconds, they gave up and went to the Contact page (_I assume to look for a phone number to call and complain about how slow that report is_). You can see the status of the original page request is _canceled_.\n\n![image][4]\n\nBoth MVC and Web API will recognize the cancelled request and signal a cancellation to the async action method for that request.  All you need to do for this to work is add a CancellationToken parameter to your controller action method and pass that token on to whatever async task is doing the work. In this case, Entity Framework:\n\n{% codeblock lang:csharp %}\npublic async Task MyReallySlowReport(CancellationToken cancellationToken)\n{\n    List<ReportItem> items;\n    using (ApplicationDbContext context = new ApplicationDbContext())\n    {\n        items = await context.ReportItems.ToListAsync(cancellationToken);\n    }\n    return View(items);\n}\n{% endcodeblock %}\n\nIf your action method already has parameters, just add the CancellationToken parameter to the end of your parameter list. That's it. Now if the browser cancels the HTTP Request, MVC will set the CancellationToken to cancelled and Entity Framework will cancel the SQL query that was executing as part of that request.\n\nThat was easy! One simple change to make sure there are fewer server resources wasted processing canceled requests.\n\n## Cancelling Additional Work\n\nOkay, so it's easy to cancel the SQL server request, but what if we were doing some long running task ourselves in the controller method. In that case all we need to do is check the IsCancellationRequested property of the cancellation token and stop the processing if it is set to true.\n\n{% codeblock lang:csharp %}\npublic async Task MyReallySlowReport(CancellationToken cancellationToken)\n{\n    List<ReportItem> items;\n    using (ApplicationDbContext context = new ApplicationDbContext())\n    {\n        items = await context.ReportItems.ToListAsync(cancellationToken);\n    }\n   \n    foreach (var item in items)\n    {\n        if (cancellationToken.IsCancellationRequested)\n        {\n            break;\n        }\n        //Do some fairly slow operation\n    }\n    return View(items);\n}\n{% endcodeblock %}\n\nBy exiting the for loop when a cancellation is requested, we can avoid using server CPU resources for HTTP requests that no longer matter.\n\n## SPAs and Ajax Requests\n\nIf you are working in a Single Page Application, your app will likely spawn a number of ajax requests to get data from the server. In this case, the browser will not automatically cancel requests when the user navigates to another 'page' in your app. That's because you are handling page navigation yourself in JavaScript rather then using traditional web page navigation. In a single page app, you will need to cancel HTTP requests yourself. This is usually fairly easy to do. For example in jQuery all you need to do is call the abort() method on the XMLHttpRequest instance:\n\n{% codeblock lang:javascript %}\nvar xhr = $.get(\"/api/myslowreport\", function(data){\n  //show the data\n});\n \n//If the user navigates away from this page\nxhr.abort();\n{% endcodeblock %}\n\nYou will of course need to tie in to the page/component lifecycle of whatever framework you are using. This varies a lot from framework to framework so I won't specifically cover it here.\n\n## UPDATE: Making this work in MVC 5\n\nA huge thank you to [Muhammad Rehan Saeed](https://disqus.com/by/RehanSaeedUK/) for pointing out that in MVC 5, the cancellation token is not actually being signaled when the browser cancels the request. Everything works as expected in both MVC 6 and in Web API, but for some reason, MVC 5 only supports cancellation if you use the AsyncTimeout attribute. I was able to reproduce this and even cloned to MVC 5 / Web API and MVC 6 repositories to confirm that the implementation are in fact different.\n\nI did find a workaround that should achieve desired results. It involves grabbing the ClientDisconnectedToken from the Response property and creating a linked token source.\n\n{% codeblock lang:csharp %}\npublic async Task<ActionResult> MyReallySlowReport(CancellationToken cancellationToken)\n{\n     CancellationToken disconnectedToken = Response.ClientDisconnectedToken;            \n     var source = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken, disconnectedToken);\n\n    List<ReportItem> items;\n    using (ApplicationDbContext context = new ApplicationDbContext())\n    { \n        items = await context.ReportItems.ToListAsync(source.Token);\n    }\n    return View(items);\n}\n{% endcodeblock %}\n\nUsing this workaround, the linked token will be signaled when the browser cancels the request and also if a timeout has expired if you choose to use the AsyncTimeout attribute.\n\n## Conclusion\n\nIf you are using MVC 5, Web API or MVC 6 in combination with any modern data access layer, it should be extremely easy to pass your cancellation token and cancel long running queries when a request from the client is aborted. This is a simple approach that can help to avoid situations where a small number of users can accidentally overload your web server and database server.\n\n[1]: http://www.asp.net/mvc/overview/getting-started/getting-started-with-ef-using-mvc/async-and-stored-procedures-with-the-entity-framework-in-an-asp-net-mvc-application\n[2]: https://msdn.microsoft.com/en-us/data/jj819165\n[3]: https://channel9.msdn.com/Events/TechEd/NorthAmerica/2013/DEV-B337\n[4]: http://www.davepaquette.com/wp-content/uploads/2015/07/image_thumb.png \"image\"\n","categories":[],"tags":[]},{"title":"Extracting a Service to Interact With Azure Table Storage","authorId":"james_chambers","slug":"extracting-a-service-to-interact-with-azure-table-storage","date":"2015-07-20 20:06:03+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/extracting-a-service-to-interact-with-azure-table-storage/","link":"","permalink":"https://westerndevs.com/_/extracting-a-service-to-interact-with-azure-table-storage/","excerpt":"In this series we are looking at the basic mechanics of interacting with cloud-based Table Storage from an MVC 5 Application, using the Visual Studio 2013 IDE and Microsoft Azure infrastructure.","raw":"---\nlayout: post\ntitle:  Extracting a Service to Interact With Azure Table Storage\ndate: 2015-07-20T12:06:03-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/day-3-extracting-a-service-to-interact-with-azure-table-storage/\n---\n\n_In [this series][1] we are looking at the basic mechanics of interacting with cloud-based Table Storage from an MVC 5 Application, using the Visual Studio 2013 IDE and Microsoft Azure infrastructure._\n\n<!--more-->\n\nOur controllers are not supposed to be about anything more than getting models together so that our views have something to present. When we start mixing concerns, our application starts to become very difficult to test, controllers start getting quite complex and the difficulty in maintaining our application can skyrocket.\n\nLet's avoid that.\n\n> If you want to follow along, please [pop into Azure][2] and make sure you've got an account ready to go. The trial is free, so get at it!\n\n## Defining Our Service\n\nLet's look at the operations we're going to need, from what we've already implemented, and knowing what we're planning from [our outline][1]:\n\n* Insert a record\n* Get a filtered set of records\n* Update a record\n* Deleting a record\n\nCool beans. At first blush it seems like we've got a pretty simple set of concerns, but notice that I didn't include things like \"connecting to Azure\", \"creating table references\" or \"reading configuration information\", as those are all separate concerns that our controller doesn't actually care about.  Remember, we're trying to isolate our concerns.\n\nHrmâ€¦so, manipulating records, adding, deleting, filtering, separating concerns from our business logicâ€¦this is starting to sound familiar, right? \n\n> Use a **repository** to separate the logic that retrieves the data and maps it to the entity model from the business logic that acts on the model. The business logic should be agnostic to the type of data that comprises the data source layer. Source: [MSDN][3].\n\nSo, we're going to want to build something using the repository pattern. We'll use that repository here in our application's controllers, but in a larger project you might go even further to have an application services layer where you map between the domain models and the view models that we have in our views. All in, our interface might look like the following:\n\n{% codeblock lang:csharp %}\npublic interface ITableStorageRepository where T : TableEntity\n{\n    void Insert(T entity);\n    void Update(T entity);\n    void Delete(T entity);\n    IEnumerable GetByPartition(string partitionKey);\n}\n{% endcodeblock %}\n\nThe public interface simply gives us a way to do our CRUD operations and treat a filtered result set [as a collection][4] in order to minimize duplicate constructs and operations related to table queries. You can think of the CloudTableClient and TableQueries from the Azure SDK as part of a [Data Mapper][5] layer that enables us to build this abstraction.\n\n> Note: For the purpose of illustration, I'm going to continue to use TableEntity here, which doesn't completely abstract the Azure Table Storage concern away from my controller. I understand that; in a real-world scenario, I would typically have a view model that is used in the MVC application and an intermediary service would handle mapping as required.\n\n## Implementing the Service\n\nLeveraging this is going to be awesome, but we need to move some of the heavy-lifting out of our controller first.  Let's start by creating a Repositories folder and adding a class called KittehRepository, which will of course implement ITableStorageRepository.\n\n**Don't peek!** As an exercise for the reader, you can use the interface noted above to implement the class. Use the interface above to craft your KittehRepository class. You should be able to find all the bits you need by exploring the objects already in use in the controller.\n\nWhen you're ready, here's my version of the solution below:\n\n{% codeblock lang:csharp %}\npublic class KittehRepository : ITableStorageRepository\n{\n    private readonly CloudTableClient _client;\n\n    public KittehRepository()\n    {\n        var storageAccount = CloudStorageAccount.Parse(CloudConfigurationManager.GetSetting(\"StorageConnectionString\"));\n        _client = storageAccount.CreateCloudTableClient();\n    }\n\n    public void Insert(KittehEntity entity)\n    {\n        var kittehTable = _client.GetTableReference(\"PicturesOfKittehs\");\n        var insert = TableOperation.Insert(entity);\n        kittehTable.Execute(insert);\n    }\n\n    public void Update(KittehEntity entity)\n    {\n        var kittehTable = _client.GetTableReference(\"PicturesOfKittehs\");\n        var insert = TableOperation.Replace(entity);\n        kittehTable.Execute(insert);\n    }\n\n    public void Delete(KittehEntity entity)\n    {\n        var kittehTable = _client.GetTableReference(\"PicturesOfKittehs\");\n        var insert = TableOperation.Delete(entity);\n        kittehTable.Execute(insert);\n    }\n\n    public IEnumerable GetByPartition(string partitionKey)\n    {\n        var kittehTable = _client.GetTableReference(\"PicturesOfKittehs\");\n        var kittehQuery = new TableQuery()\n                        .Where(TableQuery.GenerateFilterCondition(\"PartitionKey\", QueryComparisons.Equal, partitionKey));\n        var kittehs = kittehTable.ExecuteQuery(kittehQuery).ToList();\n\n        return kittehs;\n    }\n}\n{% endcodeblock %}\n\nOne thing to note is that I've pushed _most_ of the initialization up to the constructor, and I've not implemented any kind of seeding code. The table seeding that I illustrated in [Day 1][6] is a concern that should be implemented outside of a repository, likely as part of a process that starts up the application in first-run scenarios, or something that would be run as part of a deployment to a test/QA environment.\n\n## Cleaning up Our Controller\n\nI love this. The controller can now do what we need it to do. Here's the complete class, with an added constructor that accepts a reference to the repository (we'll wire that up shortly):\n\n{% codeblock lang:csharp %}\npublic class HomeController : Controller\n{\n    private readonly ITableStorageRepository _kittehRepository;\n\n    public HomeController(ITableStorageRepository kittehRepository)\n    {\n        _kittehRepository = kittehRepository;\n    }\n\n    public ActionResult Index()\n    {\n        var kittehs = _kittehRepository.GetByPartition(\"FunnyKittehs\");\n        return View(kittehs);\n    }\n\n    [HttpPost]\n    public ActionResult Index(KittehEntity entity)\n    {\n        _kittehRepository.Insert(entity);\n        return RedirectToAction(\"Index\");\n    }\n\n    public ActionResult About()\n    {\n        ViewBag.Message = \"Your application description page.\";\n\n        return View();\n    }\n\n    public ActionResult Contact()\n    {\n        ViewBag.Message = \"Your contact page.\";\n\n        return View();\n    }\n}\n{% endcodeblock %}\n\nNotice how we've reduce the amount of code in this class _significantly_, to the point that anyone should be able to read it â€“ with little or no exposure to Azure Table Storage â€“ and still have a sense of what's going on. We've taken our controller from over 50 lines of code (non-cruft/whitespace) to about 5.\n\nJust to see how much more clear we've made things, do a \"remove and sort\" on your usings. You'll notice that everything to do with Azure has all but disappeared; our repository has served it's purpose!\n\nOkay, so the repository is in place, and our controller is dramatically simplified. Now we need to do a bit of wiring to let the MVC Framework know that we'd like an instance of the class when the controller fires up. Here's how.\n\n## Adding Dependency Injection\n\nFirst, open the Package Manager Console (View â€“> Other Windows â€“> Package Manager Console) and type the following:\n\n    install-package Ninject.MVC5\n\nThe Ninject packages required for interoperation with the MVC Framework are installed, and you get a new class in _AppStart called NinjectWebCommon. This class contains an assembly-level attribute that allows it to properly wire dependency injection up in your application at startup, you'll see this at the top of the file.\n\nWhat happens now is quite interesting: when the MVC Framework tries to create an instance of a controller (i.e., when someone makes a request to your application), it looks for a constructor with no parameters. This no longer exists on our controller because we require the ITableStorageRepository parameter.  Ninject will now step in for us and say, \"Oh, you want something that looks like that? Here's one I made for you!\"\n\nTo get that kind of injection love, you need to go into the NinjectWebCommon class and update the RegisterServices method to include this line of code:\n\n    kernel.Bind>().To();\n\nThis simply says, \"When someone asks for that interface, give them this concrete class.\n\nSo at this point, the wiring is done, and you can run your app! It will have the exact same functionality and user experience, but it will be much more technically sound.\n\n## Notes and Improvements\n\nJust a few things to note:\n\n* I've kept things simple and omitted ViewModels\n* You'd likely want to have a layer between your controller and repository class in most real-word scenarios\n* The repository class should have it's dependencies injected as well, namely, the configuration information it needs to connect to Azure. A proper configuration helper class would do the trick and, once registered with Ninject, could also be accepted as a parameter on the constructor\n\n## Summary\n\nWith the repository in place we can now lighten the load on the controller and more easily implement features with our concerns more clearly separated. In the next couple of posts, we're going to start allowing the user to manipulate the entities in the table.\n\nHappy coding! ![Smile][7]\n\n[1]: http://jameschambers.com/2015/01/day-0-8-days-of-working-with-azure-table-storage-from-asp-net-mvc-5/\n[2]: http://www.microsoft.com/click/services/Redirect2.ashx?CR_CC=200575119\n[3]: https://msdn.microsoft.com/en-us/library/ff649690.aspx\n[4]: http://martinfowler.com/eaaCatalog/repository.html\n[5]: http://martinfowler.com/eaaCatalog/dataMapper.html\n[6]: http://jameschambers.com/2015/01/day-1-the-basics-of-the-basics-with-azure-table-storage/\n[7]: http://jameschambers.com/wp-content/uploads/2015/07/wlEmoticon-smile.png\n  ","categories":[],"tags":[]},{"title":"Discussion: Hosting git in-house","slug":"discussion-hosting-git-in-house","date":"2015-07-16 16:46:11+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/discussion-hosting-git-in-house/","link":"","permalink":"https://westerndevs.com/_/discussion-hosting-git-in-house/","excerpt":"We had a good discussion on Slack recently on getting a client up and running with Git. For many, the decision boils down to: GitHub or BitBucket. In this case, the repositories need to be stored in-house. Thus sparking the first debate.","raw":"---\nlayout: post\ntitle:  \"Discussion: Hosting git in-house\"\ndate: 2015-07-16T08:46:11-04:00\ncategories: \ncomments: true\nslack: true\n---\n\nWe had a good discussion on Slack recently on getting a client up and running with Git. For many, the decision boils down to: GitHub or BitBucket. In this case, the repositories need to be stored in-house. Thus sparking the first debate.\n\n<!--more-->\n\n### In-house vs. hosted\n\nDespite the strong cultural shift, some companies are still wary about having assets stored in frozen droplets of water floating in the atmosphere. Hosting in-house feels safer. What if GitHub goes down? What if VSO gets hacked? WHERE IN THE &*%$# IS MY DATA?!?\n\nThere's a psychological aspect to hosting yourself that's related to these fears. If it's on your server and it goes down, that's on you, the organization. And in many organizations, they can shoulder that blame better than if a provider goes down or comes under attack. In fact, in some respects, the more incompetent your IT department, the easier it is to bear the burden of downtime as the users will be accustomed to it. (Side note: it would be interesting to see if there's a correlation between an IT department's effectiveness and whether they host their own source code.)\n\nIn-house source code hosting can be done, obviously. It *has* been done for years. But with the proliferation of online options, there are considerations to...ummm...consider. The cost of setting up and maintaining a production-grade git server for one thing. This is your source code so it can't be running on a cast-off laptop in the basement. It needs a proper back-up schedule complete with testing the restore process. Uptime will be important as will latency. The server must be supported by the entire IT infrastructure, not just a couple of developers who know git. That means it has to be considered during OS upgrades and patches.\n\nThis isn't meant to deter people from hosting in-house but to outline the variables when you're deciding whether to do it. There remain good reasons whereby the cost of doing it yourself is mitigated by other factors. Legal requirements of where the data resides, perhaps. Or a staunch corporate/government policy that will cost too much to rebel against.\n\n### Windows/Linux\n\nAnother constraint: no Linux. This is a tough one and we wrestled with it quite a bit. Ignoring the legitimacy of the constraint, we discussed a few options. [Bonobo](https://bonobogitserver.com/) for one. It's an open-source Git server written in C# that runs in IIS. That sentence alone was enough to give us some pause. Not that we're against open source C# web projects. But through \"git\" in there and it starts to sound more like the result of someone's weekend thought experiment.\n\nProblem is: there aren't many other options once you remove Linux from the equation. It is apparently possible to [host git repositories on a Windows server directly](http://blog.chronosinteractive.com/posts/using-windows-server-host-git-repository). If you read through the steps though, it seems like you may as well be doing it on Linux.\n\nAt one point, we gave serious consideration to running a git server on Docker on Windows. Until someone pointed out the obvious: for the time being, at least, it's still a Linux VM behind the scenes.\n\nGitHub for Enterprise will [install on Windows](https://help.github.com/enterprise/2.2/admin/guides/installation/installing-github-enterprise-on-hyper-v/). But only on Hyper-V. And they don't make it clear but this strongly suggests that it's on a VM, almost certainly Linux. They also provide an image for VMWare.\n\nThat leaves [BitBucket's Stash](https://www.atlassian.com/software/stash/). It requires a JVM and TomCat but looks very much like it meets the \"Windows only\" server criteria. That said, we also have anecdotal evidence that it is a considerable drain on resources.\n\n### The \"winner\"\n\nIn the end, given the constraints, we settled (begrudgingly) on TFS as the best solution. It runs on Windows and hosts git. TFS comes with its own issues, not least of which is the cost. Whether or not that cost offsets the perceived \"benefit\" of hosting in-house remains to be seen. But that is now a business decision rather than a technical one.","categories":[],"tags":[]},{"title":"Three Types of Relationships You Need to Survive as a Remote Worker","authorId":"james_chambers","slug":"three-types-of-relationships-you-need-to-survive-as-a-remote-worker","date":"2015-07-15 06:44:35+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/three-types-of-relationships-you-need-to-survive-as-a-remote-worker/","link":"","permalink":"https://westerndevs.com/_/three-types-of-relationships-you-need-to-survive-as-a-remote-worker/","excerpt":"Getting through your work day on your own is easy enough. Over the long run, though, you're going to need to have some solid relationships in your life to help get you through the rough patches and pick you up from the falls, but more importantly, to be there when it's time to celebrate the wins.","raw":"---\nlayout: post\ntitle:  Three Types of Relationships You Need to Survive as a Remote Worker\ndate: 2015-07-14T22:44:35-04:00\ncategories:\ncomments: true\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/07/three-types-of-relationships-you-need-to-survive-as-a-remote-worker/\n---\n\nGetting through your work day on your own is easy enough.  Over the long run, though,  you're going to need to have some solid relationships in your life to help get you through the rough patches and pick you up from the falls, but more importantly, to be there when it's time to celebrate the wins.\n\n<!--more-->\n\nWhile these are going to seem obvious at first, I want to make the point that they're by no means effortless. In fact, some of the closest connections in your life can be the most affected by your choice to have your home serve also as your place of work.\n\n## Your Co-Workers\n\n<img src=\"http://jameschambers.com/wp-content/uploads/2015/07/office_relationships_thumb.jpg\" style=\"float: right;\" />\nIn the movie _Office Space_, the main character Peter Gibbons pines that his motivation to work, or rather, to \"work just hard enough not to get fired,\" stems from unhealthy relationships with management. This **cannot** be your reality as a remote worker, and you need to make efforts to establish (and maintain) trust with your boss and teammates.\n\nI have worked in blended environments where there was a head office with regular staff, but many remote workers and many office workers who had the option to work from home as they elected to do so. Being a permanent remote with very little office time (averaging about a day a month in the office) it was harder to get to know folks, but I knew it was really on me to own it.\n\nBefore you start at a company as a remote worker, or before you start working from home, talk to the management and other staff that already work remotely to see what it is like. If the atmosphere supports it, flying solo can be a great experience, but you still need the support of ground control. If you work as the only remote worker, or if management doesn't trust or understand how productivity can work remotely, it may not be the right time for you to engage in flight.\n\n## Fellow Remote Workers\n\nThese ones are pretty important, especially in a company where there is a block of folks that work full-time at the office and a group that works remotely. Why are these folks good to know?\n\nBecause they _get ya_. They're on the Skypes. They're on the Slack.\n\nThey are trying to do the same things that you are doing and likely face the same challenges as you do. They look for solutions and have found tools that help them avoid the pitfalls. You've likely had to work through something that they haven't yet, you have it figured out and it's great to be able to share that with them.\n\nDon't be afraid to share your failures or ask for help! Being a good remote worker means mentoring and being mentored by other remote workers so that collectively we can all be really good at it.\n\n## People With Absolutely Nothing to do with Work\n\nAhhhâ€¦five o'clock, am I right? It's that time when you disconnect from work and start to enjoy the more meaningful things in life. Of course, that means that you have be successful in leaving work in the first place, which can be tricky, but [isn't impossible to do][2], especially if you've [put in a good day][3].\n\nThere a measure of counter-intuitiveness here that you'll find. For example, keeping strong relationships in your own household is actually more about defining and maintaining boundaries during the workday than you might imagine. For example, if your office door is open as you're working and your spouse, kids or roommates feel free to come and chat, you're going to be less productive than you'd like to be. Over time, that loss of focus and reduced ability to create tangible outputs are going to start to turn into stress. Allowing the non-work relationships to bleed into your day can be a form of long term toxin that will erode your success as a remote worker.\n\nLikewise, allowing your work day to bleed into your evenings and weekends will rob you of the best part of your day. Maybe you're married with kids, fiancÃ©ed, or perhaps single, but the fact holds true regardless of your situation: marginalizing your family and friends to try to improve things at work will yield exactly zero positive results for your personal life. If you don't agree with that statement, it's simply because Dr. Seuss got the idea of the Grinch's heart being two sizes too small from your life story.\n\nThere will be exceptions, where some rule-bending and time-bleeding will occur, but your job as a remote is to maintain that balance and be honest with yourself about how you're doing in that regard.\n\n## An Old Adage\n\nI do have to admit that I'm quite fortunate in this regard, as I've been able to align with an employer who is very much accustomed to and accepting of remote work. Working remotely, while becoming more common, isn't yet universally accepted and I know from personal experience that if the culture isn't there in the company, it can't work on the long term.\n\nIt's long and often been said that **it takes a village to raise a child**, and the reality of that statement is that at some point we have to grow up and be part of the village. As a remote worker, you need to remember that **community isn't going to just happen for you**. Sure, you may not be raising kids, but the outcomes of your work efforts can only be best realized if you're able to establish some great relationships along the way.\n\n[2]: http://jameschambers.com/2015/03/working-from-home-and-walking-to-work-surviving-remote-work/\n[3]: http://jameschambers.com/2015/03/wake-up-and-get-st-done-a-practice-of-awesome/\n  ","categories":[],"tags":[]},{"title":"Microservices, or \"How to spread the love\"","authorId":"kyle_baley","slug":"microservices-or-how-to-spread-the-love","date":"2015-07-15 05:01:19+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microservices-or-how-to-spread-the-love/","link":"","permalink":"https://westerndevs.com/_/microservices-or-how-to-spread-the-love/","excerpt":"For some time, people have been talking about microservices. I say &quot;some time&quot; for two reasons: 1) It's a good opening line, and 2) I have no clue how long people have been talking about them. I just heard the term for the first time about four months ago. So if I start talking about them now, while I still know virtually nothing, I can get at least two more future posts on the subject talking about how I was doing it wrong in the beginning.","raw":"---\nlayout: post\ntitle:  Microservices, or \"How to spread the love\"\ndate: 2015-07-14T21:01:19-04:00\ncategories:\ncomments: true\nauthorId: kyle_baley\noriginalurl:\n---\n\nFor some time, people have been talking about [microservices](https://www.nginx.com/blog/introduction-to-microservices/). I say \"some time\" for two reasons: 1) It's a good opening line, and 2) I have no clue how long people have been talking about them. I just heard the term for the first time about four months ago. So if I start talking about them now, while I still know virtually nothing, I can get at least two more future posts on the subject talking about how I was doing it wrong in the beginning.\n\n<!--more-->\n\nIn the meantime, I _have_ been talking about them quite a bit recently. We've been using them on a project at [Clear Measure](http://www.clear-measure.com) and I'd like to think it's been successful but it's too soon to tell. I feel good about what we've done which, historically, has always been a good metric for me.\n\nThe topic has been covered at a technical and architectural level pretty well by Martin Fowler, so much so that he's even collected his discussions into a nice little [Microservices Resource Guide](http://martinfowler.com/microservices/). In it, he and other ThoughtWorkians define them (to the extent that anything in software containing the word \"services\" _can_ be defined), point out pros and cons compared to monolithic applications, describe testing strategies, and cover off the major success stories in the space.\n\nThat doesn't leave much ground for me to cover which, from a marketing standpoint, is almost surely the point. But I would like to add my voice if for no other reason than to plug the [podcast](http://www.westerndevs.com/podcasts/podcast-microservices/) on the subject.\n\nOne of the more interesting links on Fowler's Resource Guide is tucked away at the bottom. It's a series of posts on how SoundCloud is migrating from a monolith to microservices. [Part 1](https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith) discusses how they stopped working on the monolith and performed all new work in new microservices and [part 2](https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-2-breaking-the-monolith) is on how they split the monolith up into microservices. There were challenges in both cases, leading to other architectural decisions like event sourcing.\n\nThe arguments for and against are, predictably, passionate and academic. \"Overkill!\" you say. \"Clean boundaries!\" sez I. \"But...DevOps!\" you counter. \"Yes...DevOps!\" I respond. But SoundCloud's experience, to me, is the real selling point of microservices. Unlike Netflix and Amazon, it's a scale that is still relatable to many of us. We can picture ourselves in the offices there making the same decisions they went through and running up against the same problems. These guys have BEEN THERE, man! Not moving to microservices because they [have to](https://plus.google.com/+RipRowan/posts/eVeouesvaVX) but because they had a real problem and needed a solution.\n\nNow if you read the posts, there's a certain finality to them. \"We ran into this problem so we solved it by doing X.\" What's missing from the narrative is doubt. When they ran into problems that required access to an internal API, did anyone ask if maybe they defined the boundaries incorrectly? Once event sourcing was introduced, was there a question of whether they were going too far down a rabbit hole?\n\nThat's not really the point of these posts, which is merely to relay the decision factors to see if it's similar enough to your situation to warrant an investigation into microservices. All the same, I think this aspect is important for something still in its relative infancy, because there are plenty of people waiting to tell you \"I told you so\" as soon as you hit your first snag. Knowing SoundCloud ran into the same doubt can be reassuring. Maybe I'm just waiting for Microservices: The Documentary.\n\nRegardless, there are already plenty of counter-arguments (or more accurately, counter-assumptions) to anecdotal evidence. Maybe the situation isn't the same. They have infrastructure. They have money and time to rewrite. They have confident, \"talented\" developers who always know how to solve architectural problems the right away.\n\nSo now I've more or less done what I always do when I talk microservices, which is talk myself into a corner. Am I for 'em or agin 'em? And more importantly, should **you**, reader, use them?\n\nThe answer is: absolutely, of course, and yes. On your current project? That's a little murkier. The experience is there and microservices have been done successfully. It's still a bit of a wild west which can be exciting if you ain't much for book learnin'. But \"exciting\" isn't always the best reason to decide on an architecture if someone else is paying the bills. As with any architectural shift, you have to factor in the human variables in your particular project.\n\nFor my limited experience, I like them. They solve one set of problems nicely and introduce a new set of problems that are not only tractable, but fun, in this hillbilly's opinion.\n\nAnd why else did you get into the industry if not to have fun?","categories":[],"tags":[]},{"title":"The Internet of Things","slug":"podcast-the-internet-of-things","date":"2015-07-13 22:47:49+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-the-internet-of-things/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-the-internet-of-things/","excerpt":"The Western Devs delve into the mire that is the Internet of Things","raw":"---\nlayout: podcast\ntitle:  \"The Internet of Things\"\ndate: 2015-07-13T14:47:49-04:00\nrecorded: 2015-07-10\ncategories: podcasts\nexcerpt: \"The Western Devs delve into the mire that is the Internet of Things\"\ncomments: true\npodcast:\n    filename: \"InternetOfThings.mp3\"\n    length: \"45:57\"\n    filesize: 55153989\n    libsynId: 5291974\n    anchorFmId: The-Internet-of-Things-evqdim\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - craig_anderson\n    - donald_belcham\n    - kyle_baley\n    - tom_opgenorth\n    - dave_white\n    - james_chambers\n    - shane_courtrille\n    - simon_timms\nlinks:\n    - Wink HUB|http://www.wink.com/products/wink-hub/\n    - Netduino|http://www.netduino.com/\n    - Arduino|https://www.arduino.cc/\n    - Raspberry Pi|https://www.raspberrypi.org/\n    - Nest|https://nest.com/\n    - Newark|http://www.newark.com/\n    - BBQduino|https://lostechies.com/derickbailey/2013/04/10/a-first-look-at-my-arduino-bbq-thermometer/\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\nalias: /whatwevesaid/podcast-the-internet-of-things/\n---\n\n### Synopsis\n\n* Is Internet of Things SCADA without the pomp and circumstance?\n* Security (and lack there of) in internet-enabled devices\n* Connecting devices together and the short-sightedness of proprietary access\n* Off-the-shelf vs. build-your-own\n* Encouraging good behaviour and learning through internet-enabled devices\n* Advantages (and risks) in the health industry\n* \"See a need, fill a need\": Sump pumps and coffeepots\n* Does the light _really_ go out when you close the fridge?\n* Accessibility of hardware to software people and vice versa\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Microsoft's Clear Message for Canadian Partners","authorId":"darcy_lussier","slug":"microsofts-clear-message-for-canadian-partners","date":"2015-07-12 23:34:10+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/microsofts-clear-message-for-canadian-partners/","link":"","permalink":"https://westerndevs.com/_/microsofts-clear-message-for-canadian-partners/","excerpt":"I attended the Microsoft Worldwide Partner Conference in 2011 and 2012. Even 3-4 years ago we were hearing about the changing focus at Microsoft. While 2000-2008 was all about winning the platform war (.NET vs Java), 2008 had ushered in the cloud era in the Microsoft world. While Azure's platform as a service seemed front and center, WPC started beating the Office 365 drums. The message from Microsoft was starting to take shape â€“ partners needed to get on board with where Microsoft was going with Azure and Office 365. Those that did would keep the attention and support of Microsoft.","raw":"---\nlayout: post\ntitle:  Microsoft's Clear Message for Canadian Partners\ndate: 2015-07-12T15:34:10-04:00\ncategories:\ncomments: true\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/07/12/165620.aspx\n---\n\nI attended the Microsoft Worldwide Partner Conference in 2011 and 2012. Even 3-4 years ago we were hearing about the changing focus at Microsoft. While 2000-2008 was all about winning the platform war (.NET vs Java), 2008 had ushered in the cloud era in the Microsoft world. While Azure's platform as a service seemed front and center, WPC started beating the Office 365 drums. The message from Microsoft was starting to take shape â€“ partners needed to get on board with where Microsoft was going with Azure and Office 365. Those that did would keep the attention and support of Microsoft.\n\n<!--more-->\n\nIn a discussion with a Microsoft Canada Tele-Managed Partner Manager within the last year, I heard this message refined and pointed â€“ Yes there were sales support dollars available but only for initiatives/opportunities where there was a clear cloud component. Have a large custom .NET app dev opportunity that would be deployed on premise to the client's infrastructure? Great, good luck, but not the marching orders Redmond gave its sales teams.\n\nThis year, if there was any doubt where Microsoft has moved its focus, this tweet should put those doubts to rest.\n\n![image][1]\n\n\"[@MPNCanada][2] _We measure our success by Cloud, Customer Acquisition & Consumption, and Usage\"_\n\nThat tweet seemed to be attributed to Janet Kennedy, President of Microsoft Canada. It lays out clearly how Microsoft Canada is measured, and by extension how the Partner community is measured.\n\n### Cloud\n\nBooking business for Azure, Office365, or any other cloud-based service/platform Microsoft offers\n\n### Customer Acquisition & Consumption\n\nGetting customers to purchase cloud services (either a-la carte or through an enterprise license agreement (ELA). Consumption is different than usage though, which we'll cover next. Consumption is getting a client to agree to move some servers to Azure, or purchase 100 licenses of Office 365. Consumption gets the Microsoft products _in production_ at the client.\n\nOver the years I heard of many large enterprise-size organizations who had an ELA which provided all sorts of licenses. Incredibly some products that the ELA provided valid licenses for were never deployed; they were never _consumed_. You can't get usage traction unless a customer actually uses (consumes) the product.\n\n### Usage\n\nUsage is the recurring revenue generated by using metered cloud services. This is the part where the meter runs and Microsoft (and the Partners) make money. Remember the example client we talked about who moved some servers to Azure? Maybe they realized some great cost savings over maintaining physical servers. Maybe they decide to move MORE servers to Azure, and more applications/processes to those servers, requiring bigger Azure VM instances. This is how _consumption_ can lead to higher _usage_.\n\n### So Look Partners...\n\nBack in 2011 and 2012, the message was less frank and more suggestive.\n\n\"Hey, we're moving in this direction. You should _really _listen to what we're saying. Things are going to be changing.\"\n\nNow in 2015, the message is loud and clear.  \"The cloud train has already left the station. A lot of partners are already on board, but if you run you can catch up and get on too. But we aren't slowing down for stragglers.\"\n\nAnd this isn't a bad thing, or a mean thing, or anything out of malice. Microsoft had to change their business to evolve, and having a partner program they rely on to move their product through means those partners need to be open to changing their focus as Microsoft does. Microsoft sets the vision for their partners, not the other way around.\n\nThose partners that were part of the program from 2000 to 2010 and built their business on custom app dev and on-premise IT services have a choice to make â€“ adapt to the new world of Microsoft, or figure out how to live in a world without Microsoft's support.\n\n[1]: https://gwb.blob.core.windows.net/dlussier/WindowsLiveWriter/MicrosoftsFrankMessageforCanadianPartner_13BC2/image_thumb.png \"image\"\n[2]: http://www.twitter.com/mpncanada","categories":[],"tags":[]},{"title":"The Changing Winnipeg Technology Consulting Landscape","authorId":"darcy_lussier","slug":"the-changing-winnipeg-technology-consulting-landscape","date":"2015-07-12 23:26:59+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/the-changing-winnipeg-technology-consulting-landscape/","link":"","permalink":"https://westerndevs.com/_/the-changing-winnipeg-technology-consulting-landscape/","excerpt":"I've been noticing a few things lately here in Winnipeg: Less large custom application development projects More adoption of packaged software More adoption of Software as a Service platforms Organizations wanting control over technology projects, only looking for augmentation roles to fill gaps The commoditization of software developers with broad skills","raw":"---\nlayout: post\ntitle:  The Changing Winnipeg Technology Consulting Landscape\ndate: 2015-07-12T15:26:59-04:00\ncategories:\ncomments: true\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/07/12/165614.aspx\n---\n\nI've been noticing a few things lately here in Winnipeg:\n\n* Less large custom application development projects\n\n* More adoption of packaged software\n\n* More adoption of Software as a Service platforms\n\n* Organizations wanting control over technology projects, only looking for augmentation roles to fill gaps\n\n* The commoditization of software developers with broad skills\n\n<!--more-->\n\nThe cloud is removing the need for organizations to manage and maintain their own infrastructure. Software as a Service provides a model where organizations can rent lower-priority applications/services while focusing their own development staff on higher priority initiatives. Consultants with specialized skills in mobile, integration, security, and platforms (SharePoint, SAP, etc.) still demand higher rates, but organizations are no longer willing to pay a premium for .NET or Java developers.\n\nConsulting firms who sell professional services need to become aware of these trends and pivot on what they offer to their marketplace. Consider the items below, all of which should have initiatives in flight or be on the radar for any professional services company.\n\n### Cloud    \nAzure and AWS are gaining more customers by the day, and releasing new features and offerings on their cloud platform. Microsoft announced at Ignite this year that organizations would be able to install the exact same Azure software that Microsoft uses in the public Azure cloud, meaning organizations can get on board with Azure on prem and move to a hybrid or hosted cloud model when ready. Consultants need to be able to show customers how to move to the cloud, the benefits of moving to the cloud, and provide the services to bridge the knowledge gaps in developing software for the cloud.\n\n### Software as a Service   \nThe _Buy/Build_ mantra has been replaced with _Buy/SaaS/...I guess we'll consider building_. If a package solution can't be found, organizations are more than willing to consider a SaaS solution before investing in a custom built application that must then be maintained. Consultants need to be aware of available solutions when advising clients on the best options for their needs. There's also an opportunity for consulting organizations to create their own SaaS and enter the product market.\n\nAlso note that even if an organization opts to build a custom application, most enterprises employ skilled developers who can deliver it instead of sourcing out to a consulting firm; more and more I've seen professional service organizations be contacted for resource augmentation rather than taking an entire project.\n\n### Security   \nSecurity has been largely ignored for years. With recent hacks, threats of cyber-attacks, and the continual onslaught of viruses, organizations are finally realizing that they need to treat security seriously. While this is a growing area, there is still more work available than service providers that can offer it.\n\n### Mobile   \nIf you aren't in the mobile space already then you're lagging behind. Companies that invested in mobile skills are reaping the rewards now. Organizations are realizing that mobile solutions aren't just about empowering the internal workforce; its also about reaching customers and the community at large. What those organizations need are partners who can help show what's possible.\n\n### Integration   \nAs organizations incorporate non-centralized IT solutions with cloud, SaaS, and mobile into their environments that probably include enterprise systems alongside custom/niche applications, sharing data across all of these will be an important consideration. Integration consultants who can help clients navigate their system landscape and implement things like ESBs, data warehouses, and BI/Analytics solutions will find opportunities â€“ although in laggard markets there will be a need to educate clients on the benefits.\n\n### What About Your Market?\n\nI'd be curious to hear if people in other markets, whether in Canada or in the US, are seeing/experiencing the same type of trends. Or maybe you're in Winnipeg and have a different take on the market. If you have comments, either leave them below or if you decide to do your own blog post please leave a link to it in the comments.  ","categories":[],"tags":[]},{"title":"Sharpening chisels","authorId":"donald_belcham","slug":"sharpening-chisels","date":"2015-07-08 21:38:46+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/sharpening-chisels/","link":"","permalink":"https://westerndevs.com/_/sharpening-chisels/","excerpt":"I'm working on a cedar garden gate for our back yard. It's all mortise and tenon joinery which means I make a lot of use of my Narex bench and mortise chisels. The more you use chisels the duller they get. Dull chisels cause you two problems; you can't be as precise with them, and you run the very real risk of amputating a finger. As much as I have two of each finger I really do want to keep all eleven of them. Getting tight fitting tenons requires fine tuning of their thicknesses by the thousandth of an inch. Both of those fly directly in the face of what dull chisels are good atâ€¦so tonight was all about sharpening them up.","raw":"---\nlayout: post\ntitle:  Sharpening chisels\ndate: 2015-07-08T13:38:46-04:00\ncategories:\ncomments: true\nauthorId: donald_belcham\noriginalurl: http://www.igloocoder.com/2785/sharpening-chisels\n---\n\nI'm working on a cedar garden gate for our back yard. It's all mortise and tenon joinery which means I make a lot of use of my [Narex][1] [bench][2] and [mortise][3] chisels. The more you use chisels the duller they get. Dull chisels cause you two problems; you can't be as precise with them, and you run the very real risk of amputating a finger. As much as I have two of each finger I really do want to keep all eleven of them. Getting tight fitting tenons requires fine tuning of their thicknesses by the thousandth of an inch. Both of those fly directly in the face of what dull chisels are good atâ€¦so tonight was all about sharpening them up.\n\n<!--more-->\n\nThere are a number of different ways that you can sharpen edged tools (chisels and hand planes). There are machines, you can use water stones, Arkansas stones, diamond stones or, my personal choice, the \"[Scary Sharp Technique][4]\". For those of you that couldn't be bothered click that link and read through the original usenet posting on the topic in detail (and who can blame you, this is a software blog after all) here's the TL;DR; for Scary Sharp.\n\n* Sharpening is done with wet dry automotive sand paper, not stones\n* Progression is made to finer and finer grits as you get sharper. i.e.  400 â€“> 800 â€“> 1200 â€“> 2000 grit\n* Sandpaper is glued down to a perfectly flat surface such as float glass, a tile, or granite counter top (ask the missus first if you're planning on using the kitchen)\n\nMy station for tonight looked like this (400 grit to the left, 2000 grit on the far right):\n\n![sharpening station][5]\n\nSo, why am I boring you with all this detail about sharpening my chisels? There's a story to be told about software tools and woodworking tools. The part of it that I'm going tell in this post is the part about maintaining and fine tuning them.\n\n### Maintaining your tools\n\nFor me to be able to effectively, and safely, create my next woodworking project I need to constantly maintain my chisels (amongst other tools). I have to stop my project work and take the time to perform this maintenance. Yes, it's time that I could be getting closer to being finished, but at what cost? Poor fitting joinery? Avoidable gouges? Self amputation? The trade off is project velocity for project quality.\n\nNow think about a development tool that you use on a regular basis for your coding work. Maybe it's Visual Studio, or IntelliJ, or ReSharper, or PowerShell, orâ€¦orâ€¦or. You get the point. You open these tools on an almost daily basis. You're (hopefully) adept at using them. But do you ever stop and take the time to maintain them? If it weren't for auto-updating/reminder services would you even keep as close to the most recent release version as you currently are? Why don't we do more? I currently have an install of Resharper that I use almost daily that doesn't correctly perform the clean-up command when I hit Ctrl-Shift-F. I know this. Every time I hit Ctrl-Shift-F I cuss about the fact it doesn't work. But I haven't taken the time to go fix it. I've numbed myself to it.\n\nAlternatively, imagine if you knew that once a week/month/sprint/ you were going to set aside time to go into the deep settings of your tool (i.e. ReSharper &#124; Options) and perform maintenance on it. What if you smoke tested the shortcuts, cleaned up the templates, updated to the latest bits? Would your (or mine in the above example) development experience be better? Would you perform better work as a result? Possibly. Probably.\n\n### Tools for tools\n\nI have tools for my woodworking tools. I can't own and use my chisels without having the necessary tools to keep them sharp. I need a honing guide, sand paper, and granite just to be able to maintain my primary woodworking tools. None of those things directly contribute to the production of the final project. All their contributions are indirect at best. This goes for any number of tools that I have on my shelves. Tools for tools is a necessity, not a luxury. Like your first level tools, your second level of tools need to be maintained and cared for too. Before I moved to the Scary Sharp system I used water stones. As you repetitively stroke the edge over the stone it naturally creates a concave in the middle of the stone. This rounded surface makes it impossible to create a proper bevel angle. To get the bevel angle dead on I needed to constantly flatten my stone. Sharpen a tool for a while, flatten the stone surfaceâ€¦repeat. Tools to maintain tools that are used to maintain tools.\n\nNow think about your software development tools. How many of you have tools for those tools? Granted, some of them don't require tools to maintain themâ€¦.or do they? So you do some configuration changes for git bash's .config file. Maybe you open that with Notepad ++. Now you have a tool (Notepad++), for your tool (git bash). How do you install Notepad++? With [chocolatey][6] you say? Have you been maintaining your chocolatey install? Tools to maintain tools that are used to maintain tools.\n\nSadly we developers don't put much importance on the secondary and tertiary tools in our toolboxes. We should. We need to. If we don't our primary tools will never be in optimal working condition and, as a result, we will never perform at our peaks.\n\n### Make time\n\nFind the time in your day/week/sprint/month to pay a little bit of attention to your secondary and tertiary tools. Don't forget to spend some quality time with your primary tools. Understand them, tweak them, optimize them, keep them healthy. Yes, it will take time away from delivering your project/product. Consider that working with un-tuned tools will take time away as well.\n\n[1]: http://www.narexchisels.com/Narex_Chisels/Home.html\n[2]: http://www.leevalley.com/en/wood/page.aspx?p=67707&cat=1,41504\n[3]: http://www.leevalley.com/en/wood/page.aspx?p=66737&cat=1,41504\n[4]: https://groups.google.com/forum/?hl=en#!topic/rec.woodworking/rGAGAPR-6ks\n[5]: https://farm1.staticflickr.com/373/19487481376_c527907bae_z.jpg\n[6]: https://chocolatey.org/\n  ","categories":[],"tags":[]},{"title":"Microservices","slug":"podcast-microservices","date":"2015-07-08 00:54:52+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"podcasts/podcast-microservices/","link":"","permalink":"https://westerndevs.com/podcasts/podcast-microservices/","excerpt":"In our inaugural podcast, the Western Devs tackle what is either the new hotness, retro SOA, or a flash in the pan: microservices.","raw":"---\nlayout: podcast\ntitle:  \"Microservices\"\ndate: 2015-07-07T16:54:52-04:00\nrecorded: 2015-07-01\ncategories: podcasts\nexcerpt: \"In our inaugural podcast, the Western Devs tackle what is either the new hotness, retro SOA, or a flash in the pan: microservices.\"\ncomments: true\npodcast:\n    filename: Microservices.mp3\n    length: \"53:58\"\n    filesize: 51810324\n    libsynId: 4662899\n    anchorFmId: Microservices-evqdi3\nparticipants:\n    - dave_paquette\n    - amir_barylko\n    - david_wesst\n    - dave_white\n    - donald_belcham\n    - dylan_smith\n    - jason_row\n    - kyle_baley\n    - tom_opgenorth\nlinks:\n    - SOA with a dash of PubSub|http://geekswithblogs.net/Optikal/archive/2013/05/19/152956.aspx\n    - Microservices, cutting through the Gordian Knot (NDC 2015)|https://vimeo.com/132194544\n    - Steve Yegge's post on Amazon's switch to services|https://plus.google.com/+RipRowan/posts/eVeouesvaVX\n    - Principles of Microservices presentation (NDC 2015)|https://vimeo.com/131632250\n    - Strangler Application|http://www.martinfowler.com/bliki/StranglerApplication.html\n    - Martin Fowler on Microservices|http://martinfowler.com/articles/microservices.html\n    - Docker|https://www.docker.com/\n    - Docker images for asp.net v5|https://registry.hub.docker.com/u/microsoft/aspnet/\n    - Splunk|http://www.splunk.com/\n    - Spotify culture|https://labs.spotify.com/2014/03/27/spotify-engineering-culture-part-1/\n    - Human benefits of a microservice architecture|http://damianm.com/articles/human-benefits-of-a-microservice-architecture\nmusic:\n    song:\n        title: Doctor Man\n        artist: Johnnie Christie and the Boats\n        url: https://www.youtube.com/user/jwcchristie\nalias: /whatwevesaid/podcast-microservices/\n---\n","categories":[{"name":"podcasts","slug":"podcasts","permalink":"https://westerndevs.com/categories/podcasts/"}],"tags":[]},{"title":"Outside the shack, or \"How to be a technology gigolo\"","authorId":"kyle_baley","slug":"outside-the-shack-or-how-to-be-a-gigolo","date":"2015-07-06 05:11:15+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/outside-the-shack-or-how-to-be-a-gigolo/","link":"","permalink":"https://westerndevs.com/_/outside-the-shack-or-how-to-be-a-gigolo/","excerpt":"The world outside is just awesome.","raw":"---\nlayout: post\ntitle:  Outside the shack, or \"How to be a technology gigolo\"\ndate: 2015-07-05T21:11:15-04:00\ncategories:\nexcerpt: The world outside is just awesome.\ncomments: true\nauthorId: kyle_baley\n---\n\nAlmost [four years ago](http://kyle.baley.org/2011/11/staying-home-for-the-night), I waxed hillbilly on how nice it was to stick with what you knew, at least for side projects. At the time, my main project was Java and my side projects were .NET. Now, my main project is .NET and for whatever reason, I thought it would be nice to take on a side project.\n\nThe side project is [Western Devs](http://www.westerndevs.com), a fairly tight-knit community of developers of similar temperament but only vaguely similar backgrounds. It's a fun group to hang out with online and in person and at one point, someone thought \"Wouldn't it be nice to build ourselves a website and have Kyle manage it while we lob increasingly ridiculous feature requests at him from afar?\"\n\nAlas, I suffer from an unfortunate condition I inherited from my grandfather on my mother's side called \"Good Idea At The Time Syndrome\" wherein one sees a community in need and charges in to make things right and damn the consequences on your social life because dammit, these people need help! The disease is common among condo association members and school bus drivers. Regardless, I liked the idea and we're currently trying to pull it off.\n\nThe first question: what do we build it in? WordPress was an option we came up with early so we could throw it away as fast as possible. Despite some dabbling, we're all more or less entrenched in .NET so an obvious choice was one of the numerous blog engines in that space. Personally, I'd consider [Miniblog](https://github.com/madskristensen/miniblog) only because of its author.\n\nThen someone suggested Jekyll hosted on GitHub pages due to its simplicity. This wasn't a word I usually assocated with hosting a blog, especially one in .NET, so I decided to give it a shot.\n\nCut to about a month later, and the stack consists of:\n\n* Jekyll\n* GitHub Pages\n* Markdown\n* SASS\n* Slim\n* Rake\n* Travis\n* Octopress\n* [Minimal Mistakes Jekyll theme](https://github.com/mmistakes/minimal-mistakes)\n\nOf these, the one and only technology I had any experience with was Rake, which I used to automate UI tests at [BookedIN](http://www.getbookedin.com). The rest, including Markdown, were foreign to me.\n\nAnd Lord Tunderin' Jayzus I can not believe how quickly stuff came together. With GitHub Pages and Jekyll, infrastructure is all but non-existent. Octopress means no database, just file copying. Markdown, Slim and SASS have allowed me to scan and edit content files easier than with plain HTML and CSS. The Minimal Mistakes theme added so much built-in polish that I'm still finding new features in it today.\n\nThe most recent addition, and the one the prompted this post, was Travis. I'm a TeamCity guy and have been for years. I managed the [TeamCity server](http://teamcity.codebetter.com) for CodeBetter for many moons and on a recent project, had 6 agents running a suite of UI tests in parallel. So when I finally got fed up enough with our deploy process (one can type `git pull origin source && rake site:publish` only so many times), TeamCity was the [first hammer](http://brendan.enrick.com/image.axd?picture=Golden-Hammer_1.png)<sup>*</sup> I reached for.\n\nOne thing to note: I've been doing all my development so far on a MacBook. My TeamCity server is on Windows. I've done Rake and Ruby stuff on the CI server before without too much trouble but I still cringe inwardly whenever I have to set up builds involving technology where the readme says \"Technically, it works on Windows\". As it is, I have an older version of Ruby on the server that is still required for another project and on Windows, Jekyll requires Python but not the latest version, and I need to install a later version of DevKit, etc, etc, and so on and so forth.\n\nA couple of hours later, I had a build created and running with no infrastructure errors. Except that it hung somewhere. No indication why in the build logs and at that moment, my 5-year-old said, \"Dad, let's play hockey\" which sounded less frustrating than having to set up a local Windows environment to debug this problem.\n\nAfter a rousing game where I schooled the kid 34-0, I left him with his mother to deal with the tears and I sat down to tackle the CI build again. At this point, it occurred to me I could try something non-Windows-based. That's where [Travis](http://travis-ci.org) came in (on a suggestion from [Dave Paquette](http://localhost:4000/bios/dave_paquette/) who I also want to say is the one that suggested Jekyll but I might be wrong).\n\nFifteen minutes. That's how long it took to get my first (admittedly failing) build to run. It was frighteningly easy. I just had to hand over complete access to my GitHub repo, add a config file, and it virtually did the rest for me.\n\nTwenty minutes later, I had my first passing build which only built the website. Less than an hour later and our dream of continuous deployment is done. No mucking with gems, no installing frameworks over RDP. I updated a grand total of four files: .travis.yml, _config.yml, Gemfile, and rakefile. And now, whenever someone checks into the `source` branch, I am officially out of the loop. I had to do virtually nothing on the CI server itself, including setting up the Slack notifications.\n\nThis is a long-winded contradiction of my post of four years ago where my uncertainty with Java drove me to the comfort of .NET. And to keep perspective, this isn't exactly a mission critical, LOB application. All the same, for someone with 15-odd years of .NET experience under his obi, I'd be lying if I said I wasn't amazed at how quickly one can put together a functional website for multiple authors with non-Microsoft technology you barely have passing knowledge of.\n\nTo be clear, I'm fully aware of what people say about these things. I know Ruby is a fun language and I feel good about myself whenever I do anything substantial with it. And I know Markdown is all the rage with the kids these days. It's not really one technology on its own that made me approach epiphaniness. It's the way all the tools and libraries intermingle so well. Which has this optimistic hillbilly feeling like his personal life and professional life are starting to mirror each other.\n\nIs there a lesson in here for others? I hope so as it would justify me typing all this out and <s>clicking publish</s> committing to the repository. But mostly, like everything else, I'm just happy to be here. As I've always said, if you've learned anything, that's your fault, not mine.\n\nKyle the Coalescent\n\n<sub><sup>* With credit to Brendan Enrick's and Steve Smith's [Software Craftsmanship Calendar 2016](http://brendan.enrick.com/post/Making-The-Software-Craftsmanship-Calendar-Images)</sup></sub>\n","categories":[],"tags":[]},{"title":"Running your first code camp","authorId":"darcy_lussier","slug":"running-your-first-code-camp","date":"2015-07-05 19:33:21+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/running-your-first-code-camp/","link":"","permalink":"https://westerndevs.com/_/running-your-first-code-camp/","excerpt":"Thoughts on how to run a one-day, multi-track code camp","raw":"---\nlayout: post\ntitle:  Running your first code camp\ndate: 2015-07-05T11:33:21-04:00\ncategories:\nexcerpt: Thoughts on how to run a one-day, multi-track code camp\ncomments: true\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/07/01/165460.aspx\n---\n\nEvery now and then I get people asking me about how to run a conference. One thing I encourage is that people start small and build from there. I ran the Winnipeg Code Camp for a number of years before evolving it into Prairie Dev Con, and the foundation of the code camp is the base that Prairie Dev Con grew out of.\n\nSo below are my thoughts on how to run a one day, multi-track, Code Camp.\n\n### What's a Code Camp\n\nCode Camps became popular in the 00's. They were free one day technology conferences that focused on showing off technology (so more code, less marketechture). This is the true volunteer event â€“ low budget, high volunteerism, but still high quality and lots of fun. All costs are covered by sponsors and there's never an entry fee for attendees.\n\nOk, so let's start planning our Code Camp!\n\n### The Code Camp Format\n\nFor a first-time code camp, I would suggest doing a single day event on a Saturday, running two or three tracks of sessions (this will be based on your market size and speaker pool). Schedule will look like this:\n\n8:30 â€“ 9:30 Breakfast/Registration   \n9:30 â€“ 9:45 Welcome   \n9:45 â€“ 10:45 Session   \n11:00 â€“ 12:00 Session   \n12:00 â€“ 1:00 Lunch   \n1:00 â€“ 2:00 Session   \n2:15 â€“ 3:15 Session   \n3:30 â€“ 4:30 Session   \n4:30 â€“ 5:00 Wrap-Up\n\nThat gives you 10 sessions for a two track or 15 sessions for a three track setup.\n\n### Step 1 â€“ Gauge Interest\n\nA big part of a Code Camp's success is the energy and commitment that an organizer brings to it, but you also need to know if your community shares your vision and will support the event. Reaching out to local technology user groups to see if their organizers and members share the same excitement is your starting point. It also makes it easier to promote the event if you can get leaders of those communities on board.\n\nNow realize you're just gauging interest here, not commitment. I ran a PrDC where community leaders â€“ who were all very well meaning â€“ said I'd get well over 300 attendees out; I struggled to get 180. The reality is that until you run your first event you won't have an idea of how many people you'll actually get out, so early on you're just gauging _interest_ and not looking for _commitment_.\n\nYou also need to gauge interest with the people who will be speaking at the Code Camp. We'll talk about doing a call-for-speakers later, but especially for an initial event you want to have a good number of speakers already lined up and committed to the event.\n\n### Step 2 â€“ Source Venues\n\nIf there's enough interest, now is the time to look at venues. For a Code Camp you want to do this on the cheap. Don't even bother with hotels or conference centers. You're first hit should be to local schools, colleges, and universities. Here's why:\n\nThey'll already have lecture halls and classrooms set up with projectors (verify that projectors are included in room rental prices)\n\nThey're usually cheaper to rent rooms space from then hotels/conference centers\n\nNo classes happen on Saturdays (typically) so space will be more generally available\n\nThey *can* be located on more convenient public transportation routes (buses, subways, etc.) so easier to get to for people\n\nIf you have any contacts with the administration, you may be able to pitch this as a good community event that students will benefit from and get a discount on the rooms.\n\nWhen sourcing venues keep in mind that you need n+1 rooms, where n is the number of tracks you're running. The +1 is for your _plenary_ room â€“ the place that all attendees will meet for meals, for the welcome/kickoff in the morning, and for the wrapup at the end of the day. All rooms should be close together so attendees aren't required to go walking all over the place.\n\nMake sure that you ask about parking â€“ if its free, if there's paid lots nearby and what the costs are, and what the street parking rules are. You'll want to communicate this to your attendees.\n\nAlso ask about internet access â€“ is there public wi-fi, is there a charge, is a passcode required, etc. This information will be important to provide to speakers as well if any require internet access for their planned talks.\n\nFinally, while this shouldn't be an issue with building codes and current laws, make sure that the venue you select is easily accessible for people with disabilities.\n\n#### Step 2.1 â€“ Food\n\nFor your first code camp food is definitely optional â€“ although if you decide to not do food you should try to ensure that there's enough restaurant options and coffee shops for attendees nearby the venue.\n\nIf you do decide to do food check what your venue's policies are. Most venues will have a policy that you must use their own food services and can't bring from outside sources; and that also means that price will be higher because venue-based food is almost always more expensive (always in my experience). This is where gauging interest is important because since you aren't charging a fee there's no way to know how many people will show up. If you charged a fee, even if they no-show you could still cover their food, but in this case its all about estimation.\n\nYou could run your first code camp, gauge attendees, and use that to base future events off of and incorporate food later. Or if you're confident in your estimations then figure out a reasonable menu. Code camps are the *only* event where I think continental breakfasts are ok. Also look at sandwiches, pizza, or chicken fingers & salad for lunch â€“ typically on the lower side of cost and generally liked by most people. Do take into account people's dietary needs (allergies, cultural preferences, etc.). I avoid any food option based on pork or seafood and stick to chicken or beef from a meat point of view. You could have vegetarian and vegan folks as well. Just make sure when you review the available menu that there are options in case you need alternate meals.\n\n### Step 3 â€“ Who's Running This?\n\nNow that you have your venue, a date, and food cost, it's time to start approaching sponsors. But first, this is when you should tighten up your leadership organization if you haven't already. These types of events get run by very well meaning individuals who want to improve their communities, but they're also run by people who aren't perfect and stuff can happen ([I blogged about a community event gone bad here][1]). So let's talk about how you can organize this.\n\nFor the Winnipeg Code Camp although I'd organize it we'd use the Winnipeg .NET User Group as a neutral and already set up organization to run finances, sponsorship, and communication through. Finances because a community group account was already set up and the guy who handles finances was willing to be the pass through for everything. It's also good to have a neutral body be the host for sponsorship â€“ some companies don't want the perception that they're _partnering with competition_ to put on an event, but they'll definitely sponsor a neutral organization's event (so they're sponsoring the .NET User Group's Code Camp, not putting one on with competitors).\n\nHere are options:\n\nLeverage an existing organization to act as the \"host\". A technology user group is ideal for this, however note that whoever is seen as hosting is also owning liability for the event as well. We'll talk about that in a second.\n\nWrite up an organizer's agreement stating who is responsible for what and who is accepting liability for the event. Yes this sounds scary, but its a necessity. The reality is that putting on any type of event, free or not, holds some level of risk that needs to be mitigated. This also protects all organizers.\n\nCreate a corporation to run your events out of. This is probably extreme though, but its what I ended up doing for my conferences. For code camp events you probably don't want to go through this rigor but it does dot ALL the i's and cross ALL the t's. It's also costly and time consuming.\n\nA note on liability â€“ this is always a consideration when running an event. Even if you're the nicest person in the world putting on a free community event for the betterment of the community, if someone eats bad food or trips and breaks their leg you could still be named in a lawsuit. Event insurance is very inexpensive. For Prairie Dev Con I pay $300 - $500 which covers me for food-borne illness and any type of injury that could occur while at the conference. The insurance will need to be made by a person or entity though, and unless you're running this under a structured legal entity then somebody may be the one to *own* the liability coverage (and any liability).\n\n### Step 3 â€“ Sponsors\n\nNow that you have venue costs and an estimate on food costs, you can approach sponsors â€“ which is how code camps are typically funded. You should create a package you can present to sponsors:\n\nWhat is the event and what is the vision for it, what are the goals.\n\nWho is involved and who will the attendees be.\n\nWhat is the ask of the sponsor.\n\nWhat will they get in return (logo recognition on marketing, website, opportunity to do a presentation, etc.)\n\nI would *not* put a limit on the number of sponsorships you have available. Have a number that you need to get to in mind to cover your costs, but if you get more sponsors that's ok â€“ you can spend the money for prizes or extra perks at the code camp. Just have the mindset that you want to spend ALL of the money on the event â€“ there's rules/tax implications for volunteer groups who carry money forward and I don't know them all (I have an incorporation now so I just run stuff through that).\n\n### Step 4 â€“ Website and Ticketing\n\nYou may want to get a website set up before you approach Sponsors, just so they can see that there's an online presence and the event is legit â€“ or you may not if you have personal connections to those you're looking to get sponsorship from Regardless, you should get some website up as early as possible once you have a good idea on whether the event will be a go or not.\n\nYou will also need some way for attendees to register. There's lots of ticketing/event-registration sites out there, my fav is [Picatic][2]. Your registration method needs to be more than just collecting a name/email and tracking registration numbers. There's a few key pieces of info you need to ask.\n\nFood Allergies/Preferences â€“ Does the person have any food allergies? Are there any preferences you need to be aware of (personal preference, religious/cultural, etc.)?\n\nEmergency Contact Info â€“ In the event something happens, who should be contacted.\n\nConsent to Media â€“ Are you planning on taking pictures and posting it to social media? You may want to get their permission to use them in pictures online. At one conference I had an attendee ask that she not be included in any event pictures because of fears about an ex-boyfriend who was looking for her.\n\nMost good ticketing sites will let you add custom questions to the process, which is the easiest way to collect this information. (Note â€“ one way to handle the \"can I take your picture or not\" practically is to provide a slightly different name badge (colour, ribbon, etc.) to identify those who wish to not be photographed).\n\n### Step 5 â€“ Speakers\n\nYou can approach speakers and sponsors at the same time, but I put it here in the order because you need to have sponsors lined up first to ensure you can cover venue costs and book the venue. Ideally you start gauging speaker interest early and continue looking throughout the organization process so you can have some people/sessions ready to post on the website when it goes live.\n\nFor code camps, speakers tend to be locally sourced and while outside speakers can definitely be invited a code camp usually doesn't have the dollars to pay for travel or hotel. I've known many speakers (and have done this myself as a speaker) who will pay their own way for a code camp, but I see code camps as a great opportunity to groom new and upcoming speakers and give them a stage to help improve their presentation skills. The point of a code camp is to _learn from each other_, not come out and see big-name speakers.\n\nIn fact for my code camps I wouldn't vette out speaker submissions â€“ I ran it as a first come/first served basis. I would however discuss with a speaker if they submitted a duplicate talk as someone else, or if we had an imbalance of sessions (i.e. you don't need 5 talks on Intro to ASP.NET) and work it that way. I would also offer guidance and coaching for those that are new to speaking.\n\nCall for speakers can be as simple as providing the info to various community leaders to spread through their membership groups, leveraging social media, posting to services like [SpeakNet][3], and also asking sponsors if they have anyone in their organizations who are interested (but not for a sales/marketing point of view â€“ has to be about code/technology). Of course you should have a method for people to submit their session submissions that can be shared on your event website and social media. I use [Survey Monkey][4] for this â€“ you can build a \"survey\" that captures speaker info and their session details _for free_.\n\n### Step 6 â€“ Promotion\n\nTime to promote the event! Social media helps out a lot here, but there's still some things that will require in-person contact of some sort. Working personal connections to get the word out works wonders, especially if its seen as a low cost, community driven, learning event open to everyone (like a code camp typically is).\n\nAlso look for non-traditional local industry groups to help get the word out â€“ groups that deal with management/decision-maker level folks in IT (i.e. [ICTAM][5]) or Chambers of Commerce. Newspapers will sometimes allow events to be posted for free in their business sections under upcoming events.\n\nSpeaking of newspapers, definitely reach out to local media at print, radio, and TV either to get visibility before the event or at the event.\n\n### Step 7 â€“ Prep for The Event\n\nThe event is coming up and its time to start prepping for it! Whether a low-budget code camp or a for-pay single day event, here are some tips for getting ready!\n\nOne rule of thumb I've learnt is that you need to invest money into the areas of your event that provide the most value. Especially for a code camp, you shouldn't worry too much about all the \"nice-to-haves\"; you don't _need_ t-shirts or fancy badge holders or big banners. If, after you cover the basics, you still have budget left over then by all means look at adding some special things to the event. But don't put them at the top of your list.\n\nNametags â€“ Amazon is your friend here! You can get plastic name-tag holders and lanyards for much cheaper than paying retail at places like Staples. If you're only doing a small amount to 200 nametags, you can print them off yourself with a home printer and nametag printer sheets (also available at Amazon).\n\nSignage â€“ Staples is actually a great place to get posters made. a 4x3 feet poster is about $35 here in Canada, which is pretty inexpensive.\n\nMobile App â€“ I used [Guidebook][6] for Prairie Dev Con Regina 2015 and was very happy with it! It provides iOS and Android apps with full schedule and the ability to build your own schedule, as well as other features like showing venue maps and custom lists. And its FREE for events under 200!\n\nPrint Schedules â€“ Event with the mobile app, many people still like to have a physical, paper schedule in hand. Black and white is fine unless you have extra funds to do colour. Make sure that whatever you print matches what's on the website and mobile app\n\nConfirm Venue Access and Review Schedule â€“ Make sure that you confirm that you'll have access to your venue space before your event is scheduled to start. You'll at least need to be there a few hours before your event starts to do any setup (registration table, signage, etc.). Make sure that security for the venue is aware of your event and what time you'll be getting access to the rooms. Confirm numbers and times for food. If you need to drop anything off the night before, know where the materials will be locked up and who will be able to get you access.\n\nCommunicate with Your Attendees â€“ Make sure via email, social media, blog, etc. to remind attendees (and speakers) about venue, times, locations in the venue, and the schedule, and where to get more information. Don't forget to include information about venue parking! One thing I strongly suggest is to have an Attendee FAQ area on your event website where you can post all this information and make it easy to refer people to it (just send them the URL). People are busy and providing a friendly reminder is definitely appreciated as they may have registered a while back and not had a chance to keep up with event announcements.\n\nSocial Media â€“ Make sure you have all your social media accounts created and hashtags decided on.\n\n### Step 8 â€“ Run the Event!\n\nTime for the big day! Running a code camp is a lot of fun, but if its your first time it can seem daunting. No worries though, you'll do great!\n\nRemember the timing format we talked about earlier:\n\n8:30 â€“ 9:30 Breakfast/Registration   \n9:30 â€“ 9:45 Welcome   \n9:45 â€“ 10:45 Session   \n11:00 â€“ 12:00 Session   \n12:00 â€“ 1:00 Lunch   \n1:00 â€“ 2:00 Session   \n2:15 â€“ 3:15 Session   \n3:30 â€“ 4:30 Session   \n4:30 â€“ 5:00 Wrap-Up\n\nLet's break this down.\n\n**_7:00 AM â€“ 8:30 AM_**\n\nGet to the venue early, at least an hour ahead of time based on how much pre-event prep you've completed. Make sure the venue has a table outside your main meeting room for registration. I usually put out nametags alphabetically on a table and let attendees pick them up, with one or two people available to help and re-organize the tags. If there's any swag or materials (like schedule hand outs), have them available at the registration desk as well. If there's any signage you want to post giving attendees directions get that up during this time.\n\n**_8:30 AM â€“ 9:30 AM_**\n\nDirect attendees to where food is and the plenary room. In the room, have a laptop setup with a rolling PowerPoint with information like venue wi-fi (if available), link to session surveys (Survey Monkey is great for this as well), thanks to sponsors with sponsor logos displayed, and any other important information.\n\n**_9:30 AM â€“ 9:45 AM_**\n\nThis is where you welcome everyone to the event, introduce yourself and the organizers, thank the sponsors, and go over housekeeping things like reviewing the venue map (point out both rooms and things like where bathrooms are), reviewing the schedule, where attendees can submit session surveys, encouraging them to use social media and what accounts/hashtags to use, and how you'll be drawing for prizes at the end of the day (if you are). Don't forget to thank the speakers and the attendees â€“ code camps need everyone to succeed and the effort people put forward, even just giving up a Saturday to attend, should be acknowledged.\n\n**_9:45 AM â€“ 12:00 PM_**\n\nThe rest of the day will be all about the sessions. As an organizer you should be making rounds ensuring that everything is going well. Your biggest issue during this time will be technology issues â€“ people not being able to connect to a projector, laptops crashing, projector bulbs burning out, etc. If you can, have a backup projector of your own on hand and a back up laptop so that worst case scenario files can be transferred over. At one code camp I had a presenter have his VGA port die between when he successfully practiced that morning to when his session was that afternoon. Weird stuff can happen.\n\nAlso gauge social media â€“ watch for how people are enjoying the event and if they post any concerns or issues.\n\n**_12:00 PM â€“ 1:00 PM_**\n\nLunch time! Use this time to make any announcements, updates, or reminders in the plenary room.\n\n**_1:00 PM â€“ 4:30 PM_**\n\nSame as the morning sessions.\n\n**_4:30 PM â€“ 5:00 PM_**\n\nHere you bring everyone together for a wrap-up. Here's where you thank everyone again for coming, thank the speakers and sponsors, review the days events, and do any prize draws.\n\nDon't be surprised if the number of people you started the day with is smaller at the end. Not everyone can make it the whole day, and that's ok.\n\nOnce you're done, all materials you had are packed up and you're ready to leave, this is a great opportunity to go out somewhere for after-code-camp beers/food/whatever and continue the awesome community building!\n\n**_Post-Event_**\n\nOnce the event is done, your role as organizer isn't. There's still some after-event items you need to do.\n\nSend out an email to your sponsors thanking them for their support and also providing information on how well the event went. Sponsors want to know that their sponsorship dollars were put to good use, so let them know!\n\nSend out an email to attendees thanking them for coming out and encouraging them to continue the conversations started at the code camp â€“ give links to various user groups in the community, show where they can get session materials, and let them know where to submit feedback for a post-event survey (you should have a post-event survey btwâ€¦Survey Monkey is great for this).\n\nDo a post-event review with the other organizers. Talk about what you could do better next year, what you'd want to keep the same, and how you can make the event better.\n\nDo you have money left over? You shouldn't but if you do figure out what to do with it. Finding a local tech-related (or not) charity to donate the money to is a good option if you have no other ideas. Remember that the idea is to have no left-over dollars by the end of the event.\n\n### And That's It!\n\nWe covered a LOT of information in this post, and if you felt a little hesitant before you _may_ be feeling very hesitant now. PLEASE DON'T BE! Running the Winnipeg Code Camp was one of the most rewarding and fun experiences for me, and putting on a code camp can be a great experience for you too! If you have any questions or comments, or want to discuss in more detail how to get your code camp or one-day event off the ground, please either leave a comment below or hit me up on [Twitter][7]!\n\nThanks for reading!\n\nD\n\n[1]: http://geekswithblogs.net/dlussier/archive/2013/07/05/153351.aspx\n[2]: https://www.picatic.com/\n[3]: https://groups.google.com/forum/#!forum/speaknet\n[4]: https://www.surveymonkey.com/\n[5]: http://www.ictam.ca/\n[6]: https://guidebook.com/\n[7]: http://www.twitter.com/darcy_lussier\n  ","categories":[],"tags":[]},{"title":"MVC 6 Image Tag Helper","authorId":"dave_paquette","slug":"mvc-6-image-tag-helper","date":"2015-07-02 17:39:40+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/mvc-6-image-tag-helper/","link":"","permalink":"https://westerndevs.com/_/mvc-6-image-tag-helper/","excerpt":"ASP.NET 5 Beta 5 shipped yesterday and it includes a new tag helper: the [Image tag helper](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/ImageTagHelper.cs. While this is a very simple tag helper, it has special meaning for me. Implementing this tag helper was my first pull request submitted to the aspnet/mvc repo.","raw":"---\nlayout: post\ntitle:  MVC 6 Image Tag Helper\ndate: 2015-07-02T09:39:40-04:00\ncategories:\ncomments: true\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/07/01/mvc-6-image-tag-helper.aspx\n---\n\n[ASP.NET 5 Beta 5](http://blogs.msdn.com/b/webdev/archive/2015/06/30/asp-net-5-beta5-now-available.aspx) shipped yesterday and it includes a new tag helper: the [Image tag helper](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/ImageTagHelper.cs. While this is a very simple tag helper, it has special meaning for me. Implementing this tag helper was [my first pull request](https://github.com/aspnet/Mvc/pull/2516) submitted to the aspnet/mvc repo.\n\n<!--more-->\n\nSo, what does this tag helper do? If you add the asp-file-version=â€trueâ€ attribute to an image tag, the tag helper will automatically append a version tag to the image file path. This allows you to aggressively cache an image without worrying about updated images not being sent to the client.\n\nUsing it is simple. Just add asp-file-version=â€trueâ€ to a standard img tag:\n\n{% codeblock lang:html %}\n<img src=\"~/images/logo.png\"\n     alt=\"company logo\"\n     asp-file-version=\"true\" />\n{% endcodeblock %}\n\nwhich will generate something like this:\n\n{% codeblock lang:html %}\n<img src=\"/images/logo.png?v=W2F5D366_nQ2fQqUk3URdgWy2ZekXjHzHJaY5yaiOOk\"\n     alt=\"company logo\"/>\n{% endcodeblock %}\n\nThe value of the v parameter is calculated based on the contents of the image file. If the contents of the image change, the value of the parameter will change. This forces the browser to download the new version of the file, even if the old version was cached locally. This technique is often called cache busting.\n\nNote that the attribute is named asp-file-version in Beta 5 but if you are using the Dev or Beta 6 bits it has been renamed to asp-append-version.\n\nAs I said, this is very simple tag helper but I find it to be very useful. I have been caught more than once with updated images not showing up for clients that had older versions cached locally. One recent example was when I was iterating quickly through logo designs for a site that was live in production. I could have changed the logo filename every time I updated the logo but this would have been tedious. Cache busting with the image tag helper allows me to update the image contents without having to rename the file or worry about manually changing the references to that image.\n","categories":[],"tags":[]},{"title":"On UI Testing","authorId":"kyle_baley","slug":"on-ui-testing","date":"2015-06-30 12:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/on-ui-testing/","link":"","permalink":"https://westerndevs.com/_/on-ui-testing/","excerpt":"What happens when 12 people gather to talk about UI testing?","raw":"---\nlayout: post\ntitle:  \"On UI Testing\"\ndate: 2015-06-30 08:09:05\nauthorId: kyle_baley\ncategories:\nexcerpt: What happens when 12 people gather to talk about UI testing?\ncomments: true\n---\n\nA short while ago, a group of [Devs of the Western variety](http://www.westerndevs.com) had a chat. It was the latest in a series, depending on how you define \"series\", where we gather together to discuss some topic, be it JavaScript frameworks, OO practices, or smoked meat. On this particular day, it was UI testing.\n\nI don't recall all the participants but it was a good number of the people on [this list](/whoweare). Here, I'm going to attempt to summarize the salient points but given my memory, it'll more likely be a dissertation of my own thoughts. Which is just as well as I recall doing more talking than I should have.\n\n### Should you UI test?\n\nThis was a common thread throughout. Anyone who has done a significant amount of UI testing has asked a variant of this question. Usually in the form, \"Why the &*%$ am I doing this?\"\n\nLet it not be said that UI testing is a \"set it and forget it\" affair. Computers are finicky things, UI's seemingly more so. Sometimes things can take just that one extra second to render and all of a sudden your test starts acting out a Woody Allen scene: Where's the button? There's supposed to be a button. YOU TOLD ME THERE WOULD BE A BUTTON!!!\n\nEventually, we more or less agreed that they are _probably_ worth the pain. From my own experience, working on a small team with no QA department, they saved us on several occasions. Yes, there are the obvious cases where they catch a potential bug. But there was also a time when we had to re-write a large section of functionality with no change to the UI. I felt _really_ good about having the tests then.\n\nOne counter-argument was whether you could just have a comprehensive suite of integration tests. But there's something to be said for having a test that:\n\n1) Searches for a product\n2) Adds it to the shopping cart\n3) Browses more products\n4) Checks out\n5) Goes to PayPal and pays\n6) Verifies that you got an email\n\nThis kind of integration test is hard to do, especially when you want to verify all the little UI things in between, like whether a success message showed up or whether the number of items in the shopping cart incremented by 1.\n\nWe also had the opposite debate: If you have a comprehensive suite of UI tests and are practicing BDD, do you still need TDD and unit tests? That was an interesting side discussion that warrants a separate post.\n\n### Maintenance\n\n...is ongoing. There's no getting around that. No matter how bullet-proof you make your tests, the real world will always get in the way. Especially if you integrate with third-party services (&lt;cough&gt;PayPal&lt;cough&gt;). If you plan to introduce UI tests, know that your tests will be needy at times. They'll fail for reasons unknown for several consecutive runs, then mysteriously pass again. They'll fail only at certain times of the day, when Daylight Savings Time kicks in, or only on days when Taylor Swift is playing an outdoor venue in the western hemisphere. There will be no rhyme or reason to the failures and you will never, _ever_ be able to reproduce them locally.\n\nYou'll add `sleep` calls out of frustration and check in with only a vague hope that it will work. Your pull requests will be riddled with variations of \"I swear I wouldn't normally do this\" and \"I HAVE NO IDEA WHAT'S GOING ON\". You'll replace elegant CSS selectors with XPath so grotesque that Alan Turing will rise from his grave only to have his rotting eyeballs burst into flames at the sight of it.\n\nThis doesn't really jibe with the \"probably worth it\" statement earlier. It depends on how often you have to revisit them and how much effort goes into it. From my experience, early on the answer to that is often and a lot. As you learn the tricks, it dwindles significantly.\n\nOne of those tricks is the [PageObject pattern](http://martinfowler.com/bliki/PageObject.html). There was universal agreement that it is required when dealing with UI tests. I'll admit I hadn't heard of the pattern before the discussion but at the risk of sounding condescending, it sounds more like common sense than an actual pattern. It's something that, even if you don't implement it right away, you'll move toward naturally as you work with your UI tests.\n\n### Data setup\n\n...is hard, too. At least in the .NET world. Tools like [Tarantino](https://github.com/HeadspringLabs/Tarantino) can help by creating scripts to prime and tear down a database. You can also create an endpoint (on a web app) that will clear and reset your database with known data.\n\nThe issue with these approaches is that the \"known\" data has to actually _be_ known when you're writing your tests. If you change anything in it, Odin knows what ramifications that will have.\n\nYou can mitigate this a little depending on your technology. If you use SpecFlow, then you may have direct access to the code necessary to prime your database. Otherwise, maybe you can create a utility or API endpoints that allow you to populate your data in a more transparent manner. This is the sort of thing that a ReST endpoint can probably do pretty well.\n\n### Mobile\n\nConsensus for UI testing on mobile devices is that it sucks more than that time after the family dinner when our cousin, Toothless Maggie, cornered---...umm... it's pretty bad...\n\nWe would love to be proven wrong but to our collective knowledge, there are no decent ways to test a mobile UI in an automated fashion. From what I gather, it's no picnic doing it in a manual fashion. Emulators are laughably bad. And there are more than a few different types and versions of mobile device so you have to use these laughably bad options about a dozen different ways.\n\n### Outsourcing\n\nWhat about companies that will run through all your test scripts on multiple browsers and multiple devices? You could save some development pain that way. But I personally wouldn't feel comfortable unless the test scripts were _extremely_ prescriptive. And if you're going to that length, you could argue that it's not a large effort to take those prescriptive steps and automate them.\n\nThat said, you might get some quick bang for your buck going this route. I've talked to a couple of them and they are always eager to help you. Some of them will even record their test sessions which I would consider a must-have if you decide to use a company for this.\n\n### Tooling\n\nI ain't gonna lie. I like Cucumber and [Capybara](https://github.com/jnicklas/capybara). I've tried [SpecFlow](http://www.specflow.org/) and it's probably as good as you can get in C#, which is decent enough. But it's hard to beat `fill_in 'Email', :with => 'hill@billy.edu'` for conciseness and readability. That said, do **not** underestimate the effort it takes to introduce Ruby to a .NET shop. There is a certain discipline required to maintain your tests and if everyone is scared to dive into your rakefile, you're already mixing stripes with plaid.\n\nWe also discussed [Canopy](http://lefthandedgoat.github.io/canopy/) and there was a general appreciation for how it looks though [Amir](/bios/amir_barylko) is the only one who has actually used it. Seems to balance the readability of Capybara with the \"it's still .NET\" aspect of companies that fear anything non-Microsoft.\n\nOf course, there's Selenium both the IDE and the driver. We mentioned it mostly because you're supposed to.\n\nSome version of Visual Studio also provided support for UI tests, both recorded and coded. The CodedUI tests are supposed to have a pretty nice fluent interface and we generally agreed that coded tests are the way to go instead of recorded ones (as if that were ever in doubt).\n\nEd. note: Shout out to [Protractor](https://angular.github.io/protractor/#/) as well. We didnâ€™t discuss it directly but as [Dave Paquette](http://www.westerndevs.com/bios/dave_paquette/) pointed out later, it helps avoid random Sleep calls in your tests because it knows how to wait until binding is done. Downside is that itâ€™s specific to Angular.\n\nAlso: [jasmine](http://jasmine.github.io/) and [PhantomJS](http://phantomjs.org/) got passing mentions, both favorable.\n\n### Continuous Integration\n\nThis is about as close as we got to disagreement. There was a claim that UI tests shouldn't be included in CI due to the length of time it takes to run them. Or if they are included, run them on a schedule (i.e. once a night) rather than on every \"check in\" (by which we mean, every feature).\n\nTo me, this is a question of money. If you have a single server and a single build agent, this is probably a valid argument. But if you want to get full value from your UI tests, get a second agent (preferably more) and run only the UI tests on it. If it's not interfering with your main build, it can run as often as you like. Yes, you may not get the feedback right away but you get it sooner than if you run the UI tests on a schedule.\n\n***\n\nThe main takeaway we drew from the discussion, which you may have gleaned from this summary, is: damn, we should have recorded this. That's a mistake we hope to rectify for future discussions.\n","categories":[],"tags":[]},{"title":"Repository nightmares","authorId":"amir_barylko","slug":"repository-nightmares","date":"2015-06-24 06:17:24+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/repository-nightmares/","link":"","permalink":"https://westerndevs.com/_/repository-nightmares/","excerpt":"The Repository pattern is a famous (or infamous?) pattern that we can find in Martin Fowler's Patterns of Enterprise Application Architecture. It was meant to be used as an interface to a collection, but what I have seen more often is that it becomes an abstraction to the data layer or ORM framework. Not so long ago I did a presentation on Who killed object oriented programming, and I mentioned the Repository implementation as one of the culprits. So what's so bad about it? What can we do to improve it? I have a counter-proposal: What if we don't need it at all?","raw":"---\nlayout: post\ntitle: \"Repository nightmares\"\ndate: 2015-06-23 21:17:24 -0500\nauthorId: amir_barylko\noriginalurl: http://orthocoders.com/blog/2015/06/23/repository-nightmares\ncategories: \ncomments: true\n---\n\nThe ```Repository``` pattern is a famous (or infamous?) pattern that we can find in Martin Fowler's _[Patterns of Enterprise Application Architecture](http://martinfowler.com/eaaCatalog/repository.html)_.\n\nIt was meant to be used as an interface to a collection, but what I have seen more often is that it becomes an abstraction to the data layer or ORM framework. \n\nNot so long ago I did a presentation on _[Who killed object oriented programming](http://www.slideshare.net/amirbarylko/who-killed-object-oriented-design)_, and I mentioned the ```Repository``` implementation as one of the culprits.\n\nSo what's so bad about it? What can we do to improve it? \n\nI have a counter-proposal: What if we don't need it at all?\n\n<!--more-->\n\n## Repository overpopulation\n\nHave you ever felt that the ``Repositories`` in your code replicate at night? You remember that when you started the project you had one or two, but now the number has grown to almost one per data model.\n\nI remember starting with something like this:\n\n{% codeblock lang:csharp %}\npublic interface ICustomerRepository {\n  IEnumerable<Customer> GetCustomers();\n}\n{% endcodeblock %}\n\nAnd then I needed to add queries such as find by ```Address```, find by ```Id``` and find which customers have pending invoices:\n\n{% codeblock lang:csharp %}\npublic interface ICustomerRepository {\n  IEnumerable<Customer> FindByAddress(Address address);\n  IEnumerable<Customer> FindById(CustomerId id);\n  IEnumerable<Customer> FindWithPendingInvoices();    \n}\n{% endcodeblock %}\n\nOnce one was completed, I replicated the recipe for each model: ```Invoice```, ```Car```, ```Chair```, etc.\n\n### Eliminating the repetition\n\nMaybe we don't need one per model. What if we could use a generic class?\n\n{% codeblock lang:csharp %}\npublic interface IRepository<T> {\n  IEnumerable<T> GetAll();\n}\n{% endcodeblock %}\n\nNow that's slightly better, but what happened to the queries? \n\n### Extracting custom queries\n\nInstead of adding queries following the business rules and modifying the ```Repository``` each time, why not use ```IQueryable``` instead? That way we can combine the results with further queries.\n\n{% codeblock lang:csharp %}\npublic interface IRepository<out T> {\n  IQueryable<T> GetAll();\n}\n{% endcodeblock %}\n\nThen we just need to combine the filtering and ordering after:\n\n{% codeblock lang:csharp %}\nvar customers = ... // instantiate a concrete IRepository<Customer>\n  \nreturn customers\n    .Where(c => c.Address != null)\n    .OrderBy(c => c.Name)\n    .Take(100);\n{% endcodeblock %}\n\n\n### Common queries\n\nWe can easily put this logic (if it's really commonly used) into an extension method (or another class) to avoid repeating the same query and to capture complexity.\n\n{% codeblock lang:csharp %}\npublic static class CustomerQueries {\n  public static IQueryable<Customer> WithAddress(this IQueryable<Customer> customers) {\n    return customers\n        .Where(c => c.Address != null)\n        .OrderBy(c => c.Name)\n        .Take(100);\n  }\n}\n{% endcodeblock %}\n\n\nThis would allow us to reuse the query when needed:\n\n{% codeblock lang:csharp %}\nvar customers = ... // instantiate a concrete IRepository<Customer>\n  \nreturn customers.WithAddress();\n{% endcodeblock %}\n\n\n## Abstracted abstraction\n\nWeâ€™ve done a great job (pat on the back) so far! Now itâ€™s much simpler...so simple that when we look at the implementation, we see we are just using the ORM (NHibernate, Entity Framework, etc.) to return the main collection and nothing else.\n\nThe result in this case is that the ```Repository``` pattern is abstracting us from another abstraction! What are we gaining from this?\n\nWhy not just use the ORM? Are we planning on changing ORMs in the middle on the project?\n\nWe should take advantage of all the power that the ORM is giving us. We don't want to have to copy the ability to do joins, sorting, etc.\n\nWeeping...is cathartic...\n\n## What about testing?\n\nAnother reason to introduce the ```Repository``` could be testing.\n\nBut what are we going to test? Should we be testing the method ```GetAll```?\n\nAre we planning to mock ```IQueryable```? Or, even further, mock all the methods that come with the ORM session?\n\nThe logic is mostly in the queries or, for example, in controllers doing queries in a web application.\n\nUnit tests won't work in this case. We don't want to depend on which methods are used or in which order theyâ€™re used, so we need to make sure the inputs are defined and write assertions on the outputs enforcing the pre- and post-condition.\n\nThis case calls for integration tests (small tests setting up the database before and cleaning it up after) to help us ensure it works as expected.\n\n## Summary\n\nMake your life simpler and your work more enjoyable. Abstractions should make the code easier to read. If that is not the case, then it is time to reflect and review.\n\nPerhaps it is a bit late to change the project we are working on right now, but next time letâ€™s make sure we give quite a bit more thought to using the ```Repository``` pattern.\n","categories":[],"tags":[]},{"title":"Creating custom MVC 6 Tag Helpers","authorId":"dave_paquette","slug":"creating-custom-mvc-6-tag-helpers","date":"2015-06-23 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/creating-custom-mvc-6-tag-helpers/","link":"","permalink":"https://westerndevs.com/_/creating-custom-mvc-6-tag-helpers/","excerpt":"In the last few blog posts, I have spent some time covering the tag helpers that are built in to MVC 6. While the built in tag helpers cover a lot of functionality needed for many basic scenarios, you might also find it beneficial to create your tag helpers from time to time. In this post, I will show how you can easily create a simple tag helper to generate a Bootstrap progress bar. NOTE: Thank you to James Chambers for giving me the idea to look at bootstrap components for ideas for custom tag helpers.","raw":"---\nlayout: post\ntitle:  \"Creating custom MVC 6 Tag Helpers\"\ndate:   2015-06-22 20:09:05\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/06/22/creating-custom-mvc-6-tag-helpers.aspx\ncategories:\ncomments: true\n---\nIn the last few blog posts, I have spent some time covering the tag helpers that are built in to MVC 6. While the built in tag helpers cover a lot of functionality needed for many basic scenarios, you might also find it beneficial to create your tag helpers from time to time.\n\nIn this post, I will show how you can easily create a simple tag helper to generate a Bootstrap progress bar. _NOTE: Thank you to [James Chambers][1] for giving me the idea to look at bootstrap components for ideas for custom tag helpers._\n\n<!--more-->\n\nBased on the documentation for the bootstrap [progress bar component][2], we need to write the following HTML to render a simple progress bar:\n\n{% codeblock lang:html %}\n<div class=\"progress\">\n  <div class=\"progress-bar\" role=\"progressbar\" aria-valuenow=\"60\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: 60%;\">\n    <span class=\"sr-only\">60% Complete</span>\n  </div>\n</div>\n{% endcodeblock %}\n\nThis is a little verbose and it can also be easy to forget some portion of the markup like the aria attributes or the role attribute. Using tag helpers, we can make the code easier to read and ensure that the HTML output is consistent everywhere we want to use a bootstrap progress bar.\n\n## Choosing your syntax\n\nWith tag helpers, you can choose to either create your own custom tag names or augment existing tags with special tag helper attributes. Examples of custom tags would be the [environment tag helper][3] and the [cache tag helper][4]. Most of the other built in MVC 6 tag helpers target existing HTML tags. Take a look at the [input tag helper][5] and the [validation tag helper][6] for examples.\n\nAt this point, I'm not 100% sure when one is more appropriate than the other. Looking at the built in MVC 6 tag helpers, the only tag helpers that target new elements are those that don't really have a corresponding HTML element that would make sense. Mostly, I think it depends on what you want your cshtml code to look like and this will largely be a personal preference. In this example, I am going to choose to target the `<div>` tag but I could have also chosen to create a new `<bs_progress_bar>` tag. The same thing happens in the angularjs world. Developers in angularjs have the option to create declare directives that are used as attributes or as custom elements. Sometimes there is an obvious choice but often it comes down to personal preference.\n\nSo, what I want my markup to look like is this:\n\n{% codeblock lang:html %}\n<div bs-progress-value=\"@Model.CurrentProgress\"\n     bs-progress-max=\"100\"\n     bs-progress-min=\"0\">\n</div>\n{% endcodeblock %}\n\nNow all we need to do is create a tag helper turn this simplified markup into the more verbose markup needed to render a bootstrap progress bar.\n\n## Creating a tag helper class\n\nTag helpers are pretty simple constructs. They are classes that inherit from the base TagHelper class. In this class, you need to do a few things.\n\nFirst, you need to specify a TargetElement attribute: this is what tells Razor which HTML tags / attributes to associate with this tag helper. In our case, we want to target any\n element that has the bs-progress-value element.\n\n{% codeblock lang:csharp %}\n[TargetElement(\"div\", Attributes = ProgressValueAttributeName)]\npublic class ProgressBarTagHelper : TagHelper\n{\n    private const string ProgressValueAttributeName = \"bs-progress-value\";    \n    //....\n}\n{% endcodeblock %}\n\nThe Attributes parameter for the TargetElement is a coma separated list of attributes that are _required _for this tag helper. I'm not making the bs-progress-min and bs-progress-max attributes required. Instead, I am going to give them a default of 0 and 100 respectively. This brings us to the next steps which is defining properties for any of the tag helper attributes. Define these as simple properties on the class and annotate them with an HtmlAttributeName attribute to specify the attribute name that will be used in markup.\n\n{% codeblock lang:csharp %}\n[TargetElement(\"div\", Attributes = ProgressValueAttributeName)]\npublic class ProgressBarTagHelper : TagHelper\n{\n    private const string ProgressValueAttributeName = \"bs-progress-value\";\n    private const string ProgressMinAttributeName = \"bs-progress-min\";\n    private const string ProgressMaxAttributeName = \"bs-progress-max\";\n\n    [HtmlAttributeName(ProgressValueAttributeName)]\n    public int ProgressValue { get; set; }\n\n    [HtmlAttributeName(ProgressMinAttributeName)]\n    public int ProgressMin { get; set; } = 0;\n\n    [HtmlAttributeName(ProgressMaxAttributeName)]\n    public int ProgressMax { get; set; } = 100;\n\n    //...\n}\n{% endcodeblock %}\n\nThese attributes are strongly typed which means Razor will give you errors if someone tries to bind a string or a date to one of these int properties.\n\nFinally, you need to override either the Process or the ProcessAsync method. In this example, the logic is simple and does not require any async work to happen so it is probably best to override the Process method. If the logic required making a request or processing a file, you would be better off overriding the ProcessAsync method.\n\n{% codeblock lang:csharp %}\n[TargetElement(\"div\", Attributes = ProgressValueAttributeName)]\npublic class ProgressBarTagHelper : TagHelper\n{\n    private const string ProgressValueAttributeName = \"bs-progress-value\";\n    private const string ProgressMinAttributeName = \"bs-progress-min\";\n    private const string ProgressMaxAttributeName = \"bs-progress-max\";\n\n    /// <summary>\n    /// An expression to be evaluated against the current model.\n    /// </summary>\n    [HtmlAttributeName(ProgressValueAttributeName)]\n    public int ProgressValue { get; set; }\n\n    [HtmlAttributeName(ProgressMinAttributeName)]\n    public int ProgressMin { get; set; } = 0;\n\n    [HtmlAttributeName(ProgressMaxAttributeName)]\n    public int ProgressMax { get; set; } = 100;\n\n    public override void Process(TagHelperContext context, TagHelperOutput output)\n    {\n        var progressTotal = ProgressMax - ProgressMin;\n\n        var progressPercentage = Math.Round(((decimal) (ProgressValue - ProgressMin) / (decimal) progressTotal) * 100, 4);\n\n        string progressBarContent =\n            string.Format(\n@\"<div class='progress-bar' role='progressbar' aria-valuenow='{0}' aria-valuemin='{1}' aria-valuemax='{2}' style='width: {3}%;'>\n<span class='sr-only'>{3}% Complete</span>\n</div>\", ProgressValue, ProgressMin, ProgressMax, progressPercentage);\n\n        output.Content.Append(progressBarContent);\n\n        string classValue;\n        if (output.Attributes.ContainsKey(\"class\"))\n        {\n            classValue = string.Format(\"{0} {1}\", output.Attributes[\"class\"], \"progress\");\n        }\n        else\n        {\n            classValue = \"progress\";\n        }\n\n        output.Attributes[\"class\"] = classValue;\n    }\n}\n{% endcodeblock %}\n\nThe Process method has 2 parameters: a TagHelperContext and a TagHelperOutput. In this simple example, we don't need to worry about the TagHelperContext. It contains information about the input element such as the attributes that were specified there and a unique ID that might be needed if multiple instances of the tag helpers were used on a single page. The TagHelperOutput is where we need to specify the HTML that will be output by this tag helper. We start by doing some basic math to calculate the percentage complete of the progress bar. Next, I used a string.Format to build the inner HTML for the bootstrap progress bar with the specified min, max, value and calculated percentages. I add this to the contents of the output by calling output.Content.Append. The last step is to add class=\"progress\" to the outer div. I can't just add the attribute though because there is a chance that the developer has already specified another class for this div (it is possible that we want the output to be _class=\"green progress\"_.\n\nIf you need to build more complicated HTML in the content, you should consider using the [TagBuilder class][7]. If a tag helper grows too complex, you might want to consider creating a View Component instead.\n\nFinally, we should add some argument checking to make sure that the Min / Max and Value properties are appropriate. For example, the ProgressMin value should be less than the ProgressMax value. We can throw argument exceptions to clearly indicate errors. Here is the finally implementation of the ProgressBarTagHelper:\n\n{% codeblock lang:csharp %}\n[TargetElement(\"div\", Attributes = ProgressValueAttributeName)]\npublic class ProgressBarTagHelper : TagHelper\n{\n    private const string ProgressValueAttributeName = \"bs-progress-value\";\n    private const string ProgressMinAttributeName = \"bs-progress-min\";\n    private const string ProgressMaxAttributeName = \"bs-progress-max\";\n\n    /// <summary>\n    /// An expression to be evaluated against the current model.\n    /// </summary>\n    [HtmlAttributeName(ProgressValueAttributeName)]\n    public int ProgressValue { get; set; }\n\n    [HtmlAttributeName(ProgressMinAttributeName)]\n    public int ProgressMin { get; set; } = 0;\n\n    [HtmlAttributeName(ProgressMaxAttributeName)]\n    public int ProgressMax { get; set; } = 100;\n\n    public override void Process(TagHelperContext context, TagHelperOutput output)\n    {\n        if (ProgressMin >= ProgressMax)\n        {\n            throw new ArgumentException(string.Format(\"{0} must be less than {1}\", ProgressMinAttributeName, ProgressMaxAttributeName));\n        }\n\n        if (ProgressValue > ProgressMax || ProgressValue < ProgressMin)\n        {\n            throw new ArgumentOutOfRangeException(string.Format(\"{0} must be within the range of {1} and {2}\", ProgressValueAttributeName, ProgressMinAttributeName, ProgressMaxAttributeName));\n        }\n        var progressTotal = ProgressMax - ProgressMin;\n\n        var progressPercentage = Math.Round(((decimal) (ProgressValue - ProgressMin) / (decimal) progressTotal) * 100, 4);\n\n        string progressBarContent =\n            string.Format(\n@\"<div class='progress-bar' role='progressbar' aria-valuenow='{0}' aria-valuemin='{1}' aria-valuemax='{2}' style='width: {3}%;'>\n<span class='sr-only'>{3}% Complete</span>\n</div>\", ProgressValue, ProgressMin, ProgressMax, progressPercentage);\n\n        output.Content.Append(progressBarContent);\n\n        string classValue;\n        if (output.Attributes.ContainsKey(\"class\"))\n        {\n            classValue = string.Format(\"{0} {1}\", output.Attributes[\"class\"], \"progress\");\n        }\n        else\n        {\n            classValue = \"progress\";\n        }\n\n        output.Attributes[\"class\"] = classValue;\n\n        base.Process(context, output);\n    }\n}\n{% endcodeblock %}\n\n## Referencing the custom Tag Helper\n\nBefore we can start using our custom tag helper, we need to add a reference to the tag helpers in the current assembly using the @addTagHelper Razor command. We can do this in individual Razor files or we can add it the _GlobalImports.cshtml file so it is applied everywhere:\n\n{% codeblock lang:csharp %}\n@using WebApplication3\n@using WebApplication3.Models\n@using Microsoft.Framework.OptionsModel\n@using Microsoft.AspNet.Identity\n@addTagHelper \"*, Microsoft.AspNet.Mvc.TagHelpers\"\n@addTagHelper \"*, WebApplication3\"  \n{% endcodeblock %}\n\nNow we can reference the tag helper in any of our Razor views. Here is an example binding the PercentComplete property from the model to the ProgressValue property of the tag helper.\n\n{% codeblock lang:html %}\n<div bs-progress-value=\"@Model.PercentComplete\"></div>\n{% endcodeblock %}\n\nHere is another example that would display progress for a 5 step process:\n\n{% codeblock lang:html %}\n<div bs-progress-min=\"1\"\n     bs-progress-max=\"5\"\n     bs-progress-value=\"@Model.CurrentStep\">\n</div>\n{% endcodeblock %}\n\n### Unit Testing\n\nTag Helpers are easy enough to unit test and I would definitely recommend that you do test them. Take a look at [these examples][8] in the MVC 6 repo for reference.\n\n## Conclusion\n\nBy creating this simple tag helper, we are able to greatly simplify the Razor code required to create a bootstrap progress bar with a value from the our model. As a result, our Razor view is easier to understand and we can ensure a consistent output for all progress bars in our app. If we needed to change the way we rendered progress bars, we would only need to change the code in one place. This is of course a relatively simple example but I think it shows the potential for tag helpers in MVC 6.\n\n[1]: http://jameschambers.com/\n[2]: http://getbootstrap.com/components/#progress\n[3]: http://www.davepaquette.com/archive/2015/05/05/web-optimization-development-and-production-in-asp-net-mvc6.aspx\n[4]: http://www.davepaquette.com/archive/2015/06/03/mvc-6-cache-tag-helper.aspx\n[5]: http://www.davepaquette.com/archive/2015/05/13/mvc6-input-tag-helper-deep-dive.aspx\n[6]: http://www.davepaquette.com/archive/2015/05/14/mvc6-validation-tag-helpers-deep-dive.aspx\n[7]: https://github.com/aspnet/Mvc/blob/master/src/Microsoft.AspNetCore.Mvc.ViewFeatures/Rendering/TagBuilder.cs\n[8]: https://github.com/aspnet/Mvc/tree/dev/test/Microsoft.AspNetCore.Mvc.Test\n","categories":[],"tags":[]},{"title":"Review - Guidebook, mobile app for conferences/events","authorId":"darcy_lussier","slug":"guidebook-mobile-app-for-conferences","date":"2015-06-22 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/guidebook-mobile-app-for-conferences/","link":"","permalink":"https://westerndevs.com/_/guidebook-mobile-app-for-conferences/","excerpt":"For Prairie Dev Con Regina I decided to use Guidebook â€“ a service that provides a mobile application for conferences and events along with online administration/content-management tools. If you're running any sort of conference/event, you should definitely check it out! Overall I was really happy with it, only a few minor gripes. Here's my review.","raw":"---\nlayout: post\ntitle:  \"Review - Guidebook, mobile app for conferences/events\"\ndate:   2015-06-21 20:09:05\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/06/21/165250.aspx\ncategories:\ncomments: true\n---\n\nFor Prairie Dev Con Regina I decided to use [Guidebook][1] â€“ a service that provides a mobile application for conferences and events along with online administration/content-management tools. If you're running any sort of conference/event, you should definitely check it out! Overall I was really happy with it, only a few minor gripes. Here's my review.\n\n<!--more-->\n\n![Image result for guidebook logo][2]\n\nThe way Guidebook works is that you create your \"Guidebook\" on their website. Then your attendees download the Guidebook app and access your specific guidebook. There's different pricing tiers which grants different features, but for the free tier (which is capped at 200 downloads comes with almost all the key features. For PrDC Regina I used the following:\n\nGeneral Info â€“ Info about the event, contact information etc.\n\nSchedule â€“ Mobile version of the schedule, complete with session description and the ability to \"add to favorites\"\n\nMy Schedule â€“ Allows you to see all your favourited sessions by day and time! Worked really well.\n\nMaps â€“ You upload your own map images for the venue, its displayed here. Multiple maps are allowed.\n\nToDo â€“ Just a place for attendees to add notes.\n\nTwitter â€“ A twitter feed based on whatever accounts you pre-configure.\n\nInbox â€“ This allows you to message attendees with updates, although I don't think you can get push messages unless you do a higher tier of service.\n\nCustom List â€“ You can add custom lists of information. I'll talk about what I used this for later.\n\nFor most events, this is more than sufficient â€“ it worked great and it was free! Feedback on the app from attendees was very positive as well. Critique wise, there's only a few things I'd mention.\n\n### Maps\n\nIf you want to see multiple maps, you have to press on the navigation arrows at the top. Some people didn't realize that, because their default was to try and swipe left or right from the map image to get to the next one.\n\n### Speaker Integration\n\nIt would have been great to have a Speakers section as well, and integrate the speaker information with the schedule data â€“ so attendees could see the abstract for a session as well as the speaker details.\n\n### Feedback and Surveys\n\nRemember that custom list I mentioned about? So you don't get feedback and surveys unless you go for a higher tier of service. What I did was put a link to the Survey Monkey survey for session feedback in a custom list and made it available. Now, I don't want to necessarily suggest that this needs to be available in the free edition because I get it â€“ Guidebook is a business and they incentivize people to get more features by paying. But this leads me to my final pointâ€¦\n\n### Payment Tiers\n\nThe tiers seem a littleâ€¦off. You get a lot of features for free up to 200 attendees. My events usually cap out at 150 so I'm good. I'd love to offer the surveys and feedback feature in the app directly, but I can't justify the $1750 price tag *just* for that feature. It would be awesome if they had an a-la-carte type of scheme, where for x dollars I could get different features, or have another tier for smaller events that provide some of the other features but for a cost (just capped at 200 downloads).\n\nI love what Guidebook is doing and I applaud them for providing such an awesome service at a free tier that so many community-based events can take advantage of! Very happy with the experience and I'll be looking to use them again at next year's PrDC!\n\n[1]: https://guidebook.com/\n[2]: data:image/png;base64,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\n  ","categories":[],"tags":[]},{"title":"Rethinking our practices with the MVC framework","authorId":"james_chambers","slug":"rethinking-our-practices-with-the-mvc-framework","date":"2015-06-15 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/rethinking-our-practices-with-the-mvc-framework/","link":"","permalink":"https://westerndevs.com/_/rethinking-our-practices-with-the-mvc-framework/","excerpt":"We get set in our ways, don't we? It's funny how the sharper and more confident we get with our frameworks and the tooling we employ to work with them, we also get a little more attached to our way of doing things. And then along comes a major version change, ripe with breaking changes and new bits to twiddle and we're left saying, &quot;But, that's not how we've always done it!&quot;.","raw":"---\nlayout: post\ntitle:  \"Rethinking our practices with the MVC framework\"\ndate:   2015-06-14 20:09:05\nauthorId: james_chambers\noriginalurl: http://jameschambers.com/2015/06/rethinking-our-practices-with-the-mvc-framework/\ncategories:\ncomments: true\n---\n\nWe get set in our ways, don't we? It's funny how the sharper and more confident we get with our frameworks and the tooling we employ to work with them, we also get a little more attached to our way of doing things. And then along comes a major version change, ripe with breaking changes and new bits to twiddle and we're left saying, \"But, that's not how we've always done it!\".\n\n<!--more-->\n\nCase in point: **service injection into views**. In ASP.NET's MVC Framework 6 we get this new concept which, if we're going to accept, requires that we relax on our thinking of how we've always done things.\n\n> My friends [Dave Paquette][1], [Simon Timms][2] and [myself][3] have been ruffling through a few of these types of changes, and Simon did a great job of illustrating how we used to get data into our views, and how we might do it in [tomorrow's MVC][4].  For a walkthrough of service injection I highly recommend his article on it.\n\nHow does it work? The new inject feature gives us the ability to asynchronously invoke methods on classes that are dynamically created and given to our view. It's IoC for your UI.\n\nPersonally, I'd been wrestling with a good use case here because we had a way to do it, and it seems an obvious one (illustrated by Simon) had been missing my thought stream, likely because it's been clouded for a few years with ViewBag. In all reality, the idea of using the ViewBag â€“ a dynamic object that is double-blind, easily forgotten about and easily polluted â€“ to push bits of data to the view has always kind of bugged me, no less than using filters did, but we didn't have an elegant, framework-driven mechanism to make it happen more gracefully.  We do now.\n\n> Also, let's not confuse things here: In more cases than not, your ViewModel is going to be the correct place to put your data, and where I've put my data for most things â€“ like drop down lists â€“ but this type of feature is exciting because it opens the door to explore new options in building views and experiences for our users.\n\n## But, doesn't it break the design of MVC?\n\n![Source: http://www.nv.doe.gov/library/photos/][5]\n\nSometimes things blow up when you try them out, but you still gotta try.\n\nPerhaps. Maybe, if you want to say, \"The only definition valid for any framework is the original definition.\" But we have more tools today to do our job, and in particular for this case dependency injection which has become a first-class citizen in ASP.NET. So, let's rewind a bit and ask, why is it a bad practice to give a component the pieces it needs to do its work?\n\nLet's think of the type of problem that we're trying to solve here, as Simon did in his article: a view needs to populate a dropdown list. It doesn't need to access the database, and it shouldn't have it. It doesn't need to know a connection string, or if data is coming from a cache, a web service or otherwise, it just needs the data. Giving it an interface by which to look it up, well, to me that seems like a good idea.\n\nIf instead you favor the approach of using the controller to populate the ViewBag or use filters (or other techniques) you inherently introduce coupling to a specific view in the controller by forcing it to look up data to populate a dropdown box. _You are still injecting data into the view._ In my mind, the controller should know as little as possible about the view.  Why should I have to change my controller if I need to change my view?\n\n> I want to make a clear distinction here, though, as I do believe the controller answers very specific concerns, namely, those that deal with a particular entity. But the PersonController shouldn't have to know the list of Canadian Provinces, should it?\n\n## Don't need to know where I'm going, just need to know where I've been\n\nThe assumption that the controller provides everything the view needs is guided by past pretence. It was true in MVC5 and earlier because it was what we had to work with. My point is that in MVC6 we now have a construct that allows:\n\n* Separation of concern/single responsibility\n* Testability\n* Type safety\n* Injectable dependencies\n\nIn my mind, the controller is just a component. So is the view. The controller's concerns are related to the entity in question. The view is required to render correct UI such that a form can be filled out in a way that satisfies the requirements of the view model. Again, why use a person controller to retrieve details about countries and states?\n\nI don't see controllers as any more important than any other component. They have things they need, and they should have those things injected. My controllers don't talk to the database, they talk to command objects and query objects via interface and those are injected from an IoC container.\n\nI think now, with views as first-class components, that we can look at views in the same way.\n\n## But what about ViewBag?\n\nWith ViewBag (and filters) we have a problem that we're not really talking about in the best interest of not upsetting anyone. The fact that my controller has to do the lifting for the combo boxes is awkward and doesn't really help us out too much with maintaining SRP. But we didn't previously have a good way to address this.\n\nWe also tend to overlook the fact that Views are effectively code. Why can't our principles apply to them as well? Of course I shouldn't access the database from the view, but why can't I know about an interface that does (and have it injected)?\n\nThis is a great use case of this new feature, and one that demonstrates that \"not changing for the sake of not changing\" isn't a good mantra. If my view code/class/script is responsible for rendering the view, I see no problem injecting into it the things it needs to do so.\n\nAfter all, isn't that what you're doing with ViewBag? Just injecting things into the view through the Dynamic? Except, with ViewBag, no one sees type problems and everyone has to cast. Now we've got run time errors.\n\nThere is the argument that says that even if we're abstracting away the data access, we're introducing the ability for the view to call the database. Again, I don't think the view is any less important a component in the scheme of things, and there is a level of appropriateness with which we must use the feature. Will it be abused? Likely. You don't want to be injecting database change-capable components into the view, but that is more a case of bad choices in implementation. You can completely destroy the maintainability of a project and wreak havoc on your users with service injection, but that doesn't mean you should avoid it. I've seen people write 1,000 lines of code in a method, but that doesn't mean I don't use methods any more.\n\n**When changes come to frameworks, I think it's okay to rethink our best practices**. Taking Simon's approach we have:\n\n* Interface-based injection\n* Abstraction from underlying data access strategy (db, cache, text file, whatever)\n* Testable components\n* Maintaining SRP in our controller and view\n* No casting from dynamic to proper types\n\nI'm okay with this approach and will be using this approach in MVC 6 projects.\n\nI highly encourage you to do your own reading on this and explore the feature in greater detail. Here are a few links for your consideration.\n\nHappy coding! ![Smile][6]\n\n_Image credit: http://www.nv.doe.gov/library/photos/_\n\n[1]: https://twitter.com/dave_paquette\n[2]: https://twitter.com/stimms\n[3]: https://twitter.com/canadianjames\n[4]: http://blog.simontimms.com/2015/06/09/getting-lookup-data-into-you-view/\n[5]: http://jameschambers.com/wp-content/uploads/2015/06/nuke-300x188.jpg\n[6]: http://jameschambers.com/wp-content/uploads/2015/06/wlEmoticon-smile.png\n  ","categories":[],"tags":[]},{"title":"Thoughts on building, making, being positive, fearing failure, and smoking meat","authorId":"darcy_lussier","slug":"thoughts-on-building-making","date":"2015-06-15 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/thoughts-on-building-making/","link":"","permalink":"https://westerndevs.com/_/thoughts-on-building-making/","excerpt":"I was recently gifted a smoker from my awesome Prairie Dev Con speakers â€“ an electric Bradley 4 rack smoker! Upon reading the instructions, I realized that due to the &quot;electronic&quot; nature of the device, it can't just be left outside to the elements like a BBQ can. So I started looking around at custom enclosures people were building and got some ideas. Now, I'm not what you would call a handy-man type of guy. I didn't grow up doing carpentry projects or working on cars or anything like that, but regardless yesterday I built it. Here's what it looks like (still needs stain and one more piece of plywood for the back)â€¦","raw":"---\nlayout: post\ntitle:  \"Thoughts on building, making, being positive, fearing failure, and smoking meat\"\ndate:   2015-06-14 20:09:05\nauthorId: darcy_lussier\noriginalurl: http://geekswithblogs.net/dlussier/archive/2015/06/14/165100.aspx\ncategories:\ncomments: true\n---\n\nI was recently gifted a smoker from my awesome Prairie Dev Con speakers â€“ an electric Bradley 4 rack smoker! Upon reading the instructions, I realized that due to the \"electronic\" nature of the device, it can't just be left outside to the elements like a BBQ can. So I started looking around at custom enclosures people were building and got some ideas. Now, I'm *not* what you would call a handy-man type of guy. I didn't grow up doing carpentry projects or working on cars or anything like that, but regardless yesterday I built it. Here's what it looks like (still needs stain and one more piece of plywood for the back)â€¦\n\n<!--more-->\n![SmokerEnclosure][1]\n\nThe experience was fantastic â€“ I had my daughters help me with most of the construction, so we were able to make this a quality-time project building something together. I also got to use some tools I received as an office Christmas gift from years back that I hadn't before. And I learned a lot â€“ even though I know there are things I would have done differently in retrospect, and things I know I'd like to tweak with it even after its \"finished\", it was a fantastic learning experience.\n\nI was really encouraged at Prairie Dev Con earlier this month in Regina. We had a number of hands-on-labs that were very well attended and I was in awe of how willingly people were to jump in and try something hands on that's new to them. This 'makers' movement in the IoT space, and also people willing to learn new ways of doing web development (as one commenter put it \"ready to start using new skills at work right away!\") was really exciting to see. And it inspired me to \"make\" more.\n\nI've been scared at times to try something new â€“ actually it wasn't even the \"starting\", but more of the possible failing. I don't know if its [a case of imposter syndrome][2] or not wanting to come across as being inept or what. But the first step, the first movement on a journey, can be the beginning of something awesome. The positive What If as opposed to the negative. The possibilities instead of the pitfalls. The rewards instead of the costs.\n\nOne thing I've learned is that the true feeling of success doesn't come from outside, but from within. That feeling of accomplishment that you receive when you complete something â€“ even if that something isn't 100% perfect. I approached the enclosure project like an agile software project â€“ this was iteration 1, done in a day, and with a retrospective of what I'd have done differently next time. And its functional, which is great because I want to start smoking some meat. :)\n\nI feel like I'm rambling a bit â€“ look, here's the thing. MAKE STUFF. BUILD STUFF. EXPERIMENT WITH STUFF. Don't allow external pressures or fears of \"Am I good enough\", \"Will I be accepted?\", \"Doesn't anything I do need to be perfect?\", and their ilk ruin your ability to have new experiences, in life or at work.\n\nIn a few weeks I'll be starting on a new adventure. I'll be working with a new team, in a new domain, on systems that are new to me. And I'm excited, even with all the unknowns, because I can't wait to see what I can help build with my colleagues. I never want those nagging, negative \"What Ifs\" to get in the way of what's possible.\n\n[1]: https://gwb.blob.core.windows.net/dlussier/WindowsLiveWriter/ThoughtsonBuildingMakingBeingPositiveFea_A71D/SmokerEnclosure_thumb.png \"SmokerEnclosure\"\n[2]: http://www.hanselman.com/blog/ImAPhonyAreYou.aspx","categories":[],"tags":[]},{"title":"Conference recap extravaganza!","authorId":"david_wesst","slug":"conference-recap-extravaganza","date":"2015-06-12 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/conference-recap-extravaganza/","link":"","permalink":"https://westerndevs.com/_/conference-recap-extravaganza/","excerpt":"I've been head down preparing and delivering presentations all over the place for the past few weeks, and wanted to share the latest happenings, along with where you can get your hands on the material if you weren't able to attend.","raw":"---\nlayout: post\ntitle:  \"Conference recap extravaganza!\"\ndate:   2015-06-11 20:09:05\nauthorId: david_wesst\noriginalurl: http://www.davidwesst.com/conference-recap-extravaganza/\ncategories:\ncomments: true\n---\n\nI've been head down preparing and delivering presentations all over the place for the past few weeks, and wanted to share the latest happenings, along with where you can get your hands on the material if you weren't able to attend.\n\n<!--more-->\n\nIf you're not interested in the retrospective, then you can head over to my [talks page][1] to get the skinny and the resources.\n\n### Computer Science Career Awesomeness\n\nGrant Park High School was kind of to invite me down and give me control of the computer science class for an hour to talk about \"Careers in Computer Science\". Being that this is something I often end up hearing or discussing with my peers in the tech community, I figured it would be a good chance to share my knowledge with the future Zuckerberg's and Gates' of the world.\n\nAlthough I didn't record the engaging conversation with the students, I did manage to make an [online presentation ][2] that goes through the supporting slides. If you're at all interested, definitely take a look and share your feedback with me here in the comments or via [Twitter][3].\n\n### Prairie Dev Con 2015 -- Regina, SK\n\n![][4] \n\nHere I delivered two presentations: Learning to be IDE Free (Java Edition) and 5 Reasons Why Your Website Isn't a Native App. The latter is a re-imagining of my Winnipeg talk entitled \"From Web to Device: A JavaScript Story\", where I received some great feedback from [Lori Lalonde][5] and [Mario Cardinal][6].\n\nThe presentation transformed from, what was supposed to be a happy tale of how fantastic is it is to share JavaScript code between web and device projects, to a blunt conversation about just because you _can_ do something doesn't mean you _should_. We focused on the development points, and so far the feedback has been great.\n\nNo online presentation or demo recordings are available yet, but you can get the slides and source code [here][1].\n\n### CSONHS 2015 Annual Meetup\n\n![][7] \n\nWhat you've never heard of the Canadian Society of Otolaryngology - Head & Neck Surgery? Well, I have and managed to meet and work with some fantastic physicians and educators during my time working for the Faculty of Health Sciences. One of my many projects, along with being part of the Committee for Online Learning (or CoOL the group) lead me to becoming the sort of \"tech expert\" on some of the new systems installed and used for improving medical education.\n\nI realize it's bit of a departure from what I normally discuss, but that was the idea: to change it up a bit. So far, it has proven to be a very interesting area for me and I hope to stay involved and provide technical expertise to physician educators looking to engage new tech. Plus, with things like [HoloLens ][8]and [Microsoft Band][9], I think the healthcare and the health education landscape is about to see some major changes.\n\n## The Point\n\nNo real point other than to share the awesomely busy month I've had sharing and networking around technology and that you can expect to find more of my \"talks\" show up on the [talks page][10] of the site.\n\nThanks for playing. ~ DW\n\n[1]: http://www.davidwesst.com/talks\n[2]: https://mix.office.com/watch/1awyqexjvlhq4\n[3]: https://twitter.com/davidwesst\n[4]: http://www.davidwesst.com/content/images/2015/06/PrDCLogo_Small.png\n[5]: https://twitter.com/loriblalonde\n[6]: https://mariocardinal.wordpress.com/\n[7]: http://www.davidwesst.com/content/images/2015/06/entcanada-program.png\n[8]: http://www.case.edu/hololens/\n[9]: https://www.microsoft.com/microsoft-band/en-us/developer\n[10]: http://davidwesst.com/talks/","categories":[],"tags":[]},{"title":"Getting lookup data into your view ASP.net MVC 6 version","authorId":"simon_timms","slug":"getting-lookup-data-into-you-view","date":"2015-06-10 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/getting-lookup-data-into-you-view/","link":"","permalink":"https://westerndevs.com/_/getting-lookup-data-into-you-view/","excerpt":"This is a super common problem I encounter when building ASP.net MVC applications. I have a form that has a drop down box. Not only do I need to select the correct item from the edit model to pick from the drop down but I need to populate the drop down with the possible values.","raw":"---\nlayout: post\ntitle:  \"Getting lookup data into your view ASP.net MVC 6 version\"\ndate:   2015-06-09 20:09:05\nauthorId: simon_timms\noriginalurl: http://blog.simontimms.com/2015/06/09/getting-lookup-data-into-you-view\ncategories:\ncomments: true\n---\n\nThis is a super common problem I encounter when building ASP.net MVC applications. I have a form that has a drop down box. Not only do I need to select the correct item from the edit model to pick from the drop down but I need to populate the drop down with the possible values.\n\n<!--more-->\n\nOver the years I've used two approaches to doing this. The first is to push into the ViewBag a list of values in the controller action. That looks like\n\n    public ActionResult Edit(int id){\n        var model = repository.get(id);\n        ViewBag.Provinces = provincesService.List();\n    \n        return View(model);\n    }\n\nThen in the view you can retrieve this data and use it to populate the drop down. If you're using the HTML helpers then this looks like\n\n    @Html.DropDownListFor(x=>x.province, (IEnumerable)ViewBag.Provinces)\n\nThis becomes somewhat messy when you have a lot of drop downs on a page. For instance consider something like\n\n    public ActionResult Edit(int id){\n      var model = repository.get(id);\n    \n        ViewBag.Provinces = provincesService.List();\n        ViewBag.States = statesService.List();\n        ViewBag.StreetDirections = streetDirectionsService.List();\n        ViewBag.Countries = countriesService.List();\n        ViewBag.Counties = countiesService.List();\n    \n        return View(model);\n    }\n\nThe work of building up the data in the model becomes the primary focus of the view. We could extract it to a method but then we have to go hunting to find the different drop downs that are being populated. An approach I've taken in the past is to annotate the methods with an action filter to populate the ViewBag for me. This makes the action look like\n\n    [ProvincesFilter]\n    [StatesFilter]\n    [StreetDirectionsFilter]\n    [CountriesFilter]\n    [CountiesFilter]\n    public ActionResult Edit(int id){\n      var model = repository.get(id);\n      return View(model);\n    }\n\nOne of the filters might look like\n\n    public override void OnActionExecuting(ActionExecutingContext filterContext)\n    {\n        var countries = new List();\n        if ((countries = (filterContext.HttpContext.Cache.Get(GetType().FullName) as List)) == null)\n        {\n            countries = countriesService.List();\n        }\n        filterContext.Controller.ViewBag.Countries = countries;\n        base.OnActionExecuting(filterContext);\n    }\n\nThis filter also adds a degree of caching to the request so that we don't have to keep bugging the database.\n\nKeeping a lot of data in the view bag presents a lot of opportunities for error. We don't have any sort of intellisense with the dynamic view object and I frequently use the wrong name in the controller and view, by mistake. Finally building the drop down box using the HTML helper requires some nasty looking casting. Any time I cast I feel uncomfortable.\n\n    @Html.DropDownListFor(x=>x.province, (IEnumerable)ViewBag.Provinces)\n\nNow a lot of people prefer transferring the data as part of the model; this is the second approach. There is nothing special about this approach you just put some collections into the model.\n\nI've always disliked this approach because it mixes the data needed for editing with the data for the drop downs which is really incidental. This data seems like a view level concern that really doesn't belong in the view model. This is a bit of a point of contention and I've challenged more than one person to a fight to the death over this very thing.\n\nSo neither option is particularly palatable. What we need is a third option and the new dependency injection capabilities of ASP.net MVC open up just such an option: we can inject the data services directly into the view. This means that we can consume the data right where we retrieve it without having to hammer it into some bloated DTO. We also don't have to worry about annotating our action or filling it with junk view specific code.\n\nTo start let's create a really simple service to return states.\n\n    public interface IStateService\n    {\n        IEnumerable List();\n    }\n    \n    public class StateService : IStateService\n    {\n        public IEnumerable List() {\n            return new List\n            {\n                new State { Abbreviation = \"AK\", Name = \"Alaska\" },\n                new State { Abbreviation = \"AL\", Name = \"Alabama\" }\n            };\n        }\n    }\n\nUmm, looks like we're down to only two states, sorry Kentucky.\n\nNow we can add this to our container. I took a singleton approach and just registered a single instance in the Startup.cs.\n\n    services.AddInstance(typeof(IStateService), new StateService());\n\nThis is easily added the the view by adding\n\n    @inject ViewInjection.Services.IStateService StateService\n\nAs the first line in the file. Then the final step is to actually make use of the service to populate a drop down box:\n\n    <div class=\"col-lg-12\">\n        @Html.DropDownList(\"States\", StateService.List().Select(x => new SelectListItem { Text = x.Name, Value = x.Abbreviation }))\n    </div>\n\nThat's it! Now we have a brand new way of getting the data we need to the view without having to clutter up our controller with anything that should be contained in the view.\n\nWhat do you think? Is this a better approach? Have I brought fire down upon us all with this? Post a comment. Source is at <https://github.com/stimms/ViewInjection>","categories":[],"tags":[]},{"title":"MVC 6 cache tag helper","authorId":"dave_paquette","slug":"mvc-6-cache-tag-helper","date":"2015-06-04 00:09:05+0000","updated":"2024-02-25 23:53:49+0000","comments":true,"path":"_/mvc-6-cache-tag-helper/","link":"","permalink":"https://westerndevs.com/_/mvc-6-cache-tag-helper/","excerpt":"In this post in my series exploring the ASP.NET 5 MVC 6 tag helpers, I will dig into the Cache Tag Helper. The Cache Tag Helper is a little different than most of the other tag helpers we talked about because it doesn't target a standard HTML tag. Instead, it wraps arbitrary content and allows those contents to be cached in memory based on the parameters you specify.","raw":"---\nlayout: post\ntitle:  \"MVC 6 cache tag helper\"\ndate:   2015-06-03 20:09:05\nauthorId: dave_paquette\noriginalurl: http://www.davepaquette.com/archive/2015/06/03/mvc-6-cache-tag-helper.aspx\ncomments: true\ncategories:\n---\n\nIn this post in my series exploring the ASP.NET 5 MVC 6 tag helpers, I will dig into the Cache Tag Helper. The Cache Tag Helper is a little different than most of the other tag helpers we talked about because it doesn't target a standard HTML tag. Instead, it wraps arbitrary content and allows those contents to be cached in memory based on the parameters you specify.\n\n<!--more-->\n\n## How it works?\n\nSimply wrap the contents you want cached with a __ tag and the contents of the tag will be cached in memory. Before processing the contents of the cache tag, the tag helper will check to see if the contents have been stored in the MemoryCache. If the contents are found in the cache, then the cached contents are sent to Razor. If the contents are not found, then Razor will process the contents and the tag helper will store it in the memory cache for next time. By default, the cache tag helper is able to generate a unique ID based on the context of the cache tag helper.\n\nHere is a simply example that would cache the output of a view component for 10 minutes.\n\n    <cache expires-after=\"@TimeSpan.FromMinutes(10)\">    \n        @Html.Partial(\"_WhatsNew\")\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>\n\nThe Cache tag will not be included in the generated HTML. It is purely a server side tag. In the example above, only the results of the _WhatsNew_ partial view and the _*last updated _text would be sent to the browser. Any subsequent requests within the 10 minute span simply return the cached contents instead of calling the partial view again. On the first request after the 10 minutes has passed, the contents would be regenerated and cached again for another10 minutes.\n\n> Note, this should work out-of-the box but there is a strange bug with the way the memory cache is initialized in MVC 6 beta 4. For the cache to work properly in this version, add _services.AddSingleton(); _to the end of your Startup.ConfigurServices method.\n\n## Cache Expiry\n\nIf no specific expiry is specified, the contents will be cached as long as the memory cache decides to hang on to the item which is likely the lifetime of the application. Chances are this is not the behaviour you want. You will likely want to use one of the 3 options for expiring the cached contents for the Cache Tag Helper: expires-after, expires-on and expires-sliding.\n\n#### expires-after\n\nUse the _expires-after_ attribute to expire the cache entry after a specific amount of time has passed since it was added to the cache. This attribute expects a TimeSpan value. For example, you expire an item 5 seconds after it was cached:\n\n    <cache expires-after=\"@TimeSpan.FromSeconds(5)\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>  \n\n#### expires-on\n\nUse the _expires-on _attribute to expire the cache entry at a specific time. This attribute expects a DateTime value. For example, imagine your system has some backend processing that you know will be updated by the end of each day. You could specify the cache to expire at the end of the day as follows:\n\n    <cache expires-on=\"@DateTime.Today.AddDays(1).AddTicks(-1)\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>  \n\n#### expires-sliding\n\nUse the _expires-sliding_ attribute to expire the cache entry after it has not been accessed for a specified amount of time. This attribute expects a TimeSpan value.\n\n    <cache expires-sliding=\"@TimeSpan.FromMinutes(5)\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>   \n\n## Vary-by / Complex Cache Keys\n\nThe cache tag helper builds cache keys by generating an id that is unique to the context of the cache tag. This ensures that you can have multiple cache tags on a single page and the contents will not override each other in the cache. You can also tell the tag helper to build more complex cache keys using a combination of the _vary-by_ attributes. Building these complex keys allows the cache tag helper to cache different contents for different requests based on nearly any criteria you can conceive. A very simple example is caching different contents for each user by adding the _vary-by-user_ attribute:\n\n    <cache vary-by-user=\"true\">\n        <!--View Component or something that gets data from the database-->\n        *last updated @DateTime.Now.ToLongTimeString()\n    </cache>\n\nYou can specify any combination of _vary-by_ attributes. The cache tag helper will build a key that is a composite of the generated unique id for that tag plus all the values from the _vary-by_ attributes.\n\n#### vary-by-user\n\nUse this attribute to cache different contents for each logged in user. The username for the logged in user will be added to the cache key. This attribute expects a boolean value. _See example above._\n\n#### vary-by-route\n\nUse this attribute to cache different contents based on a set of route data parameters. This attribute expects a comma-separated list of route data parameter names. The values of those route parameters will be added to the cache key.\n\nFor example, the following cache tag would cache different contents based on the _id_ route parameter:\n\n    <cache vary-by-route=\"id\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>  \n\n#### vary-by-query\n\nThe _vary-by-query_ attribute allows you to cache different contents based on the query parameters for the current request. This attribute expects a comma-separated list of query string parameter names. The value of those query string parameters will be added to the cache key.\n\nFor example, the following cache tag would cache different contents for each unique value of the _search_ query parameter:\n\n    <cache vary-by-query=\"search\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>\n\n#### vary-by-cookie\n\nThe _vary-by-cookie_ attributes allows you to cache different contents based on values stored in a cookie. This attribute expects a comma-separated list of cookie names. The values of the specified cookie names will be added to the cache key.\n\nFor example, the following cache tag would cache different contents based on the value of the _MyAppCookie_.\n\n    <cache vary-by-cookie=\"MyAppCookie\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>\n\n#### vary-by-header\n\nThe _vary-by-header_ attribute allows you to cache different contents based on the value of a specific request header. This attribute expects a single header name. For example, the following cache tag would cache different results based on the User-Agent header:\n\n    <cache vary-by-header=\"User-Agent\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>\n\n#### vary-by\n\nFinally, the _vary-by_ attribute allows you to cache different contents based on any arbitrary string value. This attribute can be used as a fall-back in case any of the other vary-by attributes do not meet your needs.\n\nFor example, the following cache tag would cache different results based on the value of a ProductId that is available on the ViewBag:\n\n    <cache vary-by=\"@ViewBag.ProductId\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>  \n\n#### Complex Keys\n\nAs mentioned earlier, you can specify any number of vary-by parameters and the cache tag helper will build a composite key. Here is an example of a cache tag that will cache different results for each user and _id_ route parameter:\n\n    <cache vary-by-user=\"true\" vary-by-route=\"id\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>  \n\n## Cache Priority\n\nThe contents of a cache tag are stored in an IMemoryCache which is limited by the amount of available memory. If the host process starts to run out of memory, the memory cache might purge items from the cache to release memory. In cases like this, you can tell the memory cache which items are considered a lower priority using the _priority _attribute. For example, the following cache tag is specified as low priority:\n\n    @using Microsoft.Framework.Caching.Memory\n\n    <cache vary-by-user=\"true\"\n           priority=\"@CachePreservationPriority.Low\">\n        <!--View Component or something that gets data from the database-->\n        *last updated  @DateTime.Now.ToLongTimeString()\n    </cache>\n\nPossible values for CacheItemPriority are Low, Normal, High and NeverRemove.\n\nThe [CacheTagHelper implementation][1] uses an instance of an IMemoryCache which stores cache entries in memory in the local process. Anything that causes the host process to shutdown / restart will cause a full loss of all entries in the cache. For example, restarting an IIS App Pool or scaling down an Azure instance would cause the memory cache to reset. In this case, the CacheTagHelper would rebuild the contents on the next request. In a cloud service like Azure you never know when your website might get moved to a new server so this could happen at any time. The [MemoryCache][2] is not a durable storage mechanism so it is important not to treat it as one.\n\nAnother important scenario to consider is when you have multiple load balanced servers. You might get strange / unexpected results if you have [server affinity / application request routing][3] turned off. The [MemoryCache][2] is not a distributed cache. Each server would have it's own memory cache with potentially different contents for each cache tag helper. If the client refreshes a page 3 times and those requests are routed to 3 different servers, then the client could potentially see 3 different contents. Depending on the scenario, this could be very confusing for the user. The solution here would be to avoid turning off ARR / server affinity in a load balanced deployment scenario. By turning this feature on you will ensure that a specific client's requests are always routed to the same server.\n\nThe Cache cache tag helper is one of the more unique tag helpers in MVC 6. It provides a flexible and convenient approach to caching the output of a portion of a page and can be a useful tool for improving performance of MVC 6 applications.\n\n_May 4, 2015: Updated with Limitations sections as suggested by Rick Anderson in the comments_\n\n[1]: https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.TagHelpers/CacheTagHelper.cs\n[2]: https://github.com/aspnet/Caching/blob/dev/src/Microsoft.Extensions.Caching.Memory/MemoryCache.cs\n[3]: https://technet.microsoft.com/en-us/library/dd443543(v=ws.10).aspx\n","categories":[],"tags":[]}]}