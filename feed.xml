<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Western Devs</title>
  
  <link href="/feed.xml" rel="self" type="application/atom+xml"/>
  <link href="https://westerndevs.com" rel="alternate" type="application/atom+xml"/>
  
  <updated>2020-12-02T23:16:38.518Z</updated>
  <id>https://westerndevs.com/</id>
  
  <author>
    <name>Western Devs</name>
	<uri>https://westerndevs.com</uri>
    <email>info@westerndevs.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title type="html">Advent of Code 2020 - Day 2</title>
    <link href="https://westerndevs.com/_/Advent-Of-Code-Day02/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Advent-Of-Code-Day02/</id>
    <published>2020-12-02T22:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dylan Smith</name>
	  <email>optikal@shaw.ca</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Day 2 solution with a new ParseLines() extension method, some Tuples, and the XOR operator</p><a id="more"></a><p>Previous Posts:</p><ul><li><a href="https://www.westerndevs.com/_/Advent-Of-Code-Intro/" target="_blank" rel="noopener">Intro</a></li><li><a href="https://www.westerndevs.com/_/Advent-Of-Code-Day01/" target="_blank" rel="noopener">Day 1</a></li></ul><p>My code can be found <a href="https://github.com/dylan-smith/AdventOfCode2020/blob/master/src/Days/Day02.cs" target="_blank" rel="noopener">here on GitHub</a></p><p>Here's quick walkthrough of my final solution:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/WXh7do8l54I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Day 2 solution with a new ParseLines() extension method, some Tuples, and the XOR operator&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Advent of Code 2020 - Day 1</title>
    <link href="https://westerndevs.com/_/Advent-Of-Code-Day01/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Advent-Of-Code-Day01/</id>
    <published>2020-12-01T05:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dylan Smith</name>
	  <email>optikal@shaw.ca</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>There was a slight hiccup with the Advent Of Code site going down right at launch today.  Within about 5-6 minutes they had solved the problem, and the site author gave a couple comments about it on reddit:</p><a id="more"></a><p><img src="https://imgur.com/ei64QeP.png" alt="Postmortem"><img src="https://imgur.com/2fZOaJt.png" alt="Postmortem comment"></p><p>My code can be found <a href="https://github.com/dylan-smith/AdventOfCode2020/blob/master/src/Days/Day01.cs" target="_blank" rel="noopener">here on GitHub</a></p><p>Here's quick walkthrough of my final solution:</p><iframe width="560" height="315" src="https://www.youtube.com/embed/CvMvjvUGz8Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;There was a slight hiccup with the Advent Of Code site going down right at launch today.  Within about 5-6 minutes they had solved the problem, and the site author gave a couple comments about it on reddit:&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Advent of Code 2020 - C# Framework</title>
    <link href="https://westerndevs.com/_/Advent-Of-Code-Intro/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Advent-Of-Code-Intro/</id>
    <published>2020-11-24T19:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dylan Smith</name>
	  <email>optikal@shaw.ca</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="https://adventofcode.com/" target="_blank" rel="noopener">Advent Of Code 2020</a> is just around the corner.  This year my hope is to create a short video after solving each day to show off my solution.  Here's an intro video where I walk-through the framework/GUI I've created where I write and run my code.</p><a id="more"></a><iframe width="560" height="315" src="https://www.youtube.com/embed/BDeR8KBJnv0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://adventofcode.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Advent Of Code 2020&lt;/a&gt; is just around the corner.  This year my hope is to create a short video after solving each day to show off my solution.  Here&#39;s an intro video where I walk-through the framework/GUI I&#39;ve created where I write and run my code.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Allocating a Serverless Database in SQL Azure</title>
    <link href="https://westerndevs.com/_/serverless-sql-azure-terraform/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/serverless-sql-azure-terraform/</id>
    <published>2020-11-18T19:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I'm pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I'm provisioning a bunch of databases for a client and we're not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out.</p><a id="more"></a><p>Traditionally we've been using the resource <code>azurerm_sql_database</code> for our databases but this provider is starting to be deprecated in favour of <code>azurerm_mssql_database</code> which has better support for some of the more modern concept in SQL Azure. The <a href="https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/mssql_database#state" target="_blank" rel="noopener">documentation</a> is pretty good for it but while there was a <code>min_capacity</code> we couldn't find an equivalent <code>max_capacity</code>. Turns out you can set the max capacity using the SKU. So we had something like</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">resource <span class="string">"azurerm_mssql_database"</span> <span class="string">"database"</span> &#123;</span><br><span class="line">  <span class="attr">name</span>                        = var.database_name</span><br><span class="line">  <span class="attr">server_id</span>                   = var.database_server_id</span><br><span class="line">  <span class="attr">max_size_gb</span>                 = var.database_max_size_gb</span><br><span class="line">  <span class="attr">auto_pause_delay_in_minutes</span> = -<span class="number">1</span></span><br><span class="line">  <span class="attr">min_capacity</span>                = <span class="number">1</span></span><br><span class="line">  <span class="attr">sku_name</span>                    = <span class="string">"GP_S_Gen5_6"</span></span><br><span class="line">  <span class="attr">tags</span> = &#123;</span><br><span class="line">    <span class="attr">environment</span> = var.prefix</span><br><span class="line">  &#125;</span><br><span class="line">  short_term_retention_policy &#123;</span><br><span class="line">    <span class="attr">retention_days</span> = <span class="number">14</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This allocates a database with a capacity of between 1 and 6 vCPU that has auto pause disabled. The S in the GP_S_Gen5_6 stands for serverless and the 6 denotes the maximum capacity.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I&#39;m pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I&#39;m provisioning a bunch of databases for a client and we&#39;re not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">From Travis CI to GitHub Actions (and GitHub Pages)</title>
    <link href="https://westerndevs.com/devops/from-travis-ci-to-github-actions/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/devops/from-travis-ci-to-github-actions/</id>
    <published>2020-11-10T23:36:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>David Wesst</name>
	  <email>questions@davidwesst.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>We recently migrated the continuous integration and deployment workflow for the Western Devs website from Travis CI to GitHub Actions. These are the steps I followed to get it done.</p><a id="more"></a><p>Travis CI <a href="https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing" target="_blank" rel="noopener">announced a new pricing model</a> that <em>could</em> have impact on open source projects that are using Travis for continuous integration and/or deployment. For static websites, like the <a href="https://westerndevs.com">Western Devs website</a> or <a href="https://www.davidwesst.com" target="_blank" rel="noopener">personal website</a>, this could result in getting some unforeseen costs. With that in mind, we decided to take the plunge an migrate away from Travis and over to <a href="https://docs.github.com/en/free-pro-team@latest/actions" target="_blank" rel="noopener">GitHub Actions</a> as they provide CI and CD workflows free for open source projects.</p><h2>TL;DR; -- Just show me the code</h2><p>Fine. <a href="https://github.com/westerndevs/western-devs-website/blob/master/.github/workflows/ci-cd.yml" target="_blank" rel="noopener">Here is is</a>. It is open source after all.</p><p>But just to be clear, this isn't a tutorial on how to code this up, rather its a walkthrough on what it took to get our <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> based static site from Travis to GitHub Actions.</p><h2>Start with Mapping Out Your Workflow</h2><p>And I mean <em>workflow</em> and not just the build.</p><p>For the Western Devs, our workflow goes like this:</p><ol><li>Commit a change to the code (i.e. a new blog post)</li><li>Build the website</li><li>If master branch build is successful, deploy the build to production</li><li>Notify the Western Devs of the build result in Slack</li></ol><p>GitHub workflow provides everything we need to do this, and I'll walk you through the code, which you can see for yourself in <a href="https://github.com/westerndevs/western-devs-website/blob/master/.github/workflows/ci-cd.yml" target="_blank" rel="noopener">here in our GitHub repo</a>.</p><h3>1. Commit a change to the code (i.e. a new blog post)</h3><p>This is our trigger to start the workflow. That is represented by the <code>on</code> section of the YAML. In our case, we want to trigger the workflow every time there is a pull request created for the master branch, a push to the master branch (i.e. a merge), or a push to any other feature (ft) or hotfix (hf) branches.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">name:</span> <span class="string">CI/CD</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ft/*</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">hf/*</span></span><br><span class="line">  <span class="attr">pull_request:</span></span><br><span class="line">    <span class="attr">branches:</span> <span class="string">[</span> <span class="string">master</span> <span class="string">]</span></span><br></pre></td></tr></table></figure><p>Now we have a workflow that will trigger when we want to. Next, we need to actually build the website.</p><h3>2. Build the website</h3><p>Our build is exceptionally simple-- just generate the site, and if the generation is successful, the build was successful. To do this, we create a <code>build</code> job that handles the work.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">Build</span> <span class="string">and</span> <span class="string">Deploy</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span> <span class="comment"># checkout the source code</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/setup-node@v1</span> <span class="comment"># setup the environment</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">node-version:</span> <span class="number">12</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">run:</span> <span class="string">npm</span> <span class="string">install</span> <span class="comment"># setup dependencies</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span> <span class="comment"># run the build command</span></span><br></pre></td></tr></table></figure><p>The first two steps are using GitHub Actions provided by GitHub themselves. This pulls our source code and the sets up the Node environment that we need to build the website. Once that is done, we  <code>run</code> steps to run shell commands to install our project specific dependendies and run the build script itself.</p><p>The scripts have been defined in our <a href="https://github.com/westerndevs/western-devs-website/blob/master/package.json" target="_blank" rel="noopener">project <code>package.json</code> file</a> and are used by the developers to build the site locally as well.</p><h3>3. If master branch build is successful, deploy the build to production</h3><p>If we are talking about the master branch, we want to do a deployment if it is successful. For this step, we added a conditional expression using the <code>github</code> context that is provided to all actions. You can learn more about context and expressions for GitHub Actions in the <a href="https://docs.github.com/en/free-pro-team@latest/actions/reference/context-and-expression-syntax-for-github-actions#about-contexts-and-expressions" target="_blank" rel="noopener">GitHub Docs here</a>.</p><p>You might also see that were using an encrypted secret using the <code>secret.GITHUB_TOKEN</code> expression. All repositories have this feature in the settings section of the repo, and you can learn more about <a href="https://docs.github.com/en/free-pro-team@latest/actions/reference/encrypted-secrets" target="_blank" rel="noopener">creating encrypted secrets for a repository here</a> in the GitHub docs.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span> <span class="string">to</span> <span class="string">GitHub</span> <span class="string">Pages</span></span><br><span class="line">  <span class="attr">if:</span> <span class="string">github.ref</span> <span class="string">==</span> <span class="string">'refs/heads/master'</span></span><br><span class="line">  <span class="attr">uses:</span> <span class="string">peaceiris/actions-gh-pages@v3</span></span><br><span class="line">  <span class="attr">with:</span></span><br><span class="line">    <span class="attr">github_token:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.GITHUB_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line">    <span class="attr">cname:</span> <span class="string">westerndevs.com</span></span><br><span class="line">    <span class="attr">commit_message:</span> <span class="string">$&#123;&#123;</span> <span class="string">github.event.head_commit.message</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure><h4>BONUS: Free hosting with GitHub Pages</h4><p>In our case, our deployment target is <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> which provides free hosting and SSL certificates for open source static sites sites like ours.</p><p>We decided to take this opportunity to consilate everything under the GitHub umbrella because it saved us a couple of bucks, and now everything we need to manage the site is in one spot rather than spread across multiple cloud services.</p><h3>4. Notify the Western Devs of the build result in Slack</h3><p>Originally, we had forgotten this step and started to feel it right away. So an issue was created and I put a solution in place in about 15 minutes, thanks to someone else doing all the heavy lifting and publishing their work to the <a href="https://github.com/marketplace?type=actions" target="_blank" rel="noopener">GitHub Actions Marketplace</a>.</p><p>Slack supports incoming webhooks, even for for free workspaces. I set that up by following the <a href="https://api.slack.com/messaging/webhooks" target="_blank" rel="noopener">Slack documentation</a>, created another secret in our repository and voila, we were back in business wih the notifications.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Notify</span> <span class="string">Slack</span></span><br><span class="line">  <span class="attr">if:</span> <span class="string">always()</span></span><br><span class="line">  <span class="attr">uses:</span> <span class="string">8398a7/action-slack@v3</span></span><br><span class="line">  <span class="attr">with:</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">$&#123;&#123;</span> <span class="string">job.status</span> <span class="string">&#125;&#125;</span></span><br><span class="line">    <span class="attr">fields:</span> <span class="string">repo,message,commit,author,action,eventName,ref,workflow,job,took</span> <span class="comment"># selectable (default: repo,message)</span></span><br><span class="line">  <span class="attr">env:</span></span><br><span class="line">    <span class="attr">SLACK_WEBHOOK_URL:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.SLACK_WEBHOOK_URL</span> <span class="string">&#125;&#125;</span> <span class="comment"># required</span></span><br></pre></td></tr></table></figure><h2>Conclusion</h2><p>The combination of GitHub Actions and GitHub Pages provides every developer with the opportunity to get a taste of DevOps while actually producing something they can show off to their peers and community. Travis CI is, and will continue to be, a great CI/CD solution for developers...but if you're looking for a one-stop-shop for source control, workflow, and hosting. You can't really go wrong with GitHub.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We recently migrated the continuous integration and deployment workflow for the Western Devs website from Travis CI to GitHub Actions. These are the steps I followed to get it done.&lt;/p&gt;
    
    </summary>
    
      <category term="devops" scheme="https://westerndevs.com/categories/devops/"/>
    
    
      <category term="github actions" scheme="https://westerndevs.com/tags/github-actions/"/>
    
      <category term="github workflow" scheme="https://westerndevs.com/tags/github-workflow/"/>
    
      <category term="travis ci" scheme="https://westerndevs.com/tags/travis-ci/"/>
    
      <category term="continuous integration" scheme="https://westerndevs.com/tags/continuous-integration/"/>
    
      <category term="continuous deployment" scheme="https://westerndevs.com/tags/continuous-deployment/"/>
    
  </entry>
  
  <entry>
    <title type="html">Running Stored Procedures Across Databases in Azure</title>
    <link href="https://westerndevs.com/_/cross-database-procs/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/cross-database-procs/</id>
    <published>2020-11-09T19:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>In a <a href="https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/" target="_blank" rel="noopener">previous article</a> I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn't entirely accurate. You can do it if you make use of stored procedures.</p><a id="more"></a><p>Running a stored proc on a remote database is a little bit weird looking but once you get your head around that then it is perfectly usable. Let's go back to the same example we used before with a products database an an orders database. In the products database let's add a stored procedure to add a new product and return the count of products.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> addProduct</span><br><span class="line"> @item <span class="keyword">nvarchar</span>(<span class="number">50</span>)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(@item);</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> products;</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>Now over in our orders database we can use our existing database connection to call this stored proc</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sp_execute_remote ProductsSource, </span><br><span class="line">                  N'addProduct @item', </span><br><span class="line">                  @params = N'@item nvarchar(50)', </span><br><span class="line">                  @item = 'long sleeved shirts';</span><br></pre></td></tr></table></figure><p>At first glance this is a little confusing so let's break it down.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp_execute_remote ProductsSource,</span><br></pre></td></tr></table></figure><p>This line instructs that we want to run a stored procedure and that it should use the ProductsSource data connection.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N'addProduct @item',</span><br></pre></td></tr></table></figure><p>This line lists the stored proc to run and the parameters to pass to it. You'll notice that it is a NVarchar string passed as a single parameter.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@params = N'@item nvarchar(50)',</span><br></pre></td></tr></table></figure><p>This line lists all the parameters to pass and their type. If you have multiple then you'd comma separate them here: <code>N'@item nvarchar(50), @price number(10,2)'</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">@item</span> = <span class="string">'long sleeved shirts'</span>;</span><br></pre></td></tr></table></figure><p>This final line is an args-style array of the values for the parameters. Again if you had a second parameter you'd pass it in as separate item here <code>@item = 'long sleeved shirts', @price=10.99</code></p><p>Running this command gets us something like</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cnt  <span class="variable">$ShardName</span></span><br><span class="line">6  [<span class="attribute">DataSource</span>=testias.database.windows.net <span class="attribute">Database</span>=testias]</span><br></pre></td></tr></table></figure><p>You'll notice that nifty ShardName colum which tells you about the source. This is because you can use a shard map to execute the stored procedure against lots of shards at once.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In a &lt;a href=&quot;https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;previous article&lt;/a&gt; I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn&#39;t entirely accurate. You can do it if you make use of stored procedures.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">New Artwork and a Complete Rewrite...ish | Out the Door Devlog</title>
    <link href="https://westerndevs.com/devlog/out-the-door/out-the-door-post-jam-update/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/devlog/out-the-door/out-the-door-post-jam-update/</id>
    <published>2020-11-09T14:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>David Wesst</name>
	  <email>questions@davidwesst.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>My effort continues on Out the Door with some new artwork, a rewrite (of sorts) to fix the build process, which has led to something of a self-driven code review.</p><a id="more"></a><iframe width="560" height="315" src="https://www.youtube.com/embed/LLXO-6Pretk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h2>Wanna Play?</h2><p>Sure you do! Head over to my <a href="https://davidwesst.itch.io/out-the-door" target="_blank" rel="noopener">davidwesst.itch.io/out-the-door</a> to give it a whirl in your browser (no install needed) or Windows! It's totally free, and feedback is always appreciated.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My effort continues on Out the Door with some new artwork, a rewrite (of sorts) to fix the build process, which has led to something of a self-driven code review.&lt;/p&gt;
    
    </summary>
    
      <category term="devlog" scheme="https://westerndevs.com/categories/devlog/"/>
    
      <category term="out the door" scheme="https://westerndevs.com/categories/devlog/out-the-door/"/>
    
    
      <category term="game development" scheme="https://westerndevs.com/tags/game-development/"/>
    
      <category term="gamejam" scheme="https://westerndevs.com/tags/gamejam/"/>
    
      <category term="ludum dare" scheme="https://westerndevs.com/tags/ludum-dare/"/>
    
      <category term="ludum dare 47" scheme="https://westerndevs.com/tags/ludum-dare-47/"/>
    
      <category term="game design" scheme="https://westerndevs.com/tags/game-design/"/>
    
      <category term="out the door" scheme="https://westerndevs.com/tags/out-the-door/"/>
    
  </entry>
  
  <entry>
    <title type="html">Azure Processor Limits</title>
    <link href="https://westerndevs.com/_/processor-limits/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/processor-limits/</id>
    <published>2020-11-05T20:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Ran into a fun little quirk in Azure today. We wanted to allocate a pretty beefy machine, an M32ms. Problem was that for the region we were looking at it wasn't showing up on our list of VM sizes. We checked and there were certainly VMs of that size available in the region we just couldn't see them. So we ran the command</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">az </span><span class="string">vm </span><span class="built_in">list-usage</span> <span class="built_in">--location</span> <span class="string">"westus"</span> <span class="built_in">--output</span> <span class="string">table</span></span><br></pre></td></tr></table></figure><p>And that returned a bunch of information about the quota limits we had in place. Sure enough in there we had</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Name</span>                               <span class="keyword">Current</span> <span class="keyword">Value</span>   <span class="keyword">Limit</span></span><br><span class="line">Standard MS <span class="keyword">Family</span> vCPUs           <span class="number">0</span>               <span class="number">0</span></span><br></pre></td></tr></table></figure><p>We opened a support request to increase the quota on that CPU. We also had a weirdly low limit on CPUs in the region</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total Regional vCPUs               <span class="number">0</span>               <span class="number">10</span></span><br></pre></td></tr></table></figure><p>Which support fixed for us too and we were then able to create the VM we were looking for.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ran into a fun little quirk in Azure today. We wanted to allocate a pretty beefy machine, an M32ms. Problem was that for the region we we
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Querying Across Databases In SQL Azure</title>
    <link href="https://westerndevs.com/_/elasticquery/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/elasticquery/</id>
    <published>2020-11-05T19:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the <code>orders</code> database from the <code>customers</code> database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that's one of the reasons.</p><a id="more"></a><p>Unfortunately, whacking developers with a ruler and making them rewrite their business logic to observe proper domain boundaries isn't always on the cards. This is a problem when migrating them to SQL Azure because querying across databases, even ones on the same server, isn't permitted.</p><p><img src="https://blog.simontimms.com/images/elasticquery/brokenQuery.png" alt="Broken query across databases"></p><p>This is where the new <a href="https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-query-overview" target="_blank" rel="noopener">Elastic Query</a> comes in. I should warn at this point that the functionality is still in preview but it's been in preview for a couple of years so I think it is pretty stable. I feel a little bit disingenuous describing it as &quot;new&quot; now but it is new to me. To use it is pretty easy and doesn't even need you to use the Azure portal.</p><p>Let's imagine that you have two databases one of which contains a collection of Products and a second database that contains a list of Orders which contain just the product id. Your mission is to query and get a list of orders and the product name. To start we can set up a couple of databases. I called mine <code>testias</code> and <code>testias2</code> and I had them both on the same instance of SQL Azure but you don't have to.</p><p><img src="https://blog.simontimms.com/images/elasticquery/setup.png" alt="Two databases on the same server"></p><h2>Product Database</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Products( </span><br><span class="line"><span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">nvarchar</span>(<span class="number">50</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'socks'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'hats'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'gloves'</span>);</span><br></pre></td></tr></table></figure><h2>Orders Database</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orders(<span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line"><span class="built_in">date</span> <span class="built_in">date</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orderLineItems(<span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line">orderId uniqueidentifier,</span><br><span class="line">productId uniqueidentifier,</span><br><span class="line">quantity <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">foreign</span> <span class="keyword">key</span> (orderId) <span class="keyword">references</span> orders(<span class="keyword">id</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> @orderID uniqueidentifier = newid();</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(<span class="keyword">id</span>, <span class="built_in">date</span>)</span><br><span class="line"><span class="keyword">values</span>(@orderID, <span class="string">'2020-11-01'</span>);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'3829A43D-FD2A-4B7C-9A09-23DBF030C1DC'</span>, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'233BC430-BA3F-4F5C-B3EA-4B82867FC040'</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'95A20D82-EC26-4769-8840-804B88630A01'</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> @orderId = newid();</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(<span class="keyword">id</span>, <span class="built_in">date</span>)</span><br><span class="line"><span class="keyword">values</span>(@orderID, <span class="string">'2020-11-02'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'3829A43D-FD2A-4B7C-9A09-23DBF030C1DC'</span>, <span class="number">16</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'233BC430-BA3F-4F5C-B3EA-4B82867FC040'</span>, <span class="number">99</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'95A20D82-EC26-4769-8840-804B88630A01'</span>, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>Now we need to hook up the databases to be able to see each other. We're actually just going to make products visible from the orders database. It makes more sense to me to run these queries in the database which contains the most data to minimize how much data needs to cross the wire to the other database.</p><p>So first up we need to tell the Orders database about the credentials needed to access the remote database, products. To do this we need to use a SQL account on the products database. Windows accounts and integrated security doesn't currently work for this.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">master</span> <span class="keyword">key</span> encryption <span class="keyword">by</span> <span class="keyword">password</span> = <span class="string">'monkeyNose!2'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> scoped credential ProductDatabaseCredentials </span><br><span class="line"><span class="keyword">with</span> <span class="keyword">identity</span> = <span class="string">'ProductsDBUser'</span>, </span><br><span class="line">secret = <span class="string">'wouNHk41l9fBBcqadwWiq3ert'</span>;</span><br></pre></td></tr></table></figure><p>Next we set up an external data source for the products</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">data</span> <span class="keyword">source</span> ProductsSource <span class="keyword">with</span> </span><br><span class="line">(<span class="keyword">type</span>=RDBMS, location = <span class="string">'testias.database.windows.net'</span>, </span><br><span class="line">database_name = <span class="string">'testias'</span>, credential = ProductDatabaseCredentials);</span><br></pre></td></tr></table></figure><p>Finally we create a table definition in the Orders database that matches the remote table (without any defaults or constraints).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> Products( <span class="keyword">id</span> uniqueidentifier,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">nvarchar</span>(<span class="number">50</span>))</span><br><span class="line"><span class="keyword">with</span> ( data_source = ProductsSource)</span><br></pre></td></tr></table></figure><p>We now have a products table in the external tables section in the object explorer</p><p><img src="https://blog.simontimms.com/images/elasticquery/testtableview.png" alt="Tables from both databases"></p><p>We can query the external table and even cross it against the tables in this database</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, ol.quantity <span class="keyword">from</span> orderLineItems ol <span class="keyword">inner</span> <span class="keyword">join</span> products p <span class="keyword">on</span> ol.productId = p.id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">socks   16</span><br><span class="line">socks   10</span><br><span class="line">gloves  1</span><br><span class="line">gloves  99</span><br><span class="line">hats    2</span><br><span class="line">hats    0</span><br></pre></td></tr></table></figure><p>So it is possible to run queries across databases in Azure but it takes a little set up and a little bit of thought about how to best set it up.</p><h1>Possible Gotchas</h1><ul><li>I forgot to set up the database to be able to talk to Azure resources in the firewall so I had to go back and add that</li><li>Inserting to the external table isn't supported, which is good, make the changes directly in the source database</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the &lt;code&gt;orders&lt;/code&gt; database from the &lt;code&gt;customers&lt;/code&gt; database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that&#39;s one of the reasons.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">A Solo Gamejam Experience | A Ludum Dare 47 Story</title>
    <link href="https://westerndevs.com/devlog/a-solo-gamejam-experience-ld47/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/devlog/a-solo-gamejam-experience-ld47/</id>
    <published>2020-10-23T13:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>David Wesst</name>
	  <email>questions@davidwesst.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I submitted a game to <a href="https://ldjam.com/events/ludum-dare/47/out-the-door" target="_blank" rel="noopener">Ludum Dare 47</a> I call <a href="https://davidwesst.itch.io/out-the-door" target="_blank" rel="noopener">Out the Door (Play Now in your Browser)</a> as a solo, amateur game developer with a non-gamedev day job, family responsibilities, and household to maintain.</p><!-- more --><iframe width="560" height="315" src="https://www.youtube.com/embed/AFnGMS24qvg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      Wonder what it takes to be a solo amateur game developer in a global gamejam? DW summarizes his Ludum Dare 47 experience in this video.
    
    </summary>
    
      <category term="devlog" scheme="https://westerndevs.com/categories/devlog/"/>
    
    
      <category term="gamedev" scheme="https://westerndevs.com/tags/gamedev/"/>
    
      <category term="game development" scheme="https://westerndevs.com/tags/game-development/"/>
    
      <category term="devlog" scheme="https://westerndevs.com/tags/devlog/"/>
    
      <category term="gamejam" scheme="https://westerndevs.com/tags/gamejam/"/>
    
      <category term="ludum dare" scheme="https://westerndevs.com/tags/ludum-dare/"/>
    
      <category term="github gameoff" scheme="https://westerndevs.com/tags/github-gameoff/"/>
    
  </entry>
  
  <entry>
    <title type="html">The trimStart rabbit hole</title>
    <link href="https://westerndevs.com/_/typescript-definition/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/typescript-definition/</id>
    <published>2020-09-28T18:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I was bragging to <a href="https://westerndevs.com/bios/dave_paquette/">David</a> about a particularly impressive piece of TypeScript code I wrote last week</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (body.<span class="keyword">trim</span>().<span class="keyword">startsWith</span>(<span class="string">'&lt;'</span>)) &#123; <span class="comment">//100% infallible xml detection</span></span><br></pre></td></tr></table></figure><p>He, rightly, pointed out that <code>trimStart</code> would probably be more efficient. Of course it would! However when I went to make that change there was only <code>trim</code>, <code>trimLeft</code> and <code>trimRight</code> in my TypeScript auto-complete drop down.</p><p><img src="https://blog.simontimms.com/images/trimStart/missing.png" alt="TrimStart and TrimEnd are missing"></p><p>Odd. This was for sure a real function because it appears in the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/trimStart" target="_blank" rel="noopener">MDN docs</a>.</p><p>A reasonable person would have used trimLeft and moved on but it was Monday and I was full of passion for programming. So I went down the rabbit hole.</p><p>Checking out the TypeScript directory in my node_modules I found that there were quite a few definition files in there. These were the definition files that described the JavaScript language itself rather than any libraries. Included in that bunch was one called <code>lib.es2019.string.d.ts</code>. This file contained changes which were made to the language in es2019.</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> String &#123;</span><br><span class="line">    <span class="comment">/** Removes the trailing white space and line terminator characters from a string. */</span></span><br><span class="line">    trimEnd(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the leading white space and line terminator characters from a string. */</span></span><br><span class="line">    trimStart(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the leading white space and line terminator characters from a string. */</span></span><br><span class="line">    trimLeft(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the trailing white space and line terminator characters from a string. */</span></span><br><span class="line">    trimRight(): <span class="built_in">string</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So I must be targeting the wrong thing! Sure enough in my <code>tsconfig.js</code> I was targeting <code>es5</code> on this project. When we started this was using an older version of node on lambda that didn't have support for more recent versions of ES. I checked and the lambda was running node 12.18.3 and support for ES2020 landed in node 12.9 so I was good to move up to es2020 as a target.</p><p>Incidentally you can check the running node version in JavaScript by running</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">console</span>.log(<span class="string">'Versions: '</span> + <span class="built_in">JSON</span>.stringify(process.versions));</span><br></pre></td></tr></table></figure><p>After updating my <code>tsconfig.js</code> and restarting the language server all was right in the world.</p><p><img src="https://blog.simontimms.com/images/trimStart/there.png" alt="The missing functions appear"></p>]]></content>
    
    <summary type="html">
    
      If you&#39;re missing expected functions in your TypeScript app the problem might be an incorrect target
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Game Portfolio Self Evaluation (in prep for Ludum Dare 47)</title>
    <link href="https://westerndevs.com/devlog/game-portfolio-review-2020/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/devlog/game-portfolio-review-2020/</id>
    <published>2020-09-18T13:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>David Wesst</name>
	  <email>questions@davidwesst.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Ludum Dare 47, a weekend long global gamejam, is coming up in a few weeks. In order to prep for the event, I decided to take the time for review and reflect on my game portfolio to see what I learning objective and goals I can set for myself.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/V_zCHtZIsYw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h2>My Portfolio</h2><p>What was that? You wanted to know where to find and play my games?! Well then, if you're inclined to try some of my games (and hopefully leave some feedback), here they are:</p><ul><li><a href="https://davidwesst.itch.io/vagabondgame" target="_blank" rel="noopener">Vagabond Game</a> -&gt; A top-down Unity based prototype where I learned the technical ropes of putting together a small, game experience, with narrative, graphics, and animation.</li><li><a href="https://davidwesst.itch.io/leaps-and-bounds" target="_blank" rel="noopener">Car Scientist</a> -&gt; My first gamejam submission and evolution of the Vagabond Game prototype, except this time built in Godot.</li><li><a href="https://davidwesst.itch.io/little-shop-of-wall-street" target="_blank" rel="noopener">Little Shop of Wall Street</a> -&gt; My Ludum Dare 46 submission, co-authored by <a href="https://westerndevs.com/bios/darcy_lussier/">D'Arcy Lussier</a> where you trade stocks online in order to feed an interesting plant, before it feeds on YOU!</li></ul>]]></content>
    
    <summary type="html">
    
      Ludum Dare 47, a weekend long global gamejam, is coming up in a few weeks. In order to prep for the event, I decided to take the time for review and reflect on my game portfolio to see what I learning objective and goals I can set for myself.
    
    </summary>
    
      <category term="devlog" scheme="https://westerndevs.com/categories/devlog/"/>
    
    
      <category term="gamedev" scheme="https://westerndevs.com/tags/gamedev/"/>
    
      <category term="game development" scheme="https://westerndevs.com/tags/game-development/"/>
    
      <category term="devlog" scheme="https://westerndevs.com/tags/devlog/"/>
    
      <category term="gamejam" scheme="https://westerndevs.com/tags/gamejam/"/>
    
      <category term="ludum dare" scheme="https://westerndevs.com/tags/ludum-dare/"/>
    
      <category term="github gameoff" scheme="https://westerndevs.com/tags/github-gameoff/"/>
    
  </entry>
  
  <entry>
    <title type="html">Release Notes for Little Shop of Wall Street 0.1.0-beta</title>
    <link href="https://westerndevs.com/devlog/little-shop-of-wall-street-01-release-notes/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/devlog/little-shop-of-wall-street-01-release-notes/</id>
    <published>2020-07-02T15:13:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>David Wesst</name>
	  <email>questions@davidwesst.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Finally! The 0.1 beta has arrived for Little Shop of Wall Street!</p><p>In this video, DW walks through the new features rolled out both in-game and behind the scenes for his LD46 game jam title.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/baMlNqGgiV4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p>You can play the game here on <a href="https://davidwesst.itch.io/little-shop-of-wall-street" target="_blank" rel="noopener">Itch.io</a>.</p><h2>Side Notes</h2><p>This release is an important one for me.</p><p>First, off it's the first &quot;beta&quot; release which I've categorized as a moderately stable release, and includes a &quot;complete gameplay loop&quot; on purpose. There are still plenty of bugs (as the video even showed) but it works and playable.</p><p>Second, this release is the original vision of what I pictured the gamejam submission to be when D'Arcy and I came up with the idea back in April. Many months later, I have that release which says a lot about my prototyping and experimenting process (i.e. I'm too slow).</p><p>Lastly, I have multiple versions of the game out there including Linux/X11 and Windows versions. There's still a lot more to learn and do with the while devops setup for my projects, but this is a great step forward and can be reused with all my Godot-based projects moving forward.</p><p>Until the next one-- thanks for playing.</p><p>~ DW</p>]]></content>
    
    <summary type="html">
    
      Little Shop of Wall Street has a 0.1-beta release!
    
    </summary>
    
      <category term="devlog" scheme="https://westerndevs.com/categories/devlog/"/>
    
    
      <category term="gamedev" scheme="https://westerndevs.com/tags/gamedev/"/>
    
      <category term="game development" scheme="https://westerndevs.com/tags/game-development/"/>
    
      <category term="devlog" scheme="https://westerndevs.com/tags/devlog/"/>
    
      <category term="little shop of wall street" scheme="https://westerndevs.com/tags/little-shop-of-wall-street/"/>
    
      <category term="godot" scheme="https://westerndevs.com/tags/godot/"/>
    
  </entry>
  
  <entry>
    <title type="html">Scaling Azure Functions from Consumption Plan to Premium Plan (and back again)</title>
    <link href="https://westerndevs.com/Azure/Azure-Functions/scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/Azure/Azure-Functions/scaling-azure-functions-from-consumption-plan-to-premium-hosting-plan/</id>
    <published>2020-05-23T16:30:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave Paquette</name>
	  <email>contactme@davepaquette.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Azure Functions, when hosted on a consumption plan, are great for most scenarios. You pay per use which is great for keeping costs down but there are some downsides and limitations. One of those is the time it takes to cold start your function app. If your function app hasn't been triggered in some time, it can take a while for the a new instance to start up to run your app. Likewise, if a very sudden spike in load occurs, it can take some time for the consumption plan to start up enough instances to handle that load. In the meantime, you might have clients getting timeouts or failed requests.</p><p>Azure Functions offers another hosting model called <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan" target="_blank" rel="noopener">Azure Functions Premium Plan</a>. With premium plans, instead of paying per function execution, you pay for the underlying compute instances that are hosting your functions. This is often more expensive, but it also ensures there are always a pre-set number of warmed instances ready to execute your function.</p><p>That's great, but what if I only really need those pre-warmed instances for a short period of time when I'm expecting a lot of incoming traffic. The rest of the time, I would rather use a Consumption Plan to save on hosting costs.</p><p>I thought the choice of hosting plan was something you needed to make up front but it turns out that you can actually move an Azure Function App from a consumption plan to a premium plan (and back again).</p><p>Thanks to <a href="https://twitter.com/stimms/" target="_blank" rel="noopener">Simon Timms</a> for starting this discussion on Twitter. We got very helpful responses from folks on the Azure Functions team:</p><p><a href="https://twitter.com/jeffhollan/" target="_blank" rel="noopener">Jeff Hollan</a> has a great <a href="https://github.com/Azure-Samples/functions-csharp-premium-scaler" target="_blank" rel="noopener">sample</a> using an Azure Durable Function to scale an Azure Function App to a premium plan for a specified amount of time, then automatically scale back down to a consumption plan.</p><blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/6C9l3PQDoZ" target="_blank" rel="noopener">https://t.co/6C9l3PQDoZ</a></p>&mdash; Jeff Hollan (@jeffhollan) <a href="https://twitter.com/jeffhollan/status/1245779682961674240?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">April 2, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>This is a super cool sample. It uses the <a href="https://docs.microsoft.com/en-us/rest/api/resources/" target="_blank" rel="noopener">Azure Resource Manager REST API</a> to make changes to the target function app resources. For my project however, I didn't really want to spin up another Azure Function to manage my Azure Functions. I just wanted an easy way to scale my 12 function apps up to premium plans for a couple hours, then scale them back down to a consumption plan.</p><p>I decided to try using the AZ CLI for this and it turned out really well. I was able to write a simple script to scale up and down.</p><h2>Setting up the AZ CLI</h2><p>First up, <a href="https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest" target="_blank" rel="noopener">install the az cli</a>.</p><p>Once installed, you'll need to login to your Azure Subscription.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az login</span><br></pre></td></tr></table></figure><p>A browser window will popup, prompting you to log in to your Azure account. Once you've logged in, the browser window will close and the az cli will display a list of subscriptions available in your account. If you have more than one subscription, make sure you select the one you want to use.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az account <span class="built_in">set</span> --subscription YourSubscriptionId</span><br></pre></td></tr></table></figure><h2>Create a Resource Group</h2><p>You will need a resource group for your Storage and CDN resources. If you don't already have one, create it here.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az group create --name DavesFunctionApps --location WestUS2</span><br></pre></td></tr></table></figure><p>Most commands will require you to pass in a <code>--resource-group</code> and <code>--location</code> parameters. These parameters are <code>-g</code> and <code>-l</code> for short, but you can save yourself even more keystrokes by setting defaults for <code>az</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">az configure -d group=DavesFunctionApps</span><br><span class="line">az configure -d location=WestUS2</span><br></pre></td></tr></table></figure><h2>Creating a (temporary) Premium Hosting Plan</h2><p>There is a strange requirement with Azure Functions / App Service. As per Jeff Hollan's sample:</p><blockquote><p>The Azure Functions Premium plan is only available in a sub-set of infrastructure in each region. Internally we call these &quot;webspaces&quot; or &quot;stamps.&quot; You will only be able to move your function between plans if the webspace supports both consumption and premium. To make sure your consumption and premium functions land in an enabled webspace you should create a premium plan in a new resource group. Then create a consumption plan in the same resource group. You can then remove the premium plan. This will ensure the consumption function is in a premium-enabled webspace.</p><footer><strong>Jeff Hollan</strong><cite><a href="https://github.com/Azure-Samples/functions-csharp-premium-scaler" target="_blank" rel="noopener">github.com/Azure-Samples/functions-csharp-premium-scaler</a></cite></footer></blockquote><p>First, add an Azure Functions Premium plan to the resource group.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp plan create -n dave_temp_premium_plan --sku EP1 --min-instances 1</span><br></pre></td></tr></table></figure><p>You can delete this premium plan using the command below <em>after</em> you've deployed a function app to this resource group . <strong>Don't forget to delete the premium plan. These cost $$$</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp plan delete -n dave_temp_premium_plan</span><br></pre></td></tr></table></figure><h2>Creating a Function App</h2><p>There are many options for creating a new function app. I really like the <code>func</code> command line tool which I installed using npm. Check out the <a href="https://github.com/Azure/azure-functions-core-tools" target="_blank" rel="noopener">Azure Functions Core Tools GitHub Repo</a> for details on other options for installing the <code>func</code> tooling.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i -g azure-functions-core-tools@3 --unsafe-perm <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>The focus of this blog post is around scaling a function app. If you don't already have an app built, you can follow along with <a href="https://docs.microsoft.com/azure/azure-functions/functions-run-local?tabs=windows%2Ccsharp%2Cbash" target="_blank" rel="noopener">this walkthrough</a> to create a function app.</p><p>A function app requires a Storage Account resource. An Application Insights resource is also highly recommended as this really simplifies monitoring your function app after it has been deployed. Let's go ahead and create those 2 resources.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">az storage account create -n davefuncappstorage</span><br><span class="line">az extension add -n application-insights</span><br><span class="line">az monitor app-insights component create --app davefuncappinsights</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now we can create our Azure Function App resource with a consumption plan, passing in the name of the storage account and app insights resources that we just created. In my case, I'm specifying the dotnet runtime on a Windows host.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp create --consumption-plan-location WestUS2 --name davefuncapp123 --os-type Windows --runtime dotnet --storage-account davefuncappstorage --app-insights davefuncappinsights --<span class="built_in">functions</span>-version 3</span><br></pre></td></tr></table></figure><p>Remember to delete that temporary Premium Hosting Plan now!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp plan delete -n dave_temp_premium_plan</span><br></pre></td></tr></table></figure><h3>Deploying your Function App using the az cli</h3><p>This is a bit outside the scope of this blog post but I like using the <code>az</code> cli to deploy my function apps because it's easy to incorporate that into my CI/CD pipelines. Since my app is using the dotnet runtime, I use the <code>dotnet publish</code> command to build the app.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dotnet publish -c release</span><br></pre></td></tr></table></figure><p>Then, zip the contents of the publish folder (<code>bin\release\netcoreapp3.1\publish\</code>).</p><p>In PowerShell:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Compress-Archive -Path .\bin\release\netcoreapp3.1\publish\* -DestinationPath .\bin\release\netcoreapp3.1\package.zip</span><br></pre></td></tr></table></figure><p>or in Bash</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -r ./bin/release/netcoreapp3.1/package.zip ./bin/release/netcoreapp3.1/publish/</span><br></pre></td></tr></table></figure><p>Finally, use the <code>az functionapp deployment</code> command to deploy the function app.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp deployment <span class="built_in">source</span> config-zip  -n davefuncapp123 --src ./bin/release/netcoreapp3.1/package.zip</span><br></pre></td></tr></table></figure><h2>Scale up to a premium plan</h2><p>Okay, now that we have a functioning (pun intended) app deployed and running on a consumption plan, let's see what it takes to scale this thing up to a premium plan.</p><p>First, create a new Premium Hosting Plan with the parameters that make sense for the load you are expecting. The <code>--sku</code> parameter refers to the size of the compute instance: EP1 is the smallest. The <code>--min-instancs</code> parameter is the number of pre-warmed instances that will always be running for this hosting plan. The <code>--max-burst</code> parameter is the upper bounds on the number of instances that the premium plan can elastically scale out if more instances are needed to handle load.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp plan create -n davefuncapp123_premium_plan --sku EP1 --min-instances 4 --max-burst 12</span><br></pre></td></tr></table></figure><p>Next, move the function app to that premium hosting plan.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp update --plan davefuncapp123_premium_plan -n davefuncapp123</span><br></pre></td></tr></table></figure><p>That's it! All it took was those 2 command and your function app is now running on a premium plan!</p><h2>Scale back down to a consumption plan</h2><p>Of course, that premium plan isn't cheap. You might only want your function app running on the premium plan for a short period of time. Scaling back down is equally easy.</p><p>First, move the function app back to the consumption based plan. In my case, the name of the consumption plan is <code>WestUS2Plan</code>. You should see a consumption plan in your resource group.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp update --plan WestUS2Plan -n davefuncapp123</span><br></pre></td></tr></table></figure><p>Next, delete the premium hosting plan.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">az functionapp plan delete -n davefuncapp123_premium_plan </span><br></pre></td></tr></table></figure><h2>Wrapping it up</h2><p>In this post, we saw how easy it is to move a function app between Premium and Consumption plans. A couple very simple <code>az</code> commands can help you get the performance and features of the Premium plan <em>only</em> when you need it while taking advantages of the simplicity and cost savings of a Consumption plan the rest of the time.</p>]]></content>
    
    <summary type="html">
    
      In this post, we use the az cli to move an Azure Function app from a Consumption Plan to a Premium Plan (and back again).
    
    </summary>
    
      <category term="Azure" scheme="https://westerndevs.com/categories/Azure/"/>
    
      <category term="Azure Functions" scheme="https://westerndevs.com/categories/Azure/Azure-Functions/"/>
    
    
      <category term="Azure" scheme="https://westerndevs.com/tags/Azure/"/>
    
      <category term="Web Dev" scheme="https://westerndevs.com/tags/Web-Dev/"/>
    
      <category term="AZ CLI" scheme="https://westerndevs.com/tags/AZ-CLI/"/>
    
      <category term="Azure Functions" scheme="https://westerndevs.com/tags/Azure-Functions/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey/</id>
    <published>2020-05-22T17:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<h1>The Journey</h1><p>This is a series of articles chronicling my learning journey as I was asked to build an IdentityServer4-based authentication system for one of my clients. This story included details about my adoption of Kubernetes, Azure Kubernetes Service, and all the things that I had to do to stand-up this client's new IdentityServer4 authentication implementation.</p><p>As with most learning experiences, I've made mistakes, arrived at working applications, adjusted my implementations, and continued to grow. My hope is that this series will continue to grow and evolve and be a bit of a living series of documents. I'll actively change articles when I discover something new or a better way to describe things. I'll add new articles or maybe alternate paths through the series as new topics present themselves. And I'm certainly not an authority on all of these topics, so as I get feedback from friends and peers, I'll certainly be making adjustments.</p><p>I have two primary goals with this series. Documenting the learning journey is the first one, but the second and almost as important goal was to provide someone (you) with a complete set of steps to get a Kubernetes cluster up and running with an IdentityServer4 implementation running inside of it. Once someone has this platform up and running, they can continue to learn and grow in the same manner that I will but hopefully they're path getting to this point was a little smoother than mine was.</p><p>As always, comments and feedback are encouraged and very welcome.</p><p>Now, on to the articles!</p><h2>Series Table of Contents</h2><ol><li><a href="/kubernetes/kubernetes-my-journey-part-1">Business problems</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-2">Initial Assumptions, Technologies, Learning resources</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-3">Tools in Use</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-4">Building an ASP.NET Core IdentityServer Implementation</a></li><li>Getting Started with Kubernetes - Minikube<ul><li><a href="/kubernetes/kubernetes-my-journey-part-5a">Part A - Getting Started with Kubernetes - Minikube</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-5b">Part B - Getting Started with Kubernetes - Minikube</a></li></ul></li><li><a href="/kubernetes/kubernetes-my-journey-part-6">Pause to reflect</a></li><li>Moving to Azure Kubernetes Service<ul><li><a href="/kubernetes/kubernetes-my-journey-part-7a">Part A - Moving to Azure Kubernetes Service</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-7b">Part B - Moving to Azure Kubernetes Service</a></li></ul></li><li><a href="/kubernetes/kubernetes-my-journey-part-8">Making Your Kubernetes Cluster Publicly Accessible</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-9">Adding Cluster Logging (fluentd)</a></li><li><a href="/kubernetes/kubernetes-my-journey-part-10">Tuning resource usage</a></li></ol><h2>Approach to this series</h2><p>I've decided to break this series into discrete posts to make this easier to write and consume. I'll have specific topics for a post that are aligned with the overall vision, and you'll be able to read the parts that are important to you.</p><p>This series is going to strive to demonstrate work that is completely done. Every bit of code in each post should work and be complete. I'll point out bits of knowledge and wisdom I've learned along the way, but the intention is to give you a working system that you can then alter/re-create and learn from that experience.</p><p>All the code, projects, manifests, etc. are (or will be) in <a href="https://github.com/agileramblings/my-kubernetes-journey" target="_blank" rel="noopener">Github here</a>.</p><p><strong>Enjoy</strong></p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-1">Business problems</a></p><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }    figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style>]]></content>
    
    <summary type="html">
    
      &lt;h1&gt;The Journey&lt;/h1&gt;
&lt;p&gt;This is a series of articles chronicling my learning journey as I was asked to build an IdentityServer4-based authen
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, aks, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey - Part 1</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey-part-1/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey-part-1/</id>
    <published>2020-05-22T16:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="/kubernetes/kubernetes-my-journey">Series Table of Contents</a></p><h1>The Business Problem</h1><p>My client has been in business since 1993. As you can imagine, software has been an integral part of their ability to deliver services to their customers. About 10 years ago, they had a re-platforming initiative that was the start of their current monolithic, critical LOB system. It is an ASP.NET MVC 5.x application that has evolved over time and is certainly reaching the limits of what its architecture can provide. It isn't &quot;cloud-native&quot; and it basically continues to be enhanced/evolved based on assumptions that were made 10 years ago.</p><p>For this project, the important part to understand is that it used forms authentication and has a user store internally that manages all of the user accounts and authorization rules. It currently only uses cookies to store authenticated user details on the client and doesn't support any modern sort of token-based activities that are needed for modern applications.</p><p>It is also important to understand that the platform is growing as users and customers demand more modern and specific user experiences. New web applications (React), mobile devices (native), IoT devices, and system-to-system integrations are all being added to the platform and all these new applications need to participate in the authentication scheme. In some cases, new applications have been created but because the current LOB system doesn't support modern authentication techniques, they cloned the existing user store and used the data to implement the appropriate authentication techniques for their specific use case. This has led to a bunch of applications re-implementing their own authentication, with everyone sharing the user store (or a copy of it) of the core LOB system.</p><p>So, the current system has demonstrated a few key problems that the new system needs to address.</p><h3>Security is hard and incredibly important</h3><p>One of the plagues of modern business is bad actors attacking various systems trying to gain access and cause lots of troubles. This is an ever-present problem for most corporations. Security needs to be a first-class citizen in all projects. Using modern authentication platforms that are proven and community-reviewed is seen as a requirement.</p><p>Speaking of security, in this series, you will find a lot of usernames, passwords, and internal details about the systems that is being built. I would ask that you <strong>please take a great deal of care when building your systems!</strong> I'm not worried about presenting these details as I'm tearing all of this down all the time and it won't be left running on my Azure accounts. I will change passwords, usernames, and all kinds of other details to make it harder for anyone to break into a running system. I strongly encourage you to do the same.</p><h3>Decentralized identity management sucks</h3><p>The platform is currently a monolithic LOB system that provides <strong>all</strong> functional aspects for the various business groups. The platform is also growing a collection of loosely related applications that basically represent specific implementations that are tailored for each individual area of the business. All the systems are re-implementing authentication while using, in most cases, a copy of the LOB user store. This basically means that we have multiple user stores that all must be managed independently. There are SSIS workflows trying to keep the right data in sync while not clobbering specific customizations to the user store data.</p><p>This is all complicated, complex, and error prone. We generally don't have a lot of troubles with authentication, but it is expensive and time-consuming to setup and maintain and as new applications are added, it adds more things to manage and has more things that can go wrong.</p><h3>New applications require modern Identity technologies</h3><p>In 2020, business that used to be well-served by a monolithic web application, are finding that this is no longer the case. Native applications, new web applications based on new technologies (SPA), IoT devices and System-to-System integrations all require more modern authentication technologies and these applications are being added to platform ecosystems at an increasing pace.</p><h3>Reduce the costs of running applications</h3><p>One of the overall initiatives for this client is to become more cloud-native and reduce the running costs of applications. There is already an initiative to move all the platform servers to Azure, but we didn't want any new applications continuing the pattern of &quot;lift and shift&quot; used for the existing apps. There is a strong desire to leverage containers (docker) and orchestrators (Kubernetes) at an increasing rate for all the applications in order to reduce overall run costs.</p><h3>Being able to scale up is important</h3><p>As with all businesses, we expect growth. My client is heavily reliant on devices for their current business needs and will be adding customers, native apps, and IoT devices at an ever-increasing rate. This means that having the ability to scale vertically or horizontally needs to be present from the start. Growth was anticipated (pre-COVID19) to be &gt; 30% year over year for the next 3-5 years so there were going to be lots of people and devices needing to use the system in the future.</p><h2>Summary (TL;DR;)</h2><p>Security is hard! De-centralized (read: many) authentication systems are expensive. Old Application are ... well... old! Modern applications need support! Moving to the cloud is happening! And businesses grow!!</p><p>I hope that this gives some context that helps you understand some of the decisions that I'll be sharing in the next section!</p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-2">Initial Assumptions, Technologies, Learning resources</a></p><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }        figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/kubernetes/kubernetes-my-journey&quot;&gt;Series Table of Contents&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;The Business Problem&lt;/h1&gt;
&lt;p&gt;My client has been in busin
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, aks, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey - Part 2</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey-part-2/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey-part-2/</id>
    <published>2020-05-22T15:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="/kubernetes/kubernetes-my-journey">Series Table of Contents</a></p><p><strong>Previously:</strong><a href="/kubernetes/kubernetes-my-journey-part-1">Business problems</a></p><h1>Initial Assumptions, Technologies, Learning Resources</h1><p>There are some assumptions that were made before my involvement with this company that made some of this decision making easy. Those assumptions were driven by the business problems we discussed previously.</p><p>Some of these decisions were made by me throughout this project. It was important for me to understand the organization's business, goals, and culture when making these decisions so they weren't as easy to make. The good thing is that I've got a great group of leaders and developers in this organization that helped me and trusted me to make better decisions on their behalf.</p><h2>Pre-defined Assumptions</h2><p>So, there are a bunch of things that we're decided before me arriving on the scene! These decisions governed my initial decision making to just get the project started.</p><h3>We're going to the cloud</h3><p>I'm not sure this needs a lot of elaboration. Everyone is going to the cloud. Owning and operating data centres is hard. Companies like Microsoft, Amazon, and Google have commoditized hardware setup, management, and maintenance so much so that it almost doesn't makes sense to own your own data centre. So, anything I did should strive to be cloud-native.</p><h4>Azure</h4><p>Azure was chosen as the vendor for all our cloud services prior to my involvement. This certainly plays to my strength as a Microsoft-based technologist and an avid Azure user for all my own projects.</p><h4>ASP.NET Core (C#)</h4><p>This company is a Microsoft shop. All the developers are C# developers who are familiar with Microsoft technologies, so it only makes sense that we are going to leverage the existing people and skills that are present in the organization. Using .NET Core and ASP.NET Core 3+ in all projects going forward was made a requirement (and is probably simple common sense) by the assumption of moving to the cloud.</p><p>I couldn't build something that only I could maintain or would require the company to go and try to find new developers or acquire new skills in their current developers. There is going to be a lot of learning being done by everyone here, but we don't need to add to the pile of new stuff to learn.</p><h4>HTTPS</h4><p>One of the things that seems obvious, but turns out isn't always obvious, is the fact that in today's world, we should be using HTTPS everywhere. It has become easier and easier to do this, and part of this project's mandate was to ensure that we accelerated the adoption of HTTPS throughout the deployed platform and make it <em>easier to own and maintain</em>. Simply doing an Identity implementation will drive this.</p><h2>Decisions I made</h2><p>During the planning of this project, I had the flexibility to make a lot of decisions. I didn't make them in isolation, but I did get to make the final decisions.</p><h3>Containers and Orchestration</h3><p>Going to the cloud can be as simple as lift-and-shift. Pick up your server, virtualize it if necessary, and place it in the cloud. In some cases, we can make this even simpler by provisioning a VM with all the pre-installed parts we need and simply deploying our application into that new VM. This is certainly &quot;going to the cloud&quot; but it isn't considered <em>cloud-native</em>.</p><h4>Docker</h4><p>I'm not an expert on what <em>cloud-native</em> entirely means, but I've discerned that one thing you need to do is containers. So today, that means <strong>Docker</strong>. Being a Microsoft shop, going to Azure, using Visual Studio and the az cli as tools, Docker was a no-brainer for a container engine with all the great tool support in Visual Studio, Azure DevOps Server, and the other tools, so I just rolled with it.</p><h4>Kubernetes</h4><p>When using Azure and running containers in the cloud, you have several options. Two easy ones are Azure Container Instances (server-less container hosting) and <strong>Azure Kubernetes Services (aka AKS)</strong>. Since our environment was going to be far more complicated than running a bunch of single containers in the cloud, we needed an orchestration platform. There are two options for that today: Kubernetes and DockerCompose and we chose Kubernetes. The community support for Kubernetes is incredible, the momentum it has in the marketplace can't be denied, and it is natively supported by Azure via the <strong>AKS</strong> product, so it was another easy decision.</p><h3>Cloud-Hosted</h3><p>I sort of covered this one off, but there is an option to stand up your own Kubernetes cluster manually in Azure simply using Virtual Machines. <strong>AKS</strong> effectively took this option off the table. In my experience so far, <strong>AKS</strong> is a polished product, easy to use, and the control plane components are included for free. You simply pay for all the other Azure products that will make up your cluster. These include VMs, Storage, Networking, and any other products you leverage in your implementation.</p><h4>Azure Kubernetes Service (<strong>AKS</strong>)</h4><p><strong>AKS</strong> also makes some of our <em>ability to scale</em> requirements much easier to address. You can scale vertically by dynamically increasing the size of the VMs in the cluster and we can scale horizontally by dynamically adding (or removing) nodes from the cluster.</p><h3>Identity Implementation</h3><p>While this project has many objectives, they key objective for all this work was to deliver a new Identity management implementation to the platform that would enable a secure, modern way for all users, applications, and devices to authenticate with each other.</p><p>One of the underlying cultural desires at this client that I should state is a strong desire to minimize subscriptions and dependencies on 3rd party vendors. This desire precluded us from selecting any of the current Identity providers that exist on the marketplace today. AuthO and Okta are two such companies and while I'm sure they have great products, wedecided to build our own Identity system.</p><h4>IdentityServer4</h4><p>Now, we didn't want to build all the identity system from scratch! That doesn't make sense. We'd have to become experts in OAuth, OpenID Connect, flows, encryption, tokens, and all those technologies that are implemented by great communities out there already. This OSS frameworks are also scrutinized and vetted by the community, so you know that a lot of people care that they are done right. Basically, we wanted to take the best implementation of a security framework in the Microsoft open-source community, host it in our infrastructure, and be able to customize it to our specific use-cases. This led us to IdentityServer4, which is the core technical component of our Identity implementation.</p><h4>Skoruba IdentityServer4 Admin</h4><p>One of the things about identity implementations is that there is a lot of configuration and user data to manage. To be honest, client configuration and user data is one of the biggest PITA about doing Identity work. Once you get it, it isn't so bad but so much of a successful roll-out with an identity platform hinges on getting all this data right. And to do that, it helps to have a good administrative platform.</p><p>While I'd love to have had the time to write an application that gives a good user experience for the administration of this data, I didn't have that time. So, I went looking for something in the community that used IdentityServer4 as a foundational component and added the user experience I wanted. And after several pilots, I found and selected Skoruba IdentityServer4 Admin. This is an ASP.NET Core 3.x web application that provides the Security Token Service (STS), and Administrative Application, and an Administrative API, all in one easy to use package! I've been really happy that I found it.</p><h3>Data Persistence</h3><p>The Skoruba templates allows for the selection of one from three different built-in persistence providers via Entity Framework Core. SqlServer, MySql and Postgres.</p><h4>Postgres</h4><p>Postgres was the database provider the I selected. This reduced our licensing costs, provides the appropriate level of performance and functionality, and with us being in Azure, we always have the option of moving to <strong>Azure Database for PostgreSQL</strong> which has single instance SKU and an HA/Hyperscale SKU if that eventually becomes a needed capability. There is also a ton of community support around the product, which means libraries, documentation, and examples are plentiful.</p><h3>Logging</h3><p>I cannot under-state how important logging is when developing, maintaining, and owning a multi-container environment, spread across multiple servers (nodes) that are hosted in the cloud. This isn't the first and hopefully won't be the last blog post to emphasize this point. The community building cloud-based applications already blogs about this, you've almost certainly read about it somewhere before here, but it still may not have sunk in that, perhaps, this needs to be the first thing you figure out in your new container-based platform. I paid some attention at the beginning, but not enough.</p><h4>Seq - Log Ingestion and Analytics</h4><p>When I first arrived at my client, they were still relying on file-based logging, spread over all the servers in the farm. They had a lot of logging in the application, but it wasn't easily accessible, and it wasn't easy at all to find out what happened across a workflow, sometimes broken, that traversed a bunch of servers.</p><p>The other thing that wasn't possible was querying and basic analytics of the log data that was being generated. It was inconsistent, unstructured, and just barely helping them solve problems.</p><p>At a previous engagement, I had used Splunk and really came to love it! The tool was great. Powerful queries, visualization, dashboard, alerts, integrations! Splunk has it all. There was only one problem. Splunk is expensive. That isn't a problem if your needs justify the spend and you have the required expertise in owning and operating Splunk, but that wasn't the case here, so I needed to find something easier to bring into the ecosystem. This was when I found Seq!</p><p>Seq is a great entry-level tool (that keeps on getting better) for organizations that are just getting started on getting logs into a central product and using that log data to analyze problems and operational aspects of the platform. Seq is built by Datalust.co, who also makes Serilog the .NET library that we use for logging internally in the application. This combination has turned out to be a cost-effective, easy to learn, setup and maintain logging infrastructure for our apps, so it was quite easy for me to bring this into the Identity project and the <strong>Kubernetes (k8s)</strong> infrastructure.</p><h4>Fluentd</h4><p>In addition to the identity applications that are in the <strong>k8s</strong> cluster that will be logging to Seq, the infrastructure applications in the cluster <em>itself</em> will generate an incredible amount of information about its operations and problems. This is a non-trivial log ingestion problem that has been made very approachable with a product called <strong>fluentd</strong>. Built to live inside of <strong>k8s</strong> clusters, fluentd is basically an event processing pipeline implementation that takes events from log files that are written all over the <strong>k8s</strong> cluster. It takes care of finding the files, processing them as they get additional entries, and sending those events &quot;somewhere&quot;. In this case, I used an fluentd docker image that emits logs to <strong>graylog</strong> consumers, which thankfully, Seq is with an addon. So, with Serilog in the applications, fluentd in the cluster using a graylog variant and Seq running in the cluster to ingest and aggregate all the log information, we have a very compact, useful logging strategy to get us started.</p><p>I don't know how far this logging implementation will grow with the cluster into the future, but it is designed to have pieces replaced as we outgrow their capabilities.</p><h3>Learning Resources</h3><p>This adventure required an incredible amount of learning on my part. The way that I learn best is by reading/learning enough to get started, and then building like crazy, discovering deficiencies in what I know, resolving the deficiency, and then finding the next thing I don't know.</p><p>This series of blog posts will hopefully capture some of that learning experience, but I wanted to make sure I shared the places that I started.</p><h4>Pluralsight</h4><p>There are a lot of courses on Pluralsight. I think I've enjoyed all them, with some courses being better than others for me at that point in my learning. For this project, I didn't need any help with C# or ASP.NET Core or any of that part. I needed to get familiar with Kubernetes and I needed to become familiar with authentication flows using OAuth2 and OpenID Connect.</p><p>The <strong>k8s</strong> courses were from Nigel Poulton were great places to get started with <strong>k8s</strong>. The ones that I watched are:</p><ul><li><a href="https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents" target="_blank" rel="noopener">Getting Started with Kubernetes</a></li><li><a href="https://app.pluralsight.com/library/courses/docker-kubernetes-big-picture/table-of-contents" target="_blank" rel="noopener">Docker and Kubernetes: The Big Picture</a></li></ul><p>When getting more details on OAuth2 and OpenId Connect, Kevin Dockx and Micah Silverman had two nice courses to review.</p><ul><li><a href="https://app.pluralsight.com/library/courses/securing-aspdotnet-core2-oauth2-openid-connect/table-of-contents" target="_blank" rel="noopener">Securing ASP.NET Core 2 with OAuth2 and OpenID Connect</a></li><li><a href="https://app.pluralsight.com/library/courses/that-conference-2019-session-07/table-of-contents" target="_blank" rel="noopener">THAT Conference '19: OAuth 2.0 and OpenID Connect (In Plain English)</a></li></ul><blockquote><p>There is a new version of Kevin Dockx course available using ASP.NET Core 3 <a href="https://app.pluralsight.com/library/courses/securing-aspnet-core-3-oauth2-openid-connect/table-of-contents" target="_blank" rel="noopener">here</a></p></blockquote><h4>Kubernetes.io</h4><p>Based on what and how I was learning <strong>k8s</strong> from Nigel on Pluralsight, Kubernetes.io became an invaluable resource for me to learn about <strong>k8s</strong> and its ecosystem. Nigel's courses encourage you to work with manifests, building your <strong>k8s</strong> cluster in a declarative manner, and kubernetes.io supported that approach well. I would highly recommend working with manifests and understanding them and how they work when learning <strong>k8s</strong>. We'll discover later that I don't work directly with manifests when provisioning my cluster resources, but you have to know manifests because all the examples in the communities are in manifests!</p><h4>Community blogs</h4><p>This would have been much harder without the incredibly vibrant community of bloggers in the world who are sharing what they are doing, the problems, and how they are solving those problems. I'm hoping that my addition in writing this blog fills a gap and makes it a bit easier for you or someone else to put this all together, but this would have been incredibly difficult without this vibrant community.</p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-3">Tools in Use</a></p><h5>Links</h5><ul><li><a href="https://azure.microsoft.com/en-us/" target="_blank" rel="noopener">Azure</a></li><li><a href="https://azure.microsoft.com/en-us/-services/kubernetes-service/" target="_blank" rel="noopener">Azure Kubernetes Services</a></li><li><a href="https://www.kubernetes.io" target="_blank" rel="noopener">Kubernetes</a></li><li><a href="https://identityserver4.readthedocs.io/en/latest/" target="_blank" rel="noopener">IdentityServer4</a></li><li><a href="https://github.com/skoruba/IdentityServer4.Admin" target="_blank" rel="noopener">Skoruba IdentityServe4 Administration</a></li><li><a href="https://www.pulumi.com/" target="_blank" rel="noopener">Pulumi</a></li><li><a href="https://datalust.co/seq" target="_blank" rel="noopener">Seq Log Ingestion</a></li><li><a href="https://docs.fluentd.org/" target="_blank" rel="noopener">Fluentd</a></li><li><a href="https://www.postgresql.org/" target="_blank" rel="noopener">Postgres</a></li><li><a href="https://www.postgresql.org/" target="_blank" rel="noopener">pgAdmin4</a></li><li><a href="https://github.com/jetstack/cert-manager" target="_blank" rel="noopener">CertManager</a></li><li><a href="https://www.nginx.com/" target="_blank" rel="noopener">Nginx</a></li><li><a href="https://containo.us/traefik/" target="_blank" rel="noopener">Traefix</a></li></ul><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }    figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/kubernetes/kubernetes-my-journey&quot;&gt;Series Table of Contents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Previously:&lt;/strong&gt;
&lt;a href=&quot;/kubernetes/kuberne
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, **AKS**, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-AKS-identityserver-docker-containers/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey - Part 3</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey-part-3/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey-part-3/</id>
    <published>2020-05-22T14:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="/kubernetes/kubernetes-my-journey">Series Table of Contents</a></p><p><strong>Previously:</strong><a href="/kubernetes/kubernetes-my-journey-part-2">Initial Assumptions, Technologies, Learning resources</a></p><h1>Tools in Use</h1><p>Tooling is an incredibly important aspect of a developer's daily life. IDEs, CLIs, Automations, Visualizations; the list goes on and on. Sifting through the set of tools that are available and finding out if they work for you can take a lot of time. As a former Microsoft MVP (Dev Tools), I'm always interested in tooling because I know the importance it can make in developer productivity and the overall productivity and quality for an organization! I'm hoping that by sharing my base set of tools, and what I use them for, you'll get a bit of tool-curation time savings back to spend on learning other things.</p><p>These are the tools I use <strong>every day</strong> on this project.</p><h2>Visual Studio</h2><table><thead><tr><th></th><th style="text-align:center"></th></tr></thead><tbody><tr><td><a href="https://visualstudio.microsoft.com/vs/" target="_blank" rel="noopener">Visual Studio</a> has come a long, long way since the first time I ever used is as Microsoft Visual Studio 97 (5.x). Wow! This IDE has been my constant companion for the entirety of my development career, and I have to say that the current VS 2019 version of the tool is a pleasure to work with. Given the complex nature of building projects and supporting all of the ecosystems that we build projects for, VS 2019 does a fantastic job of supporting <em>my</em> needs.</td><td style="text-align:center"><img width="200px" src="https://s3.amazonaws.com/neowin/news/images/uploaded/2017/02/1486663278_visual-studio-97.jpg" alt="Visual Studio 97"></td></tr></tbody></table><p>With this project, I obviously appreciate all of the normal C# coding functions that are present, but the docker/docker-compose support is particularly important and the ability to debug applications running in docker containers is great.</p><p>I still have <a href="https://www.jetbrains.com/resharper/" target="_blank" rel="noopener">ReSharper</a> and I still have many extensions (Thanks <a href="https://marketplace.visualstudio.com/publishers/MadsKristensen" target="_blank" rel="noopener">Mads</a>!) running in Visual Studio, but this IDE is the workhorse of my day-to-day activities when I'm working in C#.</p><blockquote><p>I don't recommend that you download and try to use Microsoft Visual Studio 97!</p></blockquote><h2>VS Code</h2><p><a href="https://code.visualstudio.com/" target="_blank" rel="noopener">Visual Studio Code</a> is a fantastic, light-weight text editor with an incredible extensibility feature that the community has taken full advantage of! Out of the box, it is very good at one thing. Editing text files. With all the extensions being written and placed on the marketplace, it has become some people's full-time IDE. The great thing about VS Code is that it runs on macOS, Linux, and Windows, so you take it wherever you go! And, it's free! It has mostly replaced all my other text editors, with the exception of <strong>Notepad</strong>. Still use that one from time to time.</p><p>I use VS Code for editing all my manifests, <strong>Pulumi</strong> Typescript applications, and markdown files for documentation. This blog post was written in Markdown using VS Code! With an integrated terminal window using PS Core, I can do pretty much all my <strong>k8s</strong> deployment and resource work without leaving VS Code.</p><h2>Pulumi</h2><p>I've mentioned that while I find manifests a great way to learn <strong>k8s</strong>, I've adopted a different approach for deploying <strong>k8s</strong>. That approach is from a company called <a href="https://www.pulumi.com/" target="_blank" rel="noopener">Pulumi</a>! What they've basically done is built a platform and multiple SDKs in various languages that allow us to do <strong>Infrastructure as Code</strong> in a programming language of our choice!! It's a great way to define and manage your cloud resources. Once you have written your application that knows what you want to do, you simply tell the <strong>pulumi cli</strong> to <code>up</code> or <code>destroy</code> your infrastructure! <code>pulumi up</code> and it will create or update your infrastructure resources as needed, and <code>pulumi destroy</code> tears it all down and cleans everything up!</p><h2>Git</h2><p><a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a> is the most popular and arguably the defacto source control system in the world today. All my manifests and pulumi typescript files are version controlled in git, as are all the Identity application projects that we'll be building later on.</p><h2>Azure DevOps Services (Server 2019)</h2><p><a href="https://azure.microsoft.com/en-ca/services/devops/" target="_blank" rel="noopener">Azure DevOps (Server)</a> is our DevOps management software. Source control (Git and TFVC), Work Item Management, Builds, and Deployments are all provided by this service. It provides us a tremendous amount of automation, and it gives us a place to make our knowledge about how to build and deploy our applications concrete! If you haven't tried Azure DevOps recently, you really should give it another go. It's been a tremendously valuable addition to the development process.</p><h2>Kubernetes Web UI (Dashboard)</h2><p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/" target="_blank" rel="noopener">Kubernetes Web UI (Dashboard)</a> is a great dashboard that is built into your <strong>k8s</strong> cluster that provides a User experience to help you visualize and manage your <strong>k8s</strong> cluster.</p><h2>Octant</h2><p><a href="https://octant.dev/" target="_blank" rel="noopener">Octant</a> is a dashboard for your <strong>k8s</strong> cluster, similar to the Kubernetes Dashboard, but it doesn't require you to port-forward to your local machine to get at the dashboard, it simply runs on your local development machine and uses the kubectl current context to access the cluster! This makes is quite easy to spin up. It provides port-forwarding capabilities in addition to many of the capabilities present in the Kubernetes Web UI. I find myself using them both quite often throughout the day.</p><h2>K9s</h2><p><a href="https://k9scli.io/" target="_blank" rel="noopener">K9s</a> is a console-based user experience for managing your <strong>k8s</strong> cluster! As with Octant, it uses kubectl and your current context to determine which <strong>k8s</strong> cluster it is managing. I quite like it! I also find it really interesting how I have three different tools for managing the <strong>k8s</strong> cluster and depending on what I'm doing or even my mood, I can pick from any of the three tools.</p><h2>Honorable Mention - Docker Desktop for Windows</h2><p><a href="https://hub.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener">Docker Desktop for Windows</a> is the easiest way to get docker containers running on a windows desktop. You need a CPU that supports virtualization and you have to have Windows 10 Pro for the HyperV/WSL2 support. There are other options if you aren't running Windows 10 Pro and up, but I'll leave that as an exercise for you to figure out. But during initial development of the Skoruba project, it was immensely helpful to be able to build, deploy, and debug the application without having to deal with <strong>k8s</strong> or <strong>minikube</strong>. There are additional complexities involved with that that you will want to defer for a while.</p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-4">Building an ASP.NET Core IdentityServer Implementation</a></p><h3>Links</h3><ul><li><a href="https://visualstudio.microsoft.com/" target="_blank" rel="noopener">Visual Studio</a></li><li><a href="https://code.visualstudio.com/" target="_blank" rel="noopener">VS Code</a></li><li><a href="https://www.pulumi.com/" target="_blank" rel="noopener">Pulumi</a></li><li><a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a></li><li><a href="https://www.github.com" target="_blank" rel="noopener">Github</a><ul><li><a href="http://git-school.github.io/visualizing-git/" target="_blank" rel="noopener">Git Simulator - Github</a></li></ul></li><li><a href="https://azure.microsoft.com/en-ca/services/devops/" target="_blank" rel="noopener">Azure DevOps (Server)</a></li><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/" target="_blank" rel="noopener">Kubernetes Dashboard</a></li><li><a href="https://octant.dev/" target="_blank" rel="noopener">Octant</a> - <a href="https://github.com/vmware-tanzu/octant" target="_blank" rel="noopener">Github</a></li><li><a href="https://k9scli.io/" target="_blank" rel="noopener">K9s</a> - <a href="https://github.com/derailed/k9s" target="_blank" rel="noopener">Github</a></li><li><a href="https://www.virtuallyghetto.com/2020/04/useful-interactive-terminal-and-graphical-ui-tools-for-kubernetes.html" target="_blank" rel="noopener">Useful Interactive Terminal And Graphical UI Tools For Kubernetes - Virtually Ghetto</a></li></ul><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }    figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/kubernetes/kubernetes-my-journey&quot;&gt;Series Table of Contents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Previously:&lt;/strong&gt;
&lt;a href=&quot;/kubernetes/kuberne
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, aks, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey - Part 4</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey-part-4/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey-part-4/</id>
    <published>2020-05-22T13:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="/kubernetes/kubernetes-my-journey">Series Table of Contents</a></p><p><strong>Previously:</strong><a href="/kubernetes/kubernetes-my-journey-part-3">Tools in Use</a></p><h1>Building an ASP.NET Core IdentityServer Implementation</h1><p>This is where the articles start to get a bit longer. I've broken a couple up into a-b parts. We won't be talking about tasks in <strong>k8s</strong> for a bit, so if you want to skip on to that, you can go <a href="/kubernetes/kubernetes-my-journey-part-5">here</a>.</p><p>There are, in my opinion, 2 parts that need to be considered when building up this IdentityServer4 implementation, and the 2nd part can arguably be split into 2 smaller parts, which is how this implementation is built.</p><p>The first part of the system needs to be the <strong>Security Token Services (STS)</strong> implementation. This part of the system is simply responsible for managing tokens. This means validating log in credentials, creating tokens, refreshing tokens, and sending along the appropriate metadata and claims. I would want this broken into a separate service because it will probably have different operational characteristics then the other parts of the application, and so we can scale is separately. If there are 1000s of request per minute to generate/validate/refresh tokens and send along metadata, that is different than administrating the data that the system relies on. Additionally, it makes it easier to secure the administrative access if it is a separate application.</p><p>The second part of the system is the <strong>Administrative (Admin)</strong> implementation. The application that provides a user experience that will help identity administrators do the right things.</p><p>In our implementation, which is based on the Skoruba project, there is actually a third part to the administrative side of the system and that is a <strong>Admin WebAPI</strong> that does all of the actual CRUD on the database. I'm happy with this setup and I'm glad the Skoruba project went in this direction.</p><p>Let's get into the details of what I did.</p><h2>Start Learning with the IdentityServer4 Quickstart</h2><p>If you are new to IdentityServer4 or even newer to OAuth2 and OpenID Connect, your first stop needs to be the <a href="https://identityserver4.readthedocs.io/en/latest/quickstarts/0_overview.html" target="_blank" rel="noopener">IdentityServer4 Quickstarts</a>. These quickstart modules will walk you through all of the steps required to incrementally build an IdentityServer4 server. They are all self-contained, building your knowledge from one to the next, and by the end of the quickstarts, you've built a functional IdentityServer4 implementation. I'm not going to re-produce any of these quickstarts or even how to get you started on them, other than providing the <a href="https://identityserver4.readthedocs.io/en/latest/quickstarts/0_overview.html" target="_blank" rel="noopener">link</a> and some encouragement. Go do the quickstarts, then come back here and continue reading!</p><blockquote><p>It was during the quickstarts that I side-stepped and did the Pluralsight courses on OAuth2 and OpenID Connect.</p></blockquote><h2>Starting your project with Skoruba</h2><p>So, you've done the quickstarts, or you're already experienced with IdentityServer4 and the real reason you are here is because the quickstarts left you wanting more! You wanted an Administrative application!! You wanted databases to persist all of this configuration and the quickstarts don't give you any of that! You're in the right place! I was in that position just a little while ago myself.</p><p>After doing the quickstarts, I realized I had to build/find an administrative experience for our platform. It isn't hard to find a couple when you google <em>IdentityServer4 Administration</em>. There are a number of Github repos that you'll find, and perhaps a couple products that you can purchase. Again, because of the desire to build/own vs. buy/subscribe, we continued looking until we found the <a href="https://github.com/skoruba/IdentityServer4.Admin" target="_blank" rel="noopener">Skoruba IdentityServer4 Admin</a> project on Github. This is a great project and I've been very happy that I found it. It is put together well and is super easy to get started with.</p><p>In order to use the project, you'll need to have the latest ASP.NET Core 3.x SDK and development tools.</p><p>The first thing I did was get the awesome template that the Skoruba team build to get you started using their project. This command tells the dotnet tooling to go off to nuget.org and get the template.</p><p><code>dotnet new -i Skoruba.IdentityServer4.Admin.Templates::1.0.0-rc1-update2</code></p><p>Once you have the template, you are ready to start creating your solution. You can start by using the template.</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dotnet new skoruba.is4admin -<span class="literal">-name</span> MyProject -<span class="literal">-title</span> MyProject `</span><br><span class="line">-<span class="literal">-adminemail</span> <span class="string">"admin@codingwithdave.xyz"</span> -<span class="literal">-adminpassword</span> <span class="string">"P@ssw0rd!"</span> `</span><br><span class="line">-<span class="literal">-adminrole</span> IdentityAdminRole -<span class="literal">-adminclientid</span> MyClientId `</span><br><span class="line">-<span class="literal">-adminclientsecret</span> MyClientSecret -<span class="literal">-dockersupport</span> true</span><br></pre></td></tr></table></figure><p>A couple notes about that command that make sense once you have done the ID4 Quickstarts:</p><ul><li><code>-adminrole IdentityAdminRole</code> is the User Role that will be used in the Admin application and the AdminAPI to know if a user is an administrator of the Identity system. This role will be assigned to your admin user account.</li><li><code>-adminclientid MyClientId</code> is the ClientId (username of your application) for the Admin application in the STS. Your Admin application needs this id to be allowed to access the STS.</li><li><code>-adminclientsecret MyClientSecret</code> is the ClientSecret (password of your application) for the Admin application in the STS. It is used with ClientId.</li></ul><p>Since we are planning to run this all in Docker locally and eventually deploying this to a <strong>k8s</strong> cluster, Docker support is required. All of the yaml examples in this post are in the docker-compose.yml file.</p><p>Once that command has run, you should have a project that looks something like this:</p><img src="/images/dwhite/Skoruba-projects-initial.png" alt="Skoruba Initial Projects Setup" height="250px"><h3>Select your DB Platform</h3><p>One of the cool things about the Skoruba template is that it comes out of the box with 3 different database persistence mechanism already waiting for you. You can just select the one you want to use, and off it goes. I don't need to provide the ability to switch mechanisms, so I'm simply going to remove the templated <strong>MySql</strong> and <strong>SqlServer</strong> options.</p><p>If you are planning to use SqlServer in your <strong>k8s</strong> cluster, you can <strong>skip</strong> these instructions and the next Postgres/pgAdmin4 instructions. You could delete everything except for SqlServer implementation details. I understand why the template spits it out, but your running application probably won't need persistence choices.</p><h3>So you selected PostgreSQL</h3><p><a href="https://www.postgresql.org/" target="_blank" rel="noopener">PostgreSQL (Postgres) is an open-source database</a> that is more than sufficient for our use-cases. It is free to use/run and has great support in the community. In order to minimize costs, I choose to use PostgresSQL.</p><p>The first thing I did was delete the MySql and SqlServer projects. I wasn't planning to use them and as such, just deleting them is the right course. They can always be put back in if the need ever arose to migrate to a different database platform. This is going to immediately break your solution. You can just go and delete/fix all of the broken code. You're just removing <code>using</code> statements and adjusting/removing <code>if {}</code> blocks. I also deleted all of the database types/selection code and configuration settings.</p><img src="/images/dwhite/delete-switch-block.png" alt="Deleting a Switch Block" height="250px"><p>That should leave you with a solution that builds and that only contains these projects.</p><img src="/images/dwhite/skoruba-projects-only-postgres.png" alt="Skoruba Final Projects Setup" height="250px"><p>You probably also want to delete the <code>db</code> entry in the docker-compose.yml file. The SqlServer image is a large image and you don't necessarily need to download it.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># remove this entry</span></span><br><span class="line">  <span class="attr">db:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">"mcr.microsoft.com/mssql/server"</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">1433</span><span class="string">:1433</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">skoruba-identityserver4-DB</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">SA_PASSWORD:</span> <span class="string">"$&#123;DB_PASSWORD:-Password_123&#125;"</span></span><br><span class="line">      <span class="attr">ACCEPT_EULA:</span> <span class="string">"Y"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">dbdata:/var/opt/mssql</span></span><br></pre></td></tr></table></figure><blockquote><p>In case you didn't know, Docker Desktop is the local image registry for all things Docker on your Windows workstation. All images will be downloaded and cached here. Docker Desktop is <em>not</em> the local image registry for <strong>k8s</strong> (minikube).</p></blockquote><h2>Backend Containers - Postgres,  pgAdmin4 &amp; Seq</h2><p>Now, your solution is building and ready to run, but our local infrastructure isn't quite there yet. We're going to need a Postgres database instance on our local machine to run this in Visual Studio, so we can do that in the docker-compose.yml file.</p><p>To bring a Postgres database container into Docker Desktop, add the following:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">postgresdb:</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">postgres:alpine</span></span><br><span class="line">  <span class="attr">hostname:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">5432</span><span class="string">:5432</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">postgresdb</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_USER=admin"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_PASSWORD=P@ssw0rd!"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_DB=identity"</span> <span class="comment"># this is the DB name that will be generated by EFCore</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">postgresdata:/var/lib/postgresql/data</span></span><br><span class="line">  <span class="attr">networks:</span></span><br><span class="line">    <span class="attr">default:</span></span><br><span class="line">      <span class="attr">aliases:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">postgres</span></span><br></pre></td></tr></table></figure><p>What this does is get the latest version of the Postgres container image from DockerHub. It exposes the default Postgres port of <strong>5432</strong>, gives this container an alias in the DNS of <strong>postgres</strong>, and hooks the container up to a persistent volume, mapped to a volume on the local host, to store its identity data. We need to add that mapping near the bottom of the existing docker-compose.yml file. We will add a mapping for <strong>pgAdmin4</strong> while we are at it.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="attr">postgresdata:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">local</span></span><br><span class="line">  <span class="attr">pgdata:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">local</span></span><br></pre></td></tr></table></figure><blockquote><p>There is a lot of YAML in <strong>k8s</strong> and <strong>docker</strong>. You are going to have to become familiar with it. I'm going to assume that you'll work through any yaml syntax errors that may come out of working through the articles.</p></blockquote><p>After adding in Postgre, we can now add in our pgAdmin4 container instance. pgAdmin4 is a database management tool built as a web application. If you have another Postgre management tool, you don't need to follow these steps.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pgAdmin4:</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">dpage/pgadmin4:4.20</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">5050</span><span class="string">:80</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">pgAdmin4</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGADMIN_DEFAULT_EMAIL=admin@codingwithdave.xyz"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGADMIN_DEFAULT_PASSWORD=P@ssw0rd!"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGDATA=/mnt/data/pgdata"</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pgdata:/mnt/data/pgdata</span></span><br></pre></td></tr></table></figure><p>The pgAdmin4 container doesn't need an alias. We gave the Postgre instance a DNS alias so that all of the running containers in the cluster can simply reference the database (in the connection string) by the DNS entry of postgres. The IdentityServer4 apps and pgAdmin4 don't really need this kind of mapping so we'll leave them as accessible at the <strong>http://127.0.0.1.xip.io:port</strong> addresses.</p><p>If you want to explore more options in the pgAdmin4 configuration that are available to you, you can go to the <a href="https://www.pgadmin.org/docs/pgadmin4/latest/container_deployment.html" target="_blank" rel="noopener">pgAdmin4 Container Configuration</a> page for more details.</p><p>We're also going to add a Seq instance into our docker-compose.yml file.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">seq:</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">seq</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">datalust/seq:preview</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"5341:80"</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"ACCEPT_EULA=Y"</span></span><br></pre></td></tr></table></figure><p>When I added the <strong>Seq</strong> container, I did <em>not</em> give it a persistent volume on the host to store log data. In the case of Seq, I'm using it for transient purposes in docker so I don't really care if the history of log entries goes away when the container is re-started. If you do want to keep your log files, you can give it a persistent volume on the host, in the same way as the databases.</p><blockquote><p>Persistent Volumes do not survive a Docker Desktop data purge.</p></blockquote><p>We are going to add <em>one</em> more little container that I want to introduce now before it really appeared in my story, but I wish that I had found it earlier. It is a small little container from <strong>Google</strong> that helps me do DNS diagnostics and debugging in the containers/<strong>k8s</strong> network. You can quickly and easily add this container at just about any time, do your diagnostics, and then remove it from the cluster. It automatically stops after 1 hour as well.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">dnsutil:</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">dnsutils</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">gcr.io/kubernetes-e2e-test-images/dnsutils:1.3</span></span><br><span class="line">  <span class="attr">command:</span> <span class="string">"sleep 3600"</span></span><br></pre></td></tr></table></figure><h2>Configuration Changes</h2><p>We aren't quite ready to run things yet. We need to make some more configuration adjustments.</p><blockquote><p>You should be prepared to make a lot of configuration adjustments in your IdentityServer4 and <strong>k8s</strong> career!</p></blockquote><p>Here are the things I've done to make this easier/cleaner for running locally.</p><ol><li>Change all usernames to <strong>admin</strong></li><li>Change all admin passwords to <strong>P@ssw0rd!</strong></li><li>Change all Role entries to <strong>IdentityAdminRole</strong></li><li>Delete all of the unnecessary launchSettings profiles</li><li>Tweak the URL configuration values</li><li>Change all of the database connection strings to point at our local Postgres instance</li><li>Tweak the code to always run migrations and seeding data on startup</li><li>Configure Serilog to use Seq and the console</li></ol><h3>Change Credentials and Role</h3><p>You will have lots of chances once you've explored and learned about all of these configurations to change usernames, potentially move to Azure Managed Identities, or use something like Azure KeyVault to store your credentials. For the time being, we want this to be quite easy to do, so we're going to simplify this and make all the admin accounts the same.</p><ul><li><strong>Postgres</strong> - If you revisit the Postgre yaml snippet, you see the default Postgre power user is <strong>admin</strong> and <strong>P@ssword!</strong>.</li><li><strong>pgAdmin4</strong> - You'll see that the pgAdmin4 wants an email address, so in the yaml snippet we see the username <strong>admin@codingwithdave.xyz</strong> and <strong>P@ssw0rd!</strong></li><li><strong>Seq</strong> - currently requires no authentication</li></ul><p>There are three .json files in a file in the root of the solution called <strong>shared</strong>. These files hold seed data for the applications. They are joined to the docker containers and are used by the running instances of the applications when they are in docker. You need to edit the copies of these files.</p><img src="/images/dwhite/Shared_seed_data_files_location.png" alt="Seed data JSON files" height="175px"><p>I added a <strong>solution folder</strong> to the solution and added these files to it.</p><img src="/images/dwhite/solution-add-existing-shared-files.png" alt="Add shared files to solution" height="175px"><p>I changed the following:</p><ul><li><strong>identitydata.json</strong><ul><li>change the initial username credentials to <strong>admin</strong> and <strong>P@ssw0rd!</strong></li><li>change the <strong>Roles</strong> entry to <strong>IdentityAdminRole</strong>.</li></ul></li></ul><p>In the three projects, we need to make a few tweaks in the appsettings.json files. I did this for a little added clarity.</p><ul><li>appsettings.json in MyProject.Admin</li><li>appsettings.json in MyProject.Admin.Api</li><li>appsettings.json in MyProject.STS.Identity<ul><li>change the <strong>AdminApiConfiguration:AdministrationRole</strong> to <strong>IdentityAdminRole</strong></li></ul></li></ul><br/><p>Here is an example from the STS <strong>appsettings.json</strong> file.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">"AdminConfiguration": &#123;</span><br><span class="line">    "PageTitle": "Skoruba IdentityServer4",</span><br><span class="line">    "HomePageLogoUri": "/images/skoruba-icon.png",</span><br><span class="line">    "FaviconUri": "/favicon.ico",</span><br><span class="line">    "IdentityAdminBaseUrl": "http://127.0.0.1.xip.io:9000",</span><br><span class="line">    "AdministrationRole": "IdentityAdminRole"</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h3>Delete Unnecessary LaunchSettings Profiles</h3><p>This one is fairly simple and just a cleaning exercise. I can debug with Docker Desktop and the DockerCompose project, so good riddance to all the other clutter! I don't plan to run this in IIS or single Docker containers, so I can remove those profiles and settings. I will leave the Kestrel settings since I can envision <em>someone</em> doing that. Otherwise, I'm only planning to run ecosystem this via DockerCompose or sending it to a <strong>k8s</strong> cluster.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"iisSettings"</span>: &#123;</span><br><span class="line">    <span class="attr">"windowsAuthentication"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"anonymousAuthentication"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"iisExpress"</span>: &#123;</span><br><span class="line">      <span class="attr">"applicationUrl"</span>: <span class="string">"http://127.0.0.1.xip.io:5000"</span>,</span><br><span class="line">      <span class="attr">"sslPort"</span>: <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"profiles"</span>: &#123;</span><br><span class="line">    <span class="attr">"IIS Express"</span>: &#123;</span><br><span class="line">      <span class="attr">"commandName"</span>: <span class="string">"IISExpress"</span>,</span><br><span class="line">      <span class="attr">"launchBrowser"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"environmentVariables"</span>: &#123;</span><br><span class="line">        <span class="attr">"ASPNETCORE_ENVIRONMENT"</span>: <span class="string">"Development"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"MyProject.AspNetIdentity"</span>: &#123;</span><br><span class="line">      <span class="attr">"commandName"</span>: <span class="string">"Project"</span>,</span><br><span class="line">      <span class="attr">"launchBrowser"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"environmentVariables"</span>: &#123;</span><br><span class="line">        <span class="attr">"ASPNETCORE_ENVIRONMENT"</span>: <span class="string">"Development"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"applicationUrl"</span>: <span class="string">"http://127.0.0.1.xip.io:5000"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"Docker"</span>: &#123;</span><br><span class="line">      <span class="attr">"commandName"</span>: <span class="string">"Docker"</span>,</span><br><span class="line">      <span class="attr">"launchBrowser"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"launchUrl"</span>: <span class="string">"&#123;Scheme&#125;://&#123;ServiceHost&#125;:&#123;ServicePort&#125;"</span>,</span><br><span class="line">      <span class="attr">"environmentVariables"</span>: &#123;&#125;,</span><br><span class="line">      <span class="attr">"httpPort"</span>: <span class="number">10000</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>becomes</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"profiles"</span>: &#123;</span><br><span class="line">    <span class="attr">"MyProject.AspNetIdentity"</span>: &#123;</span><br><span class="line">      <span class="attr">"commandName"</span>: <span class="string">"Project"</span>,</span><br><span class="line">      <span class="attr">"launchBrowser"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="attr">"environmentVariables"</span>: &#123;</span><br><span class="line">        <span class="attr">"ASPNETCORE_ENVIRONMENT"</span>: <span class="string">"Development"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"applicationUrl"</span>: <span class="string">"http://127.0.0.1.xip.io:5000"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Repeat for all of the launchSettings.json files.</p><h3>The URL configuration values</h3><p>Identity implementations require a lot of information about people and applications in order to work. One of those pieces of information is the URL of the caller. The Skoruba template spits out all of the URLs in the form of <strong>127.0.0.1.xip.io</strong>. These URLs are using the <a href="http://xip.io/" target="_blank" rel="noopener">xip.io</a> service provided by the makers of Basecamp. This DNS server basically takes the DNS entry request, pulls the IP address out of it, and send that IP address base as the resolved IP. You can basically get a public DNS to resolve to an IP address that is your local machine.</p><img src="/images/dwhite/xip-io-landingpage.png" alt="Xip.io Landing Page" height="200px"><p>One thing I found while looking over the project was a blend of <strong>localhost</strong> configurations and <strong>127.0.0.1.xip.io</strong>. Because of the way that xip.io works, we can get rid of all of the localhost references. So, you can do a <strong>Replace in Files</strong> and replace all of the localhost with 127.0.0.1.xip.io.</p><blockquote><p>We could also replace all of the 127.0.0.1.xip.io with <strong>lvh.me</strong> which is a public DNS entry that resolves to 127.0.0.1 as well. <a href="https://nickjanetakis.com/blog/ngrok-lvhme-nipio-a-trilogy-for-local-development-and-testing" target="_blank" rel="noopener">This link</a> describes lvh.me and some other good utilities.</p></blockquote><p>These URLs are in the <strong>launchSettings.json</strong> file and the <strong>appsettings.json</strong>. Remember that in ASP.NET Core applications, by convention the <strong>environmental variable</strong> configurations are loaded last and overwrite anything in the appsettings.json files.</p><blockquote><p>One thing about configuration in this adventure is making sure that your configuration supports anywhere you run. That is why you see this crazy overlap of appsettings.json files or environmental variables managed all over the place. You have to develop a good understanding of how the configuration systems work in the runtime environments and take advantage of them.</p></blockquote><p>While we are doing this work, I also adjusted the ports that everything lives at. There are some mistakes in the Skoruba template that state that the Admin API is at 5001. So in this exercise I've set:</p><ul><li>STS to port 80 (no port)</li><li>Admin is at port 9000</li><li>Admin API is at port 5000</li></ul><p>These go along with pgAdmin4 already being at port 5050 and Seq being at port 5341.</p><h3>Database Connection strings</h3><p>The Skoruba uses 5 different DbContext to do its work. This in theory allows you to maintain smaller, more specific entity sets and could also spread some of this persistence work across different servers, but in practice, we'll only use one server. You can replace all connection strings with this one connection string.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Server&#x3D;postgres; User Id&#x3D;admin; Database&#x3D;identity; Port&#x3D;5432; Password&#x3D;P@ssw0rd!; SSL Mode&#x3D;Prefer; Trust Server Certificate&#x3D;true;</span><br></pre></td></tr></table></figure><blockquote><p>Notice that we can reference the Postgre DB at its DNS alias of <strong>postgres</strong>.</p></blockquote><h3>Tweak Code to Run Migrations Always</h3><p>You can see that the Skoruba team already had this idea in mind when they created the template. You just need to make the switch in <strong>Program.cs</strong> around line <strong>32</strong> and the MyProject.Admin project.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Uncomment this to seed upon startup, alternatively pass in `dotnet run /seed` to seed using CLI</span></span><br><span class="line"><span class="keyword">await</span> DbMigrationHelpers.EnsureSeedData&lt;IdentityServerConfigurationDbContext,</span><br><span class="line">                                        AdminIdentityDbContext,</span><br><span class="line">                                        IdentityServerPersistedGrantDbContext,</span><br><span class="line">                                        AdminLogDbContext,</span><br><span class="line">                                        AdminAuditLogDbContext,</span><br><span class="line">                                        UserIdentity,</span><br><span class="line">                                        UserIdentityRole&gt;(host);</span><br><span class="line"><span class="comment">//if (seed)</span></span><br><span class="line"><span class="comment">//&#123;</span></span><br><span class="line"><span class="comment">//    await DbMigrationHelpers</span></span><br><span class="line"><span class="comment">//        .EnsureSeedData&lt;IdentityServerConfigurationDbContext, AdminIdentityDbContext,</span></span><br><span class="line"><span class="comment">//            IdentityServerPersistedGrantDbContext, AdminLogDbContext, AdminAuditLogDbContext,</span></span><br><span class="line"><span class="comment">//            UserIdentity, UserIdentityRole&gt;(host);</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br></pre></td></tr></table></figure><h3>Configure Serilog to use Seq</h3><p>This is going to require a little bit of work in Visual Studio. The Skoruba template already makes use of Serilog as the logging engine, but it doesn't have the Seq sink that will be used to send all of the structured log messages to Seq. We need to add that bit to the Identity applications.</p><ol><li>Add the <code>Serilog.Sinks.Seq</code> package to all three Identity projects<img src="/images/dwhite/add-serilog-sink-seq-package.png" alt="Add Serilog Sink Seq Package to Projects" height="250px"></li><li>Change the configuration in the <strong>/shared/serilog.json</strong> file that will be used by the docker containers. <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"Serilog"</span>: &#123;</span><br><span class="line">    <span class="attr">"Using"</span>: [ <span class="string">"Serilog.Sinks.Console"</span> ],</span><br><span class="line">    <span class="attr">"MinimumLevel"</span>: &#123;</span><br><span class="line">      <span class="attr">"Default"</span>: <span class="string">"Debug"</span>,</span><br><span class="line">      <span class="attr">"Override"</span>: &#123;</span><br><span class="line">        <span class="attr">"Microsoft"</span>: <span class="string">"Information"</span>,</span><br><span class="line">        <span class="attr">"System"</span>: <span class="string">"Error"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"WriteTo"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"Name"</span>: <span class="string">"Console"</span>,</span><br><span class="line">        <span class="attr">"Args"</span>: &#123; <span class="attr">"outputTemplate"</span>: <span class="string">"[&#123;Timestamp:o&#125;][&#123;Level:u4&#125;][&#123;SourceContext&#125;] &#123;Message&#125;&#123;NewLine&#125;&#123;Exception&#125;"</span> &#125;</span><br><span class="line">      &#125;,&#123;</span><br><span class="line">        <span class="attr">"Name"</span>: <span class="string">"Seq"</span>,</span><br><span class="line">        <span class="attr">"Args"</span>: &#123; <span class="attr">"serverUrl"</span>: <span class="string">"http://seq:5341"</span> &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"Enrich"</span>: [ <span class="string">"FromLogContext"</span>, <span class="string">"WithMachineName"</span> ],</span><br><span class="line">    <span class="attr">"Properties"</span>: &#123;</span><br><span class="line">      <span class="attr">"Product"</span>: <span class="string">"IdentityServer4"</span>,</span><br><span class="line">      <span class="attr">"Platform"</span>: <span class="string">"Docker"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><strong>Optional</strong> Change the individual <strong>serilog.json</strong> files in each project to approximately match. Remember that the docker containers all use the <strong>shared/serilog.json</strong> file, not the individual serilog.json file via the <code>volumes:</code> directive <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">"./shared/serilog.json:/app/serilog.json"</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">"./shared/identitydata.json:/app/identitydata.json"</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">"./shared/identityserverdata.json:/app/identityserverdata.json"</span></span><br></pre></td></tr></table></figure></li></ol><h2>Ready to run</h2><p>Alright! We've scaffolded the IdentityServer4/Skoruba applications, we've put all of our infrastructure in place, and we've adjusted all of our configuration! We should be ready to run this application!</p><ol><li>Select the docker-compose project in the solution explorer!<img src="/images/dwhite/select-docker-compose-project.png" alt="Select Docker Compose file" height="200px"></li><li>You should see only the docker-compose option in the Visual Studio Run button<img src="/images/dwhite/docker-compose-run-button.png" alt="Docker Compose Run Button" height="200px"></li><li>Run your docker-compose orchestration!<img src="/images/dwhite/containers-running-in-docker.png" alt="Container Explorer in Visual Studio" height="200px"></li></ol><p>Now we can start to explore what we've got running in the container network.</p><ol><li>pgAdmin4</li><li>Seq</li><li>STS</li><li>Admin</li><li>AdminApi</li></ol><h3>pgAdmin4</h3><p>You should be able now to navigate to <a href="http://127.0.0.1.xip.io:5050" target="_blank" rel="noopener">http://127.0.0.1.xip.io:5050</a> in order to see the pgAdmin4 login screen.</p><img src="/images/dwhite/pgadmin4-login.png" alt="pgAdmin4 Login Screen" height="250px"><p>Now we enter our login credentials that we set in the pgAdmin4 section in the docker-compose.yml file.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pgAdmin4:</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">dpage/pgadmin4:4.20</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"5050:80"</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">pgAdmin4</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGADMIN_DEFAULT_EMAIL=admin@codingwithdave.xyz"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGADMIN_DEFAULT_PASSWORD=P@ssw0rd!"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"PGDATA=/mnt/data/pgdata"</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pgdata:/mnt/data/pgdata</span></span><br></pre></td></tr></table></figure><img src="/images/dwhite/pgadmin4-login-details.png" alt="pgAdmin4 Login Screen" height="250px"><p>Once we've logged in! We need to add a connection to the Postgre database into pgAdmin4.</p><ol><li>Create Server Listing Entry<img src="/images/dwhite/pgadmin4-create-server-listing.png" alt="pgAdmin4 Create Server Entry" height="250px"></li><li>Enter server location (DNS)<img src="/images/dwhite/pgadmin4-server-listing-name.png" alt="pgAdmin4 Enter Server DNS location" height="150px"></li><li>Enter Postgre server admin credentials <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">postgresdb:</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">postgres:alpine</span></span><br><span class="line">  <span class="attr">hostname:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"5432:5432"</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">postgresdb</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_USER=admin"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_PASSWORD=P@ssw0rd!"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_DB=identity"</span> <span class="comment"># this is the DB name that will be generated by EFCore</span></span><br></pre></td></tr></table></figure> <img src="/images/dwhite/pgadmin4-server-login-credentials.png" alt="pgAdmin4 Postgres Admin Credentials" height="250px"></li></ol><p>And voila! We can administer our Postgre-based Identity store that is used by the 3 applications in the platform!</p><img src="/images/dwhite/pgadmin4-identity-database.png" alt="pgAdmin4 Identity Database Structure" height="250px"><p>We can now run Postgre queries and sql commands to look at or manipulate our data.</p><img src="/images/dwhite/pgadmin4-identity-query-users-table.png" alt="pgAdmin4 Identity Database Query" height="250px"><h3>Seq</h3><p>Looking at Seq is a little easier! We just need to go to the <a href="http://127.0.0.1.xip.io:5341" target="_blank" rel="noopener">http://127.0.0.1.xip.io:5341</a> URL. Since we are in a single-user license and with no authentication turned on or hooked up, we'll simply land on the query screen!</p><img src="/images/dwhite/Seq-landing-page.png" alt="Seq Landing Page" height="250px"><p>Now, we can start to just get a taste of the benefit of a embedded log-ingestion application. We can see that the apps are all logging into the Seq platform.</p><img src="/images/dwhite/Seq-application-name-query.png" alt="Seq distinct query" height="250px"><p>And now we can start to look at basic loads across all three applications <em>in the same timeframe</em>.</p><img src="/images/dwhite/seq-query-visualization-all-apps.png" alt="Seq load query for all applications" height="250px"><p>I'll leave a much deeper exploration of what Seq can do for you as homework! I know your probably already at home (or work?!?!) but I'm not going to directly explore Seq's capabilities in this blog post! I'll save that for another series. But you should definitely go check out <a href="https://www.datalust.co/seq" target="_blank" rel="noopener">Seq by DataLust.co</a>.</p><blockquote><p>I am not affiliated with Datalust or Seq in anyway. I get no money for this. I just really like the product.</p></blockquote><h3>STS</h3><p>Database. Check!Database Admin. Check!Log Ingestion. Check!</p><p>Now we get to see the applications (finally) that is the reason we are doing all of the rest of this work.</p><blockquote><p>Ensure the <strong>DockerCompose</strong> project is running in Visual Studio.</p></blockquote><p>If you navigate to <a href="http://127.0.0.1.xip.io" target="_blank" rel="noopener">http://127.0.0.1.xip.io</a> you will land on the STS login page.</p><img src="/images/dwhite/sts-landing-page.png" alt="Security Token Service Landing Page" height="250px"><blockquote><p>If you have another web server running and serving pages on port 80 you may have a conflict here.</p></blockquote><p>We can log into this application using the account that was created for us! We saw the <strong>admin</strong> account in the pgAdmin4 query of the <code>Users</code> table!</p><img src="/images/dwhite/sts-login-page.png" alt="Security Token Service Login Page" height="250px"><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"IdentityData"</span>: &#123;</span><br><span class="line">    <span class="attr">"Roles"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"Name"</span>: <span class="string">"IdentityAdminRole"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"Users"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"Username"</span>: <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"Password"</span>: <span class="string">"P@ssw0rd!"</span>,</span><br><span class="line">        <span class="attr">"Email"</span>: <span class="string">"admin@codingwithdave.xyz"</span>,</span><br><span class="line">        <span class="attr">"Roles"</span>: [</span><br><span class="line">          <span class="string">"IdentityAdminRole"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"Claims"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"Type"</span>: <span class="string">"name"</span>,</span><br><span class="line">            <span class="attr">"Value"</span>: <span class="string">"admin"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Once we are logged into the STS, we can see that users can manage <em>their</em> details. They can look at the applications they've granted permissions to, they can look at their profile data. They can delete their personal data, turn on MFA, and change their password! Phew!</p><img src="/images/dwhite/sts-user-functions.png" alt="Security Token Service User Functions" height="250px"><p>Again, I'll leave a deeper exploration of the STS for homework. You should have enough familiarity after working through the IdentityServer4 Quickstarts that most of this will seem familiar. Also, this is only the user profile information, so it isn't exciting.</p><h3>Admin</h3><p>Next, we'll open a tab to the Admin application. If you are already logged into the STS, you won't be challenged for a username/password but have no doubt, you were authenticated!</p><p>Navigate to <a href="http://127.0.0.1.xip.io:9000" target="_blank" rel="noopener">http://127.0.0.1.xip.io:9000</a> you will land on the Admin application landing page.</p><img src="/images/dwhite/admin-landing-page.png" alt="Admin Landing Page" height="250px"><p>If you were not logged in, you would have been re-directed to the STS to enter your credentials, prove who you were, and then returned to the Admin page. Give it a try. Log out of the STS and try going to the admin URL again!</p><p>There is a lot to learn about administering an IdentityServer4 implementation. I'm going to cop out again and leave you to explore this as homework.</p><h3>Admin Api</h3><p>Last but not least, the Swashbuckle Api Explorer application (swagger) that is embedded in the AdminApi project!</p><p>Navigate to <a href="http://127.0.0.1.xip.io:5000/swagger" target="_blank" rel="noopener">http://127.0.0.1.xip.io:5000/swagger</a> you will land on the Admin WebApi ApiExplorer page.</p><img src="/images/dwhite/swashbuckle-api-explorer.png" alt="Swashbuckle Api Explorer" height="250px"><p>The Api Explorer is already setup to leverage token-based authentication, so if you hit the Authorize button, you'll be directed to log in (authenticate) with the STS (if you haven't already). If you are already authenticated, you may be asked by the Api Explorer app for your consent to use your profile details, but you won't have to type in your username/password again. And if you allow your consent to be remembered, you won't be challenged for it again.</p><p>If you execute any of the API methods before you are authorized, you'll get the dreaded <strong>401</strong> Unauthorized HTTP status code!</p><img src="/images/dwhite/swashbuckle-api-explorer-unauthorized.png" alt="Swashbuckle Api Explorer Unauthorized" height="250px"><p>Now get authenticated!</p><img src="/images/dwhite/swashbuckle-api-explorer-authorize.png" alt="Swashbuckle Api Explorer Authorize" height="250px"><p>And this time, we are authorized and our API call succeeds!</p><img src="/images/dwhite/swashbuckle-api-explorer-authorized.png" alt="Swashbuckle Api Explorer Authorized" height="250px"><h2>Summary</h2><p>Now that is it! Wow! That was a lot of work! But we have been rewarded by a fully functioning IdentityServer4 running on our machine with a Postgres DB as persistence, pgAdmin4 for database administration, and Seq for log ingestion!</p><p>One thing I didn't do for demonstration purposes is do any sort of custom-branding on the Skoruba applications, but they are just ASP.NET Core 3.1 <em>web applications</em> that you already know how to modify and enhance. The Skoruba template has a little bit of built-in branding configuration, but you can certainly charge ahead and make this look however you'd like.</p><p>So, there we go. We have a fully functional system running in Docker, but that isn't going to help us where we are going. Time for the next step and we move our platform to <strong>Kubernetes</strong>!!</p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-5a">Getting Started with Kubernetes - Minikube</a></p><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }    figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style><link  href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css" rel="stylesheet"><script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js"></script><script>// View an imageconst gallery = new Viewer(document.getElementById('mainPostContent', {    "navbar": false,    "toolbar": false}));</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/kubernetes/kubernetes-my-journey&quot;&gt;Series Table of Contents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Previously:&lt;/strong&gt;
&lt;a href=&quot;/kubernetes/kuberne
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, aks, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"/>
    
  </entry>
  
  <entry>
    <title type="html">Kubernetes - My Journey - Part 5a</title>
    <link href="https://westerndevs.com/kubernetes/kubernetes-my-journey-part-5a/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/kubernetes/kubernetes-my-journey-part-5a/</id>
    <published>2020-05-22T12:00:00.000Z</published>
    <updated>2020-12-02T23:16:38.518Z</updated>
	<author>
	
	  
	  <name>Dave White</name>
	  <email>dmhwhite@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p><a href="/kubernetes/kubernetes-my-journey">Series Table of Contents</a></p><p><strong>Previously:</strong><a href="/kubernetes/kubernetes-my-journey-part-4">Building an ASP.NET Core IdentityServer Implementation</a></p><h1>Getting Started with Kubernetes - Minikube - Part A</h1><p>We're finally getting to the part of the series where we have a group of applications and we want to have them live in <strong>Kubernetes (k8s)</strong>. In order to do that, we need to have <strong>k8s</strong> on our local development machine and we are going to use <strong>Minikube</strong> to do that.</p><h2>Important Caveat</h2><p>I'm going to make an assumption that at a minimum, you've reviewed various types of <strong>k8s</strong> resources on <a href="https://kubernetes.io" target="_blank" rel="noopener">kubernetes.io</a> and in a best case scenario, you've watched Nigel Poulton's Pluralsight course <a href="https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents" target="_blank" rel="noopener">Getting Started with Kubernetes</a>. If you haven't, my discussions about these topics may be harder to understand because they do not cover these basics.</p><p>Also, my understanding of Kubernetes is certainly not as extensive as I'd like. I'm not sure I'd hazard calling myself an expert. I've got a working cluster and applications in that cluster but I am not going to make the statement that I've done it all right or with the current best practices in place. This is a learning exercise for me (and you) and while I want to get you with a cluster up and running as soon as possible, I expect you to learn/challenge/grow your <strong>k8s</strong> cluster knowledge as well.</p><h2>Getting started</h2><p>Thankfully, I don't have to create a huge blog post on this! There is already some great documentation at <a href="https://kubernetes.io" target="_blank" rel="noopener">kubernetes.io</a> that gets you setup with <a href="https://kubernetes.io/docs/setup/learning-environment/minikube/#installation" target="_blank" rel="noopener">a learning environment based on minikube</a> so I'll just let you go there and read the installation guide! I'm using v1.9.2 of minikube during the creation of this series of articles.</p><h3>A Powershell Script</h3><p>It is certainly easy enough to remember some commands when standing up your environment, but as we work through getting everything into minikube using the command-line and manifests, it can become quite a list of commands and so I'd recommend creating a powershell script that you can capture your commands in the order that you are likely to execute them.</p><p>The way that I've been structuring my source files is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">project_root</span><br><span class="line">      |- infra</span><br><span class="line">      |- manifests</span><br><span class="line">      |- src</span><br></pre></td></tr></table></figure><p>The <strong>src</strong> folder is where the ASP.NET Core applications are. You can ignore the <strong>infra</strong> directory for a while, but for this part of the journey, we'll be using the <strong>manifests</strong> folder to store all of our <strong>k8s</strong> declarative manifest files. This is also where I store my <strong>stand-up.ps1</strong> powershell script file!</p><p>The first line in your stand-up.ps1 file should probably be (if your on windows):</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">minikube start -<span class="literal">-vm</span><span class="literal">-driver</span>=hyperv -<span class="literal">-cpus</span>=<span class="number">4</span> -<span class="literal">-memory</span>=<span class="number">16</span>g -<span class="literal">-extra</span><span class="literal">-config</span>=apiserver.service<span class="literal">-node</span><span class="literal">-port</span><span class="literal">-range</span>=<span class="number">80</span><span class="literal">-33000</span></span><br></pre></td></tr></table></figure><p>This is saying:</p><ul><li>start minikube<ul><li>use the hyperv driver</li><li>with 4 logical CPUS (4 of the <em>n</em> you have in Task Manager - Performance Tab)</li><li>with 16 gb of memory</li><li>20 gb of disk space (default value)</li><li>and expanding the range of usable ports in minikube</li></ul></li></ul><p>You're going to want to adjust these numbers to what make sense for your workstation. I have a i9-9900k with 64 gbs of memory, so these numbers makes sense for me. You can certainly run all of this on 1 CPU and 4 gb of RAM with no problems. As you build <strong>k8s</strong> clusters with more hosted pods, you'll probably need to increase the values when starting minikube.</p><blockquote><p>It is important to understand that this command creates a virtual machine, running linux, in hyperv, on your local workstation. If you want to change these parameters, you'll need to destroy your current minikube instance and re-create a new one.</p></blockquote><p>Out of the box, <strong>k8s</strong> (minikube) limits the port range of containers in the cluster to 30000-33000. I've expanded this range because I want to use the same port values that we used in docker. This expanded range allows <strong>k8s</strong> in minikube to use more ports, but it doesn't claim all of them.</p><p>After the minikube VM has been created (which may take a few minutes), we can then invoke our next command to make sure it is up and running!</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start a tunnel from your local machine into minikube and the kubernetes dashboard</span></span><br><span class="line"><span class="comment"># 127.0.0.1 should open in a browser window for you and it should be the Kubernetes Web UI in minikube</span></span><br><span class="line"><span class="built_in">Start-Process</span> <span class="literal">-NoNewWindow</span> minikube dashboard</span><br></pre></td></tr></table></figure><p>Using this powershell command, the process creating the tunnel is started and control of the shell is returned to you, but the process remains running. When you close down this shell, the running process will also be closed and the tunnel will close.</p><p>Hopefully, the tunnel started, a browser window opened and you can see the Kubernetes Web UI for your new minikube <strong>k8s</strong> cluster! Let's leave this browser window open and we can keep our eyes on it.</p><h2>Putting your backend into the cluster</h2><p>Now that <strong>k8s</strong> is running on your local machine, we can start to install our backend services into the cluster. We'll do this activity first, with easy to configure pods, to get used to working with manifests. Installing our IdentityServer4-based applications will require some additional resources and automation.</p><p>In this section, we will start the process of converting our docker-compose.yml into a bunch of <strong>k8s</strong> <em>manifest</em> files. We could do this as a monolithic manifest file, but I prefer smaller manifest files. They are easier to think about and just as easy to use.</p><h3>Postgres manifests</h3><p>If we look at the postgres section of our docker-compose.yml file, we will see a couple of things.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">postgresdb:</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">postgres:alpine</span></span><br><span class="line">  <span class="attr">hostname:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"5432:5432"</span></span><br><span class="line">  <span class="attr">container_name:</span> <span class="string">postgresdb</span></span><br><span class="line">  <span class="attr">environment:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_USER=admin"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_PASSWORD=P@ssw0rd!"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"POSTGRES_DB=identity"</span> <span class="comment"># this is the db name that will be generated by EFCore</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">postgresdata:/var/lib/postgresql/data</span></span><br><span class="line">  <span class="attr">networks:</span></span><br><span class="line">    <span class="attr">default:</span></span><br><span class="line">      <span class="attr">aliases:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">postgres</span></span><br></pre></td></tr></table></figure><ul><li>an image from DockerHub</li><li>container name</li><li>ports that are exposed</li><li>environmental variable declarations</li><li>attached persistent volumes</li><li>network alias</li></ul><p>In <strong>k8s</strong>, those concepts are separated into different types of resources that we will want to provision. Generally speaking, the container-based resources will be described in a <strong>Deployment</strong> manifest and the network-based resources will be described in a <strong>Service</strong> manifest.</p><blockquote><p>I tend to clump container-based concerns into a <em>manifest</em> file. So in this case, with postgres, I will also describe the <strong>PersistentVolume</strong> resources in with the Deployment resources.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-pv-volume</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/var/lib/postgresql/data</span></span><br><span class="line"><span class="comment"># divider to separate resource declarations in a yaml file</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-pv-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-dep</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">postgres-pv-storage</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">postgres-pv-claim</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">postgres</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">postgres:alpine</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_USER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"admin"</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_PASSWORD</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"P@ssw0rd!"</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_DB</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"identity"</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5432</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">"/var/lib/postgresql/data"</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">postgres-pv-storage</span></span><br></pre></td></tr></table></figure><p>Let's decompose that Deployment manifest in more detail.</p><h4>PersistenVolume</h4><p>From <em>kubernetes.io</em>...</p><p><em>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource.</em></p><p>So we need to have some disk space made available for us to use in the cluster.</p><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">PersistentVolumes</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-pv-volume</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/var/lib/postgresql/data</span></span><br></pre></td></tr></table></figure><p>The <strong>TL;DR;</strong> (too long; didn't read it) description of this resource description is:</p><ul><li>Create a volume on the linux host (<strong>k8s</strong> <em>node</em>)<ul><li>at the path described</li><li>make it 1Gi large</li><li>only let <em>one</em> node (VM/host) connect to it</li></ul></li></ul><h4>PersistentVolumeClaim</h4><p>From <em>kubernetes.io</em>...</p><p><em>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).</em></p><p>We need to claim some of the disk resources from the cluster (the PersistentVolume) in the same way that we would request some CPU or memory capacity from the cluster. It is important to explore the relationship between PV and PVC. <a href="https://rancher.com/blog/2018/2018-09-20-unexpected-kubernetes-part-1/" target="_blank" rel="noopener">This blog article</a> provides some insights as does this <a href="https://stackoverflow.com/questions/48956049/what-is-the-difference-between-persistent-volume-pv-and-persistent-volume-clai" target="_blank" rel="noopener">Stack Overflow Question/Answer</a>.</p><p>As I understand it, we will need to do this PV+PVC technique because we are using minikube, not AKS, and the storageClassName that we are using (manual) doesn't allow for <em>dynamic</em> provisioning of the storage resource. This will change when we move to AKS.</p><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener">PersistentVolumeClaims</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-pv-claim</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><h4>Deployment</h4><p>So we have the persistent storage that we want for our database setup. Now we need to get postgres deployed into our cluster.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-dep</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span> <span class="comment">#this pod is going to claim this PVC and call it postgres-pv-storage</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">postgres-pv-storage</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">postgres-pv-claim</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">postgres</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">postgres:alpine</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_USER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"admin"</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_PASSWORD</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"P@ssw0rd!"</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POSTGRES_DB</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"identity"</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5432</span></span><br><span class="line">          <span class="attr">volumeMounts:</span> <span class="comment"># this container is going to mount the volume that is the claimed PVC</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">"/var/lib/postgresql/data"</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">postgres-pv-storage</span></span><br></pre></td></tr></table></figure><p>There is a lot that goes into a Deployment manifest. You can <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">see more details here</a> in the event that you need a refresher.</p><p>We've created this manifest that will stand-up Postgres in the <strong>k8s</strong> cluster. Now we can use <strong>kubectl</strong> to run this manifest against our cluster.</p><p>First, let's make sure our kubectl is configured to point at minikube.</p><p><code>kubectl config current-context</code> should report <strong>minikube</strong>.</p><p>Next, let's run the postgre-dep.yml file through kubectl.</p><p><code>kubectl create -f .\postgres-dep.yml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PS D:\temp\testidentity\MyProject\manifests&gt; kubectl create -f .\postgres-dep.yml</span><br><span class="line"></span><br><span class="line">persistentvolume/postgres-pv-volume created</span><br><span class="line">persistentvolumeclaim/postgres-pv-claim created</span><br><span class="line">deployment.apps/postgres-dep created</span><br><span class="line"></span><br><span class="line">PS D:\temp\testidentity\MyProject\manifests&gt;</span><br></pre></td></tr></table></figure><p>Now go to the browser window that is showing you the <strong>Kubernetes Web UI</strong> and you should see all of the new resources in your cluster!</p><h5>PersistentVolume In the Cluster</h5><img src="/images/dwhite/postgres-deployment-persistentvolume.png" alt="Postgres PersistentVolume Created" height="250px"><p>Notice how this PV is a resource <em>in the cluster</em> and not related to any particular pod.</p><h5>Postgres-based Resources</h5><p>If you'd like to see only the Postgres resources,you can use the Search bar to filter everything else out.</p><img src="/images/dwhite/kubernetes-search-bar.png" alt="Postgres Deployment Completed" height="120px"><p>And now we can see all of the resources we just deployed, and explore them in more detail individually.</p><img src="/images/dwhite/postgres-deployment-completed.png" alt="Postgres Deployment Completed" height="250px"><p>So that completes putting our Postgres database into the cluster! Now we have to expose it.</p><h4>Postgres Service Manifest</h4><p>From <em>kubernetes.io</em>...</p><p><em>An abstract way to expose an application running on a set of Pods as a network service.</em><em>With Kubernetes you don’t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.</em></p><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank" rel="noopener">Services</a></p><p>Generally, you want to present as little surface area for security reasons as possible, so I have no desire to expose the database outside of the cluster. We still need a service resource created so that the container gets a DNS name within the cluster, and as containers/pods come and go, the cluster will ensure that the DNS entry always points to the right place.</p><p>So in this service manifest, we are basically telling <strong>k8s</strong> to map the DNS entry <strong>postgres-svc</strong> to whatever pod spins up with the label <strong>app:postgres</strong>.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">postgres</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">5432</span></span><br></pre></td></tr></table></figure><h3>pgAdmin4 Deployment manifests</h3><p>The pgAdmin4 deployment manifest is very similar to the postgres-dep.yml file we've created already. So I'll just post it in here so you can take a look at it. The only things that are really different are the environmental variables and the names of things.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pgadmin4-pv-volume</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/var/lib/pgadmin4/data</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pgadmin4-pv-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pgadmin4-dep</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pgadmin4</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">pgadmin4</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">pgadmin4</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pgadmin4-pv-storage</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">pgadmin4-pv-claim</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pgadmin4</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">dpage/pgadmin4</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PGADMIN_DEFAULT_EMAIL</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"admin@codingwithdave.xyz"</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PGADMIN_DEFAULT_PASSWORD</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">"P@ssw0rd!"</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">"/var/lib/pgadmin/data"</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">pgadmin-pv-storage</span></span><br></pre></td></tr></table></figure><h3>pgAdmin4 Service manifests</h3><p>Since I do want to be able to access the pgAdmin4 application from outside of the cluster, we need to make a service manifest that exposes the pgAdmin4 pod.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pgadmin4-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pgadmin4</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">5050</span></span><br></pre></td></tr></table></figure><p>Simply speaking, this manifest says to create a NodePort service resource for this pod and expose it on port 5050. Notice that the <strong>spec: selector:</strong> is saying that this service will be applied to pods with the same label of <strong>pgadmin4</strong>. Remember, in <strong>k8s</strong>, labels are usually very important. And because of the <strong>type: NodePort</strong> on this service, we will be able to access this pod from outside of the cluster.</p><blockquote><p>If you want to expose your Postgres resource outside of the cluster, you can change that manifest to create a <strong>type: NodePort</strong> service exposing the postgres pod on port 5432.</p></blockquote><p>Now we need to use kubectl to create this resource in the cluster.</p><p><code>kubectl create -f .\pgadmin4-svc.yml</code></p><p>You should see the new pgAdmin4 resources in your cluster.</p><p>We can also now use another feature of minikube which allows you to expose services in your cluster to the outside world on minikube's IP address. In order to expose pgAdmin4, simply type:</p><p><code>minikube service pgadmin4-svc</code></p><p>Minikube should then create a tunnel into the cluster for you, and open a web browser to the URL that was generated!</p><p>You can also use <code>minikube service list</code> at any time to see a list of the exposed services in your cluster.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PS D:\temp\testidentity\MyProject\manifests&gt; minikube service list  </span><br><span class="line"></span><br><span class="line">|----------------------|---------------------------|--------------|----------------------------|</span><br><span class="line">|      NAMESPACE       |           NAME            | TARGET PORT  |            URL             |</span><br><span class="line">|----------------------|---------------------------|--------------|----------------------------|</span><br><span class="line">| default              | kubernetes                | No node port |                            |</span><br><span class="line">| default              | pgadmin4-svc              |           80 | http://172.28.129.202:5050 |</span><br><span class="line">| default              | postgres-svc              | No node port |                            |</span><br><span class="line">| kube-system          | kube-dns                  | No node port |                            |</span><br><span class="line">| kubernetes-dashboard | dashboard-metrics-scraper | No node port |                            |</span><br><span class="line">| kubernetes-dashboard | kubernetes-dashboard      | No node port |                            |</span><br><span class="line">|----------------------|---------------------------|--------------|----------------------------|</span><br></pre></td></tr></table></figure><p>You should be able to log into pgAdmin4 with the credentials we've come to know and love (<strong>user:</strong> admin@codingwithdave.xyz <strong>pwd:</strong> P@ssw0rd!). Once in there, you will be able to re-create your server list entry, but this time, the location of the server is the name of the postgres service, as described in the metadata: element of the yaml.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">postgres-svc</span></span><br></pre></td></tr></table></figure><img src="/images/dwhite/minikube-postgres-server-location.png" alt="Postgres Deployment Completed" height="250px"><h3>Seq Deployment manifests</h3><p>The Seq deployment manifest is very similar to the others we've created already. So take a look, and then put Seq into your minikube cluster.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">seq-pv-volume</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/mnt/data/seqv6/</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">seq-pv-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">seq-dep</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">seq</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">seq</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">seq</span> </span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">seq-pv-storage</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">seq-pv-claim</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">seq</span> </span><br><span class="line">        <span class="attr">image:</span> <span class="string">datalust/seq:preview</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">"/data"</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">seq-pv-storage</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ACCEPT_EULA</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">"Y"</span></span><br></pre></td></tr></table></figure><p><code>kubectl create -f .\seq-dep.yml</code> will get your resources created!</p><h3>Seq Service manifests</h3><p>We want to see Seq outside of the cluster, so we will also want to expose it with a <strong>NodePort</strong> service.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">seq-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">seq</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">5341</span></span><br></pre></td></tr></table></figure><p><code>kubectl create -f .\seq-dep.yml</code> will get your service resource created!</p><p>Then expose your service via:</p><p><code>minikube service seq-svc</code></p><p>And voila! A browser window opens and you'll see Seq!! Nothing is logging there yet, but it's there! Woo hoo!!</p><h2>Summary</h2><p>Hopefully now, you've shifted all of the backend services required for our IdentityServer4 applications into minikube! They are all up and running, and you've been able to access pgAdmin4 (connected to the postgres db) and you've been able to access Seq, even if nothing is logging to it!</p><p>This has been really long post, so I'm going to take a break, let you have a break, and continue on to Part B of putting our system in Minikube!</p><p><strong>Next up:</strong><a href="/kubernetes/kubernetes-my-journey-part-5b">Getting Started with Kubernetes - Minikube - Part B</a></p><style>    h1, h2, h3, h4, h5, h6 {       margin-top: 25px;    }    figure.highlight{        background-color: #E8EEFE;    }    figure.highlight .gutter{        color: #0033CD;    }    figure.highlight pre {        font-family: 'Cascadia Code PL', monospace;    }    code {        font-family: 'Cascadia Code PL', sans-serif;        border-width: 0.1em;        border-color: #E8EEFE;        border-style: solid;        border-radius: 0.3em;        background-color: #E8EEFE;        color: #0033CD;        padding: 0em 0.4em;        white-space: nowrap;    }</style><link  href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.css" rel="stylesheet"><script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.5.0/viewer.min.js"></script><script>// View an imageconst gallery = new Viewer(document.getElementById('mainPostContent', {    "navbar": false,    "toolbar": false}));</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/kubernetes/kubernetes-my-journey&quot;&gt;Series Table of Contents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Previously:&lt;/strong&gt;
&lt;a href=&quot;/kubernetes/kuberne
    
    </summary>
    
      <category term="kubernetes" scheme="https://westerndevs.com/categories/kubernetes/"/>
    
    
      <category term="kubernetes, azure, aks, identityserver, docker, containers" scheme="https://westerndevs.com/tags/kubernetes-azure-aks-identityserver-docker-containers/"/>
    
  </entry>
  
</feed>
