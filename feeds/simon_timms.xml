<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Western Devs</title>
  
  <link href="/feeds/simon_timms" rel="self" type="application/atom+xml"/>
  <link href="https://westerndevs.com" rel="alternate" type="application/atom+xml"/>
  
  <updated>2020-12-04T02:04:56.040Z</updated>
  <id>https://westerndevs.com/</id>
  
  <author>
    <name>Western Devs</name>
	<uri>https://westerndevs.com</uri>
    <email>info@westerndevs.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title type="html">Allocating a Serverless Database in SQL Azure</title>
    <link href="https://westerndevs.com/_/serverless-sql-azure-terraform/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/serverless-sql-azure-terraform/</id>
    <published>2020-11-18T19:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.040Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I'm pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I'm provisioning a bunch of databases for a client and we're not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out.</p><a id="more"></a><p>Traditionally we've been using the resource <code>azurerm_sql_database</code> for our databases but this provider is starting to be deprecated in favour of <code>azurerm_mssql_database</code> which has better support for some of the more modern concept in SQL Azure. The <a href="https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/mssql_database#state" target="_blank" rel="noopener">documentation</a> is pretty good for it but while there was a <code>min_capacity</code> we couldn't find an equivalent <code>max_capacity</code>. Turns out you can set the max capacity using the SKU. So we had something like</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">resource <span class="string">"azurerm_mssql_database"</span> <span class="string">"database"</span> &#123;</span><br><span class="line">  <span class="attr">name</span>                        = var.database_name</span><br><span class="line">  <span class="attr">server_id</span>                   = var.database_server_id</span><br><span class="line">  <span class="attr">max_size_gb</span>                 = var.database_max_size_gb</span><br><span class="line">  <span class="attr">auto_pause_delay_in_minutes</span> = -<span class="number">1</span></span><br><span class="line">  <span class="attr">min_capacity</span>                = <span class="number">1</span></span><br><span class="line">  <span class="attr">sku_name</span>                    = <span class="string">"GP_S_Gen5_6"</span></span><br><span class="line">  <span class="attr">tags</span> = &#123;</span><br><span class="line">    <span class="attr">environment</span> = var.prefix</span><br><span class="line">  &#125;</span><br><span class="line">  short_term_retention_policy &#123;</span><br><span class="line">    <span class="attr">retention_days</span> = <span class="number">14</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This allocates a database with a capacity of between 1 and 6 vCPU that has auto pause disabled. The S in the GP_S_Gen5_6 stands for serverless and the 6 denotes the maximum capacity.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I&#39;m pretty big on the SQL Azure Serverless SKU. It allows you to scale databases up and down automatically within a band of between 0.75 and 40 vCores on Gen5 hardware. It also supports auto-pausing which can shut down the entire database during periods of inactivity. I&#39;m provisioning a bunch of databases for a client and we&#39;re not sure what performance tier is going to be needed. Eventually we may move to an elastic pool but initially we wanted to allocate the databases in a serverless configuration so we can ascertain a performance envelope. We wanted to allocate the resources in a terraform template but had a little trouble figuring it out.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Running Stored Procedures Across Databases in Azure</title>
    <link href="https://westerndevs.com/_/cross-database-procs/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/cross-database-procs/</id>
    <published>2020-11-09T19:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.040Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>In a <a href="https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/" target="_blank" rel="noopener">previous article</a> I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn't entirely accurate. You can do it if you make use of stored procedures.</p><a id="more"></a><p>Running a stored proc on a remote database is a little bit weird looking but once you get your head around that then it is perfectly usable. Let's go back to the same example we used before with a products database an an orders database. In the products database let's add a stored procedure to add a new product and return the count of products.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> addProduct</span><br><span class="line"> @item <span class="keyword">nvarchar</span>(<span class="number">50</span>)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(@item);</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) cnt <span class="keyword">from</span> products;</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>Now over in our orders database we can use our existing database connection to call this stored proc</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sp_execute_remote ProductsSource, </span><br><span class="line">                  N'addProduct @item', </span><br><span class="line">                  @params = N'@item nvarchar(50)', </span><br><span class="line">                  @item = 'long sleeved shirts';</span><br></pre></td></tr></table></figure><p>At first glance this is a little confusing so let's break it down.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp_execute_remote ProductsSource,</span><br></pre></td></tr></table></figure><p>This line instructs that we want to run a stored procedure and that it should use the ProductsSource data connection.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N'addProduct @item',</span><br></pre></td></tr></table></figure><p>This line lists the stored proc to run and the parameters to pass to it. You'll notice that it is a NVarchar string passed as a single parameter.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@params = N'@item nvarchar(50)',</span><br></pre></td></tr></table></figure><p>This line lists all the parameters to pass and their type. If you have multiple then you'd comma separate them here: <code>N'@item nvarchar(50), @price number(10,2)'</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">@item</span> = <span class="string">'long sleeved shirts'</span>;</span><br></pre></td></tr></table></figure><p>This final line is an args-style array of the values for the parameters. Again if you had a second parameter you'd pass it in as separate item here <code>@item = 'long sleeved shirts', @price=10.99</code></p><p>Running this command gets us something like</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cnt  <span class="variable">$ShardName</span></span><br><span class="line">6  [<span class="attribute">DataSource</span>=testias.database.windows.net <span class="attribute">Database</span>=testias]</span><br></pre></td></tr></table></figure><p>You'll notice that nifty ShardName colum which tells you about the source. This is because you can use a shard map to execute the stored procedure against lots of shards at once.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In a &lt;a href=&quot;https://blog.simontimms.com/2020/11/05/2020-11-05-cross-database-queries/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;previous article&lt;/a&gt; I talked about how to run queries across database instances on Azure using ElasticQuery. One of the limitations I talked about was the in ability to update data in the source database. Well that isn&#39;t entirely accurate. You can do it if you make use of stored procedures.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Azure Processor Limits</title>
    <link href="https://westerndevs.com/_/processor-limits/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/processor-limits/</id>
    <published>2020-11-05T20:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.040Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Ran into a fun little quirk in Azure today. We wanted to allocate a pretty beefy machine, an M32ms. Problem was that for the region we were looking at it wasn't showing up on our list of VM sizes. We checked and there were certainly VMs of that size available in the region we just couldn't see them. So we ran the command</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">az </span><span class="string">vm </span><span class="built_in">list-usage</span> <span class="built_in">--location</span> <span class="string">"westus"</span> <span class="built_in">--output</span> <span class="string">table</span></span><br></pre></td></tr></table></figure><p>And that returned a bunch of information about the quota limits we had in place. Sure enough in there we had</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Name</span>                               <span class="keyword">Current</span> <span class="keyword">Value</span>   <span class="keyword">Limit</span></span><br><span class="line">Standard MS <span class="keyword">Family</span> vCPUs           <span class="number">0</span>               <span class="number">0</span></span><br></pre></td></tr></table></figure><p>We opened a support request to increase the quota on that CPU. We also had a weirdly low limit on CPUs in the region</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total Regional vCPUs               <span class="number">0</span>               <span class="number">10</span></span><br></pre></td></tr></table></figure><p>Which support fixed for us too and we were then able to create the VM we were looking for.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ran into a fun little quirk in Azure today. We wanted to allocate a pretty beefy machine, an M32ms. Problem was that for the region we we
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Querying Across Databases In SQL Azure</title>
    <link href="https://westerndevs.com/_/elasticquery/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/elasticquery/</id>
    <published>2020-11-05T19:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.040Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the <code>orders</code> database from the <code>customers</code> database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that's one of the reasons.</p><a id="more"></a><p>Unfortunately, whacking developers with a ruler and making them rewrite their business logic to observe proper domain boundaries isn't always on the cards. This is a problem when migrating them to SQL Azure because querying across databases, even ones on the same server, isn't permitted.</p><p><img src="https://blog.simontimms.com/images/elasticquery/brokenQuery.png" alt="Broken query across databases"></p><p>This is where the new <a href="https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-query-overview" target="_blank" rel="noopener">Elastic Query</a> comes in. I should warn at this point that the functionality is still in preview but it's been in preview for a couple of years so I think it is pretty stable. I feel a little bit disingenuous describing it as &quot;new&quot; now but it is new to me. To use it is pretty easy and doesn't even need you to use the Azure portal.</p><p>Let's imagine that you have two databases one of which contains a collection of Products and a second database that contains a list of Orders which contain just the product id. Your mission is to query and get a list of orders and the product name. To start we can set up a couple of databases. I called mine <code>testias</code> and <code>testias2</code> and I had them both on the same instance of SQL Azure but you don't have to.</p><p><img src="https://blog.simontimms.com/images/elasticquery/setup.png" alt="Two databases on the same server"></p><h2>Product Database</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Products( </span><br><span class="line"><span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">nvarchar</span>(<span class="number">50</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'socks'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'hats'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Products(<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'gloves'</span>);</span><br></pre></td></tr></table></figure><h2>Orders Database</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orders(<span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line"><span class="built_in">date</span> <span class="built_in">date</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> orderLineItems(<span class="keyword">id</span> uniqueidentifier primary <span class="keyword">key</span> <span class="keyword">default</span> newid(),</span><br><span class="line">orderId uniqueidentifier,</span><br><span class="line">productId uniqueidentifier,</span><br><span class="line">quantity <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">foreign</span> <span class="keyword">key</span> (orderId) <span class="keyword">references</span> orders(<span class="keyword">id</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> @orderID uniqueidentifier = newid();</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(<span class="keyword">id</span>, <span class="built_in">date</span>)</span><br><span class="line"><span class="keyword">values</span>(@orderID, <span class="string">'2020-11-01'</span>);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'3829A43D-FD2A-4B7C-9A09-23DBF030C1DC'</span>, <span class="number">10</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'233BC430-BA3F-4F5C-B3EA-4B82867FC040'</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'95A20D82-EC26-4769-8840-804B88630A01'</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> @orderId = newid();</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orders(<span class="keyword">id</span>, <span class="built_in">date</span>)</span><br><span class="line"><span class="keyword">values</span>(@orderID, <span class="string">'2020-11-02'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'3829A43D-FD2A-4B7C-9A09-23DBF030C1DC'</span>, <span class="number">16</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'233BC430-BA3F-4F5C-B3EA-4B82867FC040'</span>, <span class="number">99</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> orderLineItems(orderId, productId, quantity) <span class="keyword">values</span>(@orderID, <span class="string">'95A20D82-EC26-4769-8840-804B88630A01'</span>, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>Now we need to hook up the databases to be able to see each other. We're actually just going to make products visible from the orders database. It makes more sense to me to run these queries in the database which contains the most data to minimize how much data needs to cross the wire to the other database.</p><p>So first up we need to tell the Orders database about the credentials needed to access the remote database, products. To do this we need to use a SQL account on the products database. Windows accounts and integrated security doesn't currently work for this.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">master</span> <span class="keyword">key</span> encryption <span class="keyword">by</span> <span class="keyword">password</span> = <span class="string">'monkeyNose!2'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> scoped credential ProductDatabaseCredentials </span><br><span class="line"><span class="keyword">with</span> <span class="keyword">identity</span> = <span class="string">'ProductsDBUser'</span>, </span><br><span class="line">secret = <span class="string">'wouNHk41l9fBBcqadwWiq3ert'</span>;</span><br></pre></td></tr></table></figure><p>Next we set up an external data source for the products</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">data</span> <span class="keyword">source</span> ProductsSource <span class="keyword">with</span> </span><br><span class="line">(<span class="keyword">type</span>=RDBMS, location = <span class="string">'testias.database.windows.net'</span>, </span><br><span class="line">database_name = <span class="string">'testias'</span>, credential = ProductDatabaseCredentials);</span><br></pre></td></tr></table></figure><p>Finally we create a table definition in the Orders database that matches the remote table (without any defaults or constraints).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> Products( <span class="keyword">id</span> uniqueidentifier,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">nvarchar</span>(<span class="number">50</span>))</span><br><span class="line"><span class="keyword">with</span> ( data_source = ProductsSource)</span><br></pre></td></tr></table></figure><p>We now have a products table in the external tables section in the object explorer</p><p><img src="https://blog.simontimms.com/images/elasticquery/testtableview.png" alt="Tables from both databases"></p><p>We can query the external table and even cross it against the tables in this database</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, ol.quantity <span class="keyword">from</span> orderLineItems ol <span class="keyword">inner</span> <span class="keyword">join</span> products p <span class="keyword">on</span> ol.productId = p.id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">socks   16</span><br><span class="line">socks   10</span><br><span class="line">gloves  1</span><br><span class="line">gloves  99</span><br><span class="line">hats    2</span><br><span class="line">hats    0</span><br></pre></td></tr></table></figure><p>So it is possible to run queries across databases in Azure but it takes a little set up and a little bit of thought about how to best set it up.</p><h1>Possible Gotchas</h1><ul><li>I forgot to set up the database to be able to talk to Azure resources in the firewall so I had to go back and add that</li><li>Inserting to the external table isn't supported, which is good, make the changes directly in the source database</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I seem to be picking up a few projects lately which require migrating data up to Azure SQL from an on premise database. One of the things that people tend to do when they have on premise databases is query across databases or link servers together. It is a really tempting prospect to be able to query the &lt;code&gt;orders&lt;/code&gt; database from the &lt;code&gt;customers&lt;/code&gt; database. There are, of course, numerous problems with taking this approach not the least of which is making it very difficult to change database schema. We have all heard that it is madness to integrate applications at the database level and that&#39;s one of the reasons.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">The trimStart rabbit hole</title>
    <link href="https://westerndevs.com/_/typescript-definition/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/typescript-definition/</id>
    <published>2020-09-28T18:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.040Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I was bragging to <a href="https://westerndevs.com/bios/dave_paquette/">David</a> about a particularly impressive piece of TypeScript code I wrote last week</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (body.<span class="keyword">trim</span>().<span class="keyword">startsWith</span>(<span class="string">'&lt;'</span>)) &#123; <span class="comment">//100% infallible xml detection</span></span><br></pre></td></tr></table></figure><p>He, rightly, pointed out that <code>trimStart</code> would probably be more efficient. Of course it would! However when I went to make that change there was only <code>trim</code>, <code>trimLeft</code> and <code>trimRight</code> in my TypeScript auto-complete drop down.</p><p><img src="https://blog.simontimms.com/images/trimStart/missing.png" alt="TrimStart and TrimEnd are missing"></p><p>Odd. This was for sure a real function because it appears in the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/trimStart" target="_blank" rel="noopener">MDN docs</a>.</p><p>A reasonable person would have used trimLeft and moved on but it was Monday and I was full of passion for programming. So I went down the rabbit hole.</p><p>Checking out the TypeScript directory in my node_modules I found that there were quite a few definition files in there. These were the definition files that described the JavaScript language itself rather than any libraries. Included in that bunch was one called <code>lib.es2019.string.d.ts</code>. This file contained changes which were made to the language in es2019.</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> String &#123;</span><br><span class="line">    <span class="comment">/** Removes the trailing white space and line terminator characters from a string. */</span></span><br><span class="line">    trimEnd(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the leading white space and line terminator characters from a string. */</span></span><br><span class="line">    trimStart(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the leading white space and line terminator characters from a string. */</span></span><br><span class="line">    trimLeft(): <span class="built_in">string</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Removes the trailing white space and line terminator characters from a string. */</span></span><br><span class="line">    trimRight(): <span class="built_in">string</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So I must be targeting the wrong thing! Sure enough in my <code>tsconfig.js</code> I was targeting <code>es5</code> on this project. When we started this was using an older version of node on lambda that didn't have support for more recent versions of ES. I checked and the lambda was running node 12.18.3 and support for ES2020 landed in node 12.9 so I was good to move up to es2020 as a target.</p><p>Incidentally you can check the running node version in JavaScript by running</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">console</span>.log(<span class="string">'Versions: '</span> + <span class="built_in">JSON</span>.stringify(process.versions));</span><br></pre></td></tr></table></figure><p>After updating my <code>tsconfig.js</code> and restarting the language server all was right in the world.</p><p><img src="https://blog.simontimms.com/images/trimStart/there.png" alt="The missing functions appear"></p>]]></content>
    
    <summary type="html">
    
      If you&#39;re missing expected functions in your TypeScript app the problem might be an incorrect target
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Flutter unit testing with native channels</title>
    <link href="https://westerndevs.com/_/flutter-tests-with-native/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/flutter-tests-with-native/</id>
    <published>2020-04-01T14:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Today I was digging through some unit tests in our flutter project that seemed to be failing on my machine but not necessarily in other places like our build pipeline. The problem was that we had some calls to async methods which were not being awaited properly. I fixed those up and they uncovered a bunch of more serious problems in our tests. We were calling out to validate a phone number with libphonenumber and now we were actually awaiting the call properly we saw this error</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[master â‰¡ +0 ~2 -0 !]&gt; flutter <span class="built_in">test</span></span><br><span class="line">00:03 +27 -1: <span class="built_in">test</span>\unit\providers\create_account_provider_test.dart: Real mobile number - is valid [E]</span><br><span class="line">  MissingPluginException(No implementation found <span class="keyword">for</span> method isValidPhoneNumber on channel codeheadlabs.com/libphonenumber)</span><br><span class="line">  package:blah/providers/create_account_provider.dart 101:9  CreateAccountProvider.setMobileNumber</span><br><span class="line"></span><br><span class="line">00:03 +28 -2: <span class="built_in">test</span>\unit\providers\create_account_provider_test.dart: Valid state <span class="keyword">if</span> properties are valid [E]</span><br><span class="line">  MissingPluginException(No implementation found <span class="keyword">for</span> method isValidPhoneNumber on channel codeheadlabs.com/libphonenumber)</span><br><span class="line">  package:blah/providers/create_account_provider.dart 101:9  CreateAccountProvider.setMobileNumber</span><br></pre></td></tr></table></figure><p>As it turns out libphonenumber is actually a native implementation wrapped up with flutter. To communicate with this native code isn't possible in a test environment so it needs to be mocked. This can be done by mocking the channel.</p><p>In the setUp() for the unit tests I added a call to setMockMethodCallHandler like so</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> _channel = <span class="keyword">const</span> MethodChannel(<span class="string">'codeheadlabs.com/libphonenumber'</span>);</span><br><span class="line">setUp(() <span class="keyword">async</span> &#123;</span><br><span class="line">_channel.setMockMethodCallHandler((MethodCall methodCall) <span class="keyword">async</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;);  </span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">tearDown(()&#123;</span><br><span class="line">_channel.setMockMethodCallHandler(<span class="keyword">null</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>With this call in place I was able to run the test without issue.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Today I was digging through some unit tests in our flutter project that seemed to be failing on my machine but not necessarily in other places like our build pipeline. The problem was that we had some calls to async methods which were not being awaited properly. I fixed those up and they uncovered a bunch of more serious problems in our tests. We were calling out to validate a phone number with libphonenumber and now we were actually awaiting the call properly we saw this error&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Solve WebForms Errors with PreCompilation</title>
    <link href="https://westerndevs.com/_/find-webforms-errors-with-precompilation/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/find-webforms-errors-with-precompilation/</id>
    <published>2020-04-01T00:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I have a webforms application that I help maintain. Today I made some change and managed to break one of the pages on the site. The error was unbelievably unhelpful.</p><p><img src="https://blog.simontimms.com/images/precompilewebforms/500.png" alt="Wut? 500 error with no useful details"></p><p>In older versions of ASP.NET it is nearly impossible to diagnose these sorts of errors. Was it something with the web.config? Did I mess up the dependency injection? I messed about a bit and found that if I deleted everything out of the <code>.aspx</code> file things worked. So it was the view. But what?</p><a id="more"></a><p>I don't know where the logs go for these sorts of errors but I couldn't find them. But knowing that the error was in the ASPX file I figured I could get the errors by compiling the files. You can actually precompile the ASPX files pretty easily in your post build step. It does take some time so I don't typically have it enabled for development but this is the command</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">%windir%</span><span class="symbol">\M</span>icrosoft.NET<span class="symbol">\F</span>ramework64<span class="symbol">\v</span>4.0.30319<span class="symbol">\a</span>spnet_compiler.exe -v / -p "$(SolutionDir)$(ProjectName)"</span><br></pre></td></tr></table></figure><p>Sure enough running that build gave me the error message I was looking for:</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">error </span>ASPPARSE: Literal content ('&lt;!--') is not allowed within a 'Telerik.Web.UI.GridColumnCollection'.</span><br></pre></td></tr></table></figure><p>This is the second time in a week XML comments caught me. But the story here is that if you're running into unexpected 500 errors on a webform page try compiling the page using <code>aspnet_compiler</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have a webforms application that I help maintain. Today I made some change and managed to break one of the pages on the site. The error was unbelievably unhelpful.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.simontimms.com/images/precompilewebforms/500.png&quot; alt=&quot;Wut? 500 error with no useful details&quot;&gt;&lt;/p&gt;
&lt;p&gt;In older versions of ASP.NET it is nearly impossible to diagnose these sorts of errors. Was it something with the web.config? Did I mess up the dependency injection? I messed about a bit and found that if I deleted everything out of the &lt;code&gt;.aspx&lt;/code&gt; file things worked. So it was the view. But what?&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Allow Azure DevOps Hosted Agents Through Firewall</title>
    <link href="https://westerndevs.com/_/Allow-hosted-agent-through-firewall/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Allow-hosted-agent-through-firewall/</id>
    <published>2020-01-10T14:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I have an on-premise (well in a third party data center but close enough) database which I'd like to update via a build in a hosted agent on Azure. We've done this before in Jenkins by just allowing a specific IP address through the firewall. However we're in the process of moving to DevOps for this build. Unfortunately, the hosted build agents don't have entirely predictable IP addresses. Every week Microsoft publishes a list of all the IP addresses in Azure. It is a huge json document and for our region (Central Canada) there are about 40 IP addresses ranges the build agent could be in. We want an automated way to update our firewall rules based on this list.</p><a id="more"></a><p>To do so we make use of the Azure Powershell extensions. The commandlet Get-AzNetworkServiceTag is an API based way to get the IP ranges. You can then pass that directly into the firewall rules like so</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$addrs</span> = ((<span class="built_in">Get-AzNetworkServiceTag</span> <span class="literal">-Location</span> canadacentral).Values|<span class="built_in">Where-Object</span> &#123; <span class="variable">$_</span>.Name <span class="operator">-eq</span> <span class="string">"AzureCloud.canadacentral"</span> &#125;).Properties.AddressPrefixes</span><br><span class="line"><span class="built_in">Set-NetFirewallRule</span> <span class="literal">-DisplayName</span> <span class="string">"Allow SQL 1433 Inbound"</span> <span class="literal">-RemoteAddress</span> <span class="variable">$addrs</span></span><br></pre></td></tr></table></figure><p>Running this once a week lets us keep the firewall up to date with the hosted agent ranges.</p><p>Bonus: If you want to see the remote addresses in your firewall rule currently then this will do it</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Get</span>-NetFirewallRule -<span class="keyword">Direction</span> Inbound -Action Allow -<span class="keyword">Enabled</span> <span class="keyword">True</span> -<span class="keyword">Verbose</span> | ? DisplayName -like <span class="string">"Allow SQL 1433 Inbound"</span> | <span class="keyword">Get</span>-NetFirewallAddressFilter |<span class="keyword">Select</span>-Object RemoteAddress -ExpandProperty RemoteAddress</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have an on-premise (well in a third party data center but close enough) database which I&#39;d like to update via a build in a hosted agent on Azure. We&#39;ve done this before in Jenkins by just allowing a specific IP address through the firewall. However we&#39;re in the process of moving to DevOps for this build. Unfortunately, the hosted build agents don&#39;t have entirely predictable IP addresses. Every week Microsoft publishes a list of all the IP addresses in Azure. It is a huge json document and for our region (Central Canada) there are about 40 IP addresses ranges the build agent could be in. We want an automated way to update our firewall rules based on this list.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">UNION vs. UNION ALL in SQL Server</title>
    <link href="https://westerndevs.com/_/UNION-vs-UNION-ALL/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/UNION-vs-UNION-ALL/</id>
    <published>2019-12-30T14:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I really dislike database queries which are slow for no apparent reason. I ran into one of those today. It queries over a few thousands of well indexed rows and returned a handful, perhaps 3, records. Time to do this? 33 seconds. Well that's no good for anybody. Digging into the query I found that it actually used a <code>UNION</code> to join 3 sets of similar data together. I go by the rule of thumb that SQL operations which treat data as sets and do things with that in mind are efficient. I'm not sure where I read that but it has stuck with me over the years.  What it suggests is that you should avoid doing things like looping over rows or calling functions on masses of data.</p><p>As it turns out there are actually two different <code>UNION</code> operators in SQL Server: <code>UNION</code> and <code>UNION ALL</code>. They differ in how they handle duplicate entries. <code>UNION</code> will check each entry to ensure that it exists in the output only one time.</p><a id="more"></a><p>So if you had results like</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a</span><br><span class="line"></span><br><span class="line"><span class="keyword">ID</span>     <span class="keyword">Name</span></span><br><span class="line"> <span class="number">1</span>     Bob</span><br><span class="line"> <span class="number">2</span>     Jane</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> b</span><br><span class="line"></span><br><span class="line"><span class="keyword">ID</span>     <span class="keyword">Name</span></span><br><span class="line"> <span class="number">3</span>     Sally</span><br><span class="line"> <span class="number">2</span>     Jane</span><br></pre></td></tr></table></figure><p>The result of running</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a</span><br><span class="line"><span class="keyword">union</span> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> b</span><br><span class="line"></span><br><span class="line"><span class="keyword">ID</span>     <span class="keyword">Name</span></span><br><span class="line"> <span class="number">1</span>     Bob</span><br><span class="line"> <span class="number">3</span>     Sally</span><br><span class="line"> <span class="number">2</span>     Jane</span><br></pre></td></tr></table></figure><p>Here the duplicate record 2 is only returned once. On the other hand <code>UNION ALL</code> will assume that the result sets are already unique and return whatever it is given</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> a</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> b</span><br><span class="line"></span><br><span class="line"><span class="keyword">ID</span>     <span class="keyword">Name</span></span><br><span class="line"> <span class="number">1</span>     Bob</span><br><span class="line"> <span class="number">2</span>     Jane</span><br><span class="line"> <span class="number">3</span>     Sally</span><br><span class="line"> <span class="number">2</span>     Jane</span><br></pre></td></tr></table></figure><p><code>UNION ALL</code> is, as a result of not doing this duplicate check, far faster than <code>UNION</code>. On the data sets I tried the savings were between 40% and 95%. It isn't always the right answer but is another tool on your toolbelt.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I really dislike database queries which are slow for no apparent reason. I ran into one of those today. It queries over a few thousands of well indexed rows and returned a handful, perhaps 3, records. Time to do this? 33 seconds. Well that&#39;s no good for anybody. Digging into the query I found that it actually used a &lt;code&gt;UNION&lt;/code&gt; to join 3 sets of similar data together. I go by the rule of thumb that SQL operations which treat data as sets and do things with that in mind are efficient. I&#39;m not sure where I read that but it has stuck with me over the years.  What it suggests is that you should avoid doing things like looping over rows or calling functions on masses of data.&lt;/p&gt;
&lt;p&gt;As it turns out there are actually two different &lt;code&gt;UNION&lt;/code&gt; operators in SQL Server: &lt;code&gt;UNION&lt;/code&gt; and &lt;code&gt;UNION ALL&lt;/code&gt;. They differ in how they handle duplicate entries. &lt;code&gt;UNION&lt;/code&gt; will check each entry to ensure that it exists in the output only one time.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Bulk Load and Merge Pattern</title>
    <link href="https://westerndevs.com/_/bulk-load-and-merge/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/bulk-load-and-merge/</id>
    <published>2019-10-12T04:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>The more years you spend programming the more you run into situations you've run into before. Situations you now know, instinctively, how to address. I suppose this is &quot;experience&quot; and is what I'm paid the medium dollars for. One such problem I've solved at least a dozen times over the years is updating a bunch of data in a database from an external source. This, as it turns out, can be a great source of poor performance if you don't know how to address it. Let's dig into my approach.</p><a id="more"></a><p>To start with let's give some examples of the problem</p><ul><li>update a database with the result of a web service call</li><li>load an Excel workbook or CSV file into the database</li><li>synchronize an external data source with your own cache in a database</li></ul><p>The approach that I commonly see people take is to get the records to load into the database and loop over them, checking them against existing records. It looks something like</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Update</span>(<span class="params">DbContext context, IEnumerable&lt;ExternalData&gt; toLoad</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">foreach</span>(<span class="keyword">var</span> record <span class="keyword">in</span> toLoad)&#123;</span><br><span class="line">        <span class="keyword">var</span> dbRecord = context.Find(record.Id);</span><br><span class="line">        <span class="keyword">if</span>(dbRecord != <span class="literal">null</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            dbRecord.Field1 = record.Field1;</span><br><span class="line">            ...</span><br><span class="line">            context.Save(dbRecord);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            context.Add(record);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is a pretty logical approach. Any existing record is updated, any new record is inserted. Problem is that you're running 2 database operations for every record that comes in. Try to load 10k records and all of a sudden you're in for a world of hurt. It gets even scarier if you're running all this inside a transaction which might live a minute or two. Operations like this are likely to be subject to lock escalation up to table locks which is certainly not something you want.</p><h1>Bulk Loading</h1><p>Way back in my university days we had a database class which was so popular the professor taught 2 sessions back to back. This professor was famous for wearing brown sweaters no matter the time of year. Because we had two sessions every day I'd grab somebody from the previous class and ask what the class covered that day.</p><p>On this day the professor was talking about bulk loading, it was the subject of some of his research. I asked a fellow in the previous class what they covered</p><p>&quot;Bulk loading and how much faster it is than regular inserts&quot;</p><p>&quot;Oh yeah? How much faster?&quot;</p><p>&quot;312 times, I think it was&quot;</p><p>So down into the basement I trudged, into the windowless class room. The class started and the professor asked</p><p>&quot;How many times faster do you think bulk loading is?&quot;</p><p>He sat back, waiting for the ridiculous answers, comfortable in the knowledge that he'd spend the last month writing paper on exactly this subject.</p><p>&quot;312 times, sir&quot; I answered</p><p>He was flabbergasted that somebody would know this. He'd just spent the last month figuring out that exact number. Eventually I let him off the hook and told him where I'd found out his precise number but not until I span him some story about how Donald Kunth was my uncle.</p><p>Anyway the point of this story is that I'm a better person now and that bulk loading is way faster than doing individual inserts. When loading data into the database I like to load the data into a bulk loading table instead of directly into the destination table. That provides a staging area where changes can be made.</p><p>In C# the bulk loading API is a bit <a href="https://blogs.msdn.microsoft.com/nikhilsi/2008/06/11/bulk-insert-into-sql-from-c-app/" target="_blank" rel="noopener">comically dated</a> and relies on data tables. There are some nice wrappers for it including <a href="https://dapper-plus.net/bulk-insert" target="_blank" rel="noopener">dapper-plus</a>. Using bulk copy speeds up loading substantially, perhaps not 312 times but I've certainly seen 50-100x. This reduces the chances that the transaction will run for a long time and having it run against a non-production table makes things even less likely to be problematic.</p><p>With the data loaded we can now merge it into the live data, for this we can make use of merge.</p><h1>Merge Statement</h1><p>I have a long list of features that SQL server is, frustratingly, missing. On that list is a simple upsert statement where you can tell the database what to do if there is a conflict. Both <a href="http://www.postgresqltutorial.com/postgresql-upsert/" target="_blank" rel="noopener">Postgresql</a> and <a href="https://www.techbeamers.com/mysql-upsert/" target="_blank" rel="noopener">MySQL</a> have a nice syntax for upsert. On SQL Server you have to wade through the complex <code>MERGE</code> statement. The <a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=sql-server-ver15" target="_blank" rel="noopener">documentation</a> for <code>MERGE</code> has on off the longest grammars for a statement I've ever seen; as you would expect for such a powerful a command.</p><p>A very simple example looks like this</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MERGE</span> HotelRooms <span class="keyword">AS</span> target  </span><br><span class="line">    <span class="keyword">USING</span> (<span class="keyword">SELECT</span> @roomNumber, @occupants <span class="keyword">from</span> bulkLoadHotelRooms) <span class="keyword">AS</span> <span class="keyword">source</span> (roomNumber, occupants)  </span><br><span class="line">    <span class="keyword">ON</span> (target.roomNumber = source.roomNumber)  </span><br><span class="line">    <span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></span><br><span class="line">        <span class="keyword">UPDATE</span> <span class="keyword">SET</span> <span class="keyword">Name</span> = source.occupants  </span><br><span class="line">    <span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span>  </span><br><span class="line">        <span class="keyword">INSERT</span> (roomNumber, occupants)  </span><br><span class="line">        <span class="keyword">VALUES</span> (source.roomNumber, source.occupants)</span><br></pre></td></tr></table></figure><p>This will insert records into the table <code>HotelRooms</code> from the bulk load table <code>BulkLoadHotelRooms</code> matching them on the room number (the <code>MATCHED</code> clause). If there is already a room number there then the occupants are updated (the <code>NOT MATCHED</code> clause). Not shown here there is also the ability to delete records which aren't in the target table. Also not shown are about 10 more clauses. The documentation is certainly worthwhile reading.</p><h1>Wrapping Up</h1><p>Bulk loading and merging is the best approach I've found so far to load data into a database. I've loaded millions of records on dozens of projects using this approach. If there is a better way that you've found, I'd love to hear about it.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The more years you spend programming the more you run into situations you&#39;ve run into before. Situations you now know, instinctively, how to address. I suppose this is &amp;quot;experience&amp;quot; and is what I&#39;m paid the medium dollars for. One such problem I&#39;ve solved at least a dozen times over the years is updating a bunch of data in a database from an external source. This, as it turns out, can be a great source of poor performance if you don&#39;t know how to address it. Let&#39;s dig into my approach.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Debug PHP Inside A Container with VSCode</title>
    <link href="https://westerndevs.com/_/Debugging-php-in-container/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Debugging-php-in-container/</id>
    <published>2019-07-05T23:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Sometimes good things happen to bad people and sometimes bad things happen to good people. I'll let you decide which one me developing a PHP application is. Maybe it is a bit of a mixture. This particular PHP app was a bit long in the tooth (what PHP app isn't) and ran on full VMs. My first operation was was to get it running inside a docker container because I couldn't be sure that my Windows development environment would be representative of production, then I wanted to be able to debug it. This is the story of how to do that.</p><a id="more"></a><p>Fortunately, I had some instructions on how to set up the VMs based on CentOS. Ah the good old days when installation instructions were written in word documents. The setup was a pretty standard LAMP stack and translated okay to a docker container.</p><p><img src="https://blog.simontimms.com/images/phpincontainer/lamp_stack.jpg" alt="LAMP stack - Linux, Apache, MySQL and PHP/Perl/Python"></p><p>Running the application was one thing, but I decided that what I really needed was the ability to attach a debugger to my PHP process. The PHP process inside the container. We are living in an age were VS Code is pretty amazing and it didn't fail to fulfill that promise this time. A recently announced feature in VS Code is the ability to split the front and back end of the editor. The persistence and language engines can run on a different machine from the user interface. In effect you're attaching to a remote sharing session except that the other end is inside the container.</p><p>Mind blown.</p><p>To set it up for PHP took a little bit it work. The first thing I did was install the xdebug extension for PHP. This I did by adding some lines to my Dockerfile.</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##### debugging environment #####</span></span><br><span class="line"><span class="comment"># install pecl</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install php-pear git</span></span><br><span class="line"><span class="comment"># specific version of xdebug for the ancient php we're running</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pecl install xdebug-2.2.4</span></span><br><span class="line"><span class="comment"># add module to loaded modules</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'zend_extension=/usr/lib64/php/modules/xdebug.so'</span> &gt;&gt; /etc/php.d/xdebug.ini</span></span><br><span class="line"><span class="comment"># set up the environment</span></span><br><span class="line"><span class="keyword">ENV</span> XDEBUG_CONFIG <span class="string">"remote_host=localhost remote_port=9000 remote_enable=1"</span></span><br></pre></td></tr></table></figure><p>This installs xdebug and tells it to connect to the development environment found on port 9000 which is going to be the VS Code back-end running in the container. You can connect to any host using this so if you needed to debug on some remote machine you could have xdebug connect to your local machine no problem.</p><p>Next I set up a <code>launch.json</code> in VS Code's <code>.vscode</code> directory which allowed VS Code to start debugging by listening to port 9000.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.2.0"</span>,</span><br><span class="line">    <span class="attr">"configurations"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"name"</span>: <span class="string">"Listen for XDebug"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"php"</span>,</span><br><span class="line">            <span class="attr">"request"</span>: <span class="string">"launch"</span>,</span><br><span class="line">            <span class="attr">"port"</span>: <span class="number">9000</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Finally I created a <code>.devcontainer/devcontainer.json</code> file which gives the remote extensions in VS Code knowledge of how to start up a container for development.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"Portal"</span>,</span><br><span class="line">    <span class="attr">"image"</span>: <span class="string">"portal"</span>,</span><br><span class="line">    <span class="attr">"appPort"</span>: [<span class="string">"8080:80"</span>],</span><br><span class="line">    <span class="attr">"extensions"</span>: [</span><br><span class="line">        <span class="string">"felixfbecker.php-debug"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"postCreateCommand"</span>: <span class="string">"git config --global core.autocrlf true"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The container is to forward port 80 to my localhost 8080 so that I can use the web browser to connect to the site. Inside the container I want the debugger extension installed so I list it under extensions. Any additional extensions like <code>bmewburn.vscode-intelephense-client</code> could also be listed here. Finally because I'm running on Windows and copying the source code over to a Linux image I run a command to change the line endings in git.</p><p>The last step is to install the Remote Development extensions which is what allows VS Code to be split between machines. Once it is installed you'll be prompted to reopen the folder inside a container.</p><p><img src="https://blog.simontimms.com/images/phpincontainer/launch_in_container.png" alt="The prompt when you open VS Code"></p><p>If you click reopen in container VS code restarts with the back end running in the container. You can tell by looking at the bottom left corner of the editor.</p><p><img src="https://blog.simontimms.com/images/phpincontainer/in_container.png" alt="Showing you're in a container"></p><p>With that all set up I was able to add break points and actually intercept calls to the PHP code.</p><p><img src="https://blog.simontimms.com/images/phpincontainer/at_breakpoint.png" alt="Hitting a breakpoint"></p><p>Super-cool!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sometimes good things happen to bad people and sometimes bad things happen to good people. I&#39;ll let you decide which one me developing a PHP application is. Maybe it is a bit of a mixture. This particular PHP app was a bit long in the tooth (what PHP app isn&#39;t) and ran on full VMs. My first operation was was to get it running inside a docker container because I couldn&#39;t be sure that my Windows development environment would be representative of production, then I wanted to be able to debug it. This is the story of how to do that.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Durable Functions Analyzer</title>
    <link href="https://westerndevs.com/DurableFunctions/durable_functions_analyzer/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/DurableFunctions/durable_functions_analyzer/</id>
    <published>2019-02-18T00:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>When it was announced the Roslyn would become the default compiler for C# in Visual Studio I was <a href="https://blog.simontimms.com/2014/04/04/roslyn-changes-everything/" target="_blank" rel="noopener">super excited</a>. I felt like it would generate all sorts of domain specific languages, custom flavors of C#, tons of custom error providers. So here we are 5 years later and almost none of it has come to pass. Why not?</p><a id="more"></a><p>Well turns out the compiler stuff is kind of hard. It is just a bridge too far for people to do any of the cool things I thought they would do. I guess we can add this to the long list of things that I'm wrong about.</p><p>But a few weeks ago I broke some code in a durable function because I was returning the wrong shaped data. I didn't find out until the code was deployed which is obviously later than I wanted. Because of the way that Durable Functions were constructed favoring magic strings and Objects it tends to be more susceptible to bugs which you wouldn't normally see in a statically typed language.</p><p>For instance consider this code</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">FunctionName(<span class="meta-string">"HireEmployee"</span>)</span>]</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">async</span> Task&lt;Application&gt; <span class="title">RunOrchestrator</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    [OrchestrationTrigger] DurableOrchestrationContext context,</span></span></span><br><span class="line"><span class="function"><span class="params">    ILogger log</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">var</span> applications = context.GetInput&lt;List&lt;Application&gt;&gt;();</span><br><span class="line">    <span class="keyword">var</span> approvals = <span class="keyword">await</span> context.CallActivityAsync&lt;List&lt;Application&gt;&gt;(<span class="string">"ApplicationsFiltered"</span>, Guid.NewGuid());</span><br><span class="line">    log.LogInformation(<span class="string">$"Approval received. <span class="subst">&#123;approvals.Count&#125;</span> applicants approved"</span>);</span><br><span class="line">    <span class="keyword">return</span> approvals.OrderByDescending(x =&gt; x.Score).First();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There are two magic strings in this code. The first is the name of the function in the annotation before the function declaration. The second is in the <code>CallActivityAsync</code> where a function called <code>ApplicationsFiltered</code> is called. If there is a typo in either of these strings the orchestration will fail at runtime.</p><p>You'll note too that we pass a Guid into that function. The function definition simply has an Object as the second argument so there is no type checking. Instead of passing in the Guid which is required there would be no compiler issues if we instead passed in a <code>Frog</code> or a <code>Puppy</code> or even an <code>int</code>.</p><p>I started working on a Roslyn analyzer which could solve some, or all of these short comings. I won't get into the technical parts of how to build an analyzer here (although I did just submit a conference talk on that).</p><p>It produces warnings (for now) when your functions aren't used correctly. Right now it will detect</p><ul><li>Incorrectly named functions</li><li>Incorrect return types from functions</li><li>Incorrect argument for functions</li><li>Orchestration annotations misapplied to arguments</li></ul><p>Here are some screenshots of it in action.</p><p><img src="https://blog.simontimms.com/images/roslynanalyzer/poc.png" alt="A misnamed function and a suggestion for what it should be called.">A misnamed function and a suggestion for what it should be called.</p><p><img src="https://blog.simontimms.com/images/roslynanalyzer/poc2.png" alt="An incorrect argument being detected">An incorrect argument being detected</p><p><img src="https://blog.simontimms.com/images/roslynanalyzer/poc3.png" alt="An incorrect return type being detected">An incorrect return type being detected</p><p><img src="https://blog.simontimms.com/images/roslynanalyzer/poc4.png" alt="Orchestration trigger on the wrong data type">Orchestration trigger on the wrong data type</p><p>If you want to try this out on your own project it is as easy as installing a <a href="https://www.nuget.org/packages/DurableFunctionsAnalyzer/" target="_blank" rel="noopener">nuget package</a>.</p><p>I'm looking for suggestions for new features or bugs in existing features. My tests are limited so any bug people can contribute will improve the product. Open an issue on <a href="https://github.com/stimms/DurableFunctionsAnalyzer" target="_blank" rel="noopener">github</a></p><p>Now I know how to build these analyzers I think I'll try to build more for internal applications.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When it was announced the Roslyn would become the default compiler for C# in Visual Studio I was &lt;a href=&quot;https://blog.simontimms.com/2014/04/04/roslyn-changes-everything/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;super excited&lt;/a&gt;. I felt like it would generate all sorts of domain specific languages, custom flavors of C#, tons of custom error providers. So here we are 5 years later and almost none of it has come to pass. Why not?&lt;/p&gt;
    
    </summary>
    
      <category term="DurableFunctions" scheme="https://westerndevs.com/categories/DurableFunctions/"/>
    
    
  </entry>
  
  <entry>
    <title type="html">Durable Azure Functions vs. NServiceBus Sagas</title>
    <link href="https://westerndevs.com/DurableFunctions/durablefunctions_sagas/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/DurableFunctions/durablefunctions_sagas/</id>
    <published>2019-02-15T00:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I've been on a bit of an Azure Functions kick over the last little while. I've blogged a bunch on Durable Functions and deployed a bunch more. When you're as old as me then you tend to draw comparisons between new technologies and existing ones. For instance I'm constantly telling people about how web pages are a lot like the cave paintings I use to do in my youth.</p><h1>The Twitter Exchange</h1><p>The technology that draws the closest comparison I've seen to Durable Functions are NServiceBus Sagas. A few weeks ago I tweeted out wondering if any body had done a comparison. The good folks at Particular stepped up and answered.</p><a id="more"></a><blockquote class="twitter-tweet" data-partner="tweetdeck"><p lang="en" dir="ltr">Recently <a href="https://twitter.com/stimms?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">@stimms</a> asked about comparisons between Azure Durable Functions and NServiceBus sagas. <a href="https://t.co/7sB5t9KMtH" target="_blank" rel="noopener">https://t.co/7sB5t9KMtH</a></p>&mdash; Particular Software (@ParticularSW) <a href="https://twitter.com/ParticularSW/status/1085212877034844160?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">January 15, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>I'll reproduce the thread here so you don't have to click through and I'll add some comments.</p><div style="background: #f3f3f3">Before continuing, a quick disclaimer: Durable Functions are extremely new. We haven't fully assessed them yet. This is just a few engineers throwing thoughts against the wall. Our position is guaranteed to evolve over time. Now, on with the showâ€¦<p>There's definitely some similarities between durable functions and sagas. After all, an NServiceBus saga is essentially an NServiceBus message handler with durable state, and a durable function is an Azure Function with durable state.</p><p>One big difference is that a durable function is a non-message-bound orchestrator for other functions or processes. That means continuation of the durable function is driven by awaits, not by additional messages.</p></div><p>This isn't entirely accurate. You can trigger durable function continuations using external events <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-external-events" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-external-events</a> but the mechanism is certainly less seamless than with NSB. You have to define a message handler and then raise the event. There is no nice way to find the running function like NSB does with Saga finding. <a href="https://docs.particular.net/nservicebus/sagas/saga-finding" target="_blank" rel="noopener">https://docs.particular.net/nservicebus/sagas/saga-finding</a></p><div style="background: #f3f3f3">A very cool part about durable functions is that they automatically checkpoint their progress whenever the function awaits. Local state is never lost if the process recycles or the VM reboots.<p>But this behavior is not free and does have consequences! The orchestrator stores the history of its past executions in Azure Storage, then executes the ENTIRE function again from the BEGINNING, skipping awaits already finished in previous runs.</p></div><p>Yeah this is certainly something to be aware of. You need to be careful about how long orchestrations run for. There is a concept of eternal orchestrations but basically they throw away all the history and start over again. If you have a complex orchestration then you can break it up into sub-orchestrations. But again, this isn't as nice as NSB.</p><p>One thing it does afford is the ability to rewind an orchestration and play it again.</p><div style="background: #f3f3f3">So with durable functions you need to be careful that your orchestrator's implementation is deterministic. DateTime.Now, random numbers, Guids, etc. will get you into trouble! [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints)<p>A Saga will typically focus on the interaction of one or two messages at a time. When thinking about a large process flowchart, one Saga will typically govern just the interactions at one point on the flowchart, not the entire process.</p><p>On the other hand, with a durable function, it's easy to have an entire process represented as one durable orchestrator function that can call other functions. This is both a strength (easily seeing the big picture) and a weakness (lots of coupling, can grow very complex).</p><p>Additionally, if you're trying to divide your system along service boundary lines (the vertical slices with independent databases mentioned in @MikhailShilkov's article) a giant orchestrator function covering an entire process flowchart makes this really difficult.</p></div>This seems like another place where sub-orchestrations could come into play. Probably a better approach is to have a Durable Function simply drop messages on a queue which will trigger other functions which may or may not be durable. This provides the logical separation similar to what you'd get with a series of sagas in NSB.<div style="background: #f3f3f3">A Saga on the other hand is a kind of "policy" object which statefully handles a few interrelated messages. It is completely isolated in its own vertical slice, and can communicate with other vertical slices in a lightly-coupled manner by publishing events.<p>The dangers of an all-encompassing durable function could be mitigated somewhat by doing all the real work in normal azure functions which are called from the orchestrator, and having the durable function be very lightweight, but this is a bit of a slippery slope.</p><p>Speaking of calling other functions, to do that you use the DurableOrchestrationContext's <code>CallActivityAsync&lt;TResult&gt;(&quot;FunctionName&quot;)</code>. This requires the use of &quot;magic strings&quot; for the function names, definitely not refactoring-friendly. <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-sequence</a></p></div><p>This is a limitation I've called out on twitter and in my durable functions talks. There are a few hacks you can use like leaning on <code>nameof</code> but they're not satisfactory. I've been working on a Roslyn Analyzer which will detect incorrect names, wrong parameters and broken return types. This is an addon nuget package, though, and not something that comes out of the box (unless I can convince the team to include it in the template). You can find it at <a href="https://www.nuget.org/packages/DurableFunctionsAnalyzer" target="_blank" rel="noopener">https://www.nuget.org/packages/DurableFunctionsAnalyzer</a> and eventually I'll get around to blogging the details.</p><div style="background: #f3f3f3">The capabilities of durable functions are impressive, but with an orchestrator function spanning multiple service boundaries and calling multiple other functions identified by magic strings in a command/control style, it would be VERY easy to end up with a distributed monolith.<p>Also, chaining functions together is itself a form of coupling, and this raises versioning challenges when changes to a system need to be made. <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-versioning" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-versioning</a></p><p>In NServiceBus a Saga is a stateful object with some Handle(message) methods on it where that state is persisted to a database. Even timeouts are represented as messages.</p></div>There is some really quite good unit testing advice at [https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-unit-testing](https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-unit-testing) for Durable Functions but it certainly isn't as elegant as NSB's implementations. For complex orchestrations I think the tests would be quite hacky. <div style="background: #f3f3f3">The messages are strongly-typed classes. No magic strings. Message comes in, stored state is read, make some decisions, message(s) come out. Easy to test, easy to refactor. [https://docs.particular.net/nservicebus/testing/#testing-a-saga](https://docs.particular.net/nservicebus/testing/#testing-a-saga)<p>And if anything goes wrong, the message rolls back to the queue and automatic retries kick in. This makes it easy for your system to recover from transient outages of databases or 3rd party APIs: <a href="https://docs.particular.net/tutorials/quickstart/#transient-failures" target="_blank" rel="noopener">https://docs.particular.net/tutorials/quickstart/#transient-failures</a></p><p>Also, when reproducing a bug in your system, having the actual messages which caused the failure in an error queue is a huge help, not to mention being able to &quot;replay&quot; them in production to complete the business process for your customer: <a href="https://docs.particular.net/tutorials/message-replay/" target="_blank" rel="noopener">https://docs.particular.net/tutorials/message-replay/</a></p></div><p>This I can't argue with. I've reprocessed thousands of messages in NSB and the ability to do so has saved the companies for whom I've worked thousands of dollars and avoided angry customer.</p><p>You can rewind durable orchestrations after correcting the problem. However the lack of an error queue is really tricky. The restful APIs to list orchestrations and their status and potentially rewind them aren't scalable or generally usable. There is a serious lack of tooling here (start up idea, alert!).</p><div style="background: #f3f3f3">Of course, this wouldn't be complete without our real-time performance monitoring capabilities. Keep an eye on message processing time, retries, and queues backing up - resolving issues before they hurt the business: [https://particular.net/real-time-monitoring](https://particular.net/real-time-monitoring)</div><p>I am pretty confident that you could build the equivalent of Service Pulse and Service Control on top of durable functions. If you were building a serious production system on top of Durable Functions I think you'd have to build some tooling in this space. That should be a cost consideration when you do your analysis of technologies to use.</p><div style="background: #f3f3f3">But it's naÃ¯ve to think that a system would have to be ALL sagas or ALL durable functions. Don't fall for the golden hammer fallacy, use each where it makes sense!<p>Azure Functions can be really great integration glue to bridge from various Azure Services like Blobs, Tables, Event Grid, CosmosDB, etc. to NServiceBus sagas and from there to your core business logic.</p><p>Azure functions are also great for small bits of infrastructure - we even recommend using them with NServiceBus to clean up data bus entries: <a href="https://docs.particular.net/samples/azure/blob-storage-databus-cleanup-function/" target="_blank" rel="noopener">https://docs.particular.net/samples/azure/blob-storage-databus-cleanup-function/</a></p><p>No matter what, remember that Azure Durable Functions are in their infancy. Going &quot;all in&quot; may not be the best bet. Start small.</p></div><p>This is totally accurate. I first used NSB in something like 2008 and even then it was a pretty mature product. Particular have built up a first class organization around NSB and you won't get better support anywhere.</p><div style="background: #f3f3f3">Also keep "credit-card-driven development" in mind. Azure Functions billing is all about actual usage. @troyhunt runs the haveibeenpwned API for pennies because the function is ridiculously efficient. You have to design for that, or you might get a bill you didn't expect.<p>Weâ€™re still assessing Azure Functions, so if you are actively using them in production or even thinking about using them some day, weâ€™d love to get your input! Leave your comments here: <a href="https://discuss.particular.net/t/azure-functions/872" target="_blank" rel="noopener">https://discuss.particular.net/t/azure-functions/872</a></p></div><h1>Other Aspects</h1><p>No brief tweet thread can't quite capture the entirety of the comparison. Let me go through a few other thoughts I had about the comparison.</p><h2>Platform Independence</h2><p>Durable Functions are highly coupled with Azure. This can be a good thing or a bad thing depending on your point of view. It speeds up development to have a lot of the decisions made for you already. Using existing Azure services together takes away from concerns about scalability and maintenance. However, if you ever need to migrate off Azure it would be a significant engineering effort.</p><p>NSB on the other hand is an embarrassment of configuration options. You can use SQL transport, MSMQ, SQS, Service Bus and so on and so forth. You can host on windows hardware, on VMs, docker images on K8S on Linux or Windows. No matter your environment be it cloud or infrastructure based there is an NSB configuration which will work for you. There is an added burden of figuring out what the right solution is for your application.</p><h2>Support</h2><p>I don't think the support aspect of NSB should be ignored either. Certainly there is a support system in place for Functions but it isn't going to be as good as the support you'll get from Particular. On multiple occasions I've been able to chat with the actual developers of a feature of NSB at some length. I think that's something that any users of NSB could get. I've had some discussions with the Functions team but I think that that is really through the benefit of being an MVP and being kind of vocal about Functions.</p><p>Cost is a factor too. I'm hesitant to bring this up because any cost comparison between free (which is Functions) and a paid product is bound to be one sided. You're going to have to pay for running your code one way or another. There isn't likely to be a huge price difference between running 10k messages through a cloud hosted NSB and Durable Functions. If you're a startup then there is free licensing available from Particular for NSB. I could certainly see an advantage for people in companies where buying licenses is harder than buying compute time on the cloud.  On the other hand I think that the advantages of using a mature framework such as NSB offer huge advantages in reducing bugs and maintaining a production environment.</p><h2>Code Structure</h2><p>The message based paradigm of NSB is another advantage. You could write your Durable functions in a similar way to use concrete messages but boy is it tempting not to. I've found myself using a lot of tuples to chuck messages around which really I shouldn't. The constraints that NSB puts around messages are helpful in establishing good engineering practices. You can follow the same practices in Durable Functions but you'd need a strong code review culture and coding guidelines to keep you honest. There is an opportunity here for some Roslyn Analyzers or even a framework on top of Durable Functions.</p><p>On the surface the await/async model of Durable Functions is really cool. The .NET Framework provides the syntactic framework for doing orchestrations so why not lean on it? Well because it is complicated looking. You end up defining your entire workflow in a single function and being able to trace though it in your mind is limiting. Message handlers let you focus on smaller parts of the system at a time which is helpful to those of us without giant brains. Any reasonably complex orchestration is liable to spread over 50 lines and to mix business logic and the plumbing of doing things like running fan-outs and chaining.</p><h2>The Golden Hammer</h2><p>Functions can be trigged though a lot of different mechanisms. One of those is HTTP which means you can run your entire web application inside of Functions. In the same project as your web API you can also put in place Durable Functions and they will just work. This provides a low friction way of doing background tasks or multi-step processes with checkpoints. But it also mixes a lot of code together and removes the nice separation of code you can get with deploying a bunch of NSB processes. At the moment I'm leaning towards the ease of being able to stand up Durable Functions inside your API code as outweighing the increased maintenance cost of mixing code up. However it is really something which we'll have to watch evolve as Durable Function products mature.</p><h2>Scaling</h2><p>Finally I wanted to talk about the ease of doing things like scaling out. Particular have done a lot of work around ethereal instance as of late but for the longest time the handler processes were treated much more like pets than cattle. That mentality is totally different in the land of Functions where you don't even know what hardware is being used. Being able to transparently scale up and down is really nice.</p><h1>Shut Up Already and Tell me What to Use</h1><p>I wish decisions were that easy. There are advantages to both systems and I'm sure I've missed a number of key aspects worth examining here. Durable Functions are great and I'm happy to see some competition for Particular who tend to fall very much on the slow and stable side of the spectrum as opposed to the move fast and break stuff side. Durable Functions have some growing up to do. The tooling around them isn't there yet and I don't expect that tooling is going to come out of Microsoft so much as it will come out of a vendor.</p><p>If such tooling does emerge then you'll be back in the same boat as having to pay Particular for their product which does have an impact on the cost equation.</p><p>If I were that ThoughtWorks Tech Radar thing then I would probably put Durable Functions in the <code>evaluate</code> quadrant while NSB would remain in the <code>adopt</code> quadrant.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I&#39;ve been on a bit of an Azure Functions kick over the last little while. I&#39;ve blogged a bunch on Durable Functions and deployed a bunch more. When you&#39;re as old as me then you tend to draw comparisons between new technologies and existing ones. For instance I&#39;m constantly telling people about how web pages are a lot like the cave paintings I use to do in my youth.&lt;/p&gt;
&lt;h1&gt;The Twitter Exchange&lt;/h1&gt;
&lt;p&gt;The technology that draws the closest comparison I&#39;ve seen to Durable Functions are NServiceBus Sagas. A few weeks ago I tweeted out wondering if any body had done a comparison. The good folks at Particular stepped up and answered.&lt;/p&gt;
    
    </summary>
    
      <category term="DurableFunctions" scheme="https://westerndevs.com/categories/DurableFunctions/"/>
    
    
  </entry>
  
  <entry>
    <title type="html">Accessing B2C Claims in an Azure Function</title>
    <link href="https://westerndevs.com/B2C/Getting-b2c-claims-in-an-azure-function/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/B2C/Getting-b2c-claims-in-an-azure-function/</id>
    <published>2019-02-14T00:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>In a <a href="2019-01-30-Functions-aad-authentication">previous article</a> I talked about how to authenticate your function application against Azure Active Directory Business to Consumer (which we're going to call B2C for the sake of my fingers). Chances are in your function you're going to want to get some of the information which is available as a claim from the bearer token. Here is how to do it.</p><a id="more"></a><p>On the surface this seems like a really simple problem. After all you can take the bearer token you got and paste it into <a href="https://jwt.io/" target="_blank" rel="noopener">jwt.io</a> and get back all the information you want. However we should probably take some effort to validate that what's coming back is what was originally derived from the B2C login. We can do this by making use of the System.IdentityModel.Tokens.Jwt NugGet package along with a bunch of other cryptographic stuff from the framework.</p><p>In my case I'm most interested in the <code>emails</code> claim so I can send the user an e-mail. The first thing we need to do is get the Issuer Signing key from Azure B2C. The easiest way to do this is to use the json metadata endpoint your service provided.</p><p><img src="https://blog.simontimms.com/images/functions-claims/metadata.png" alt="The metadata URL for your B2C"></p><p>If you click on that link you'll get a document like</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">issuer: <span class="string">"https://yourtenant.b2clogin.com/a04a23ad-1e6a-4114-a91b-b8f8f7ac9660/v2.0/"</span>,</span><br><span class="line">authorization_endpoint: <span class="string">"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/authorize?p=b2c_1_siupin"</span>,</span><br><span class="line">token_endpoint: <span class="string">"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/token?p=b2c_1_siupin"</span>,</span><br><span class="line">end_session_endpoint: <span class="string">"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/oauth2/v2.0/logout?p=b2c_1_siupin"</span>,</span><br><span class="line">jwks_uri: <span class="string">"https://yourtenant.b2clogin.com/yourtenant.onmicrosoft.com/discovery/v2.0/keys?p=b2c_1_siupin"</span>,</span><br><span class="line">response_modes_supported: [</span><br><span class="line"><span class="string">"query"</span>,</span><br><span class="line"><span class="string">"fragment"</span>,</span><br><span class="line"><span class="string">"form_post"</span></span><br><span class="line">],</span><br><span class="line">response_types_supported: [</span><br><span class="line"><span class="string">"code"</span>,</span><br><span class="line"><span class="string">"code id_token"</span>,</span><br><span class="line"><span class="string">"code token"</span>,</span><br><span class="line"><span class="string">"code id_token token"</span>,</span><br><span class="line"><span class="string">"id_token"</span>,</span><br><span class="line"><span class="string">"id_token token"</span>,</span><br><span class="line"><span class="string">"token"</span>,</span><br><span class="line"><span class="string">"token id_token"</span></span><br><span class="line">],</span><br><span class="line">scopes_supported: [</span><br><span class="line"><span class="string">"openid"</span></span><br><span class="line">],</span><br><span class="line">subject_types_supported: [</span><br><span class="line"><span class="string">"pairwise"</span></span><br><span class="line">],</span><br><span class="line">id_token_signing_alg_values_supported: [</span><br><span class="line"><span class="string">"RS256"</span></span><br><span class="line">],</span><br><span class="line">token_endpoint_auth_methods_supported: [</span><br><span class="line"><span class="string">"client_secret_post"</span>,</span><br><span class="line"><span class="string">"client_secret_basic"</span></span><br><span class="line">],</span><br><span class="line">claims_supported: [</span><br><span class="line"><span class="string">"emails"</span>,</span><br><span class="line"><span class="string">"idp"</span>,</span><br><span class="line"><span class="string">"name"</span>,</span><br><span class="line"><span class="string">"sub"</span>,</span><br><span class="line"><span class="string">"tfp"</span></span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this document is the <code>jwks_uri</code> which, if you click on it will give you something like this (keys altered to protect the innocent)</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">keys: [&#123;</span><br><span class="line">kid: <span class="string">"X5eXjf93njNFum1kl2Ytv8dlNP4-c57dO6QGTVBwaNk"</span>,</span><br><span class="line">nbf: <span class="number">1493764781</span>,</span><br><span class="line">use: <span class="string">"sig"</span>,</span><br><span class="line">kty: <span class="string">"RSA"</span>,</span><br><span class="line">e: <span class="string">"AQAB"</span>,</span><br><span class="line">n: <span class="string">"tVKUtcx_n9rt5afY_2WFNvU6PlFMggCatsZ3l4RjKxH0jgdLqab345de3ZGXYbPzXvmmLiWZizpb-h0qup5jznOvOr-Dhw9908584BSgC83YacjWNqEK3urxhyE2jWjwRm2N95WGgb5mzE5XmZIvkvyXnn7X8dvgFPF5QwIngGsDG8LyHuJWlaDhr_EPLMW4wHvH0zZCuRMARIJmmqiMy3VD4ftq4nS5s8vJL0pVSrkuNojtokp84AtkADCDU_BUhrc2sIgfnvZ03koCQRoZmWiHu86SuJZYkDFstVTVSR0hiXudFlfQ2rOhPlpObmku68lXw-7V-P7jwrQRFfQVXw"</span></span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The information in here is what's needed to decode the JWT and validate it. The keys here can rotate so if you're going to cache the information you'll want to make sure it expires with some frequency. I've seen an hour recommended but your mileage may vary. I followed the very helpful post <a href="https://stackoverflow.com/a/47390593/361" target="_blank" rel="noopener">here</a> for how to consume this information.</p><p>In my function I grabbed the bearer token out of the request and passed it</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> bearerToken = request.Headers[<span class="string">"Authorization"</span>].ToString().Split(<span class="string">' '</span>).Last();</span><br><span class="line"><span class="keyword">var</span> json = <span class="keyword">await</span> client.GetStringAsync(configService.TokenMetadataEndpoint()); <span class="comment">//client is a static HTTP Client</span></span><br><span class="line">JwtSecurityTokenHandler handler = <span class="keyword">new</span> JwtSecurityTokenHandler();</span><br><span class="line"><span class="keyword">var</span> claims = handler.ValidateToken(bearerToken,</span><br><span class="line">    <span class="keyword">new</span> TokenValidationParameters</span><br><span class="line">    &#123;</span><br><span class="line">        ValidateAudience = <span class="literal">true</span>,</span><br><span class="line">        ValidateIssuer = <span class="literal">true</span>,</span><br><span class="line">        ValidateLifetime = <span class="literal">true</span>,</span><br><span class="line">        ValidIssuer =  configService.TokenIssuer(),</span><br><span class="line">        ValidAudience = configService.TokenAudience(),</span><br><span class="line">        IssuerSigningKeys = GetSecurityKeys(JsonConvert.DeserializeObject&lt;JsonWebKeySet&gt;(json))</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="keyword">out</span> <span class="keyword">var</span> validatedToken).Claims;</span><br></pre></td></tr></table></figure><p>The token audience is the ID of the API application in Azure B2C. The token issuer I had trouble figuring out, in the end the only place I could find it was in a known JWT that I decoded. For me it ended up being <code>https://yourtenant.b2clogin.com/a04a23ad-1e6a-4114-a91b-b8f8f7ac9660/v2.0/</code>. I'm not sure if that GUID in there changes from tenant to tenant but certainly the host name in the URL will change to yours. We want to make sure to validate that the JWT hasn't expired (lifetime) that it was issued by our B2C instance (issuer) and that it was intended for this application (audience).</p><p>The claims object here will contain all of the claims for the JWT. I'm interested in <code>emails</code> so I run</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> email = claims.Single(x =&gt; x.Type == <span class="string">"emails"</span>);</span><br></pre></td></tr></table></figure><p>Getting the security keys is done via this handy function which deserializes the keys into a list of SecurityKeys.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;SecurityKey&gt; <span class="title">GetSecurityKeys</span>(<span class="params">JsonWebKeySet jsonWebKeySet</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> keys = <span class="keyword">new</span> List&lt;SecurityKey&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">foreach</span> (<span class="keyword">var</span> key <span class="keyword">in</span> jsonWebKeySet.Keys)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (key.Kty == <span class="string">"RSA"</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (key.X5C != <span class="literal">null</span> &amp;&amp; key.X5C.Length &gt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">string</span> certificateString = key.X5C[<span class="number">0</span>];</span><br><span class="line">                    <span class="keyword">var</span> certificate = <span class="keyword">new</span> X509Certificate2(Convert.FromBase64String(certificateString));</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">var</span> x509SecurityKey = <span class="keyword">new</span> X509SecurityKey(certificate)</span><br><span class="line">                    &#123;</span><br><span class="line">                        KeyId = key.Kid</span><br><span class="line">                    &#125;;</span><br><span class="line"></span><br><span class="line">                    keys.Add(x509SecurityKey);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="keyword">string</span>.IsNullOrWhiteSpace(key.E) &amp;&amp; !<span class="keyword">string</span>.IsNullOrWhiteSpace(key.N))</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">byte</span>[] exponent = Base64UrlDecode(key.E);</span><br><span class="line">                    <span class="keyword">byte</span>[] modulus = Base64UrlDecode(key.N);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">var</span> rsaParameters = <span class="keyword">new</span> RSAParameters</span><br><span class="line">                    &#123;</span><br><span class="line">                        Exponent = exponent,</span><br><span class="line">                        Modulus = modulus</span><br><span class="line">                    &#125;;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">var</span> rsaSecurityKey = <span class="keyword">new</span> RsaSecurityKey(rsaParameters)</span><br><span class="line">                    &#123;</span><br><span class="line">                        KeyId = key.Kid</span><br><span class="line">                    &#125;;</span><br><span class="line"></span><br><span class="line">                    keys.Add(rsaSecurityKey);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"JWK data is missing in token validation"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> NotImplementedException(<span class="string">"Only RSA key type is implemented for token validation"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> keys;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The final pieces are the models</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Model the JSON Web Key Set</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">class</span> <span class="title">JsonWebKeySet</span></span><br><span class="line">    &#123;</span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"keys"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> JsonWebKey[] Keys &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//Model the JSON Web Key object</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">class</span> <span class="title">JsonWebKey</span></span><br><span class="line">    &#123;</span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"kty"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> Kty &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"use"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> Use &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"kid"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> Kid &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"x5t"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> X5T &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"e"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> E &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"n"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> N &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"x5c"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span>[] X5C &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">JsonProperty(DefaultValueHandling = DefaultValueHandling.Ignore, NullValueHandling = NullValueHandling.Ignore, PropertyName = <span class="meta-string">"alg"</span>, Required = Required.Default)</span>]</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">string</span> Alg &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>and the Bas64Decoder</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">byte</span>[] <span class="title">Base64UrlDecode</span>(<span class="params"><span class="keyword">string</span> arg</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">string</span> s = arg;</span><br><span class="line">    s = s.Replace(<span class="string">'-'</span>, <span class="string">'+'</span>); <span class="comment">// 62nd char of encoding</span></span><br><span class="line">    s = s.Replace(<span class="string">'_'</span>, <span class="string">'/'</span>); <span class="comment">// 63rd char of encoding</span></span><br><span class="line">    <span class="keyword">switch</span> (s.Length % <span class="number">4</span>) <span class="comment">// Pad with trailing '='s</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span>: <span class="keyword">break</span>; <span class="comment">// No pad chars in this case</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>: s += <span class="string">"=="</span>; <span class="keyword">break</span>; <span class="comment">// Two pad chars</span></span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>: s += <span class="string">"="</span>; <span class="keyword">break</span>; <span class="comment">// One pad char</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> System.Exception(</span><br><span class="line">        <span class="string">"Illegal base64url string!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> Convert.FromBase64String(s); <span class="comment">// Standard base64 decoder</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As with all things security related this stuff is confusing and convoluted. Hopefully this post will help out somebody in the future.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In a &lt;a href=&quot;2019-01-30-Functions-aad-authentication&quot;&gt;previous article&lt;/a&gt; I talked about how to authenticate your function application against Azure Active Directory Business to Consumer (which we&#39;re going to call B2C for the sake of my fingers). Chances are in your function you&#39;re going to want to get some of the information which is available as a claim from the bearer token. Here is how to do it.&lt;/p&gt;
    
    </summary>
    
      <category term="B2C" scheme="https://westerndevs.com/categories/B2C/"/>
    
    
  </entry>
  
  <entry>
    <title type="html">Running a single instance of a durable function</title>
    <link href="https://westerndevs.com/_/Running-a-single-durable-function/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Running-a-single-durable-function/</id>
    <published>2019-01-31T14:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I have a durable functions project which orchestrates several thousand function calls the purpose of which is to scrape and load a bunch of data. It is scheduled to run once a day but one of my concerns was that I didn't want to accidentally run to functions at the same time. They would duplicate a bunch of the data loading and, at least until the function ran again, chaos would reign. I'm not a huge fan of chaos reigning so I set out to find a way around this.</p><a id="more"></a><p>If you've ever launched a durable function from the default HTTP triggered template you might have noticed that the JSON returned contains a number of interesting looking urls.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"id"</span>: <span class="string">"ad6b8019-b356-4099-b70c-a2b503168db3"</span>,</span><br><span class="line"><span class="string">"statusQueryGetUri"</span>: <span class="string">"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3?taskHub=DataIngest&amp;connection=Storage&amp;code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA=="</span>,</span><br><span class="line"><span class="string">"sendEventPostUri"</span>: <span class="string">"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/raiseEvent/&#123;eventName&#125;?taskHub=DataIngest&amp;connection=Storage&amp;code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA=="</span>,</span><br><span class="line"><span class="string">"terminatePostUri"</span>: <span class="string">"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/terminate?reason=&#123;text&#125;&amp;taskHub=DataIngest&amp;connection=Storage&amp;code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA=="</span>,</span><br><span class="line"><span class="string">"rewindPostUri"</span>: <span class="string">"http://localhost:7071/runtime/webhooks/durabletask/instances/ad6b8019-b356-4099-b70c-a2b503168db3/rewind?reason=&#123;text&#125;&amp;taskHub=DataIngest&amp;connection=Storage&amp;code=ra8WlNh5Vadbj0tqQddXXoZKpkamqMMt2zzfhnmais0SD1K1VzppuA=="</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The one we're interested in there is the <code>statusQueryGetUri</code>. Poking at that gets you something like</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="string">"instanceId"</span>: <span class="string">"ad6b8019-b356-4099-b70c-a2b503168db3"</span>,</span><br><span class="line"><span class="string">"runtimeStatus"</span>: <span class="string">"Completed"</span>,</span><br><span class="line"><span class="string">"input"</span>: <span class="string">"ad6b8019-b356-4099-b70c-a2b503168db3"</span>,</span><br><span class="line"><span class="string">"customStatus"</span>: <span class="literal">null</span>,</span><br><span class="line"><span class="string">"output"</span>: <span class="literal">null</span>,</span><br><span class="line"><span class="string">"createdTime"</span>: <span class="string">"2019-01-30T23:09:20Z"</span>,</span><br><span class="line"><span class="string">"lastUpdatedTime"</span>: <span class="string">"2019-01-30T23:10:25Z"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Notice that there is a <code>runtimeStatus</code> field there which reports the status of the orchestration. If we could look at the running orchestrations then perhaps we could see if there is already one running. Playing with the status URL I found that you could see all the running instances by simply looking at <code>http://localhost:7071/runtime/webhooks/durabletask/instances</code>.</p><p>Even better than that the DurableOrchestrationClient used to start an orchestration has on it a function called GetStatusAsync which returns a list of all the running orchestrations currently running on the task hub. This means we can do something like</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> statuses = <span class="keyword">await</span> starter.GetStatusAsync();</span><br><span class="line"><span class="keyword">if</span> (statuses.Any(x =&gt; x.RuntimeStatus == OrchestrationRuntimeStatus.Running &amp;&amp; x.Name == <span class="string">"OrchestrationName"</span>))</span><br><span class="line">&#123;</span><br><span class="line">    log.LogWarning(<span class="string">"An a running instance of the orchestration was detected. Terminating run."</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> HttpResponseMessage(System.Net.HttpStatusCode.Conflict);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This will prevent multiple instances of the orchestrator running. Note that we need to filter on orchestration name because it is possible the same task hub has multiple different orchestrations.</p><p>Is this perfect? Oh heck no. There are lots of windows in which multiple orchestrations could be started. For my purposes, however, this works just fine. If your orchestrations are run more tightly than this then you might want to look at a distributed lock. My buddy Matt has a solution on the <a href="https://stackify.com/distributed-method-mutexing/" target="_blank" rel="noopener">Stackify blog</a> for this exact problem.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have a durable functions project which orchestrates several thousand function calls the purpose of which is to scrape and load a bunch of data. It is scheduled to run once a day but one of my concerns was that I didn&#39;t want to accidentally run to functions at the same time. They would duplicate a bunch of the data loading and, at least until the function ran again, chaos would reign. I&#39;m not a huge fan of chaos reigning so I set out to find a way around this.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Azure Functions and Azure B2C Authentication</title>
    <link href="https://westerndevs.com/_/Functions-aad-authentication/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/Functions-aad-authentication/</id>
    <published>2019-01-30T18:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>I had a pretty good struggle setting up Azure Functions and Azure B2C to work together. There are a few guides out there but I wanted to put my own together because I had a terrible time finding these posts initially. The scenario here is that we want a single page application written in React to talk to an API hosted entirely in Azure Functions such that the functions are authenticated.</p><a id="more"></a><h2>Azure</h2><p>First up you'll need to create a new tenant for Azure B2C. This is a weird two step process which I'm given to understand is going to be improved at some point in the near future. For now in the Azure Portal select <code>Create a resource</code> then <code>Azure B2C</code>. You'll be presented with these two options.</p><p><img src="https://blog.simontimms.com/images/functions-aad/create_tenant.png" alt="Creating a tenant"></p><p>To start you'll need to create a tenant, so pick the first one. You'll be presented with a few options for organization name and initial domain. These are specific to your organization so I'll leave them up to you but think on the names a little because they cannot be changed.</p><p>With the B2C tenant created you'll now need the second option to link an existing Azure AD B2C tenant to the Azure subscription. This will create a reference to your tenant in your main Azure subscription.</p><p><img src="https://blog.simontimms.com/images/functions-aad/linking_tenant.png" alt="Linking a tenant"></p><p>Managing tenants is very confusing because you need to actually switch your Azure portal over to the new tenant. This is the least intuitive part of the process in my mind and shouldn't have been implemented this way. So click on the account selector in the top right of the portal to switch directories. Once you're in the new directory you'll see all the various resources you're use to. Don't both touching those, head straight over to Azure AD B2C in All resources.</p><p>In here you'll want to start in the Applications section. We'll be creating two applications and defining some claims between them. One application will be for the functions and the other for the SPA. You can expand this to more applications as your application grows.</p><p>The SPA should be the first one you create. I, creatively, called mine <code>SPA</code>. This one does not need an App ID URI but you should allow implicit flows and include web app/web API.</p><p><img src="https://blog.simontimms.com/images/functions-aad/spa_properties.png" alt="Properties for the SPA"></p><p>The reply URLs should contain a list of all the page from which a user can authenticate. Unfortunately, wildcards are not permitted here so you need to be explicit.</p><p>With that created we can now move onto the API app. This one is similar except that you'll want to put in place an App ID. Mine is simply called <code>API</code></p><p><img src="https://blog.simontimms.com/images/functions-aad/api_properties.png" alt="Properties for the API"></p><p>In that screenshot you'll notice a reply URL. You likely don't have that yet so leave it blank. We'll come back to it. Now the API app will need to publish a scope. This is what ties your two apps (API and SPA) together. I created a single scope called <code>access</code> because I don't have anything too complex in my API. You can publish multiple scopes if you wish.</p><p><img src="https://blog.simontimms.com/images/functions-aad/scopes.png" alt="Scopes"></p><p>Publishing scopes is half the equation, now back to the API application and select <code>API access</code>. In there select <code>Add</code> and select <code>API</code> in the first dropdown and then select both scopes in the second. Users who have authenticated against the SPA will now be able to access the API.</p><p>We're almost done in the B2C for now but we also want to set up a <code>User Flow</code>. This is a hosted login experience. You can customize the experience but that's outside of this tutorial. We want a basic Signup and Sign in policy. I named mine <code>SignupSignin</code> but you can pick whatever you like. Be sure to check the Email Signup identity provider. I also drilled into the <code>User attributes and claims</code> to select Email Address as a collected attribute and Email Addresses as a returned claim. This means that during the signup process we'll ask for the Email address and when authenticating in the SPA the JWT passed back will contain a collection of Email addresses. You can select more if you like.</p><p><img src="https://blog.simontimms.com/images/functions-aad/policy.png" alt="Policy"></p><p>Once the policy is created you'll want to select it and click <code>Run user flow</code> in there you'll find a metadata URL which will be used in your SPA application and functions. Make a note of it. While we're here also make a not of the ID of the API application.</p><p>Now all this is created we're now ready to jump over to the function app you want to authenticate. Phew, that took a while.</p><p>Switch back to your primary directory and head over to your function app. If you don't have one created already just create a blank C# one. In the function app click through to the platform features and select Authentication.</p><p><img src="https://blog.simontimms.com/images/functions-aad/platform_features.png" alt="Authentication"></p><p>In authentication turn on <code>App Service Authentication</code> and select <code>Azure Active Directory</code>. Switch over to advanced and enter the API application Id in the Client ID field and the metadata URL in the Issuer Url field. These are the two things you made notes about before leaving the B2C.</p><p><img src="https://blog.simontimms.com/images/functions-aad/aad_settings.png" alt="Active Directory Auth"></p><p>Finally change the <code>Action to take when request is not authenticated</code> over to <code>Log in with Azure Active Directory</code>. Save the authentication page. Now you'll need to make one final change over in the B2C. You'll need to find the URL for you function app. This is easily found by clicking on a function and running it. Take the URL and back in the B2C head to the API application.</p><p>In the API application you'll need to set the <code>Reply URL</code>. The URL should be of the format <code>https://&lt;functionapp name&gt;/.auth/login/aad/callback</code> so if you function app is called <code>bob</code> the result would be <code>https://bob/.auth/login/aad/callback</code>. With that in place we're done the Azure setup.</p><h2>In React</h2><p>Open up a terminal and install the handy package <code>react-aad-msal</code>. (Note: If you run into problems authenticating you might want to try the <code>react-aad-msal-jfm</code> package instead. It works around a new domain that Microsoft is using for B2C which isn't accepted by the <code>react-aad-msal</code> package.). This package provides some React components you can put on your page to do the actual authentication.</p><p>In a page I put in this control. I'm using Create ReactApp so the values for authority, clientID and scope are taken from a config file. The Authority is the Url for the policy so something like <code>https://yourtenant.b2clogin.com/tfp/yourtenant.onmicrosoft.com/B2C_1_SiUpIn</code>. The client Id is the ID of the SPA app from the B2C. Finally the scope is the fully qualified scope <code>https://yourtenant.onmicrosoft.com/API/access</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import &#123; AzureAD, LoginType, MsalAuthProviderFactory &#125; from 'react-aad-msal-jfm';</span><br><span class="line">...</span><br><span class="line"> <span class="tag">&lt;<span class="name">AzureAD</span></span></span><br><span class="line"><span class="tag">    <span class="attr">provider</span>=<span class="string">&#123;new</span> <span class="attr">MsalAuthProviderFactory</span>(&#123;</span></span><br><span class="line"><span class="tag">        <span class="attr">authority:</span> <span class="attr">process.env.REACT_APP_AUTHORITY</span>,</span></span><br><span class="line"><span class="tag">        <span class="attr">clientID:</span> <span class="attr">process.env.REACT_APP_CLIENT_ID</span>,</span></span><br><span class="line"><span class="tag">        <span class="attr">scopes:</span> [<span class="attr">process.env.REACT_APP_SCOPE</span>],</span></span><br><span class="line"><span class="tag">        <span class="attr">type:</span> <span class="attr">LoginType.Popup</span>,</span></span><br><span class="line"><span class="tag">        <span class="attr">persistLoginPastSession:</span> <span class="attr">true</span>,</span></span><br><span class="line"><span class="tag">        <span class="attr">validateAuthority:</span> <span class="attr">false</span></span></span><br><span class="line"><span class="tag"></span></span><br><span class="line"><span class="tag">    &#125;)&#125;</span></span><br><span class="line"><span class="tag">    <span class="attr">unauthenticatedFunction</span>=<span class="string">&#123;this.unauthenticatedFunction&#125;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">authenticatedFunction</span>=<span class="string">&#123;this.authenticatedFunction&#125;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">userInfoCallback</span>=<span class="string">&#123;this.userJustLoggedIn&#125;</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>With this in place a login button is rendered. Click on the button and a popup will be shown with your login page. I saved off the bearer token from the <code>userInfoCallback</code> which is passed an object containing the <code>jwtAccessToken</code>. The emails we permitted as a claim are passed back as <code>idToken.emails</code>. My reducer looks a bit like</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> getType(login.loginSuccess):</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        ...state,</span><br><span class="line">        loggedIn: <span class="literal">true</span>,</span><br><span class="line">        bearerToken: action.payload.body.jwtAccessToken,</span><br><span class="line">        displayName: action.payload.body.user.name,</span><br><span class="line">        userId: action.payload.body.user.userIdentifier,</span><br><span class="line">        email: action.payload.body.user.idToken.emails[<span class="number">0</span>]</span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure><p>This jwtAccess token should be used in the headers of any fetches against the function app.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> result = <span class="keyword">await</span> fetch(<span class="string">`<span class="subst">$&#123;process.env.REACT_APP_API_URL&#125;</span>/Practices`</span>, &#123;</span><br><span class="line">            method: <span class="string">'GET'</span>,</span><br><span class="line">            headers: &#123;</span><br><span class="line">                <span class="string">'Authorization'</span>: <span class="string">`Bearer <span class="subst">$&#123;<span class="keyword">this</span>.props.login.bearerToken&#125;</span>`</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I had a pretty good struggle setting up Azure Functions and Azure B2C to work together. There are a few guides out there but I wanted to put my own together because I had a terrible time finding these posts initially. The scenario here is that we want a single page application written in React to talk to an API hosted entirely in Azure Functions such that the functions are authenticated.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Creating Storage Queues in Azure DevOps</title>
    <link href="https://westerndevs.com/_/2018-12-06-creating-queues-in-devops/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/2018-12-06-creating-queues-in-devops/</id>
    <published>2018-12-06T18:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Storage Queues are one of the original pieces of Azure dating back about a decade now. They are great for deferring work to later or spreading it out over a bunch of consumers. If you're following best practices for DevOps you'll know that the creation of your queues should be done in code. In some cases you can create the queues on application startup but in serverless scenarios there often is no startup code so the responsibility of creating queues falls to your deployment process. Let's look at how to do that on Azure DevOps</p><a id="more"></a><p>Your first instinct might be to use your ARM templates to build queues. This makes great sense - the storage account is defined in the ARM template so why not also define the queues? Because you can't!</p><p>There is a <a href="https://feedback.azure.com/forums/281804-azure-resource-manager/suggestions/9306108-let-me-define-preconfigured-blob-containers-table" target="_blank" rel="noopener">request open to add that functionality</a> but after 3 years it remains unanswered. Talking to some Azure engineers at Microsoft it seems like the approach they would like people to take is to build the queue in the application. Problem with that is that now your application needs to have rights to create queues instead of just right to write to or read from the queue. You might also not have an appropriate place to put startup code - for instance Azure Functions don't have a good way to run code on startup. Same deal with logic apps.</p><p>The fact that creating storage queues is not officially supported in ARM templates is baloney and the argument that this is an application level create queue to do is proof that parts of Microsoft still doesn't get DevOps.</p><p>This means we have to plug something into the pipeline to create the queues. I quite like the <a href="https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest" target="_blank" rel="noopener">Azure CLI</a> which is a great little command line tool for interacting with Azure. You can add in the Azure CLI task in Azure DevOps, hook up the subscription and the give it an inline script like so:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">call az storage<span class="built_in"> queue </span>create -n <span class="string">"awesome-queue-1"</span> --connection-string <span class="string">"<span class="variable">$(storageAccountConnectionString)</span>"</span></span><br></pre></td></tr></table></figure><p>If you're using a Windows build agent then you need to include the <code>call</code> to ensure that multiple lines are executed. If you're on a Linux agent then <code>call</code> can be omitted.</p><p>That connection string can be exported from your ARM template as an output parameter and then sucked into the DevOps variables using <a href="https://github.com/keesschollaart81/vsts-arm-outputs" target="_blank" rel="noopener">ARM Outputs</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Storage Queues are one of the original pieces of Azure dating back about a decade now. They are great for deferring work to later or spreading it out over a bunch of consumers. If you&#39;re following best practices for DevOps you&#39;ll know that the creation of your queues should be done in code. In some cases you can create the queues on application startup but in serverless scenarios there often is no startup code so the responsibility of creating queues falls to your deployment process. Let&#39;s look at how to do that on Azure DevOps&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Azure Data Factory - a rapid introduction</title>
    <link href="https://westerndevs.com/_/2018-11-05-datafactory/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/2018-11-05-datafactory/</id>
    <published>2018-11-05T18:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Azure is huge. There are probably a dozen ways to host a website, a similar number of different data storage technologies, tools for identity, scaling, DDoS protection - you name it Azure has it. With that many services it isn't unusual for me to find some service I didn't even know existed. Today that service is <a href="https://azure.microsoft.com/en-ca/services/data-factory/" target="_blank" rel="noopener">Data Factory</a>. Data factory is a batch based Extract, Transform and Load(ETL) service which means that it moves data between locations. I mention that it is batch to distinguish it from services which are online and process events as they come in. Data Factory might be used to move data between a production database and the test system or between two data sources.</p><a id="more"></a><p>In most cases I'd recommend solutions which were more tightly integrated into your business processes than copying data between databases. It is easier to test, closer to real time and easier to update. However, moving to event based systems can be long and difficult so there is certainly a niche for Data Factory. A good application might be migrating data from a service you don't own to your internal database - the external system is unlikely to have data change notifications you could use to drive data population. Azure Data Factory plays in the same space that SQL Server Integration Services played in the past - in fact you can build your Azure Data Factory pipeline in SSIS and simply upload it.</p><p>Let's take a look at loading some data from an Azure SQL database, and putting it into Cosmos DB. I've chosen these two systems but there are literally dozens of different data sources and destinations you can use at the click of a mouse. They are not limited to Microsoft offerings, either. There are connectors for Cassandra, Couchbase, MongoDB, Google BigQuery even Oracle.</p><p><img src="https://blog.simontimms.com/images/datafactory/datasources.png" alt="A small sample of the various datasources which exist within data factory"></p><p>The first step is to create a new data factory in Azure. This is as simple as searching for data factory in the Azure portal and clicking create. A few settings are required such as the name and the version. For this I went with v2 because it is a larger number than v1 and ergo way better. I'm pretty sure that's how numbers work. With the factory created the next step is to click on <code>Author and Monitor</code>.</p><p><img src="https://blog.simontimms.com/images/datafactory/author.png" alt="Select author and monitor in the portal"></p><p>This opens up a whole editor experience in a new tab. It is still roughly styled like the portal so it isn't as jarring as using the man styling jumble that is AWS' console. In the left gutter click on the little <code>+</code> symbol to create a new data set.</p><p><img src="https://blog.simontimms.com/images/datafactory/newdatasource.png" alt="New SQL data source"></p><p>I found myself an old backup database I had kicking around still on my Azure account to be the source of data for this experiment. It is a database of data related to the construction of some oil extraction facility somewhere. To protect the innocent I've anonymized the data a little. We'll start by adding this as a source for the data factory. Select Azure SQL as the source and then give it a name in the general pane. Under connection set up a new linked service. This is what holds our data connection information so multiple different data sets can use the same linked service if you wanted to pull from multiple tables. In the linked service set up you can select an existing database from the drop downs and enter the login information.</p><p>With the linked service set up you can select the table you'll be using for the schema information and even preview the data.</p><p><img src="https://blog.simontimms.com/images/datafactory/preview.png" alt="A preview of the data in the SQL database"></p><p>Next follow a similar procedure for setting up the cosmos data source. My cosmos data source was brand new so it didn't have any document from which data factory could figure out the schema. This meant that I had to go in and define one in the data source.</p><p><img src="https://blog.simontimms.com/images/datafactory/schema.png" alt="Defining the Cosmos database schema"></p><p>With the two data sources in place all that is needed now is to copy the data from one to another. Data factory is obviously a lot more than being able to copy data between data bases but to do any manipulation of the data you really need to pull in other services. For instance you can manipulate the data with data bricks or HD Insights and, of course, you can analyze the data with Azure ML. What is missing, in my mind, is a really simple way of manipulating fields, concatenating them together, splitting them up that sort of thing. Because data factory is designed to scale it relies on other services which can also scale instead of internalizing too much. On one hand this is good because is formalizes your scaling and makes you think about what you do if you have huge quantities of data. On the other hand is raises the knowledge bar for entry quite high.</p><p>Originally this article was going to cover manipulating data but the difficulty meant that that content had to be pushed off to another post.</p><p>Returning to the problem at hand the copy task is added by adding a new pipeline. Within that pipeline we add a copy data task by dragging it to the canvas. In the task we configure the source as being the SQL database and, at the same time, select a query. My query filters for tags which are complete (you don't really need to know what that means).</p><p><img src="https://blog.simontimms.com/images/datafactory/selecttags.png" alt="Entering a query"></p><p>Next set up a destination sink as the cosmos db. Finally set up the mapping. Mappings determine which fields go where: from the source into the destination. Because we've gone to the trouble of ensuring field names are the same over our two data sets simply clicking <code>Import Schemas</code> is enough to set up the mappings for us. You may need to manually map fields if you're renaming as part of the copy.</p><p>Pipelines are built by coupling together various tasks to copy, filter, sort and otherwise manipulate data. Each task has a success, completion and failure output which can be wired to the next task allowing you to build pretty complex logic. Of course as with all complex logic it is nice to have automated tests around it. This is a failing of data factory - it is difficult to test the workflow logic.</p><p>The set up of the pipeline is now complete. To start using it you first need to publish it which is done by clicking on the <code>Publish All</code> button. Publishing takes a moment but once it is done testing the integration is as simple as clicking on trigger and going down to <code>Trigger Now</code>. Within a few seconds I was able to jump to my cosmos and find it filled with all the records from SQL. It was quick and easy to set up. What's really nice too is that the pipeline can easily be scheduled.</p><p>Data factory is not the right solution for every project. I'd actually argue that it isn't the right solution for most projects but it is a good stop gap until you can move to a more online version of data integration using something like change events and functions. Of course that assumes you have infinite resources to improve your projects...</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Azure is huge. There are probably a dozen ways to host a website, a similar number of different data storage technologies, tools for identity, scaling, DDoS protection - you name it Azure has it. With that many services it isn&#39;t unusual for me to find some service I didn&#39;t even know existed. Today that service is &lt;a href=&quot;https://azure.microsoft.com/en-ca/services/data-factory/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Data Factory&lt;/a&gt;. Data factory is a batch based Extract, Transform and Load(ETL) service which means that it moves data between locations. I mention that it is batch to distinguish it from services which are online and process events as they come in. Data Factory might be used to move data between a production database and the test system or between two data sources.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title type="html">Checking in packages</title>
    <link href="https://westerndevs.com/development-fundamentals/Checking-In-packages/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/development-fundamentals/Checking-In-packages/</id>
    <published>2018-10-21T15:36:36.000Z</published>
    <updated>2020-12-04T02:04:56.032Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>If there is one thing that we developers are good at it is holy wars. Vi vs. Emacs, tabs vs. spaces, Python vs. R, the list goes on. I'm usually smart enough to not get involved in such low brow exchanges... haha, who am I kidding? (vi, spaces and R, BTW) Recently I've been tilting at the windmill that is checking in package files. I don't mean the files that tell what version of files to check in but the actual library files.</p><a id="more"></a><p>Package managers aren't anything new, we've had them for years, decades even if you consider CPAN which has been online for 23 years. The idea behind them is that they provide an easy mechanism to install dependencies into your project. At the same time you can avoid checking in a bunch of library files and instead run a package restore as one of the build steps.</p><p>I've heard a couple of arguments against relying on package managers instead of checking in your libraries.</p><ol><li>What if the package manager goes away? How could you still reliably build the software years in the future?</li><li>What if the specific package being used goes away? It might be unpublished like what happened with <a href="https://www.theregister.co.uk/2016/03/23/npm_left_pad_chaos/" target="_blank" rel="noopener">left-pad</a></li><li>What if a transitive dependency, that is one that is included because it is a dependency of some other package, is revved and the package author of the included dependency left the dependency requirement open?</li><li>It takes a long time to restore packages using a package manager, we can speed up builds by not running the package restore during the build.</li><li>Everything you need to build your solution should be checked into source control.</li></ol><p>Let's break each one of these down and see why they are wrong headed.</p><ol><li>Package managers have been around for years and aren't going away anytime soon. Perl is hardly a well used language these days but CPAN survives still. Package managers will outlast the applications built using the language.</li><li>This is a totally legitimate concern. Fortunately policies have changed at <a href="https://docs.npmjs.com/cli/unpublish" target="_blank" rel="noopener">npm</a> and <a href="https://docs.microsoft.com/en-us/nuget/policies/deleting-packages" target="_blank" rel="noopener">NuGet</a> to no longer allow removing packages after a brief window.</li><li>I've seen this happen with some frequency. Again package managers have changed to ensure that we no longer have this problem. Npm has introduced a package-lock file with version 5 and finally fixed how it works with version 6. By checking in this file we can be assured that we get exactly the right version of packages restored in a build. NuGet is, well, <a href="https://github.com/NuGet/Home/wiki/Enable-repeatable-package-restore-using-lock-file" target="_blank" rel="noopener">trying to catch up</a> on package lock files. Paket, which uses NuGet files under the covers does have <a href="https://fsprojects.github.io/Paket/lock-file.html" target="_blank" rel="noopener">proper support for lock files</a>.</li><li>This is a legitimate concern as well. I'd say that greater than 50% of my build times are spend restoring packages. Some of that time is downloading packages and some of it is solving the dependency graph. <a href="https://westerndevs.com/bios/dylan_smith/">Dylan</a> has recently been <a href="https://github.com/Microsoft/hash-and-cache" target="_blank" rel="noopener">experimenting with caching packages</a> based on a checksum of the lock file. This, as it turns out, is quite a bit faster than simply downloading individual files from the package repository. This approach has been used for a while by Circle CI.</li><li>The goal is to make sure that you can always build every piece of code you own. One solution is to check everything in, but where do you stop? Do you check in the compiler? The system libraries? The operating system? Instead of checking everything in and hoping that they continue to build why not just run builds every night?</li></ol><p>We've countered every one of the big points for checking in packages. Now let me tell you why you shouldn't check packages in.</p><ol><li>It bloats the size of your repository. It does take time to download a repository and filling it up with multiple copies of no longer used packages is not helpful. Git works in a way that even if you delete a file it still exists in the image which is distributed to everybody who pulls the repository.<img src="https://blog.simontimms.com/images/checking_in_packages/weight.jpeg" alt="Heavy weight repositories"></li><li>It is really easy to upgrade a package and forget to check it in or otherwise get the package file out of sync with what's on disk.</li><li>The whole reason we have package managers is to help us handle installing and reinstalling packages - we should try to trust in them.</li><li>If there is concern that packages may stop being available or that package servers will be unavailable then we can stand up an internal package server. This is also beneficial for developer builds.</li></ol><p>For NuGet we have the added problem that the package directory is no longer local to the source control directory so you have to really go out of your way to check in the packages folder.</p><p>Stop checking in packages, it is a ridiculous outdated practice which is introducing bugs and slowing down builds.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;If there is one thing that we developers are good at it is holy wars. Vi vs. Emacs, tabs vs. spaces, Python vs. R, the list goes on. I&#39;m usually smart enough to not get involved in such low brow exchanges... haha, who am I kidding? (vi, spaces and R, BTW) Recently I&#39;ve been tilting at the windmill that is checking in package files. I don&#39;t mean the files that tell what version of files to check in but the actual library files.&lt;/p&gt;
    
    </summary>
    
      <category term="development fundamentals" scheme="https://westerndevs.com/categories/development-fundamentals/"/>
    
    
      <category term="npm" scheme="https://westerndevs.com/tags/npm/"/>
    
      <category term="nuget" scheme="https://westerndevs.com/tags/nuget/"/>
    
  </entry>
  
  <entry>
    <title type="html">DevOps and Microservices - Symbiotes</title>
    <link href="https://westerndevs.com/_/microservices-devops/" rel="alternate" type="text/html"/>
    <id>https://westerndevs.com/_/microservices-devops/</id>
    <published>2018-10-20T04:00:00.000Z</published>
    <updated>2020-12-04T02:04:56.036Z</updated>
	<author>
	
	  
	  <name>Simon Timms</name>
	  <email>stimms@gmail.com</email>
	
	  <uri>https://westerndevs.com</uri>
	</author>
    
    <content type="html"><![CDATA[<p>Two of the major ideas de jour in development circles these past few years have been DevOps and Microservices. That they rose to the forefront at the same time was not a coincidence. They are inexorably linked ideas.</p><a id="more"></a><p>Microservices Architecture is a top level software design which favours creating small, loosely connected services which maintain data autonomy. Part of this design requires that each service you deploy has its own data storage (this could be as complex as its own SQL Server instance or as simple as an in-memory cache) and that the service be independently deployable. If you've ever built out a deployment pipeline for a monolithic application and thought &quot;geez, this takes a lot of work&quot; then imagine scaling that over 20 services or 100 services. Equally deploying a lot of databases can be painful to say nothing of how complex it is to deploy copies of your microservices collection over several environments (dev, test, prod,...).</p><p>The added work of deploying this large number of services necessitates changes to the old models where releases took weeks and infrastructure was manually provisioned. Perhaps the greatest motivator for people is avoiding boring, repetitive work. Microservices accentuate the pain points which traditional, monolithic design has hidden. It is slow to provision severs, difficult to set up build and annoying to log into servers to get logs. Changes had to be made to unblock microservices. Out of the pain was born a desire to make builds and infrastructure provisioning faster, more repeatable and easier to set up.</p><p>DevOps is obviously more that just speeding up builds and unblocking infrastructure. It is a cultural mindset together the previously disjoint operations and development groups to better serve the business by providing reliable changes rapidly. DevOps permeates the entire development life-cycle.<img src="https://blog.simontimms.com/images/devops_microservices/infinity.png" alt="Image from https://medium.com/@neonrocket/devops-is-a-culture-not-a-role-be1bed149b0"></p><p>To achieve success in DevOps there do need to be some changes to how the software is written. Large applications are obviously slower to build than smaller ones, so that applies pressure to create more smaller applications. As teams become larger to maintain a rate of change which limits the scope of changes pushed to production (which you want to limit investigation when something goes wrong) that also pushes towards smaller services.</p><p>Logging and instrumentation is a necessity when there is no simple, single process path for a request. Opening a half dozen log files and trying to hunt through them all is obviously far less efficient than entering a query into a log aggregator.</p><p>If the inputs and outputs from a service are well known then that unlocks the ability to blackbox services during testing. This practice allows for much better testing: another feature of a strong DevOps culture.</p><p>Independent services are needed in order to permit different parts of the business to move at different speeds without blocking each other. A monolithic application must wait for all parties to agree before promoting a build.</p><p>Without DevOps microservices would be so much more difficult to manage that it would no longer be worth it. At the same time a lot of the advantages you get from a DevOps culture push the shape of applications built under it to be more like microservices. The two are strongly related.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Two of the major ideas de jour in development circles these past few years have been DevOps and Microservices. That they rose to the forefront at the same time was not a coincidence. They are inexorably linked ideas.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
